/**
 * Recurrent Neural Networks (RNNs) Quiz
 */
export const recurrentNeuralNetworksQuiz = [
  {
    id: 'cnn-q1',
    question:
      'Explain the vanishing gradient problem in RNNs mathematically. Why does it occur, what are its consequences for learning long-term dependencies, and how is it different from the vanishing gradient problem in deep feedforward networks?',
    sampleAnswer:
      'The vanishing gradient problem in RNNs is particularly severe due to the multiplicative nature of gradient flow through time.\\n\\n**Mathematical Explanation**:\\n\\nFor an RNN with hidden state h_t = tanh(W_hh · h_{t-1} + W_xh · x_t), the gradient of the loss with respect to an early hidden state h_0 requires the chain rule through all timesteps:\\n\\n∂L/∂h_0 = (∂L/∂h_T) × (∂h_T/∂h_{T-1}) × (∂h_{T-1}/∂h_{T-2}) × ... × (∂h_1/∂h_0)\\n\\nEach term (∂h_t/∂h_{t-1}) involves:\\n- The Jacobian matrix W_hh (recurrent weights)\\n- The derivative of tanh activation: diag(1 - tanh²(...))\\n\\nSince tanh outputs are in [-1, 1], the derivative |tanh\'(x)| ≤ 1, typically around 0.25-1.0.\\n\\n**Why Vanishing Occurs**:\\n\\n1. **Repeated Multiplication**: Gradient requires T matrix multiplications\\n2. **Sub-unity Values**: If largest eigenvalue of (W_hh · diag (tanh\')) < 1, gradients shrink exponentially\\n3. **Exponential Decay**: grad (t) ≈ (0.9)^t → After 50 steps: 0.9^50 ≈ 0.005\\n\\n**Numerical Example**:\\n```\\nt=0:  gradient = 1.000\\nt=10: gradient = 0.349 (λ=0.9)\\nt=25: gradient = 0.072\\nt=50: gradient = 0.005\\nt=100: gradient = 0.00003\\n```\\n\\n**Consequences**:\\n\\n1. **Cannot Learn Long Dependencies**: Gradient from timestep 100 to timestep 0 is effectively zero\\n2. **Short Memory**: RNN effectively remembers only ~10-20 recent timesteps\\n3. **Bias Toward Recent**: Model learns patterns in recent history, ignores distant past\\n4. **Training Difficulty**: Early layers don\'t update (no learning signal)\\n\\n**Example Task Failure**:\\n```\\nInput: "The cat, which was sitting on the mat and looking very comfortable, ___"\\nTarget: "was" (subject-verb agreement with "cat")\\n\\nRNN behavior:\\n- Sees recent words: "comfortable, ___"\\n- Lost connection to "cat" (too many steps ago)\\n- Might predict "were" or other errors\\n```\\n\\n**Difference from Feedforward Networks**:\\n\\n**Feedforward Networks**:\\n- Gradient vanishes through depth (layers)\\n- Each layer has different weights\\n- Can use: residual connections, batch norm, careful initialization\\n- Typically 10-100 layers before severe vanishing\\n\\n**RNNs**:\\n- Gradient vanishes through time (timesteps)\\n- **Same weights repeated** (W_hh used at every timestep)\\n- Multiplicative effect is worse (same matrix T times)\\n- Problems appear after just 20-50 timesteps\\n- Skip connections harder (would need connections through time)\\n\\n**Why RNN is Worse**:\\n\\n1. **Weight Sharing**: Same W_hh multiplied repeatedly amplifies eigenvalue effects\\n2. **Longer Sequences**: Text can be 1000+ tokens, much longer than network depth\\n3. **Temporal Dependencies**: Must maintain information across arbitrary distances\\n\\n**Mathematical Condition**:\\n\\nFor gradient to flow without vanishing:\\n- Need largest eigenvalue λ_max(W_hh · diag(σ\'(·))) ≈ 1\\n- Too hard to achieve with simple RNN\\n- Requires special architecture (LSTM/GRU)\\n\\n**Solutions**:\\n\\n1. **LSTM/GRU**: Gating mechanisms create additive paths for gradients\\n2. **Gradient Clipping**: Prevents exploding (doesn\'t fix vanishing)\\n3. **Identity Init**: Initialize W_hh as identity matrix\\n4. **ReLU**: No derivative saturation, but causes other problems in RNNs\\n5. **Careful Learning Rates**: Can\'t really fix the core issue\\n\\nThe vanishing gradient problem fundamentally limits vanilla RNNs to short-term dependencies, necessitating the development of LSTM and GRU architectures.',
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      'Compare parameter efficiency between RNNs and feedforward networks for sequence processing. If you have sequences of length 100 with 50-dimensional features, how many parameters would each architecture need? Explain the concept of parameter sharing in RNNs.',
    sampleAnswer:
      'Parameter sharing is the key efficiency gain of RNNs over feedforward networks for sequences.\\n\\n**Parameter Comparison**:\\n\\n**Setup**:\\n- Sequence length: T = 100\\n- Feature dimension: d = 50\\n- Hidden size: h = 128\\n- Output size: o = 10\\n\\n**Feedforward Network** (Naive Approach):\\n\\nMust process entire flattened sequence:\\n\\n```\\nInput: 100 × 50 = 5000 features\\nHidden layer: 5000 × 128 = 640,000 weights\\nOutput layer: 128 × 10 = 1,280 weights\\nTotal: 641,280 parameters\\n```\\n\\nProblems:\\n- Different sequence length → different model!\\n- For T=200: Would need 1,280,000 parameters (doubles)\\n- No generalization across positions\\n\\n**RNN** (Parameter Sharing):\\n\\nSame weights applied at each timestep:\\n\\n```\\nInput-to-hidden (W_xh): 50 × 128 = 6,400 weights\\nHidden-to-hidden (W_hh): 128 × 128 = 16,384 weights  \\nHidden-to-output (W_hy): 128 × 10 = 1,280 weights\\nBiases (b_h, b_y): 128 + 10 = 138\\nTotal: 24,202 parameters\\n```\\n\\n**Comparison**:\\n- Feedforward: 641,280 parameters (depends on T)\\n- RNN: 24,202 parameters (independent of T)\\n- **Reduction: 26.5× fewer parameters!**\\n\\n**For Longer Sequences (T=1000)**:\\n- Feedforward: ~6.4M parameters\\n- RNN: 24,202 parameters (same!)\\n- **Reduction: 264× fewer parameters!**\\n\\n**Parameter Sharing Explained**:\\n\\n**Concept**: The same weight matrices (W_xh, W_hh, W_hy) are reused at every timestep.\\n\\n**Visual**:\\n```\\nTime:    t=0      t=1      t=2      t=3\\n         ↓        ↓        ↓        ↓\\nWeights: W_xh ←──same──→ W_xh ←──→ W_xh\\n         W_hh             W_hh      W_hh\\n         W_hy             W_hy      W_hy\\n```\\n\\n**Benefits**:\\n\\n1. **Scalability**: Works with any sequence length\\n   - Train on length 50, test on length 500\\n   - Same parameters process all positions\\n\\n2. **Generalization**: Learning at one position helps others\\n   - Learn "cat" is subject at position 5\\n   - Automatically applies to position 50, 500, etc.\\n\\n3. **Statistical Efficiency**: Same pattern at different positions\\n   - Don\'t need examples of "cat" at every position\\n   - Shared weights accumulate gradient from all positions\\n\\n4. **Memory Efficiency**: Constant memory regardless of sequence length\\n   - 24K params for T=10 or T=10000\\n\\n**Analogy to CNNs**:\\n\\n**CNN**: Parameter sharing across **space**\\n- Same filter applied to all image locations\\n- Learn "edge detector" once, use everywhere\\n- 3×3 filter = 9 params for entire image\\n\\n**RNN**: Parameter sharing across **time**\\n- Same weights applied to all time positions\\n- Learn "word → embedding" once, use always\\n- Fixed params for any sequence length\\n\\n**Trade-offs**:\\n\\n**RNN Advantages**:\\n- Vastly fewer parameters\\n- Handles variable-length sequences\\n- Generalizes across positions\\n- Memory efficient\\n\\n**RNN Disadvantages**:\\n- Sequential processing (can\'t parallelize)\\n- Vanishing gradients (hard to learn long-term)\\n- All positions share exact same transformation\\n\\n**Feedforward Advantages**:\\n- Can learn position-specific patterns\\n- Parallel computation\\n- No gradient issues through time\\n\\n**Feedforward Disadvantages**:\\n- Massive parameter count\\n- Fixed sequence length\\n- No sharing → needs more data\\n\\n**Modern Hybrid: Transformers**:\\n\\nCombine best of both:\\n- Position-independent transformations (like RNN)\\n- Parallel computation (like Feedforward)\\n- Attention instead of recurrence\\n- But more parameters than RNN (still much less than naive feedforward)\\n\\n**Practical Example**:\\n\\nLanguage model on 10K vocabulary:\\n\\n**Feedforward** (sequence length 100):\\n```\\nInput: 100 positions × 10000 vocab = 1M input features\\nHidden: 1M × 512 = 512M parameters\\nImpractical!\\n```\\n\\n**RNN**:\\n```\\nEmbedding: 10000 × 256 = 2.56M\\nRNN: 256 × 256 = 65K\\nOutput: 256 × 10000 = 2.56M\\nTotal: ~5M parameters\\nPractical!\\n```\\n\\n**Key Insight**: Parameter sharing makes sequence modeling feasible. Without it, we\'d need exponentially more data and memory. This principle (exploit structure through weight sharing) is fundamental to modern deep learning.',
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'Describe the four main RNN architectural patterns (one-to-many, many-to-one, many-to-many synced, many-to-many encoder-decoder) with concrete examples and explain when to use each. How would you implement each pattern in PyTorch?',
    sampleAnswer:
      'RNNs can be arranged in different patterns depending on input/output requirements.\\n\\n**1. ONE-TO-MANY: Single Input → Sequence Output**\\n\\n**Use Cases**:\\n- Image captioning: Image → sentence\\n- Music generation: Genre → melody\\n- Image generation: Description → pixel sequence\\n\\n**Architecture**:\\n```\\nFixed Input → RNN → RNN → RNN → ... → Sequence Output\\n    x     h_0    h_1    h_2         y_0   y_1   y_2\\n```\\n\\n**Implementation**:\\n```python\\nclass OneToMany (nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size, seq_length):\\n        super().__init__()\\n        self.encoder = nn.Linear (input_size, hidden_size)\\n        self.rnn_cell = nn.RNNCell (output_size, hidden_size)\\n        self.decoder = nn.Linear (hidden_size, output_size)\\n        self.seq_length = seq_length\\n    \\n    def forward (self, x):\\n        # x: (batch, input_size) - single input\\n        batch_size = x.size(0)\\n        \\n        # Initialize hidden state from input\\n        hidden = torch.tanh (self.encoder (x))\\n        \\n        outputs = []\\n        # Start token (could be learned or zeros)\\n        rnn_input = torch.zeros (batch_size, self.output_size)\\n        \\n        # Generate sequence\\n        for t in range (self.seq_length):\\n            hidden = self.rnn_cell (rnn_input, hidden)\\n            output = self.decoder (hidden)\\n            outputs.append (output)\\n            rnn_input = output  # Feedback (or use true output in training)\\n        \\n        return torch.stack (outputs, dim=1)\\n```\\n\\n**Example - Image Captioning**:\\n```\\nInput: CNN features [2048]\\nOutput: Caption "A dog playing in park" [5 words]\\n```\\n\\n**2. MANY-TO-ONE: Sequence Input → Single Output**\\n\\n**Use Cases**:\\n- Sentiment analysis: Review → positive/negative\\n- Video classification: Frames → action label\\n- Speaker identification: Audio → speaker ID\\n- Document classification: Text → category\\n\\n**Architecture**:\\n```\\nSequence Input → RNN → RNN → RNN → ... → Single Output\\nx_0   x_1   x_2    h_0    h_1    h_2         y (final hidden)\\n```\\n\\n**Implementation**:\\n```python\\nclass ManyToOne (nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super().__init__()\\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\\n        self.fc = nn.Linear (hidden_size, output_size)\\n    \\n    def forward (self, x):\\n        # x: (batch, seq_length, input_size)\\n        \\n        # Process entire sequence\\n        output, hidden = self.rnn (x)\\n        # output: (batch, seq, hidden_size)\\n        # hidden: (1, batch, hidden_size)\\n        \\n        # Use final hidden state\\n        final_hidden = hidden.squeeze(0)\\n        \\n        # Classify\\n        logits = self.fc (final_hidden)\\n        return logits\\n```\\n\\n**Example - Sentiment Analysis**:\\n```\\nInput: "This movie was amazing!" [5 words]\\nOutput: Positive [1 class]\\n```\\n\\n**3. MANY-TO-MANY (Synced): Aligned Input/Output Sequences**\\n\\n**Use Cases**:\\n- Part-of-speech tagging: Words → POS tags\\n- Named entity recognition: Tokens → entity labels\\n- Video frame labeling: Frames → frame-level labels\\n- Stock prediction: Prices → next prices\\n\\n**Architecture**:\\n```\\nInput:  x_0 → x_1 → x_2 → x_3\\n           ↓      ↓      ↓      ↓\\nRNN:    h_0 → h_1 → h_2 → h_3\\n           ↓      ↓      ↓      ↓\\nOutput: y_0   y_1   y_2   y_3\\n```\\n\\nKey: Output at each timestep, aligned with input\\n\\n**Implementation**:\\n```python\\nclass ManyToManySync (nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super().__init__()\\n        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\\n        self.fc = nn.Linear (hidden_size, output_size)\\n    \\n    def forward (self, x):\\n        # x: (batch, seq_length, input_size)\\n        \\n        # Process sequence\\n        output, hidden = self.rnn (x)\\n        # output: (batch, seq_length, hidden_size)\\n        \\n        # Decode at each timestep\\n        batch_size, seq_length, hidden_size = output.shape\\n        output = output.contiguous().view(-1, hidden_size)\\n        output = self.fc (output)\\n        output = output.view (batch_size, seq_length, -1)\\n        \\n        return output\\n```\\n\\n**Example - POS Tagging**:\\n```\\nInput:  ["The", "cat", "sat", "down",]\\nOutput: [DET,   NOUN,  VERB,  ADV]\\n```\\nEach input token gets corresponding output label.\\n\\n**4. MANY-TO-MANY (Encoder-Decoder): Unaligned Sequences**\\n\\n**Use Cases**:\\n- Machine translation: English → French\\n- Text summarization: Article → summary\\n- Question answering: Context + Question → Answer\\n- Speech recognition: Audio → text\\n\\n**Architecture**:\\n```\\nEncoder:        Decoder:\\nInput Seq       Output Seq\\nx_0→x_1→x_2→[h] → y_0→y_1→y_2\\n                 ↑\\n            Context vector\\n```\\n\\nKey: Encoder compresses input, decoder generates output (different lengths!)\\n\\n**Implementation**:\\n```python\\nclass Seq2Seq (nn.Module):\\n    def __init__(self, input_size, hidden_size, output_size):\\n        super().__init__()\\n        # Encoder\\n        self.encoder = nn.RNN(input_size, hidden_size, batch_first=True)\\n        # Decoder\\n        self.decoder_cell = nn.RNNCell (output_size, hidden_size)\\n        self.fc = nn.Linear (hidden_size, output_size)\\n    \\n    def forward (self, source, target_length):\\n        # source: (batch, source_length, input_size)\\n        batch_size = source.size(0)\\n        \\n        # Encode\\n        _, hidden = self.encoder (source)\\n        # hidden: (1, batch, hidden_size) - context vector\\n        hidden = hidden.squeeze(0)\\n        \\n        # Decode\\n        outputs = []\\n        decoder_input = torch.zeros (batch_size, self.output_size)\\n        \\n        for t in range (target_length):\\n            hidden = self.decoder_cell (decoder_input, hidden)\\n            output = self.fc (hidden)\\n            outputs.append (output)\\n            decoder_input = output\\n        \\n        return torch.stack (outputs, dim=1)\\n```\\n\\n**Example - Translation**:\\n```\\nInput:  "Hello how are you" [4 words, English]\\nOutput: "Bonjour comment allez-vous" [3 words, French]\\n```\\nDifferent lengths, no alignment!\\n\\n**Comparison Table**:\\n\\n| Pattern | Input | Output | Example | Key Feature |\\n|---------|-------|--------|---------|-------------|\\n| One-to-Many | Fixed | Sequence | Image→Caption | Generate from fixed input |\\n| Many-to-One | Sequence | Fixed | Review→Sentiment | Aggregate sequence info |\\n| Many-to-Many (Sync) | Sequence | Sequence | POS tagging | Aligned, same length |\\n| Many-to-Many (Seq2Seq) | Sequence | Sequence | Translation | Unaligned, different lengths |\\n\\n**Decision Tree**:\\n```\\nInput type?\\n├─ Fixed input\\n│  └─ Need sequence output? → One-to-Many\\n└─ Sequence input\\n   ├─ Need single output? → Many-to-One\\n   └─ Need sequence output?\\n      ├─ Same length, aligned? → Many-to-Many (Sync)\\n      └─ Different length? → Seq2Seq (Encoder-Decoder)\\n```\\n\\nEach pattern has specific use cases based on the structure of your input and output data. Modern variants add attention mechanisms to improve all these architectures.',
    keyPoints: [],
  },
];
