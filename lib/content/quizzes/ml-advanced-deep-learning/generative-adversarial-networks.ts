/**
 * Generative Adversarial Networks (GANs) Quiz
 */

export const generativeAdversarialNetworksQuiz = [
  {
    id: 'gan-q1',
    question:
      'Explain the adversarial training process in GANs. How do the generator and discriminator interact during training, and why is this a minimax game? Include the loss functions for both networks.',
    sampleAnswer:
      'GANs train two neural networks in an **adversarial game** where the generator learns to create fake data while the discriminator learns to distinguish real from fake, resulting in a minimax optimization problem.\\n\\n**GAN Architecture**:\\n\\n```python\\nclass Generator(nn.Module):\\n    def __init__(self, latent_dim=100, img_shape=(1, 28, 28)):\\n        self.model = nn.Sequential(\\n            nn.Linear(latent_dim, 256),\\n            nn.LeakyReLU(0.2),\\n            nn.Linear(256, 512),\\n            nn.LeakyReLU(0.2),\\n            nn.Linear(512, 1024),\\n            nn.LeakyReLU(0.2),\\n            nn.Linear(1024, int(np.prod(img_shape))),\\n            nn.Tanh()  # Output in [-1, 1]\\n        )\\n        self.img_shape = img_shape\\n    \\n    def forward(self, z):\\n        # z: random noise (batch_size, latent_dim)\\n        img = self.model(z)\\n        return img.view(img.size(0), *self.img_shape)\\n\\nclass Discriminator(nn.Module):\\n    def __init__(self, img_shape=(1, 28, 28)):\\n        self.model = nn.Sequential(\\n            nn.Linear(int(np.prod(img_shape)), 512),\\n            nn.LeakyReLU(0.2),\\n            nn.Dropout(0.3),\\n            nn.Linear(512, 256),\\n            nn.LeakyReLU(0.2),\\n            nn.Dropout(0.3),\\n            nn.Linear(256, 1),\\n            nn.Sigmoid()  # Output probability [0, 1]\\n        )\\n    \\n    def forward(self, img):\\n        # Flatten image\\n        img_flat = img.view(img.size(0), -1)\\n        validity = self.model(img_flat)\\n        return validity  # P(real)\\n```\\n\\n**Adversarial Training Process**:\\n\\n**The Game**:\\n```\\nGenerator (G): Forger trying to create fake money\\n- Goal: Fool the discriminator\\n- Creates fake images from random noise\\n- Wants D(G(z)) → 1 (discriminator thinks fake is real)\\n\\nDiscriminator (D): Police trying to detect fakes\\n- Goal: Correctly classify real vs fake\\n- Wants D(x_real) → 1 (real is real)\\n- Wants D(G(z)) → 0 (fake is fake)\\n\\nAs D gets better at detecting, G must improve to fool it\\nAs G creates better fakes, D must improve to detect them\\n→ Arms race!\\n```\\n\\n**Loss Functions**:\\n\\n**Discriminator Loss** - Binary Cross-Entropy:\\n```python\\ndef discriminator_loss(D, G, real_imgs, z):\\n    # Loss on real images: D(x_real) should be 1\\n    real_pred = D(real_imgs)\\n    real_loss = BCE(real_pred, torch.ones_like(real_pred))\\n    \\n    # Loss on fake images: D(G(z)) should be 0\\n    fake_imgs = G(z).detach()  # Detach: don\'t backprop through G\\n    fake_pred = D(fake_imgs)\\n    fake_loss = BCE(fake_pred, torch.zeros_like(fake_pred))\\n    \\n    # Total discriminator loss\\n    d_loss = (real_loss + fake_loss) / 2\\n    return d_loss\\n\\n# Discriminator wants to maximize:\\n# E[log D(x)] + E[log(1 - D(G(z)))]\\n```\\n\\n**Generator Loss**:\\n```python\\ndef generator_loss(D, G, z):\\n    # Generate fake images\\n    fake_imgs = G(z)\\n    \\n    # Loss: D(G(z)) should be 1 (fool discriminator)\\n    fake_pred = D(fake_imgs)\\n    g_loss = BCE(fake_pred, torch.ones_like(fake_pred))\\n    \\n    return g_loss\\n\\n# Generator wants to maximize:\\n# E[log D(G(z))]\\n# Or equivalently minimize:\\n# E[log(1 - D(G(z)))]  # Original formulation\\n```\\n\\n**Minimax Formulation**:\\n\\n**Original GAN Objective** (Goodfellow et al., 2014):\\n```\\nmin_G max_D V(D, G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]\\n\\nDiscriminator maximizes V:\\n- Wants D(x_real) → 1 (log D(x) → 0)\\n- Wants D(G(z)) → 0 (log(1 - D(G(z))) → 0)\\n\\nGenerator minimizes V:\\n- Wants D(G(z)) → 1 (log(1 - D(G(z))) → -∞)\\n\\n→ Minimax game (zero-sum)\\n```\\n\\n**Why Minimax?**\\n\\n**Zero-Sum Game**:\\n```\\nDiscriminator\'s gain = Generator\'s loss\\n- D correctly classifies → D wins, G loses\\n- G fools D → G wins, D loses\\n- No cooperation, pure competition\\n```\\n\\n**Nash Equilibrium** (optimal solution):\\n```\\nAt equilibrium:\\n- G generates samples from true data distribution: p_g = p_data\\n- D cannot distinguish real from fake: D(x) = 1/2 everywhere\\n- Neither can improve without the other adapting\\n```\\n\\n**Training Algorithm**:\\n\\n```python\\n# Initialize networks\\nG = Generator(latent_dim=100)\\nD = Discriminator()\\n\\noptimizer_G = Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\\noptimizer_D = Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))\\n\\nfor epoch in range(num_epochs):\\n    for i, real_imgs in enumerate(dataloader):\\n        batch_size = real_imgs.size(0)\\n        \\n        # ---------------------\\n        # Train Discriminator\\n        # ---------------------\\n        # Usually train D more than G (k=1 or k=5)\\n        for k in range(1):  # k discriminator updates per generator update\\n            optimizer_D.zero_grad()\\n            \\n            # Real images\\n            real_pred = D(real_imgs)\\n            real_loss = BCE(real_pred, torch.ones(batch_size, 1))\\n            \\n            # Fake images\\n            z = torch.randn(batch_size, latent_dim)\\n            fake_imgs = G(z).detach()  # Don\'t backprop through G!\\n            fake_pred = D(fake_imgs)\\n            fake_loss = BCE(fake_pred, torch.zeros(batch_size, 1))\\n            \\n            # Total discriminator loss\\n            d_loss = (real_loss + fake_loss) / 2\\n            d_loss.backward()\\n            optimizer_D.step()\\n        \\n        # -----------------\\n        # Train Generator\\n        # -----------------\\n        optimizer_G.zero_grad()\\n        \\n        # Generate fake images\\n        z = torch.randn(batch_size, latent_dim)\\n        fake_imgs = G(z)\\n        \\n        # Generator loss: fool discriminator\\n        fake_pred = D(fake_imgs)\\n        g_loss = BCE(fake_pred, torch.ones(batch_size, 1))\\n        \\n        g_loss.backward()\\n        optimizer_G.step()\\n        \\n        # Log progress\\n        if i % 100 == 0:\\n            print(f"Epoch {epoch}, Batch {i}: "\\n                  f"D_loss={d_loss.item():.4f}, "\\n                  f"G_loss={g_loss.item():.4f}, "\\n                  f"D(x)={real_pred.mean():.4f}, "\\n                  f"D(G(z))={fake_pred.mean():.4f}")\\n```\\n\\n**Training Dynamics**:\\n\\n**Early Training** (Epoch 1-10):\\n```\\nGenerator: Produces noise/garbage\\nDiscriminator: Easily distinguishes real from fake\\n\\nMetrics:\\nD(x_real) = 0.95 (correctly identifies real)\\nD(G(z)) = 0.05 (correctly identifies fake)\\nD_loss = 0.05 (low - D is winning)\\nG_loss = 3.00 (high - G is losing)\\n\\nResult: D is too strong, provides no useful gradient to G\\n→ Vanishing gradient problem!\\n```\\n\\n**Mid Training** (Epoch 20-50):\\n```\\nGenerator: Produces recognizable shapes\\nDiscriminator: Struggles to classify\\n\\nMetrics:\\nD(x_real) = 0.70 (less confident on real)\\nD(G(z)) = 0.40 (fooled by some fakes)\\nD_loss = 0.35 (moderate)\\nG_loss = 1.00 (improving)\\n\\nResult: Balanced competition, both networks improving\\n```\\n\\n**Late Training** (Epoch 100+):\\n```\\nGenerator: Produces realistic images\\nDiscriminator: Cannot distinguish reliably\\n\\nMetrics:\\nD(x_real) = 0.52 (near random)\\nD(G(z)) = 0.48 (near random)\\nD_loss = 0.69 (≈ -log(0.5), random guessing)\\nG_loss = 0.69 (D fooled consistently)\\n\\nResult: Nash equilibrium approached\\n```\\n\\n**Practical Modifications**:\\n\\n**Problem 1: Vanishing Gradient** (early training)\\n\\n**Original G objective**: `min_G E[log(1 - D(G(z)))]`\\n```\\nWhen D is strong: D(G(z)) ≈ 0\\n→ log(1 - 0) ≈ 0\\n→ Gradient ≈ 0\\n→ G doesn\'t learn!\\n```\\n\\n**Solution**: Non-saturating loss\\n```python\\n# Instead of minimizing log(1 - D(G(z)))\\n# Maximize log(D(G(z)))\\n\\ng_loss = -torch.log(D(G(z))).mean()\\n# Or equivalently:\\ng_loss = BCE(D(G(z)), torch.ones_like(D(G(z))))\\n\\n# When D(G(z)) ≈ 0:\\n# -log(0) → ∞ (large gradient!)\\n# G receives strong learning signal\\n```\\n\\n**Problem 2: Mode Collapse**\\n\\n**Symptom**: G generates only a few types of samples\\n```\\nMNIST example:\\n- G only generates "1" and "7"\\n- Ignores other digits\\n- Fools D by finding easy samples\\n```\\n\\n**Why it happens**:\\n```\\nG discovers: "If I generate \'1", D is fooled!"\n→ Generates only "1"\n→ D adapts, learns to detect "1"\n→ G switches to generating "7"\n→ Oscillates between few modes\n→ Never explores full distribution\n```\n\n**Solution**: Minibatch discrimination, unrolled GANs, experience replay\n\n**Problem 3: Training Instability**\n\n**Symptoms**:\n```\n- Loss oscillates wildly\n- D_loss → 0 (D too strong) or D_loss → ∞ (D too weak)\n- Generated samples degrade over time\n```\n\n**Solutions**:\n\n**1. Balance D and G updates**:\n```python\n# Train D more frequently\nfor k in range(5):  # 5 D updates per G update\n    train_discriminator()\ntrain_generator()\n```\n\n**2. Use label smoothing**:\n```python\n# Instead of real=1, fake=0\nreal_labels = torch.rand(batch_size, 1) * 0.1 + 0.9  # [0.9, 1.0]\nfake_labels = torch.rand(batch_size, 1) * 0.1        # [0.0, 0.1]\n\n# Prevents D from becoming too confident\n```\n\n**3. Add noise to inputs**:\n```python\n# Add noise to discriminator inputs (annealed over training)\nreal_imgs_noisy = real_imgs + noise_std * torch.randn_like(real_imgs)\nfake_imgs_noisy = fake_imgs + noise_std * torch.randn_like(fake_imgs)\n\nnoise_std = max(0.1 * (1 - epoch/num_epochs), 0.01)  # Decay over time\n```\n\n**Monitoring Training**:\n\n**Good Signs**:\n```\n✓ D_loss ≈ 0.69 (log(2), random guessing)\n✓ G_loss decreasing gradually\n✓ D(x_real) ≈ 0.5-0.7 (not too confident)\n✓ D(G(z)) ≈ 0.3-0.5 (fooled sometimes)\n✓ Generated samples improving visually\n```\n\n**Bad Signs**:\n```\n✗ D_loss → 0 (D too strong, G can\'t learn)\n✗ D_loss → ∞ (D too weak, no useful signal)\n✗ G_loss not decreasing (G not improving)\n✗ D(G(z)) → 0 or 1 (extreme confidence)\n✗ Mode collapse (generates same samples)\n```\n\n**Key Insight**: GANs are a **minimax game** where two networks compete: the generator tries to fool the discriminator by generating realistic data, while the discriminator tries to distinguish real from fake. At equilibrium, the generator perfectly mimics the data distribution and the discriminator cannot tell them apart (D=0.5). The adversarial training creates a powerful implicit loss function that doesn\'t require explicitly defining "realistic" - it\'s learned through competition.\',',
    keyPoints: [],
  },
  {
    id: 'gan-q2',
    question:
      'Explain the major challenges in training GANs (mode collapse, vanishing gradients, convergence issues) and describe modern techniques to address them, such as Wasserstein GAN, Spectral Normalization, and Progressive Growing.',
    sampleAnswer:
      "Training GANs is notoriously difficult due to instability, mode collapse, and vanishing gradients. Modern architectures introduce theoretical and practical improvements to stabilize training.\n\n**Challenge 1: Mode Collapse**:\n\n**Problem**: Generator produces limited variety\n\n**Example**:\n```\nDataset: MNIST (digits 0-9)\nExpected: G generates all 10 digits\nActual: G only generates \"1\" and \"7\"\n\n→ Missing 8 out of 10 modes!\n```\n\n**Why It Happens**:\n```\n1. G discovers: \"Generating '1' fools D with low loss\"\n2. G collapses to only generating \"1\"\n3. D adapts: \"Oh, all fakes are '1', I'll reject '1's\"\n4. G switches: \"Now I'll generate '7' instead\"\n5. Repeat: Oscillates between few modes\n\nNever explores full distribution!\n```\n\n**Mathematical Explanation**:\n```\nG minimizes: E_z[log(1 - D(G(z)))]\n\nG only cares about:\n- Fooling D (making D(G(z)) high)\n- Not about covering all modes\n\nIf generating mode A gives loss 0.1\nAnd generating mode B gives loss 0.2\n→ G prefers mode A only (ignores B)\n```\n\n**Solutions**:\n\n**A. Minibatch Discrimination**:\n```python\nclass MinibatchDiscrimination(nn.Module):\n    def __init__(self, in_features, out_features, kernel_dims):\n        self.T = nn.Parameter(torch.randn(in_features, out_features, kernel_dims))\n    \n    def forward(self, x):\n        # x: (batch_size, in_features)\n        M = torch.mm(x, self.T.view(in_features, -1))\n        M = M.view(-1, out_features, kernel_dims)\n        \n        # Compute L1 distance between samples in batch\n        distances = torch.abs(M.unsqueeze(0) - M.unsqueeze(1)).sum(3)\n        \n        # c(x_i) = sum_j exp(-||M_i - M_j||)\n        c = torch.sum(torch.exp(-distances), dim=1)\n        \n        return torch.cat([x, c], dim=1)\n\n# Add to discriminator:\nself.minibatch = MinibatchDiscrimination(512, 100, 5)\nh = self.features(x)  # (batch, 512)\nh = self.minibatch(h)  # (batch, 512+100)\n\n# D sees: Are samples in batch diverse?\n# If G produces identical samples → D detects mode collapse\n```\n\n**B. Unrolled GAN**:\n```python\n# G optimizes against future D (after k updates)\n# Instead of current D\n\ndef train_generator_unrolled(G, D, k=5):\n    # Save current D\n    D_copy = copy.deepcopy(D)\n    \n    # Simulate k discriminator updates\n    for _ in range(k):\n        train_discriminator_step(D_copy, G)\n    \n    # Update G against future D_copy\n    z = torch.randn(batch_size, latent_dim)\n    fake = G(z)\n    g_loss = -torch.log(D_copy(fake)).mean()\n    g_loss.backward()\n    optimizer_G.step()\n\n# G considers: \"If D adapts, will my samples still fool it?\"\n# Encourages diversity\n```\n\n**Challenge 2: Vanishing Gradients**:\n\n**Problem**: Early in training, D is too strong\n\n**What Happens**:\n```\nEpoch 1:\nG produces garbage\nD easily classifies: D(G(z)) ≈ 0.001 (very confident)\n\nG loss: log(1 - D(G(z))) = log(1 - 0.001) ≈ 0\nGradient: ∂loss/∂G ≈ 0\n\n→ G receives no learning signal!\n```\n\n**Gradient Analysis**:\n```python\n# Original G loss: E[log(1 - D(G(z)))]\n# Gradient w.r.t. G:\n∂/∂G log(1 - D(G(z))) = -D'(G(z)) / (1 - D(G(z)))\n\n# When D(G(z)) ≈ 0:\nnumerator: D'(G(z)) ≈ small\ndenominator: 1 - 0 = 1\n→ Gradient ≈ 0 / 1 ≈ 0\n```\n\n**Solution: Non-saturating Loss**:\n```python\n# Instead of: min_G E[log(1 - D(G(z)))]\n# Use: max_G E[log D(G(z))]\n\ng_loss = -torch.log(D(G(z))).mean()\n\n# Gradient:\n∂/∂G log(D(G(z))) = D'(G(z)) / D(G(z))\n\n# When D(G(z)) ≈ 0:\nnumerator: D'(G(z)) ≈ small\ndenominator: 0 ≈ very small\n→ Gradient ≈ small / very_small ≈ LARGE!\n\n# G receives strong signal even when D is confident\n```\n\n**Challenge 3: Training Instability**:\n\n**Problem**: Loss oscillates, never converges\n\n**Symptoms**:\n```python\nEpoch 10: D_loss=0.05, G_loss=3.5  # D winning\nEpoch 11: D_loss=2.8, G_loss=0.1   # G winning\nEpoch 12: D_loss=0.03, G_loss=4.2  # D winning again\n\n→ Oscillates indefinitely, no Nash equilibrium\n```\n\n**Root Cause**: JS Divergence in original GAN\n\n**Jensen-Shannon Divergence**:\n```\nOriginal GAN minimizes JS divergence:\nD_JS(p_data || p_g) = 1/2[KL(p_data || m) + KL(p_g || m)]\nwhere m = (p_data + p_g) / 2\n\nProblem: When p_data and p_g have disjoint supports:\nD_JS = log(2) = 0.69 (constant!)\n→ No gradient for G to improve\n```\n\n**Modern Solution: Wasserstein GAN (WGAN)**:\n\n**Key Idea**: Use Earth Mover's Distance (Wasserstein-1 distance)\n\n**Wasserstein Distance**:\n```\nW(p_data, p_g) = inf_γ E_(x,y)~γ[||x - y||]\n\nIntuition: Minimum cost to transport mass from p_g to p_data\n- Continuous (provides gradients everywhere)\n- Meaningful even when supports don't overlap\n```\n\n**WGAN Implementation**:\n```python\nclass WGAN:\n    def __init__(self):\n        self.generator = Generator()\n        # Critic (not discriminator): outputs real number, not probability\n        self.critic = Critic()  # No sigmoid at end!\n        \n        self.opt_G = RMSprop(self.generator.parameters(), lr=5e-5)\n        self.opt_C = RMSprop(self.critic.parameters(), lr=5e-5)\n    \n    def critic_loss(self, real_imgs, fake_imgs):\n        # Maximize: E[C(x_real)] - E[C(G(z))]\n        # Or minimize: -E[C(x_real)] + E[C(G(z))]\n        real_score = self.critic(real_imgs).mean()\n        fake_score = self.critic(fake_imgs).mean()\n        \n        c_loss = fake_score - real_score  # Wasserstein distance estimate\n        return c_loss\n    \n    def generator_loss(self, fake_imgs):\n        # Maximize: E[C(G(z))]\n        # Or minimize: -E[C(G(z))]\n        fake_score = self.critic(fake_imgs).mean()\n        g_loss = -fake_score\n        return g_loss\n    \n    def train_step(self, real_imgs):\n        # Train critic more than generator (5:1 ratio)\n        for _ in range(5):\n            z = torch.randn(batch_size, latent_dim)\n            fake_imgs = self.generator(z).detach()\n            \n            c_loss = self.critic_loss(real_imgs, fake_imgs)\n            self.opt_C.zero_grad()\n            c_loss.backward()\n            self.opt_C.step()\n            \n            # Weight clipping (enforce Lipschitz constraint)\n            for p in self.critic.parameters():\n                p.data.clamp_(-0.01, 0.01)  # Clip weights to [-0.01, 0.01]\n        \n        # Train generator\n        z = torch.randn(batch_size, latent_dim)\n        fake_imgs = self.generator(z)\n        g_loss = self.generator_loss(fake_imgs)\n        self.opt_G.zero_grad()\n        g_loss.backward()\n        self.opt_G.step()\n```\n\n**WGAN Benefits**:\n```\n✓ Meaningful loss metric (correlates with sample quality)\n✓ More stable training (no mode collapse)\n✓ No need to balance G and D carefully\n✓ Works with various architectures\n\nLoss interpretation:\nWGAN loss = 50 → samples poor\nWGAN loss = 10 → samples good\nWGAN loss = 2 → samples excellent\n\n(Lower is better, unlike original GAN where loss means nothing)\n```\n\n**WGAN-GP (Gradient Penalty)**:\n\n**Problem with Weight Clipping**: Limits model capacity\n\n**Solution**: Soft constraint via gradient penalty\n```python\ndef gradient_penalty(critic, real_imgs, fake_imgs):\n    # Sample random points between real and fake\n    alpha = torch.rand(batch_size, 1, 1, 1)\n    interpolated = alpha * real_imgs + (1 - alpha) * fake_imgs\n    interpolated.requires_grad_(True)\n    \n    # Compute critic scores\n    critic_interp = critic(interpolated)\n    \n    # Compute gradients\n    gradients = torch.autograd.grad(\n        outputs=critic_interp,\n        inputs=interpolated,\n        grad_outputs=torch.ones_like(critic_interp),\n        create_graph=True\n    )[0]\n    \n    # Gradient penalty: ||∇critic(x)|| should be 1\n    gradients = gradients.view(batch_size, -1)\n    gradient_norm = gradients.norm(2, dim=1)\n    penalty = ((gradient_norm - 1) ** 2).mean()\n    \n    return penalty\n\n# Critic loss with GP:\nc_loss = fake_score - real_score + lambda_gp * gradient_penalty\n# Typically lambda_gp = 10\n```\n\n**Spectral Normalization**:\n\n**Idea**: Normalize weights to enforce Lipschitz constraint\n\n```python\nclass SpectralNorm:\n    def __init__(self, module, name='weight'):\n        self.module = module\n        self.name = name\n        self._make_params()\n    \n    def _make_params(self):\n        w = getattr(self.module, self.name)\n        # Initialize u vector\n        self.u = nn.Parameter(torch.randn(w.size(0)), requires_grad=False)\n    \n    def forward(self, *args):\n        # Power iteration to compute largest singular value\n        w = getattr(self.module, self.name)\n        \n        # u: left singular vector\n        # v: right singular vector\n        v = torch.matmul(w.t(), self.u)\n        v = v / v.norm()\n        u = torch.matmul(w, v)\n        u = u / u.norm()\n        \n        # Spectral norm (largest singular value)\n        sigma = torch.dot(u, torch.matmul(w, v))\n        \n        # Normalize weights\n        w_normalized = w / sigma\n        setattr(self.module, self.name, w_normalized)\n        \n        return self.module(*args)\n\n# Usage:\nconv = nn.Conv2d(64, 128, 3)\nconv = SpectralNorm(conv)\n# Or using PyTorch:\nconv = nn.utils.spectral_norm(conv)\n```\n\n**Progressive Growing of GANs**:\n\n**Idea**: Start with low resolution, gradually increase\n\n```python\nclass ProgressiveGAN:\n    def __init__(self):\n        # Start: 4×4 resolution\n        self.current_resolution = 4\n        self.max_resolution = 1024\n        \n        self.G_blocks = {\n            4: GeneratorBlock(latent_dim, 512),\n            8: GeneratorBlock(512, 512),\n            16: GeneratorBlock(512, 256),\n            32: GeneratorBlock(256, 128),\n            # ... up to 1024\n        }\n    \n    def train_stage(self, resolution):\n        # Train at current resolution\n        for epoch in range(epochs_per_stage):\n            train_gan_at_resolution(resolution)\n        \n        # Smoothly transition to next resolution\n        if resolution < self.max_resolution:\n            fade_in_next_layer(resolution * 2)\n            train_stage(resolution * 2)\n\n# Training schedule:\n# Stage 1: 4×4 images (50k iterations)\n# Stage 2: 8×8 images (50k iterations)\n# Stage 3: 16×16 images (100k iterations)\n# ...\n# Stage 7: 1024×1024 images (500k iterations)\n\n# Benefits:\n# - Faster training (start small)\n# - More stable (incremental complexity)\n# - Better quality at high resolution\n```\n\n**Summary of Techniques**:\n\n**Problem → Solution**:\n```\nMode Collapse → Minibatch discrimination, Unrolled GAN\nVanishing Gradients → Non-saturating loss, WGAN\nInstability → WGAN-GP, Spectral Normalization\nHigh Resolution → Progressive Growing\nConvergence → Wasserstein distance, better metrics\n```\n\n**Key Insight**: Modern GANs address training instability by replacing Jensen-Shannon divergence with **Wasserstein distance** (provides meaningful gradients everywhere) and enforcing **Lipschitz constraints** through gradient penalty or spectral normalization, resulting in stable training and high-quality generation.",
    keyPoints: [],
  },
  {
    id: 'gan-q3',
    question:
      'Explain conditional GANs (cGANs) and how they enable controlled generation. Provide examples of architectures and applications, including class-conditional generation and image-to-image translation.',
    sampleAnswer:
      'Conditional GANs (cGANs) extend standard GANs by **conditioning the generation process on additional information**, enabling controlled synthesis of specific outputs.\\n\\n**Standard GAN vs Conditional GAN**:\\n\\n**Standard GAN** (uncontrolled):\\n```python\\n# Generator: Random noise → Random output\\nz = torch.randn(1, 100)  # Random noise\\nx_fake = G(z)            # Generated image (random digit)\\n\\n# Problem: No control over what gets generated\\n# Could be "0", "1", "2", ... "9" (random)\\n```\\n\\n**Conditional GAN** (controlled):\\n```python\\n# Generator: Noise + Condition → Specific output\\nz = torch.randn(1, 100)  # Random noise\\nc = torch.tensor([7])    # Condition: "Generate digit 7"\\nx_fake = G(z, c)         # Generated image of "7"\\n\\n# Control: Specify what to generate!\\n```\\n\\n**cGAN Architecture**:\\n\\n**Class-Conditional Generation** (MNIST example):\\n\\n```python\\nclass ConditionalGenerator(nn.Module):\\n    def __init__(self, latent_dim=100, num_classes=10, img_shape=(1, 28, 28)):\\n        # Embedding for class labels\\n        self.label_emb = nn.Embedding(num_classes, num_classes)\\n        \\n        # Generator: Concat [z; embedding(c)]\\n        self.model = nn.Sequential(\\n            nn.Linear(latent_dim + num_classes, 256),\\n            nn.LeakyReLU(0.2),\\n            nn.BatchNorm1d(256),\\n            nn.Linear(256, 512),\\n            nn.LeakyReLU(0.2),\\n            nn.BatchNorm1d(512),\\n            nn.Linear(512, 1024),\\n            nn.LeakyReLU(0.2),\\n            nn.BatchNorm1d(1024),\\n            nn.Linear(1024, int(np.prod(img_shape))),\\n            nn.Tanh()\\n        )\\n        self.img_shape = img_shape\\n    \\n    def forward(self, z, labels):\\n        # Embed labels: [7] → [0,0,0,0,0,0,0,1,0,0] (one-hot)\\n        c = self.label_emb(labels)\\n        \\n        # Concatenate noise and condition\\n        gen_input = torch.cat([z, c], dim=1)  # (batch, 100+10)\\n        \\n        # Generate conditioned on label\\n        img = self.model(gen_input)\\n        return img.view(img.size(0), *self.img_shape)\\n\\nclass ConditionalDiscriminator(nn.Module):\\n    def __init__(self, num_classes=10, img_shape=(1, 28, 28)):\\n        self.label_emb = nn.Embedding(num_classes, num_classes)\\n        \\n        # Discriminator: Concat [image; embedding(c)]\\n        self.model = nn.Sequential(\\n            nn.Linear(int(np.prod(img_shape)) + num_classes, 512),\\n            nn.LeakyReLU(0.2),\\n            nn.Dropout(0.3),\\n            nn.Linear(512, 256),\\n            nn.LeakyReLU(0.2),\\n            nn.Dropout(0.3),\\n            nn.Linear(256, 1),\\n            nn.Sigmoid()\\n        )\\n    \\n    def forward(self, img, labels):\\n        # Flatten image\\n        img_flat = img.view(img.size(0), -1)\\n        \\n        # Embed labels\\n        c = self.label_emb(labels)\\n        \\n        # Concatenate image and condition\\n        disc_input = torch.cat([img_flat, c], dim=1)\\n        \\n        # Classify: Is this image+label pair real?\\n        validity = self.model(disc_input)\\n        return validity\\n```\\n\\n**Training cGAN**:\\n\\n```python\\nG = ConditionalGenerator()\\nD = ConditionalDiscriminator()\\n\\nfor epoch in range(num_epochs):\\n    for real_imgs, real_labels in dataloader:\\n        batch_size = real_imgs.size(0)\\n        \\n        # --------------------\\n        # Train Discriminator\\n        # --------------------\\n        # Real images with correct labels\\n        real_validity = D(real_imgs, real_labels)\\n        real_loss = BCE(real_validity, torch.ones(batch_size, 1))\\n        \\n        # Fake images with sampled labels\\n        z = torch.randn(batch_size, latent_dim)\\n        gen_labels = torch.randint(0, num_classes, (batch_size,))\\n        fake_imgs = G(z, gen_labels).detach()\\n        fake_validity = D(fake_imgs, gen_labels)\\n        fake_loss = BCE(fake_validity, torch.zeros(batch_size, 1))\\n        \\n        d_loss = (real_loss + fake_loss) / 2\\n        d_loss.backward()\\n        optimizer_D.step()\\n        \\n        # ---------------\\n        # Train Generator\\n        # ---------------\\n        z = torch.randn(batch_size, latent_dim)\\n        gen_labels = torch.randint(0, num_classes, (batch_size,))\\n        fake_imgs = G(z, gen_labels)\\n        fake_validity = D(fake_imgs, gen_labels)\\n        g_loss = BCE(fake_validity, torch.ones(batch_size, 1))\\n        \\n        g_loss.backward()\\n        optimizer_G.step()\\n```\\n\\n**Conditional GAN Objective**:\\n\\n```\\nOriginal GAN:\\nmin_G max_D V(D,G) = E_x[log D(x)] + E_z[log(1 - D(G(z)))]\\n\\nConditional GAN:\\nmin_G max_D V(D,G) = E_x,c[log D(x|c)] + E_z,c[log(1 - D(G(z|c)|c))]\\n\\nKey difference:\\n- Generation: G(z, c) produces output conditioned on c\\n- Discrimination: D(x, c) checks if x matches condition c\\n```\\n\\n**Generation with cGAN**:\\n\\n```python\\n# Generate specific digit\\nz = torch.randn(1, 100)\\nlabel = torch.tensor([7])  # Generate "7"\\nimg = G(z, label)\\n\\n# Generate multiple classes\\nfor digit in range(10):\\n    z = torch.randn(10, 100)  # 10 samples per digit\\n    labels = torch.full((10,), digit)\\n    imgs = G(z, labels)\\n    save_images(imgs, f\'digit_{digit}.png\')\\n\\n# Interpolation with fixed class\\nz1, z2 = torch.randn(1, 100), torch.randn(1, 100)\\nlabel = torch.tensor([3])  # Keep class "3"\\nfor alpha in np.linspace(0, 1, 10):\\n    z_interp = (1 - alpha) * z1 + alpha * z2\\n    img = G(z_interp, label)\\n    # Result: Different styles of "3"\\n```\\n\\n**Advanced Conditioning: Image-to-Image Translation**:\\n\\n**Pix2Pix** (Paired image translation):\\n\\n**Task**: Translate from domain A to domain B\\n```\\nExamples:\\n- Edges → Photo\\n- Sketch → Image\\n- Day → Night\\n- Semantic map → Photo\\n```\\n\\n**Architecture**:\\n```python\\nclass Pix2PixGenerator(nn.Module):\\n    # U-Net architecture with skip connections\\n    def __init__(self):\\n        # Encoder (downsampling)\\n        self.down1 = DownBlock(3, 64)     # 256×256 → 128×128\\n        self.down2 = DownBlock(64, 128)   # 128×128 → 64×64\\n        self.down3 = DownBlock(128, 256)  # 64×64 → 32×32\\n        self.down4 = DownBlock(256, 512)  # 32×32 → 16×16\\n        \\n        # Bottleneck\\n        self.bottleneck = DownBlock(512, 512)  # 16×16 → 8×8\\n        \\n        # Decoder (upsampling) with skip connections\\n        self.up1 = UpBlock(512, 512)      # 8×8 → 16×16\\n        self.up2 = UpBlock(1024, 256)     # 16×16 + skip → 32×32\\n        self.up3 = UpBlock(512, 128)      # 32×32 + skip → 64×64\\n        self.up4 = UpBlock(256, 64)       # 64×64 + skip → 128×128\\n        \\n        # Output\\n        self.final = nn.Sequential(\\n            nn.ConvTranspose2d(128, 3, 4, 2, 1),  # 128×128 → 256×256\\n            nn.Tanh()\\n        )\\n    \\n    def forward(self, x):\\n        # Encoder\\n        d1 = self.down1(x)\\n        d2 = self.down2(d1)\\n        d3 = self.down3(d2)\\n        d4 = self.down4(d3)\\n        \\n        # Bottleneck\\n        b = self.bottleneck(d4)\\n        \\n        # Decoder with skip connections (U-Net)\\n        u1 = self.up1(b)\\n        u2 = self.up2(torch.cat([u1, d4], dim=1))  # Concatenate skip\\n        u3 = self.up3(torch.cat([u2, d3], dim=1))\\n        u4 = self.up4(torch.cat([u3, d2], dim=1))\\n        \\n        output = self.final(torch.cat([u4, d1], dim=1))\\n        return output\\n\\nclass Pix2PixDiscriminator(nn.Module):\\n    # PatchGAN: Classify each patch as real/fake\\n    def __init__(self):\\n        self.model = nn.Sequential(\\n            # Input: Concatenate [input_image; output_image]\\n            nn.Conv2d(6, 64, 4, 2, 1),     # 256×256 → 128×128\\n            nn.LeakyReLU(0.2),\\n            nn.Conv2d(64, 128, 4, 2, 1),   # 128×128 → 64×64\\n            nn.BatchNorm2d(128),\\n            nn.LeakyReLU(0.2),\\n            nn.Conv2d(128, 256, 4, 2, 1),  # 64×64 → 32×32\\n            nn.BatchNorm2d(256),\\n            nn.LeakyReLU(0.2),\\n            nn.Conv2d(256, 512, 4, 1, 1),  # 32×32 → 31×31\\n            nn.BatchNorm2d(512),\\n            nn.LeakyReLU(0.2),\\n            nn.Conv2d(512, 1, 4, 1, 1),    # 31×31 → 30×30\\n            # Output: 30×30 patch classification\\n        )\\n    \\n    def forward(self, input_img, output_img):\\n        # Concatenate input and output\\n        x = torch.cat([input_img, output_img], dim=1)\\n        return self.model(x)\\n```\\n\\n**Pix2Pix Loss**:\\n```python\\ndef pix2pix_loss(G, D, input_img, target_img):\\n    # Generator loss\\n    fake_img = G(input_img)\\n    \\n    # 1. Adversarial loss\\n    fake_validity = D(input_img, fake_img)\\n    gan_loss = BCE(fake_validity, torch.ones_like(fake_validity))\\n    \\n    # 2. L1 reconstruction loss (encourages output to match target)\\n    l1_loss = nn.L1Loss()(fake_img, target_img)\\n    \\n    # Combined generator loss\\n    g_loss = gan_loss + lambda_l1 * l1_loss  # lambda_l1 = 100\\n    \\n    # Discriminator loss\\n    real_validity = D(input_img, target_img)\\n    fake_validity = D(input_img, fake_img.detach())\\n    \\n    real_loss = BCE(real_validity, torch.ones_like(real_validity))\\n    fake_loss = BCE(fake_validity, torch.zeros_like(fake_validity))\\n    d_loss = (real_loss + fake_loss) / 2\\n    \\n    return g_loss, d_loss\\n```\\n\\n**CycleGAN** (Unpaired translation):\\n\\n**Key Innovation**: No paired data needed!\\n\\n```python\\n# Learn mappings: X ↔ Y\\nG_X2Y = Generator()  # Horse → Zebra\\nG_Y2X = Generator()  # Zebra → Horse\\nD_X = Discriminator()  # Real horse vs fake horse\\nD_Y = Discriminator()  # Real zebra vs fake zebra\\n\\n# Cycle consistency loss\\ndef cycle_loss(real_X, real_Y):\\n    # Forward cycle: X → Y → X\\n    fake_Y = G_X2Y(real_X)\\n    reconstructed_X = G_Y2X(fake_Y)\\n    cycle_X_loss = nn.L1Loss()(reconstructed_X, real_X)\\n    \\n    # Backward cycle: Y → X → Y\\n    fake_X = G_Y2X(real_Y)\\n    reconstructed_Y = G_X2Y(fake_X)\\n    cycle_Y_loss = nn.L1Loss()(reconstructed_Y, real_Y)\\n    \\n    return cycle_X_loss + cycle_Y_loss\\n\\n# Total loss\\ntotal_loss = gan_loss + lambda_cycle * cycle_loss  # lambda_cycle = 10\\n```\\n\\n**Applications**:\\n\\n**1. Class-Conditional Generation**:\\n```\\n- Generate specific digits (MNIST)\\n- Generate specific objects (ImageNet classes)\\n- Generate faces with attributes (age, gender, expression)\\n```\\n\\n**2. Text-to-Image**:\\n```\\nText: "A red bird with blue wings"\\n    ↓\\nText encoder → embedding\\n    ↓\\nG(z, text_embedding) → Image of red bird with blue wings\\n```\\n\\n**3. Super-Resolution**:\\n```\\nLow-res image (64×64)\\n    ↓\\nG(low_res) → High-res image (256×256)\\n\\nConditioned on low-res input\\n```\\n\\n**4. Style Transfer**:\\n```\\nContent image + Style reference\\n    ↓\\nG(content, style) → Stylized output\\n\\nExample: Photo + Van Gogh painting → Photo in Van Gogh style\\n```\\n\\n**Key Insight**: Conditional GANs enable **controlled generation** by conditioning both generator and discriminator on additional information (labels, images, text). This allows targeted synthesis for specific applications, from class-conditional generation (cGAN) to paired image translation (Pix2Pix) and unpaired domain transfer (CycleGAN).',
    keyPoints: [],
  },
];
