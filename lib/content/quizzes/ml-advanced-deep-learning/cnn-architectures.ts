/**
 * CNN Architectures Quiz
 */
export const cnnArchitecturesQuiz = [
  {
    id: 'cnn-q1',
    question:
      "Explain the degradation problem that ResNet solved and how residual connections address it. Why can't we just stack more layers in a plain CNN to get better performance?",
    sampleAnswer:
      "The **degradation problem** was a surprising discovery: adding more layers to plain CNNs made them perform **worse**, even on training data (not overfitting!). Deeper plain networks had higher training error than shallow ones, contradicting the intuition that deeper = more expressive.\n\n**Why Stacking Layers Fails**:\n\n1. **Gradient Vanishing**: In very deep networks (50+ layers), gradients become exponentially small during backpropagation. After multiplying many small numbers (derivatives through each layer), gradients approach zero, preventing early layers from learning.\n\n2. **Optimization Difficulty**: Even if gradients don't vanish, deep plain networks create a difficult optimization landscape. Finding the optimal path through 50+ layers of parameters is extremely hard.\n\n3. **Identity Mapping Challenge**: If the optimal solution at a certain depth is the identity function (just pass input through unchanged), a plain network must learn to set all weights perfectly to achieve this - very difficult!\n\n**How Residual Connections Fix This**:\n\nInstead of learning H(x), learn residual F(x) where H(x) = F(x) + x\n\n```\nInput x → [Conv layers compute F(x)] → Add x → Output H(x)\n          └───────skip connection────────┘\n```\n\n**Benefits**:\n\n1. **Easy Identity Learning**: If identity is optimal, just set F(x) = 0 (all weights to zero) - much easier than learning identity in plain network\n\n2. **Gradient Highway**: Gradients flow directly through skip connections without diminishing. During backpropagation: dL/dx = dL/dH × (dF/dx + 1), the '+1' ensures gradient always flows\n\n3. **Residual Learning**: Learning H(x) - x (the difference/residual) is easier than learning H(x) from scratch\n\n**Empirical Results**:\n- ResNet-152 (152 layers) outperformed shallower plain networks\n- Won ImageNet 2015 with 3.57% error\n- Enabled training of 1000+ layer networks\n\nResidual connections transformed deep learning, enabling the ultra-deep networks powering modern AI.",
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      "Compare VGG's design philosophy (small filters, deep networks) with Inception\'s approach (multi-scale parallel branches). What are the computational and representational trade-offs of each design choice?",
    sampleAnswer:
      'VGG and Inception represent two distinct design philosophies that influenced modern CNN architecture.\\n\\n**VGG Philosophy: Simplicity and Depth**\\n\\n**Design**:\\n- Uniform 3×3 filters throughout entire network\\n- Stack many layers (VGG-16 has 16 weight layers)\\n- Simple, homogeneous architecture\\n- Max pooling for downsampling\\n\\n**Advantages**:\\n- **Simple to understand and modify**: All conv layers identical\\n- **Deep receptive fields**: Two 3×3 convs = 5×5 receptive field, three 3×3 = 7×7\\n- **More non-linearities**: Stacking adds ReLU activations between layers\\n- **Fewer parameters than large filters**: 2× (3×3) = 18 params vs 1× (5×5) = 25 params\\n\\n**Disadvantages**:\\n- **Too many parameters**: 138M (mostly in FC layers)\\n- **Slow**: Sequential processing, lots of operations\\n- **High memory**: Must store all intermediate activations\\n- **No multi-scale**: All filters same size\\n\\n**Inception Philosophy: Width and Efficiency**\\n\\n**Design**:\\n- Parallel branches with 1×1, 3×3, 5×5 filters\\n- 1×1 convolutions for dimension reduction\\n- Concatenate multi-scale features\\n- Network-in-network concept\\n\\n**Advantages**:\\n- **Multi-scale features**: Different filter sizes capture different patterns\\n- **Very parameter-efficient**: GoogLeNet 6.8M params (20× less than VGG!)\\n- **Computational efficiency**: 1×1 convs reduce channels before expensive ops\\n- **Better accuracy/parameter ratio**: Similar performance to VGG with far fewer params\\n\\n**Disadvantages**:\\n- **Complex architecture**: Harder to design and understand\\n- **Engineering complexity**: More components to tune\\n- **Memory fragmentation**: Many parallel branches\\n\\n**Computational Comparison**:\\n\\n*Example: 28×28×192 input, want 256 outputs*\\n\\n**VGG approach (5×5 conv)**:\\n- Params: 5 × 5 × 192 × 256 = 1.2M\\n- FLOPs: 1.2M × 28 × 28 = 940M\\n\\n**Inception approach (1×1→3×3 with reduction)**:\\n- 1×1 reduce: 192 → 96: 18K params\\n- 3×3 conv: 96 → 256: 221K params\\n- Total: ~240K params (5× reduction!)\\n- FLOPs: ~200M (4.7× reduction!)\\n\\n**Modern Synthesis**:\\n\\nBest practices combine both philosophies:\\n- **ResNet**: Depth (VGG) + skip connections\\n- **ResNeXt**: Depth + grouped convolutions (Inception-inspired)\\n- **EfficientNet**: Balanced scaling of depth, width, resolution\\n\\n**When to Use**:\\n- **VGG-style**: When simplicity matters, teaching, prototyping\\n- **Inception-style**: When efficiency matters, mobile/edge deployment, large-scale production\\n- **Modern (ResNet)**: Default choice - combines best of both',
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'EfficientNet introduced compound scaling (jointly scaling depth, width, and resolution). Explain why scaling these three dimensions together works better than scaling any single dimension. Provide intuition for why the scaling relationship is α · β² · γ² ≈ 2.',
    sampleAnswer:
      "EfficientNet\'s compound scaling addresses a fundamental question: **what's the best way to scale up a CNN?** The answer: scale depth, width, and resolution **together** in a balanced way.\n\n**The Three Scaling Dimensions**:\n\n1. **Depth (d)**: Number of layers\n   - Captures more complex features\n   - Larger receptive field\n   - Diminishing returns beyond certain depth\n\n2. **Width (w)**: Number of channels/filters\n   - More diverse features at each layer\n   - Better fine-grained patterns\n   - Plateaus if not supported by depth\n\n3. **Resolution (r)**: Input image size\n   - More pixel information\n   - Better spatial details\n   - Needs more capacity (depth/width) to process\n\n**Why Scaling One Dimension Fails**:\n\n**Only Depth**:\n- Narrow filters can't capture diverse features\n- Low resolution limits fine details\n- Example: 100 layers with 32 channels sees limited patterns\n\n**Only Width**:\n- Shallow network can't learn hierarchical features\n- Small receptive field limits context\n- Example: 10 layers with 1000 channels can't build deep representations\n\n**Only Resolution**:\n- Higher resolution without capacity to process it wastes computation\n- Network doesn't have parameters to utilize extra details\n- Example: 224×224 → 600×600 input with same small network\n\n**Why Balanced Scaling Works**:\n\nIntuitively:\n1. **Higher resolution** needs more layers (depth) to reduce spatial dimensions gradually\n2. **More layers** need more channels (width) to capture increased pattern complexity\n3. **More channels** can process higher resolution effectively\n\nThey're mutually reinforcing!\n\n**The Math: α · β² · γ² ≈ 2**\n\nWhere:\n- α = depth multiplier\n- β = width multiplier  \n- γ = resolution multiplier\n\n**Why Squared Terms (β², γ²)?**\n\n1. **Width (β²)**: \n   - Doubling width (β=2) increases FLOPs by ~4×\n   - Because convolutions are O(C_in × C_out)\n   - Both input and output channels double\n\n2. **Resolution (γ²)**:\n   - Doubling resolution (γ=2) means 2× height and 2× width\n   - Pixel count increases by 4×\n   - FLOPs scale with spatial dimensions\n\n3. **Depth (α)**:\n   - Doubling depth roughly doubles FLOPs\n   - Just linear factor\n\n**The Constraint α · β² · γ² ≈ 2**:\n\nThis ensures each scaling step increases FLOPs by roughly 2×\n\n**Example Scaling**:\n\nBase model: depth=1.0, width=1.0, resolution=224\n\n**Poor Scaling** (depth only):\n- φ=2: depth=2.0, width=1.0, resolution=224\n- Result: Deep but narrow, misses diverse features\n\n**Good Scaling** (compound):\n- φ=2: depth=1.2, width=1.1, resolution=260\n- Check: 1.2 × 1.1² × (260/224)² ≈ 2.0 ✓\n- Result: Balanced growth in all dimensions\n\n**Empirical Results**:\n\nEfficientNet-B7 vs. GPipe (depth-only scaling):\n- Similar accuracy\n- EfficientNet: 8.4× fewer parameters\n- EfficientNet: 6.1× fewer FLOPs\n\n**Key Insight**: The constraint ensures **computational budget grows predictably** while maintaining balance. Each dimension supports the others, creating networks that are both accurate and efficient.\n\nThis principled approach to scaling is now widely adopted in architecture design.",
    keyPoints: [],
  },
];
