/**
 * LSTM & GRU Quiz
 */
export const lstmGruQuiz = [
  {
    id: 'cnn-q1',
    question:
      "Explain mathematically how LSTM's cell state creates a 'gradient highway' that solves the vanishing gradient problem. Compare the gradient flow equations for LSTM vs vanilla RNN and show why LSTM can learn long-term dependencies.",
    sampleAnswer:
      "LSTM solves vanishing gradients through its cell state providing a nearly unobstructed path for gradients to flow backward through time.\n\n**Vanilla RNN Gradient Flow**:\n\nFor vanilla RNN with h_t = tanh(W_hh·h_{t-1} + W_xh·x_t), the gradient from loss L to early hidden state h_0 is:\n\n∂L/∂h_0 = (∂L/∂h_T) × (∂h_T/∂h_{T-1}) × ... × (∂h_1/∂h_0)\n\nEach Jacobian term:\n∂h_t/∂h_{t-1} = W_hh^T × diag(1 - tanh²(...))\n\nSince |tanh'(x)| ≤ 1 (typically ~0.25), and we multiply T such terms:\n||∂L/∂h_0|| ≈ ||W_hh||^T × (0.25)^T\n\nFor T=50: (0.25)^50 ≈ 10^{-30} → gradient vanishes!\n\n**LSTM Cell State Gradient Flow**:\n\nLSTM cell state update:\nC_t = f_t ⊙ C_{t-1} + i_t ⊙ C̃_t\n\nwhere f_t is forget gate (sigmoid, typically 0.5-1.0)\n\nGradient with respect to previous cell state:\n∂C_t/∂C_{t-1} = f_t  (element-wise)\n\nBackward through time:\n∂L/∂C_0 = (∂L/∂C_T) × f_T × f_{T-1} × ... × f_1\n\n**Key Difference - Additive vs Multiplicative**:\n\nRNN: Multiplicative path through weight matrices\n- ∂h_t/∂h_{t-1} involves matrix multiplication\n- Gradients decay exponentially\n\nLSTM: Additive path through cell state\n- C_t = f_t ⊙ C_{t-1} + ... (addition!)\n- ∂C_t/∂C_{t-1} = f_t (just the gate, no weight matrix)\n- Derivative of addition is 1\n\n**Numerical Example**:\n\nAssume forget gate f_t ≈ 0.95 (typical for important information):\n\nAfter T=50 steps:\n- RNN: gradient ≈ (0.25)^50 ≈ 10^{-30}\n- LSTM: gradient ≈ (0.95)^50 ≈ 0.077\n\nAfter T=100 steps:\n- RNN: essentially 0\n- LSTM: (0.95)^100 ≈ 0.006 (small but usable!)\n\n**Why This Works**:\n\n1. **Controlled Forgetting**: Forget gate f_t ∈ [0,1] is learned\n   - For important info: f_t → 1 (preserve)\n   - For irrelevant info: f_t → 0 (forget)\n\n2. **No Repeated Matrix Multiplication**: \n   - Vanilla RNN: Same W_hh multiplied T times\n   - LSTM: Element-wise multiplication by scalars (gates)\n\n3. **Additive Updates**:\n   - New information added: + i_t ⊙ C̃_t\n   - Addition has gradient = 1 (no decay!)\n\n4. **Gating Provides Flexibility**:\n   - Can maintain constant cell state indefinitely (f_t=1, i_t=0)\n   - Can completely reset (f_t=0, i_t=1)\n   - Can do anything in between\n\n**Mathematical Proof Sketch**:\n\nFor LSTM, if forget gates stay near 1 for important information:\n\n∂L/∂C_0 ≈ (∂L/∂C_T) × ∏_{t=1}^T f_t\n\nIf f_t ≈ 1, then ∏_{t=1}^T f_t ≈ 1 (or slowly decaying)\n\nCompare to RNN where each term < 1 and involves weight matrix:\n∏_{t=1}^T (W_hh × diag(tanh')) → 0 exponentially fast\n\n**Practical Implications**:\n\n1. **Long-Term Dependencies**: LSTM can learn dependencies 100+ steps away\n2. **Selective Memory**: Gates learn what to remember/forget\n3. **Stable Training**: Gradients don't explode or vanish as severely\n4. **Task Success**: Tasks requiring long context (e.g., 'The cat, which ... 50 words ... , was hungry') become solvable\n\n**Limitation**: While much better than vanilla RNN, very long sequences (1000+ steps) can still be challenging. Solution: Attention mechanisms (next sections) provide even better long-range modeling.",
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      "Walk through a concrete example of how LSTM gates work for the sequence 'The cat sat on the mat. The dog ___'. Show what each gate (forget, input, output) might be doing at key timesteps and how the cell state evolves.",
    sampleAnswer:
      "Let's trace how LSTM processes this sequence, focusing on subject-verb agreement: 'The cat sat on the mat. The dog ___' (predict 'sat').\n\n**Setup**:\n- Task: Predict next word\n- Challenge: Remember 'dog' (singular subject) to predict 'sat' (singular verb)\n- Sequence: ['The', 'cat', 'sat', 'on', 'the', 'mat', '.', 'The', 'dog', '___']\n\n**Timestep 1: 'The'**\n\nInput: 'The' (determiner)\n\nForget Gate f_1:\n- Decision: Nothing important yet, keep cell state neutral\n- Value: ≈ 0.5 (moderate retention)\n- C_1 = 0.5 × C_0 + ... ≈ small values\n\nInput Gate i_1:\n- Decision: 'The' signals noun coming, prepare\n- Value: ≈ 0.3 (moderate new information)\n- Stores: Expectation of noun\n\nOutput Gate o_1:\n- Decision: Output basic features\n- Value: ≈ 0.6\n\nCell State: C_1 ≈ [0.1, 0.2, ...] (preparing for noun)\n\n**Timestep 2: 'cat'**\n\nInput: 'cat' (noun, singular, subject)\n\nForget Gate f_2:\n- Decision: Keep previous context, don't forget 'The'\n- Value: ≈ 0.8 (high retention)\n\nInput Gate i_2:\n- Decision: IMPORTANT - 'cat' is subject, store it!\n- Value: ≈ 0.9 (high importance)\n- Candidate C̃_2: Strong encoding of 'cat' + singular\n- C_2 = 0.8 × C_1 + 0.9 × [cat, singular, subject, ...]\n\nOutput Gate o_2:\n- Decision: Expose subject information\n- Value: ≈ 0.8\n\nCell State: C_2 ≈ [cat:0.8, singular:0.9, subject:0.85, ...]\n\n**Timesteps 3-7: 'sat on the mat .'**\n\nFor each of these words:\n\nForget Gate f_t:\n- 'sat' (t=3): f_3 ≈ 0.6 (some forgetting of 'cat', verb seen)\n- 'on', 'the', 'mat' (t=4,5,6): f_t ≈ 0.7-0.8 (gradually forget 'cat')\n- '.' (t=7): f_7 ≈ 0.3 (sentence end, reset for new sentence)\n\nInput Gate i_t:\n- Processing these words adds context but not critical for next prediction\n- i_t ≈ 0.2-0.4 (low to moderate)\n\nCell State Evolution:\n- C_3: Still remembers 'cat' but adds 'sat' (past tense)\n- C_4-C_6: 'cat' memory slowly fading\n- C_7: After period, 'cat' mostly forgotten\n\n**Timestep 8: 'The' (second sentence)**\n\nForget Gate f_8:\n- Decision: New sentence! Forget previous subject 'cat'\n- Value: ≈ 0.2 (strong forgetting)\n- C_8 = 0.2 × C_7 + ... (mostly cleared)\n\nInput Gate i_8:\n- Decision: New determiner, noun coming\n- Value: ≈ 0.5\n\nCell State: C_8 ≈ [determiner:0.5, ...] (reset state)\n\n**Timestep 9: 'dog' (new subject)**\n\nForget Gate f_9:\n- Decision: Keep context from 'The'\n- Value: ≈ 0.7\n\nInput Gate i_9:\n- Decision: NEW SUBJECT - store it!\n- Value: ≈ 0.95 (very high importance)\n- Candidate: Strong encoding of 'dog' + singular\n- C_9 = 0.7 × C_8 + 0.95 × [dog, singular, subject, ...]\n\nOutput Gate o_9:\n- Decision: Expose subject for verb prediction\n- Value: ≈ 0.9 (high output)\n\nCell State: C_9 ≈ [dog:0.9, singular:0.95, subject:0.9, ...]\n\n**Timestep 10: Prediction (next word)**\n\nInput: Start token or padding\n\nThe model uses h_9 (which depends on C_9) to predict:\n- Cell state C_9 contains: dog, singular, subject\n- Hidden state h_9 = o_9 ⊙ tanh(C_9) has this information\n- Output layer: h_9 → probabilities over vocabulary\n- Top predictions:\n  - 'sat': high probability (singular verb, matches 'dog')\n  - 'were': low probability (plural verb, doesn't match)\n\n**Key Observations**:\n\n1. **Selective Forgetting**:\n   - 'cat' stored at t=2, gradually forgotten by t=7\n   - Period triggers strong forgetting (f_8 ≈ 0.2)\n   - 'dog' stored fresh at t=9\n\n2. **Input Gating**:\n   - High for subjects: 'cat' (i_2≈0.9), 'dog' (i_9≈0.95)\n   - Low for function words: 'the', 'on' (i_t≈0.2-0.4)\n\n3. **Output Gating**:\n   - High when subject info needed: t=2, t=9\n   - Moderate for other words\n\n4. **Cell State as Memory**:\n   - C_2: Remembers 'cat'\n   - C_7: 'cat' faded\n   - C_9: Now remembers 'dog'\n   - Continuous, differentiable memory\n\n**Why This Works**:\n\nThe gates learn these patterns from data:\n- **Linguistic knowledge**: Subjects are important for verb prediction\n- **Temporal structure**: Sentence boundaries trigger reset\n- **Selective attention**: Remember important words, forget filler\n\nThis learned gating behavior enables accurate long-range predictions that would be impossible with vanilla RNN (which would have forgotten 'dog' after just a few steps).",
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'Compare LSTM and GRU architecturally and computationally. When would you choose GRU over LSTM or vice versa? Provide specific scenarios and justify your reasoning with parameter counts and training considerations.',
    sampleAnswer:
      "LSTM and GRU are both gated RNN architectures that solve vanishing gradients, but differ in complexity and structure.\n\n**Architectural Comparison**:\n\n**LSTM (1997) - Three Gates + Cell State**:\n\nEquations:\n```\nf_t = σ(W_f·[h_{t-1}, x_t] + b_f)  # Forget gate\ni_t = σ(W_i·[h_{t-1}, x_t] + b_i)  # Input gate\no_t = σ(W_o·[h_{t-1}, x_t] + b_o)  # Output gate\nC̃_t = tanh(W_c·[h_{t-1}, x_t] + b_c)  # Candidate\nC_t = f_t⊙C_{t-1} + i_t⊙C̃_t        # Cell state\nh_t = o_t⊙tanh(C_t)                  # Hidden state\n```\n\nComponents:\n- 2 states: cell state C_t, hidden state h_t\n- 3 gates: forget, input, output\n- 4 weight matrices per layer\n\n**GRU (2014) - Two Gates, No Cell State**:\n\nEquations:\n```\nr_t = σ(W_r·[h_{t-1}, x_t] + b_r)  # Reset gate\nz_t = σ(W_z·[h_{t-1}, x_t] + b_z)  # Update gate\nh̃_t = tanh(W_h·[r_t⊙h_{t-1}, x_t] + b_h)  # Candidate\nh_t = (1-z_t)⊙h_{t-1} + z_t⊙h̃_t  # Hidden state\n```\n\nComponents:\n- 1 state: hidden state h_t only\n- 2 gates: reset, update\n- 3 weight matrices per layer\n- Update gate combines forget+input\n\n**Parameter Count Comparison**:\n\nFor input dimension d, hidden dimension h:\n\n**LSTM Parameters**:\n```\nW_f: (h+d) × h = h² + dh\nW_i: (h+d) × h = h² + dh  \nW_o: (h+d) × h = h² + dh\nW_c: (h+d) × h = h² + dh\nBiases: 4h\nTotal: 4(h² + dh) + 4h = 4h² + 4dh + 4h\n```\n\n**GRU Parameters**:\n```\nW_r: (h+d) × h = h² + dh\nW_z: (h+d) × h = h² + dh\nW_h: (h+d) × h = h² + dh\nBiases: 3h\nTotal: 3(h² + dh) + 3h = 3h² + 3dh + 3h\n```\n\n**Reduction**: GRU has 25% fewer parameters\n\n**Concrete Example**:\nd=512 (embedding), h=1024 (hidden)\n\n- LSTM: 4(1024² + 512×1024) + 4×1024 = 4,198,400 + 2,101,248 + 4,096 ≈ 6.3M params\n- GRU: 3(1024² + 512×1024) + 3×1024 = 3,145,728 + 1,572,864 + 3,072 ≈ 4.7M params\n- Savings: 1.6M parameters (25%)\n\n**Computational Complexity**:\n\n**Per Timestep**:\n- LSTM: 4 matrix multiplications + element-wise ops\n- GRU: 3 matrix multiplications + element-wise ops\n- GRU: ~25% fewer FLOPs\n\n**Training Time** (empirical):\n- GRU typically 15-30% faster than LSTM\n- Due to fewer parameters and simpler computation\n\n**Memory Usage**:\n- LSTM: Stores both C_t and h_t\n- GRU: Stores only h_t\n- GRU: ~2× less state memory (significant for long sequences)\n\n**When to Choose LSTM**:\n\n**Scenario 1: Very Long Sequences (1000+ timesteps)**\n- LSTM's separate cell state provides more capacity\n- Output gate allows hiding cell state when not needed\n- Example: Document-level NLP, long audio transcription\n\n**Scenario 2: Complex Patterns, Large Datasets**\n- More parameters = more expressiveness\n- With millions of training examples, extra capacity helps\n- Example: Large-scale machine translation, speech recognition\n\n**Scenario 3: Bidirectional Networks**\n- LSTM slightly better for bidirectional (empirically)\n- Example: NER, POS tagging, sentiment analysis\n\n**Scenario 4: When Interpretability Matters**\n- Cell state provides clear memory mechanism\n- Three gates easier to interpret than two\n- Example: Research, debugging, model analysis\n\n**When to Choose GRU**:\n\n**Scenario 1: Limited Computational Resources**\n- 25% faster training\n- Lower memory footprint\n- Example: Mobile deployment, edge devices, rapid prototyping\n\n**Scenario 2: Small to Medium Datasets**\n- Fewer parameters = less overfitting risk\n- Better generalization with limited data\n- Example: Domain-specific NLP (<100K samples)\n\n**Scenario 3: Faster Experimentation**\n- Quicker to train = faster iteration\n- Good starting point for any sequence task\n- Example: Research phase, hyperparameter search\n\n**Scenario 4: Moderate Sequence Lengths (50-500)**\n- Performance often identical to LSTM\n- Efficiency advantage matters more\n- Example: Sentence-level NLP, short time series\n\n**Scenario 5: Real-Time Inference**\n- Lower latency per timestep\n- Streaming applications\n- Example: Online speech recognition, live translation\n\n**Performance Comparison (Empirical)**:\n\n**Task-Specific Results** (typical):\n\n```\nTask                 | LSTM Accuracy | GRU Accuracy | Winner\n---------------------|---------------|--------------|--------\nSentiment Analysis   | 87.2%         | 87.4%        | Tie\nNamed Entity Recog.  | 91.5%         | 91.1%        | LSTM\nMachine Translation  | 32.1 BLEU     | 31.8 BLEU    | LSTM\nSpeech Recognition   | 18.2% WER     | 18.5% WER    | LSTM\nTime Series Forecast | 2.45 RMSE     | 2.43 RMSE    | GRU\n```\n\nOften differences are within noise!\n\n**Decision Tree**:\n\n```\nDo you have GPU/computational constraints?\n├─ Yes → GRU (faster, lighter)\n└─ No\n   │\n   Dataset size?\n   ├─ Small (<100K) → GRU (less overfitting)\n   └─ Large (>1M)\n      │\n      Sequence length?\n      ├─ Short (<200) → GRU (similar performance, faster)\n      └─ Very long (>1000) → LSTM (better capacity)\n```\n\n**Modern Context**:\n\n**2015-2017**: LSTM/GRU were state-of-the-art\n**2018-present**: Transformers dominate NLP\n**Current use cases** for LSTM/GRU:\n- Time series (still excellent)\n- Streaming/online learning\n- Resource-constrained environments\n- When sequential processing needed\n\n**Practical Recommendation**:\n1. **Start with GRU** - faster iteration\n2. **Switch to LSTM** if:\n   - Need extra 2-3% accuracy\n   - Very long sequences\n   - Large dataset available\n3. **Consider Transformers** if:\n   - Enough data (>100K samples)\n   - Can afford computation\n   - Need state-of-the-art\n\nIn practice, choice between LSTM/GRU often matters less than:\n- Network depth, hidden size\n- Regularization, training procedure  \n- Data quality and quantity\n- Feature engineering\n\nBoth are excellent, mature architectures with strong theoretical foundations and practical track records.",
    keyPoints: [],
  },
];
