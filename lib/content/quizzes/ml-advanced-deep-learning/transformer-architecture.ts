/**
 * Transformer Architecture Quiz
 */

export const transformerArchitectureQuiz = [
  {
    id: 'transformer-q1',
    question:
      'Explain how the Transformer architecture achieves parallelization during training, unlike RNNs. Walk through the key mechanisms that enable this, and explain why this matters for training efficiency.',
    sampleAnswer:
      'Transformers achieve parallelization by eliminating sequential dependencies through **self-attention** and **positional encodings**.\\n\\n**Why RNNs Cannot Parallelize**:\\n```\\nRNN: h_t = f (h_{t-1}, x_t)\\n```\\n- Must compute h_1 before h_2 before h_3...\\n- Sequential bottleneck: Cannot process positions in parallel\\n- Training time O(n) in sequence length\\n\\n**How Transformers Parallelize**:\\n\\n**1. Self-Attention (No Sequential Dependency)**:\\n```\\nFor each position i simultaneously:\\nQ_i = x_i W^Q, K_i = x_i W^K, V_i = x_i W^V\\nAttention(Q, K, V) = softmax(QK^T / √d_k) V\\n```\\n- All positions attend to all positions in one operation\\n- Matrix multiplication: Process entire sequence at once\\n- No dependency between computing h_i and h_j\\n\\n**2. Positional Encoding (Preserves Order)**:\\n```python\\nPE(pos, 2i) = sin (pos / 10000^(2i/d_model))\\nPE(pos, 2i+1) = cos (pos / 10000^(2i/d_model))\\n```\\n- Adds position information to embeddings\\n- Allows model to use order without sequential processing\\n\\n**3. Feed-Forward Layers (Position-wise)**:\\n```python\\nFFN(x) = max(0, xW_1 + b_1)W_2 + b_2\\n```\\n- Applied independently to each position\\n- Fully parallelizable across sequence\\n\\n**Training Efficiency Impact**:\\n\\n**RNN (LSTM)**:\\n- Sequential: O(n) time steps\\n- Example: 100-token sequence = 100 sequential steps\\n- Cannot use GPU parallelism effectively\\n- Training time: Hours to days\\n\\n**Transformer**:\\n- Parallel: O(1) time steps (constant)\\n- Example: 100-token sequence = all tokens processed simultaneously\\n- Full GPU parallelism: Process all 100 tokens at once\\n- Training time: Minutes to hours (10-100× faster)\\n\\n**Memory-Computation Tradeoff**:\\n- RNN: O(n) memory, O(n) sequential ops\\n- Transformer: O(n²) memory (attention matrix), O(1) parallel ops\\n- Worth it: Wall-clock time reduced dramatically\\n\\n**Real-World Impact**:\\n- BERT: Trained on 16 TPUs in 4 days (vs months for RNN)\\n- GPT-3: 175B parameters trained because of parallelization\\n- Enables scaling to massive datasets (TB of text)\\n\\n**Why It Matters**:\\n1. **Research iteration**: Train models in hours vs days\\n2. **Model scaling**: Larger models feasible (billions of parameters)\\n3. **Data scaling**: Process massive corpora (Wikipedia, web text)\\n4. **Democratization**: Smaller teams can train large models\\n\\nParallelization is **the key innovation** that enabled the deep learning revolution in NLP (2017-present).',
    keyPoints: [],
  },
  {
    id: 'transformer-q2',
    question:
      'Explain multi-head attention in Transformers. Why use multiple attention heads instead of one large attention mechanism? Provide concrete examples of what different heads might learn.',
    sampleAnswer:
      'Multi-head attention allows the model to **attend to different aspects of the input simultaneously** by learning multiple attention patterns in parallel.\\n\\n**Single-Head Attention (Baseline)**:\\n```python\\nAttention(Q, K, V) = softmax(QK^T / √d_k) V\\n# d_model = 512, one attention pattern\\n```\\n- One attention pattern for entire sequence\\n- Must capture all relationships in single representation\\n- Limited expressiveness\\n\\n**Multi-Head Attention (8 Heads)**:\\n```python\\nhead_i = Attention(QW^Q_i, KW^K_i, VW^V_i)\\n# Each head: d_k = d_model / num_heads = 512/8 = 64\\nMultiHead(Q,K,V) = Concat (head_1,...,head_8)W^O\\n```\\n- 8 different attention patterns learned independently\\n- Each head specializes in different relationships\\n- Combined via learned output projection W^O\\n\\n**Why Multiple Heads?**\\n\\n**1. Different Linguistic Relationships**:\\n\\nExample sentence: "The cat sat on the mat because it was tired"\\n\\n**Head 1 - Syntactic (Subject-Verb)**:\\n```\\n"cat" → "sat" (0.9)\\n"it" → "was" (0.8)\\nLearns grammatical structure\\n```\\n\\n**Head 2 - Coreference (Pronouns)**:\\n```\\n"it" → "cat" (0.95)\\n"it" → "mat" (0.05)\\nResolves what "it" refers to\\n```\\n\\n**Head 3 - Semantic (Related Concepts)**:\\n```\\n"cat" → "mat" (0.7)\\n"sat" → "mat" (0.6)\\nLearns spatial relationships\\n```\\n\\n**Head 4 - Long-Range (Causal)**:\\n```\\n"tired" → "sat" (0.8)\\n"because" → "tired" (0.9)\\nCaptures cause-effect across distance\\n```\\n\\n**2. Representational Capacity**:\\n\\n**Single Head (512-dim)**:\\n```\\nOne 512×512 attention pattern\\nMust encode everything in one matrix\\n```\\n\\n**8 Heads (64-dim each)**:\\n```\\nEight 64×64 attention patterns\\n8 specialized subspaces\\nMore flexible than one large space\\n```\\n\\n**Analogy**: Like having 8 specialists vs 1 generalist\\n- 1 doctor (knows everything moderately)\\n- vs 8 specialists (cardiologist, neurologist, etc.)\\n\\n**3. Parameter Efficiency**:\\n\\n**Single Head**:\\n- W^Q, W^K, W^V: 512×512 each = 786K params\\n- Total: 786K params\\n\\n**8 Heads**:\\n- W^Q_i, W^K_i, W^V_i: 512×64 each = 98K params per head\\n- 8 heads: 98K × 3 × 8 = 2.3M params\\n- More parameters, but specialized\\n\\n**4. Robustness**:\\n- If one head fails to learn useful pattern, others compensate\\n- Ensemble effect: Multiple perspectives on data\\n- Reduces overfitting to single attention pattern\\n\\n**Implementation Details**:\\n```python\\nclass MultiHeadAttention:\\n    def __init__(self, d_model=512, num_heads=8):\\n        self.num_heads = num_heads\\n        self.d_k = d_model // num_heads  # 64\\n        \\n        # Separate projections for each head\\n        self.W_Q = Linear (d_model, d_model)  # 512×512\\n        self.W_K = Linear (d_model, d_model)\\n        self.W_V = Linear (d_model, d_model)\\n        self.W_O = Linear (d_model, d_model)\\n    \\n    def forward (self, Q, K, V):\\n        batch_size = Q.size(0)\\n        \\n        # Linear projections and split into heads\\n        Q = self.W_Q(Q).view (batch_size, -1, self.num_heads, self.d_k)\\n        K = self.W_K(K).view (batch_size, -1, self.num_heads, self.d_k)\\n        V = self.W_V(V).view (batch_size, -1, self.num_heads, self.d_k)\\n        \\n        # Transpose for attention: (batch, num_heads, seq_len, d_k)\\n        Q, K, V = Q.transpose(1,2), K.transpose(1,2), V.transpose(1,2)\\n        \\n        # Scaled dot-product attention for all heads in parallel\\n        scores = torch.matmul(Q, K.transpose(-2,-1)) / math.sqrt (self.d_k)\\n        attn = F.softmax (scores, dim=-1)\\n        output = torch.matmul (attn, V)\\n        \\n        # Concatenate heads and project\\n        output = output.transpose(1,2).contiguous().view (batch_size, -1, d_model)\\n        return self.W_O(output)\\n```\\n\\n**Empirical Evidence**:\\n- Transformer (8 heads): 28.4 BLEU on WMT translation\\n- Single head: ~25 BLEU (significant degradation)\\n- Too many heads (16+): Diminishing returns\\n\\n**Key Insight**: Multi-head attention is like **committee decision-making** - multiple perspectives lead to better, more robust representations than any single view.',
    keyPoints: [],
  },
  {
    id: 'transformer-q3',
    question:
      'Explain the purpose and mechanism of the decoder causal mask in Transformers. Why is it necessary, and how does it differ from encoder self-attention?',
    sampleAnswer:
      'The **causal mask** (also called "look-ahead mask") prevents the decoder from attending to future positions during training, ensuring **autoregressive generation** at inference time.\\n\\n**The Problem Without Causal Mask**:\\n\\nDuring training, decoder has access to entire target sequence:\\n```\\nTarget: "The cat sat on the mat"\\nWithout mask: Position 3 ("sat") can see "on", "the", "mat"\\n→ Model learns to cheat by looking ahead!\\n→ At inference: No future tokens available → poor performance\\n```\\n\\n**Training vs Inference Mismatch**:\\n\\n**Training (without mask)** - Teacher forcing:\\n```python\\n# All target tokens available\\ntarget = ["<sos>", "The", "cat", "sat", "on", "the", "mat",]\\nlogits = decoder (target)  # Can see all positions\\nloss = cross_entropy (logits, target[1:])  # Predict next token\\n```\\n→ Model learns to look ahead (cheating)\\n\\n**Inference** - Autoregressive:\\n```python\\n# Generate one token at a time\\ngenerated = ["<sos>",]\\nfor i in range (max_len):\\n    logits = decoder (generated)  # Can only see previous tokens\\n    next_token = argmax (logits[-1])\\n    generated.append (next_token)\\n```\\n→ Model never saw this pattern during training → fails!\\n\\n**Solution: Causal Mask**\\n\\n**Mask Structure**:\\n```python\\n# Sequence length = 4\\nmask = [[1, 0, 0, 0],    # Position 0 sees only itself\\n        [1, 1, 0, 0],    # Position 1 sees 0,1\\n        [1, 1, 1, 0],    # Position 2 sees 0,1,2\\n        [1, 1, 1, 1]]    # Position 3 sees 0,1,2,3\\n# Lower triangular matrix: 1 = attend, 0 = mask\\n```\\n\\n**Implementation**:\\n```python\\ndef create_causal_mask (seq_len):\\n    # Create lower triangular matrix\\n    mask = torch.tril (torch.ones (seq_len, seq_len))\\n    return mask\\n\\ndef apply_causal_mask (scores, mask):\\n    # scores: (batch, num_heads, seq_len, seq_len)\\n    # mask: (seq_len, seq_len)\\n    \\n    # Set masked positions to -inf before softmax\\n    scores = scores.masked_fill (mask == 0, float(\'-inf\'))\\n    \\n    # Softmax: exp(-inf) = 0 → no attention to future\\n    attn_weights = F.softmax (scores, dim=-1)\\n    return attn_weights\\n```\\n\\n**How It Works**:\\n\\n**Step 1: Compute Attention Scores**:\\n```python\\nscores = QK^T / √d_k  # (seq_len, seq_len)\\n# Example (4×4):\\n[[2.1, 0.5, 1.2, 0.8],\\n [1.5, 2.3, 0.9, 1.1],\\n [0.8, 1.2, 2.5, 0.7],\\n [1.1, 0.9, 1.3, 2.8]]\\n```\\n\\n**Step 2: Apply Mask (Set Future to -∞)**:\\n```python\\nmasked_scores = scores.masked_fill (mask == 0, -inf)\\n# Result:\\n[[2.1,  -∞,  -∞,  -∞],\\n [1.5, 2.3,  -∞,  -∞],\\n [0.8, 1.2, 2.5,  -∞],\\n [1.1, 0.9, 1.3, 2.8]]\\n```\\n\\n**Step 3: Softmax (exp(-∞) = 0)**:\\n```python\\nattn_weights = softmax (masked_scores, dim=-1)\\n# Result:\\n[[1.00, 0.00, 0.00, 0.00],  # Only attends to self\\n [0.45, 0.55, 0.00, 0.00],  # Attends to positions 0,1\\n [0.15, 0.20, 0.65, 0.00],  # Attends to positions 0,1,2\\n [0.20, 0.15, 0.25, 0.40]]  # Attends to all positions\\n```\\n\\n**Encoder vs Decoder Self-Attention**:\\n\\n**Encoder Self-Attention (Bidirectional)**:\\n```\\nInput: "The cat sat"\\nMask: All ones (full attention)\\n[[1, 1, 1],\\n [1, 1, 1],\\n [1, 1, 1]]\\n\\n"cat" can see: "The" (left), "sat" (right)\\nBidirectional context → better understanding\\nUse case: Sentence encoding, classification\\n```\\n\\n**Decoder Self-Attention (Causal)**:\\n```\\nTarget: "The cat sat"\\nMask: Lower triangular (causal)\\n[[1, 0, 0],\\n [1, 1, 0],\\n [1, 1, 1]]\\n\\n"cat" can see: "The" (left), not "sat" (future)\\nAutoregressive → matches inference\\nUse case: Text generation, translation\\n```\\n\\n**Why This Matters**:\\n\\n**1. Training-Inference Consistency**:\\n- Training: Masked to only see past\\n- Inference: Naturally only sees past\\n- No distribution shift!\\n\\n**2. Autoregressive Property**:\\n```\\nP(y_1, y_2, y_3 | x) = P(y_1|x) × P(y_2|y_1,x) × P(y_3|y_1,y_2,x)\\n```\\n- Each token depends only on previous tokens\\n- Matches generative process\\n\\n**3. Computational Efficiency**:\\n- Can still parallelize during training (all positions at once)\\n- Only inference is sequential\\n\\n**Common Mistake**:\\n```python\\n# WRONG: Applying causal mask to encoder\\nencoder_output = encoder (source, causal_mask=True)  # ❌\\n# Encoder should see full bidirectional context!\\n\\n# CORRECT: Causal mask only in decoder self-attention\\ndecoder_output = decoder (target, causal_mask=True,   # ✓\\n                        encoder_output=encoder_output)\\n```\\n\\n**Key Insight**: The causal mask ensures the model learns to generate tokens **sequentially** during training, matching the autoregressive process at inference. Without it, the model would learn to "cheat" by looking ahead, leading to poor generation quality.',
    keyPoints: [],
  },
];
