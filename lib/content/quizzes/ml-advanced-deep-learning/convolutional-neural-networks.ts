/**
 * Convolutional Neural Networks (CNNs) Quiz
 */
export const convolutionalNeuralNetworksQuiz = [
  {
    id: 'cnn-q1',
    question:
      'Explain how CNNs achieve parameter efficiency compared to fully connected networks when processing images. Provide a concrete numerical example comparing the number of parameters needed for a small image.',
    sampleAnswer:
      'CNNs achieve parameter efficiency through **parameter sharing** and **sparse connectivity**. Instead of having unique weights for every input-output connection like fully connected networks, CNNs use the same small filter across the entire image.\\n\\n**Concrete Example**: Consider processing a 28×28 grayscale image (like MNIST):\\n\\n**Fully Connected Approach**:\\n- Input: 28 × 28 = 784 pixels\\n- Hidden layer with 128 neurons: 784 × 128 = 100,352 parameters\\n- Just the first layer needs 100K+ parameters!\\n\\n**CNN Approach**:\\n- Conv layer: 32 filters of size 3×3\\n- Parameters per filter: 3 × 3 = 9 weights + 1 bias = 10\\n- Total: 32 × 10 = 320 parameters\\n- **Reduction**: 100,352 / 320 = 313× fewer parameters!\\n\\n**Why This Works**:\\n1. **Parameter Sharing**: The same 3×3 filter is applied across all 784 positions in the image, reusing weights\\n2. **Sparse Connectivity**: Each output only connects to a 3×3 local region (9 inputs) rather than all 784 inputs\\n3. **Translation Invariance**: Same filter detects edges whether in top-left or bottom-right\\n\\n**Benefits**:\\n- Dramatically reduces model size\\n- Less prone to overfitting (fewer parameters to learn)\\n- Faster training and inference\\n- Exploits spatial structure of images\\n\\nFor larger images (e.g., 224×224×3 RGB), the difference becomes even more dramatic - millions vs billions of parameters. This efficiency is why CNNs revolutionized computer vision.',
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      'A CNN layer has input shape (H=32, W=32, C_in=3), uses kernel size 5×5, has C_out=64 output channels, stride=2, and padding=2. Calculate: (a) the output spatial dimensions, (b) the number of learnable parameters in this layer, and (c) explain how stride affects computation and output size.',
    sampleAnswer:
      "Let's systematically solve this CNN layer configuration problem.\n\n**(a) Output Spatial Dimensions**:\n\nFormula: Output_size = (Input_size - Kernel_size + 2 × Padding) / Stride + 1\n\n- Height: (32 - 5 + 2×2) / 2 + 1 = (32 - 5 + 4) / 2 + 1 = 31 / 2 + 1 = 15.5 + 1 = 16\n- Width: Same calculation = 16\n- Channels: C_out = 64\n\n**Output shape: (16, 16, 64)**\n\n**(b) Number of Learnable Parameters**:\n\nEach filter:\n- Weights: Kernel_height × Kernel_width × C_in = 5 × 5 × 3 = 75\n- Bias: 1\n- Total per filter: 76\n\nAll filters:\n- 64 filters × 76 parameters = 4,864 parameters\n\n**Total parameters: 4,864**\n\nBreakdown: 4,800 weights + 64 biases\n\n**(c) How Stride Affects Computation and Output Size**:\n\n**Stride = 1** (dense sampling):\n- Filter moves 1 pixel at a time\n- Output size ≈ Input size (with same padding)\n- More overlapping windows\n- More computation (more positions)\n- Example: 32×32 input → ~32×32 output\n\n**Stride = 2** (our case):\n- Filter moves 2 pixels at a time\n- Skips every other position\n- Output size reduced by ~2× in each dimension\n- **4× less computation** (2× in height, 2× in width)\n- Acts as downsampling\n- Example: 32×32 input → 16×16 output\n\n**Trade-offs**:\n- Larger stride = faster computation, smaller output, less detail\n- Smaller stride = slower computation, larger output, more detail\n- Stride > 1 can replace pooling layers for downsampling\n\nIn modern architectures, stride=2 in some layers reduces computational cost while maintaining performance, effectively combining feature extraction with downsampling.",
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'Compare and contrast max pooling vs average pooling. In what scenarios would you prefer one over the other? Discuss the trade-offs and provide examples of architectures that use each approach.',
    sampleAnswer:
      'Max pooling and average pooling are both downsampling operations, but they aggregate information differently.\\n\\n**Max Pooling**:\\n- **Operation**: Takes maximum value in each window\\n- **Effect**: Preserves strongest activations (most prominent features)\\n- **Behavior**: Sharpens features, emphasizes presence of patterns\\n- **Gradient**: Only flows through maximum value position\\n\\n**Average Pooling**:\\n- **Operation**: Takes mean of values in each window\\n- **Effect**: Smooth aggregation of all values\\n- **Behavior**: Softer downsampling, considers all features\\n- **Gradient**: Flows equally to all positions in window\\n\\n**When to Use Max Pooling**:\\n1. **Classification tasks** - detecting presence of features matters more than exact values\\n2. **When features are sparse** - edges, corners, key patterns\\n3. **Robustness to noise** - focuses on strong signals, ignores weak noise\\n4. **Default choice** - works well for most computer vision tasks\\n\\n**Examples**: VGG, AlexNet, most modern CNNs use max pooling\\n\\n**When to Use Average Pooling**:\\n1. **Smooth transitions** - preserving subtle variations\\n2. **Final layers** - global average pooling for classification (reduces parameters)\\n3. **When all features matter** - not just strongest activation\\n4. **Texture analysis** - overall pattern more important than peaks\\n\\n**Examples**: \\n- GoogLeNet (Inception) uses global average pooling before classification\\n- ResNet uses average pooling in final layer\\n- Some shallow networks for texture classification\\n\\n**Comparison Table**:\\n```\\nAspect          | Max Pooling        | Average Pooling\\n----------------|--------------------|-----------------\\nStrong features | Emphasized         | Smoothed\\nNoise           | More robust        | Includes noise\\nGradients       | Sparse             | Dense\\nCommon use      | Feature extraction | Final layers\\nInvariance      | Better translation | Better scale\\nDefault choice  | Yes ✓              | Specific cases\\n```\\n\\n**Modern Trends**:\\n1. **Strided convolutions** replacing pooling in some architectures\\n2. **Global average pooling** preferred over FC layers (fewer parameters)\\n3. **Max pooling** still dominant for spatial downsampling\\n4. **No pooling** in some architectures (e.g., fully convolutional networks)\\n\\n**Rule of Thumb**: Use max pooling for general feature extraction in early/middle layers, consider average pooling (especially global) in final layers before classification.',
    keyPoints: [],
  },
];
