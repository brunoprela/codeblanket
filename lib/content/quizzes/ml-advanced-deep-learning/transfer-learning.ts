/**
 * Transfer Learning Quiz
 */

export const transferLearningQuiz = [
  {
    id: 'transfer-learning-q1',
    question:
      'Explain the difference between feature extraction and fine-tuning in transfer learning. When would you use each approach, and what are the tradeoffs?',
    sampleAnswer:
      "Feature extraction and fine-tuning represent two strategies for leveraging pre-trained models, differing in **which layers are updated** during training.\n\n**Feature Extraction (Freeze Pre-trained Layers)**:\n\n**Approach**:\n```python\n# Load pre-trained ResNet\nmodel = torchvision.models.resnet50(pretrained=True)\n\n# Freeze all convolutional layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Replace and train only final classifier\nmodel.fc = nn.Linear(2048, num_classes)\n# Only model.fc parameters will be updated\n```\n\n**What Happens**:\n- Pre-trained layers: **Frozen** (no gradient updates)\n- Feature extractor: Acts as fixed transformation\n- New classifier: Trained from scratch\n- Learns: How to combine pre-trained features for new task\n\n**When to Use**:\n\n**1. Small Dataset (<1,000 samples per class)**:\n```\nDataset: 500 images per class\nFine-tuning: Risk of overfitting (millions of parameters)\nFeature extraction: Safer (only thousands of classifier params)\n```\n\n**2. Similar Domain**:\n```\nPre-trained: ImageNet (objects, animals, scenes)\nNew task: Medical images of organs\n→ Similar low-level features (edges, textures)\n→ Pre-trained features already good!\n```\n\n**3. Limited Compute**:\n```\nFeature extraction: 5-10 minutes on single GPU\nFine-tuning: 2-4 hours on single GPU\n→ 10-20× faster training\n```\n\n**4. Quick Prototyping**:\n```\nTest multiple architectures quickly:\n- ResNet-50 features: 5 min → 85% accuracy\n- EfficientNet features: 7 min → 87% accuracy\n→ Choose best, then fine-tune if needed\n```\n\n**Fine-Tuning (Update Pre-trained Layers)**:\n\n**Approach**:\n```python\n# Load pre-trained ResNet\nmodel = torchvision.models.resnet50(pretrained=True)\n\n# Replace final classifier\nmodel.fc = nn.Linear(2048, num_classes)\n\n# Train all layers (or subset) with different learning rates\noptimizer = optim.SGD([\n    {'params': model.layer4.parameters(), 'lr': 1e-4},  # Deep layers\n    {'params': model.layer3.parameters(), 'lr': 1e-5},  # Mid layers\n    {'params': model.fc.parameters(), 'lr': 1e-3}      # New classifier\n])\n```\n\n**What Happens**:\n- Pre-trained layers: **Unfrozen** (gradient updates allowed)\n- Entire network: Adapted to new task\n- Learns: Task-specific features at all levels\n\n**When to Use**:\n\n**1. Larger Dataset (>10,000 samples per class)**:\n```\nDataset: 50,000 images per class\nFeature extraction: Underfits (limited by fixed features)\nFine-tuning: Better performance (adapts features to data)\n```\n\n**2. Different Domain**:\n```\nPre-trained: ImageNet (natural images)\nNew task: Satellite imagery\n→ Different statistics, textures, objects\n→ Need to adapt low-level features\n```\n\n**3. Need Maximum Performance**:\n```\nFeature extraction: 85% accuracy\nFine-tuning: 92% accuracy\n→ Worth extra compute for 7% gain\n```\n\n**Progressive Fine-Tuning Strategy**:\n\n**Phase 1: Feature Extraction (2-5 epochs)**:\n```python\n# Freeze all except classifier\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.fc.requires_grad = True\n\n# Train with high LR\noptimizer = Adam (model.fc.parameters(), lr=1e-3)\n# Result: 83% validation accuracy\n```\n\n**Phase 2: Fine-tune Top Layers (5-10 epochs)**:\n```python\n# Unfreeze last conv block\nfor param in model.layer4.parameters():\n    param.requires_grad = True\n\n# Lower learning rate\noptimizer = Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n])\n# Result: 88% validation accuracy (+5%)\n```\n\n**Phase 3: Fine-tune All Layers (5-10 epochs)**:\n```python\n# Unfreeze all layers\nfor param in model.parameters():\n    param.requires_grad = True\n\n# Discriminative learning rates (smaller for early layers)\noptimizer = Adam([\n    {'params': model.layer1.parameters(), 'lr': 1e-6},  # Early features\n    {'params': model.layer2.parameters(), 'lr': 1e-5},\n    {'params': model.layer3.parameters(), 'lr': 1e-5},\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 1e-3}       # New layers\n])\n# Result: 91% validation accuracy (+3%)\n```\n\n**Tradeoffs**:\n\n**Training Time**:\n```\nFeature extraction: 5-10 min (only classifier gradients)\nFine-tuning: 1-4 hours (full backprop through network)\n```\n\n**Memory**:\n```\nFeature extraction: 4-6 GB GPU memory (no intermediate activations)\nFine-tuning: 10-12 GB GPU memory (store all activations)\n```\n\n**Performance**:\n```\nSmall data (1K): Feature extraction 78%, Fine-tuning 72% (overfits)\nLarge data (100K): Feature extraction 85%, Fine-tuning 94% (adapts)\n```\n\n**Overfitting Risk**:\n```\nFeature extraction: Low (few trainable params)\nFine-tuning: High (millions of trainable params)\n→ Need regularization: dropout, weight decay, data augmentation\n```\n\n**Practical Guidelines**:\n\n**Use Feature Extraction When**:\n- Dataset < 5,000 samples per class\n- New task very similar to pre-training task\n- Need fast results (prototyping)\n- Limited compute resources\n- Domain is similar (natural images → natural images)\n\n**Use Fine-Tuning When**:\n- Dataset > 10,000 samples per class\n- New task different from pre-training\n- Need maximum performance\n- Have compute resources\n- Domain shift exists (natural → medical/satellite)\n\n**Best Practice**: Start with feature extraction, then progressively fine-tune deeper layers if you have enough data and compute.",
    keyPoints: [],
  },
  {
    id: 'transfer-learning-q2',
    question:
      'Explain discriminative learning rates in transfer learning. Why should different layers use different learning rates, and how do you determine appropriate rates?',
    sampleAnswer:
      "Discriminative learning rates (also called **layer-wise learning rates** or **gradual unfreezing**) assign different learning rates to different layers, with **lower rates for early layers** and **higher rates for later layers**.\n\n**Why Different Layers Need Different Learning Rates**:\n\n**Layer Specialization in Pre-trained Networks**:\n\n**Early Layers (Layer 1-2)**:\n```\nLearn: Generic features (edges, colors, textures)\nTransferability: High (useful for almost all vision tasks)\nChange needed: Minimal (already optimal for low-level features)\n→ Need LOW learning rate (preserve learned features)\n```\n\n**Middle Layers (Layer 3-4)**:\n```\nLearn: Mid-level features (shapes, object parts)\nTransferability: Moderate (somewhat task-specific)\nChange needed: Moderate (adapt to new object types)\n→ Need MEDIUM learning rate (gradual adaptation)\n```\n\n**Late Layers (Layer 5 + Classifier)**:\n```\nLearn: High-level features (specific objects/classes)\nTransferability: Low (very task-specific)\nChange needed: Maximum (completely new classes)\n→ Need HIGH learning rate (learn new representations)\n```\n\n**The Problem with Uniform Learning Rate**:\n\n**Scenario**: Fine-tuning ResNet-50 from ImageNet (1000 classes) to medical images (10 classes)\n\n**Using Same LR for All Layers (lr=1e-3)**:\n```python\noptimizer = Adam (model.parameters(), lr=1e-3)\n```\n\n**Problem 1: Catastrophic Forgetting**:\n```\nEarly layers (edges, textures):\n- Were already optimal\n- High LR causes large updates\n- Destroys useful pre-trained features\n- Result: Layer1 weight changes by 40% → worse performance\n```\n\n**Problem 2: Slow Adaptation of Later Layers**:\n```\nClassifier:\n- Needs to learn completely new mapping\n- LR=1e-3 may be too low\n- Takes many epochs to adapt\n- Result: Slow convergence\n```\n\n**Discriminative Learning Rate Solution**:\n\n**Implementation**:\n```python\nimport torch.optim as optim\n\n# ResNet-50 with discriminative learning rates\nmodel = torchvision.models.resnet50(pretrained=True)\nmodel.fc = nn.Linear(2048, num_classes)  # New classifier\n\n# Define learning rates for each layer group\noptimizer = optim.Adam([\n    # Early layers: Very small LR (preserve generic features)\n    {'params': model.conv1.parameters(), 'lr': 1e-6},\n    {'params': model.layer1.parameters(), 'lr': 1e-6},\n    \n    # Middle layers: Small to medium LR (gradual adaptation)\n    {'params': model.layer2.parameters(), 'lr': 1e-5},\n    {'params': model.layer3.parameters(), 'lr': 1e-5},\n    \n    # Late conv layers: Medium LR (more adaptation)\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    \n    # New classifier: High LR (learn from scratch)\n    {'params': model.fc.parameters(), 'lr': 1e-3}\n], lr=1e-4)  # Default LR (used if not specified)\n```\n\n**Determining Appropriate Rates**:\n\n**Method 1: Fixed Ratios**:\n```python\nbase_lr = 1e-3  # For new classifier\n\n# Common ratio: 10× reduction per layer group\nlr_layer5 = base_lr        # 1e-3 (classifier)\nlr_layer4 = base_lr / 10   # 1e-4 (last conv block)\nlr_layer3 = base_lr / 100  # 1e-5 (mid conv blocks)\nlr_layer2 = base_lr / 100  # 1e-5\nlr_layer1 = base_lr / 1000 # 1e-6 (early layers)\n```\n\n**Method 2: Learning Rate Range Test**:\n```python\n# Test different LRs on validation set\nfor lr in [1e-6, 1e-5, 1e-4, 1e-3]:\n    model_copy = copy.deepcopy (model)\n    optimizer = Adam (model_copy.layer4.parameters(), lr=lr)\n    val_acc = train_and_evaluate (model_copy, epochs=3)\n    print(f\"LR {lr}: Val Acc {val_acc}\")\n\n# Results:\n# LR 1e-6: 82% (too small)\n# LR 1e-5: 86% (good)\n# LR 1e-4: 88% (optimal) ← Choose this\n# LR 1e-3: 79% (too large, unstable)\n```\n\n**Method 3: Gradual Unfreezing with Adaptive Rates**:\n```python\n# Stage 1: Train only classifier (5 epochs)\nfor param in model.parameters():\n    param.requires_grad = False\nmodel.fc.requires_grad = True\noptimizer = Adam (model.fc.parameters(), lr=1e-3)\n\n# Stage 2: Unfreeze layer4, use smaller LR (10 epochs)\nfor param in model.layer4.parameters():\n    param.requires_grad = True\noptimizer = Adam([\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 5e-4}  # Reduce classifier LR\n])\n\n# Stage 3: Unfreeze all, use discriminative LRs (20 epochs)\nfor param in model.parameters():\n    param.requires_grad = True\noptimizer = Adam([\n    {'params': model.layer1.parameters(), 'lr': 1e-6},\n    {'params': model.layer2.parameters(), 'lr': 1e-5},\n    {'params': model.layer3.parameters(), 'lr': 1e-5},\n    {'params': model.layer4.parameters(), 'lr': 1e-4},\n    {'params': model.fc.parameters(), 'lr': 2e-4}  # Further reduce\n])\n```\n\n**Empirical Guidelines**:\n\n**For Computer Vision (CNN)**:\n```\nNew classifier: 1e-3 to 1e-2\nLast conv block: 1e-4 to 1e-3\nMiddle conv blocks: 1e-5 to 1e-4\nEarly conv blocks: 1e-6 to 1e-5\n\nRule of thumb: 10× reduction per layer group\n```\n\n**For NLP (Transformer/BERT)**:\n```\nNew classifier: 2e-5 to 5e-5\nTop transformer layers (10-12): 1e-5 to 2e-5\nMiddle layers (6-9): 5e-6 to 1e-5\nBottom layers (1-5): 1e-6 to 5e-6\nEmbeddings: 1e-7 to 1e-6\n\nRule of thumb: Smaller LRs than vision (transformers more sensitive)\n```\n\n**Advanced Technique: Cosine Annealing with Layer-wise Decay**:\n```python\n# Start with discriminative LRs, then decay over time\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n    optimizer, T_max=num_epochs\n)\n\n# Learning rate evolves for each group:\n# Classifier: 1e-3 → 1e-5 (cosine decay)\n# Layer4: 1e-4 → 1e-6\n# Layer1: 1e-6 → 1e-8\n```\n\n**Results Comparison**:\n\n**Uniform LR (lr=1e-4 for all)**:\n```\nEpoch 10: 78% val acc\nEpoch 20: 83% val acc\nEpoch 30: 84% val acc (plateaus, early layers not adapting)\n```\n\n**Discriminative LR**:\n```\nEpoch 10: 82% val acc (faster early learning)\nEpoch 20: 88% val acc (better adaptation)\nEpoch 30: 91% val acc (continues improving)\n```\n\n**Key Insight**: Discriminative learning rates respect the **hierarchical nature** of deep networks, preserving useful low-level features while aggressively adapting high-level features to the new task.",
    keyPoints: [],
  },
  {
    id: 'transfer-learning-q3',
    question:
      'Explain the concept of domain adaptation in transfer learning. What challenges arise when the source and target domains differ significantly, and what techniques can address this?',
    sampleAnswer:
      'Domain adaptation addresses the challenge of applying models trained on one distribution (**source domain**) to a different distribution (**target domain**), dealing with **domain shift** or **covariate shift**.\\n\\n**The Domain Shift Problem**:\\n\\n**Example 1: Natural Images → Medical Images**:\\n```\\nSource (ImageNet): Natural images\\n- RGB photos, good lighting, varied backgrounds\\n- Objects: cats, dogs, cars, buildings\\n- Statistics: Mean=[0.485, 0.456, 0.406], Std=[0.229, 0.224, 0.225]\\n\\nTarget (Medical): X-ray images\\n- Grayscale, uniform background, specific anatomy\\n- Objects: bones, organs, lesions\\n- Statistics: Mean=[0.221], Std=[0.067]\\n\\n→ Distribution mismatch causes performance degradation!\\n```\\n\\n**Example 2: Synthetic → Real Data**:\\n```\\nSource (Simulation): Rendered 3D car models\\n- Perfect lighting, no noise, clean backgrounds\\n- Infinite labeled data (automatic from renderer)\\n\\nTarget (Real): Real photos of cars\\n- Varied lighting, occlusions, weather effects\\n- Limited labeled data (expensive to annotate)\\n\\n→ Model trained on synthetic fails on real images!\\n```\\n\\n**Types of Domain Shift**:\\n\\n**1. Covariate Shift**: P_source(X) ≠ P_target(X)\\n```\\nInput distributions differ, but relationship X→Y same\\nExample: Daytime images → Nighttime images\\n- Objects same, lighting different\\n- Need to adapt feature extraction\\n```\\n\\n**2. Label Shift**: P_source(Y) ≠ P_target(Y)\\n```\\nClass distributions differ\\nExample: ImageNet (balanced) → Medical (imbalanced)\\n- ImageNet: 1000 samples per class\\n- Medical: 10,000 normal, 100 disease cases\\n- Need to reweight classes\\n```\\n\\n**3. Concept Shift**: P_source(Y|X) ≠ P_target(Y|X)\\n```\\nInput-output relationship differs\\nExample: "Positive sentiment" in movies vs medical reviews\\n- "This movie killed me!" (positive in movies)\\n- "This drug killed me!" (negative in medical)\\n- Hardest to adapt!\\n```\\n\\n**Domain Adaptation Techniques**:\\n\\n**Technique 1: Fine-tuning with Target Data**:\\n\\n**Supervised Fine-tuning** (have labeled target data):\\n```python\\n# Step 1: Pre-train on source domain\\nmodel = ResNet50()\\nmodel.train_on_source (source_data_labeled)  # ImageNet\\n\\n# Step 2: Fine-tune on target domain\\nmodel.fc = nn.Linear(2048, target_classes)\\nmodel.fine_tune (target_data_labeled, lr=1e-4)  # Medical data\\n\\n# Works when: Have sufficient labeled target data (1K+ samples)\\n```\\n\\n**Semi-supervised Fine-tuning** (limited target labels):\\n```python\\n# Combine labeled and unlabeled target data\\n\\n# Step 1: Fine-tune on small labeled set\\nmodel.fine_tune (target_data_labeled_small, epochs=10)  # 100 samples\\n\\n# Step 2: Pseudo-labeling on unlabeled data\\nwith torch.no_grad():\\n    pseudo_labels = model.predict (target_data_unlabeled)\\n    confident = pseudo_labels.max (dim=1) > 0.9  # High confidence\\n    \\n# Step 3: Train on labeled + confident pseudo-labeled\\ncombined_data = target_data_labeled + target_data_unlabeled[confident]\\nmodel.train (combined_data, epochs=20)\\n\\n# Iteratively improve: More confident predictions → better model\\n```\\n\\n**Technique 2: Domain-Adversarial Training**:\\n\\n**Goal**: Learn features that are **discriminative for task** but **invariant to domain**.\\n\\n**Architecture**:\\n```python\\nclass DomainAdversarialModel (nn.Module):\\n    def __init__(self):\\n        # Shared feature extractor\\n        self.feature_extractor = ResNet50_Backbone()\\n        \\n        # Task classifier (predict labels)\\n        self.task_classifier = nn.Linear(2048, num_classes)\\n        \\n        # Domain classifier (predict source vs target)\\n        self.domain_classifier = nn.Sequential(\\n            nn.Linear(2048, 1024),\\n            nn.ReLU(),\\n            nn.Linear(1024, 2)  # Binary: source=0, target=1\\n        )\\n    \\n    def forward (self, x, alpha):\\n        # Extract features\\n        features = self.feature_extractor (x)\\n        \\n        # Task prediction\\n        task_output = self.task_classifier (features)\\n        \\n        # Domain prediction with gradient reversal\\n        reversed_features = GradientReversalLayer.apply (features, alpha)\\n        domain_output = self.domain_classifier (reversed_features)\\n        \\n        return task_output, domain_output\\n\\n# Gradient Reversal Layer: Reverse gradients during backprop\\nclass GradientReversalLayer (torch.autograd.Function):\\n    @staticmethod\\n    def forward (ctx, x, alpha):\\n        ctx.alpha = alpha\\n        return x\\n    \\n    @staticmethod\\n    def backward (ctx, grad_output):\\n        return -ctx.alpha * grad_output, None  # Reverse gradient!\\n```\\n\\n**Training**:\\n```python\\n# Loss function\\ntask_loss = CrossEntropyLoss (task_output, labels)  # Correct labels\\ndomain_loss = CrossEntropyLoss (domain_output, domain_labels)  # Source/target\\n\\ntotal_loss = task_loss + domain_loss\\n\\n# Feature extractor gradients:\\n# - Task loss pushes features to be discriminative\\n# - Domain loss (reversed) pushes features to be domain-invariant\\n# Result: Features good for task, but can\'t tell source from target!\\n```\\n\\n**Technique 3: Self-supervised Pre-training on Target Domain**:\\n\\n**Approach**: Pre-train on target domain without labels using self-supervised tasks.\\n\\n**SimCLR on Target Domain**:\\n```python\\n# Step 1: Self-supervised pre-training on unlabeled target data\\nmodel = ResNet50()\\n\\n# Contrastive learning: Same image with different augmentations → similar\\nfor epoch in range(100):\\n    for images in target_data_unlabeled:\\n        aug1 = augment (images)  # Random crop, color jitter\\n        aug2 = augment (images)  # Different augmentation\\n        \\n        z1 = model (aug1)\\n        z2 = model (aug2)\\n        \\n        # Maximize similarity between augmentations of same image\\n        loss = contrastive_loss (z1, z2)\\n        loss.backward()\\n\\n# Step 2: Fine-tune on small labeled target set\\nmodel.fc = nn.Linear(2048, num_classes)\\nmodel.fine_tune (target_data_labeled_small, epochs=20)\\n\\n# Learns target domain features without labels!\\n```\\n\\n**Technique 4: Data Augmentation for Domain Gap**:\\n\\n**CycleGAN for Style Transfer**:\\n```python\\n# Train CycleGAN to translate source → target style\\ncyclegan.train (source_images, target_images_unlabeled)\\n\\n# Generate synthetic target-style images from source\\nsynthetic_target = cyclegan.transform (source_images_labeled)\\n\\n# Train on synthetic target data (has labels from source)\\nmodel.train (synthetic_target, labels_from_source)\\n\\n# Fine-tune on real target data if available\\nmodel.fine_tune (target_data_labeled_small)\\n\\n# Bridges domain gap visually!\\n```\\n\\n**Advanced Technique: Optimal Transport**:\\n\\n```python\\n# Match source and target feature distributions\\nimport ot  # Optimal transport library\\n\\n# Extract features from both domains\\nfeatures_source = model.extract_features (source_data)\\nfeatures_target = model.extract_features (target_data)\\n\\n# Compute optimal transport plan\\nM = cost_matrix (features_source, features_target)  # Pairwise distances\\nT = ot.emd (uniform (n_source), uniform (n_target), M)  # Transport plan\\n\\n# Reweight source samples based on transport plan\\nfor i in range (n_source):\\n    weight_i = T[i, :].sum()  # How much sample i transports to target\\n    weighted_loss += weight_i * loss (source_data[i])\\n\\n# Aligns source and target distributions in feature space\\n```\\n\\n**Practical Guidelines**:\\n\\n**Small Domain Gap (Natural → Natural)**:\\n```\\nExample: ImageNet → Food images\\nSolution: Fine-tuning (5-10 epochs)\\nExpected: 85-90% of supervised performance\\n```\\n\\n**Medium Domain Gap (RGB → Grayscale)**:\\n```\\nExample: ImageNet → X-rays\\nSolution: Fine-tuning + data augmentation + discriminative LR\\nExpected: 70-80% of supervised performance\\n```\\n\\n**Large Domain Gap (Real → Synthetic)**:\\n```\\nExample: Real photos → Rendered 3D\\nSolution: Domain adversarial training + CycleGAN\\nExpected: 50-70% of supervised performance\\n```\\n\\n**Very Large Gap (Different Modalities)**:\\n```\\nExample: Images → Audio spectrograms\\nSolution: Self-supervised pre-training on target + fine-tuning\\nExpected: 30-50% of supervised performance (may need more target data)\\n```\\n\\n**Key Insight**: Domain adaptation aims to transfer knowledge across distributions by learning **domain-invariant features** that capture task-relevant information while ignoring domain-specific artifacts.',
    keyPoints: [],
  },
];
