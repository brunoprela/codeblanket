/**
 * Image Processing with CNNs Quiz
 */
export const imageProcessingWithCnnsQuiz = [
  {
    id: 'cnn-q1',
    question:
      'Explain the three-phase strategy for transfer learning: (1) training only the classifier, (2) fine-tuning top layers, and (3) fine-tuning the entire network. When would you use each phase, and what learning rates are appropriate for each?',
    sampleAnswer:
      "Transfer learning adapts pretrained models to new tasks through progressive fine-tuning with carefully controlled learning rates.\n\n**Phase 1: Train Only Classifier** (Features Frozen)\n\n**When**: Always start here, especially with small datasets (<10K images)\n\n**Setup**:\n- Freeze all convolutional layers (requires_grad=False)\n- Replace final classification layer\n- Train only the new classifier\n\n**Learning Rate**: High (0.001-0.01)\n- Safe because pretrained features won't be damaged\n- Fast convergence (few parameters)\n\n**Duration**: 5-10 epochs\n\n**Rationale**: Pretrained features (edges, textures, shapes) are already good. Just learn how to combine them for your classes.\n\n**Example**:\n```python\n# Freeze all layers\nfor param in model.parameters():\n    param.requires_grad = False\n\n# Train only classifier\noptimizer = Adam(model.fc.parameters(), lr=0.001)\n```\n\n**Phase 2: Fine-Tune Top Layers**\n\n**When**: After Phase 1, if you have medium-sized dataset (10K-100K images)\n\n**Setup**:\n- Unfreeze last 2-3 convolutional blocks\n- Keep early layers frozen (general features)\n- Fine-tune high-level features for your domain\n\n**Learning Rate**: Lower (0.0001-0.001)\n- 10-100× smaller than Phase 1\n- Prevent catastrophic forgetting of pretrained weights\n\n**Duration**: 10-20 epochs\n\n**Rationale**: High-level features (object parts, complex patterns) may need domain adaptation. Early features (edges) are universal.\n\n**Example**:\n```python\n# Unfreeze last 2 blocks\nfor param in model.layer4.parameters():\n    param.requires_grad = True\nfor param in model.layer3.parameters():\n    param.requires_grad = True\n\noptimizer = Adam(\n    filter(lambda p: p.requires_grad, model.parameters()),\n    lr=0.0001\n)\n```\n\n**Phase 3: Fine-Tune Entire Network**\n\n**When**: \n- Large dataset (>100K images)\n- Very different from ImageNet (medical imaging, satellite)\n- After Phases 1 & 2 if still improving\n\n**Setup**:\n- Unfreeze all layers\n- Fine-tune entire network end-to-end\n\n**Learning Rate**: Very low (0.00001-0.0001)\n- 100-1000× smaller than Phase 1\n- Tiny adjustments to pretrained weights\n\n**Duration**: 20-50 epochs\n\n**Rationale**: Even early features might benefit from domain-specific tuning, but must be very careful not to destroy useful pretrained representations.\n\n**Example**:\n```python\n# Unfreeze all\nfor param in model.parameters():\n    param.requires_grad = True\n\noptimizer = Adam(model.parameters(), lr=0.00001)\n```\n\n**Decision Tree**:\n```\nDataset Size?\n├─ <10K → Phase 1 only (risk overfitting)\n├─ 10K-100K → Phases 1+2\n└─ >100K → Phases 1+2+3\n\nSimilarity to ImageNet?\n├─ Very similar (cats, dogs) → Phase 1 may suffice\n├─ Somewhat similar (other objects) → Phases 1+2\n└─ Very different (medical, satellite) → All phases\n```\n\n**Common Mistakes**:\n1. Using same learning rate for all phases → destroys pretrained features\n2. Skipping Phase 1 → wastes time, unstable training\n3. Fine-tuning with small data → overfitting\n4. Too high learning rate in Phase 2/3 → catastrophic forgetting",
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      'Compare and contrast object detection (e.g., YOLO) with semantic segmentation (e.g., U-Net). What are the key architectural differences, and what types of problems is each approach best suited for?',
    sampleAnswer:
      "Object detection and semantic segmentation solve different computer vision problems with distinct architectures and use cases.\n\n**Object Detection (YOLO)**\n\n**Task**: Find and classify objects → output bounding boxes + labels\n\n**Output Format**:\n- List of detections: [(x, y, width, height, class, confidence), ...]\n- Variable number of outputs (0 to hundreds)\n- Coarse localization (rectangular boxes)\n\n**Architecture**:\n- Single forward pass through CNN\n- Grid-based prediction (e.g., 7×7 grid)\n- Each grid cell predicts:\n  - Multiple bounding boxes (x, y, w, h, confidence)\n  - Class probabilities\n- Final layer: Dense predictions for all grid cells\n- No upsampling needed (output smaller than input)\n\n**Example (YOLO)**:\n```\nInput: 416×416×3\n    ↓\nCNN backbone\n    ↓\nOutput: 13×13×125\n(13×13 grid, each cell predicts boxes + classes)\n```\n\n**Speed**: Very fast (real-time 30-60 FPS)\n- Single forward pass\n- Small output tensor\n- Optimized for speed\n\n**Accuracy**: Bounding boxes (approximate)\n- Can't capture precise object boundaries\n- Multiple objects → multiple boxes\n- IoU metric for evaluation\n\n**Semantic Segmentation (U-Net)**\n\n**Task**: Classify every pixel → output segmentation mask\n\n**Output Format**:\n- Same size as input image\n- Each pixel has class label\n- Precise object boundaries\n- Example: 256×256×C (C classes)\n\n**Architecture**:\n- Encoder-decoder structure\n- **Encoder**: Downsample (extract features)\n- **Bottleneck**: Compressed representation\n- **Decoder**: Upsample (recover spatial resolution)\n- **Skip connections**: Preserve spatial details from encoder\n\n**Example (U-Net)**:\n```\nInput: 256×256×3\n    ↓\nEncoder: 256→128→64→32→16 (downsample)\n    ↓\nBottleneck: 16×16×1024\n    ↓\nDecoder: 16→32→64→128→256 (upsample)\n    ↓\nOutput: 256×256×C (per-pixel classification)\n```\n\n**Speed**: Slower than detection\n- Upsampling computationally expensive\n- Must process every pixel\n- Typically 5-20 FPS\n\n**Accuracy**: Pixel-perfect boundaries\n- Captures precise object shapes\n- Can separate touching objects (with instance segmentation variant)\n- Dice coefficient / IoU for evaluation\n\n**Key Architectural Differences**:\n\n| Aspect | Object Detection | Semantic Segmentation |\n|--------|-----------------|----------------------|\n| Output size | Small (e.g., 7×7) | Same as input |\n| Upsampling | None | Extensive |\n| Skip connections | No | Yes (critical!) |\n| Final layer | Dense regression | 1×1 conv per pixel |\n| Loss function | Localization + classification | Per-pixel cross-entropy |\n| Memory | Low | High (store feature maps) |\n\n**Use Cases**:\n\n**Object Detection**:\n- **Autonomous driving**: Detect cars, pedestrians, signs\n- **Surveillance**: Track people, detect suspicious activity\n- **Retail**: Count products, inventory management\n- **Sports analytics**: Track players\n- When: Need to count objects, track movement, coarse localization OK\n\n**Semantic Segmentation**:\n- **Medical imaging**: Tumor boundaries, organ segmentation\n- **Autonomous driving**: Drivable area, lane markings\n- **Satellite imagery**: Land classification (forest, water, urban)\n- **Photo editing**: Background removal, object selection\n- **Agriculture**: Crop health, weed detection\n- When: Need precise boundaries, pixel-level understanding\n\n**Hybrid Approaches**:\n\n**Instance Segmentation** (Mask R-CNN):\n- Combines both: detect objects + segment each instance\n- Separate touching objects of same class\n- Best of both worlds (but slower/more complex)\n\n**Panoptic Segmentation**:\n- Unified framework for both semantic + instance\n- Every pixel labeled, objects separated\n\n**Summary Decision Tree**:\n```\nNeed precise boundaries?\n├─ No → Object Detection (YOLO, RetinaNet)\n└─ Yes\n    |\n    Need to separate instances?\n    ├─ No → Semantic Segmentation (U-Net, DeepLab)\n    └─ Yes → Instance Segmentation (Mask R-CNN)\n```",
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      "Explain how U-Net's skip connections work and why they're critical for semantic segmentation. What problem do they solve that a pure encoder-decoder architecture without skip connections would struggle with?",
    sampleAnswer:
      'Skip connections in U-Net are critical for preserving spatial information lost during downsampling, enabling precise pixel-level segmentation.\n\n**The Problem: Information Loss in Encoder-Decoder**\n\n**Pure Encoder-Decoder** (without skip connections):\n\n```\nInput 256×256 → Encoder → 8×8 → Decoder → Output 256×256\n```\n\n**What Goes Wrong**:\n\n1. **Spatial Information Loss**:\n   - Pooling/striding reduces 256×256 → 8×8 (1024× reduction!)\n   - Original pixel positions destroyed\n   - Fine details (edges, textures) lost\n\n2. **Bottleneck Problem**:\n   - All information must pass through tiny 8×8 bottleneck\n   - Impossible to preserve spatial details from 65K pixels in 64 feature maps\n\n3. **Upsampling Ambiguity**:\n   - Decoder must guess original spatial structure\n   - 8×8 → 256×256 expansion is highly ambiguous\n   - Results in blurry, imprecise boundaries\n\n**Example Problem**:\n```\nInput: Cell image with precise nuclei boundaries\n    ↓\nEncoder: Downample to 8×8 (lost exact positions)\n    ↓  \nBottleneck: 8×8 representation (no fine details)\n    ↓\nDecoder: Upsample to 256×256\n    ↓\nOutput: Blurry boundaries, lost small structures\n```\n\n**U-Net\'s Solution: Skip Connections**\n\n**Architecture**:\n```\nEncoder                  Decoder\n256×256 ──skip────────→ Concat → 256×256\n   ↓                              ↑\n128×128 ──skip────────→ Concat → 128×128  \n   ↓                              ↑\n64×64 ───skip────────→ Concat → 64×64\n   ↓                              ↑\n32×32 ───skip────────→ Concat → 32×32\n   ↓                              ↑\n16×16 ────Bottleneck────────→ 16×16\n```\n\n**How Skip Connections Work**:\n\n1. **Direct Path for Spatial Info**:\n   - Encoder feature maps copied directly to decoder\n   - Bypasses bottleneck entirely\n   - Preserves exact spatial positions\n\n2. **Concatenation** (not addition):\n   ```python\n   # Encoder output: 128×128×256\n   # Upsampled decoder: 128×128×256\n   # Concatenate: 128×128×512\n   \n   decoder_features = upsample(decoder_prev)\n   combined = torch.cat([decoder_features, encoder_features], dim=1)\n   decoder_next = conv_block(combined)\n   ```\n\n3. **Multi-Scale Information**:\n   - Early layers: Fine details (edges, textures)\n   - Deep layers: Semantic information (what object)\n   - Decoder combines both: "semantic edge" → precise boundary\n\n**What Skip Connections Provide**:\n\n**From Shallow Layers** (256×256 level):\n- Pixel-level details\n- Sharp edges\n- Texture information\n- Exact spatial positions\n\n**From Deep Layers** (32×32 level):\n- Semantic understanding\n- Object context\n- "What" information\n\n**Decoder Combines**:\n- Semantic info ("this is a cell")\n- Spatial info ("exact boundary is here")\n- Result: Precise, meaningful segmentation\n\n**Empirical Evidence**:\n\n**Without Skip Connections**:\n- Blurry boundaries\n- Missed small objects\n- Poor localization\n- Dice score: ~0.60-0.70\n\n**With Skip Connections**:\n- Sharp boundaries\n- Captures fine details\n- Accurate localization  \n- Dice score: ~0.85-0.95\n\n**Improvement**: +20-30% performance!\n\n**Why Concatenation vs Addition?**\n\nU-Net uses **concatenation**, ResNet uses **addition**:\n\n**Concatenation** (U-Net):\n```python\ncombined = torch.cat([encoder_feat, decoder_feat], dim=1)\n# Doubles channel count\n```\n- Preserves both features independently\n- Decoder learns how to weight them\n- More flexible but more parameters\n\n**Addition** (ResNet):\n```python\ncombined = encoder_feat + decoder_feat\n# Same channel count\n```\n- Forces same dimensionality\n- Residual learning\n- Fewer parameters\n\nSegmentation needs **both** low-level and high-level features explicitly → concatenation wins.\n\n**Alternative Designs**:\n\n1. **Feature Pyramid Networks (FPN)**:\n   - Top-down pathway + lateral connections\n   - Similar concept, used in detection\n\n2. **DenseNet**:\n   - Dense skip connections (all to all)\n   - More connections but more complex\n\n3. **Attention Gates**:\n   - Weighted skip connections\n   - Focus on relevant features\n   - Attention-UNet variant\n\n**Key Insight**:\n\nSkip connections solve the **localization vs semantics trade-off**:\n- Deep features: Good semantics, poor localization\n- Shallow features: Poor semantics, good localization\n- Skip connections: Best of both worlds!\n\nThis principle extends beyond U-Net - any task requiring fine-grained spatial understanding benefits from skip connections.',
    keyPoints: [],
  },
];
