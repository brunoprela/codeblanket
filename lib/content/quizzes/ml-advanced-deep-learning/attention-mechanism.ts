/**
 * Attention Mechanism Quiz
 */
export const attentionMechanismQuiz = [
  {
    id: 'cnn-q1',
    question:
      'Explain mathematically how attention mechanisms solve the bottleneck problem in Seq2Seq models. Walk through the three core steps (score, align, context) and show why this allows information to flow from any encoder position to any decoder position.',
    sampleAnswer:
      'Attention solves the bottleneck by providing a dynamic, learnable mechanism for the decoder to access ALL encoder hidden states, not just a fixed-size compressed vector.\n\n**The Bottleneck Problem (Review)**:\n\nWithout attention:\n- All input information compressed into single vector c = h_n\n- Information from early tokens lost or degraded\n- Fixed capacity regardless of input length\n\n**Attention Solution**:\n\nInstead of single context vector, compute dynamic context c_t at EACH decoder timestep t.\n\n**Step 1: Score - Measure Relevance**\n\nFor each encoder position i, compute how relevant it is to current decoder state s_{t-1}.\n\n**Bahdanau (Additive) Attention**:\n```\ne_{t,i} = v^T tanh(W_1 h_i + W_2 s_{t-1})\n```\n\nWhere:\n- h_i: Encoder hidden state at position i\n- s_{t-1}: Decoder hidden state at previous timestep\n- W_1, W_2, v: Learnable parameters\n\nThis scores how well h_i and s_{t-1} match.\n\n**Step 2: Align - Convert Scores to Weights**\n\nNormalize scores using softmax:\n```\nα_{t,i} = exp(e_{t,i}) / Σ_j exp(e_{t,j})\n```\n\nProperties:\n- α_{t,i} ∈ [0, 1]\n- Σ_i α_{t,i} = 1 (valid probability distribution)\n- High score → high weight\n- Differentiable (can train end-to-end)\n\n**Step 3: Context - Weighted Sum**\n\nCompute context as weighted combination of ALL encoder states:\n```\nc_t = Σ_i α_{t,i} × h_i\n```\n\nThis is the key breakthrough:\n- NOT compressing through bottleneck\n- Directly accessing relevant encoder states\n- Weights learned dynamically\n\n**Complete Process at Decoder Step t**:\n\n```\n1. Input: s_{t-1} (decoder state), {h_1,...,h_n} (all encoder states)\n\n2. Score each position:\n   e_{t,1} = score(h_1, s_{t-1})\n   e_{t,2} = score(h_2, s_{t-1})\n   ...\n   e_{t,n} = score(h_n, s_{t-1})\n\n3. Normalize to weights:\n   α_t = softmax([e_{t,1}, ..., e_{t,n}])\n\n4. Compute context:\n   c_t = α_{t,1}×h_1 + α_{t,2}×h_2 + ... + α_{t,n}×h_n\n\n5. Use context in decoder:\n   s_t = LSTM(s_{t-1}, [y_{t-1}; c_t])\n   output_t = softmax(W s_t)\n```\n\n**Why This Solves the Bottleneck**:\n\n1. **No Compression Loss**:\n   - All encoder states h_1,...,h_n preserved\n   - Context c_t can access any of them\n   - Information not forced through fixed-size vector\n\n2. **Dynamic Selection**:\n   - Different c_t at each decoder step\n   - c_1 might focus on h_1, h_2 (early input)\n   - c_n might focus on h_{n-1}, h_n (late input)\n   - Learned what\'s relevant when\n\n3. **Scalable Capacity**:\n   - Long input (n=100): Access all 100 states\n   - Short input (n=5): Access all 5 states\n   - Capacity scales with input length!\n\n4. **Direct Information Flow**:\n   ```\n   Without attention:\n   h_1 → h_2 → ... → h_n = c → decoder\n   (long path, information degrades)\n   \n   With attention:\n   h_1 ──────────┐\n   h_2 ──────────┤\n   ...           ├→ α_t → c_t → decoder\n   h_n ──────────┘\n   (direct path from any h_i to decoder!)\n   ```\n\n**Mathematical Comparison**:\n\n**Without Attention**:\n- Context: c (fixed, 512-dim typically)\n- Information path: x_i → h_i → ... → h_n = c → all decoder steps\n- Bottleneck: All information through single 512-dim vector\n\n**With Attention**:\n- Context: c_t (dynamic, different each step)\n- Information path: x_i → h_i → directly to any decoder step via attention\n- No bottleneck: Each c_t can access all h_i\n\n**Gradient Flow**:\n\nWithout attention:\n```\n∂L/∂h_i requires backprop through: h_i → h_{i+1} → ... → h_n → all decoder steps\n```\n\nWith attention:\n```\n∂L/∂h_i = Σ_t (∂L/∂c_t) × (∂c_t/∂h_i) = Σ_t (∂L/∂c_t) × α_{t,i}\n```\n\nDirect gradient path from any decoder timestep to any encoder position!\n\n**Concrete Example - Translation**:\n\nTranslating "The cat sat on the mat" to French:\n\nWithout attention:\n- Must remember "cat" from beginning when generating "chat" later\n- Information about "cat" in compressed context c\n- Likely degraded by intervening words\n\nWith attention:\n- When generating "chat", compute attention over input\n- α weights: [0.1, 0.8, 0.05, 0.05, 0.0, 0.0] (focuses on position 2 = "cat")\n- c_t = 0.8 × h_2 ("cat") + small contributions from others\n- Direct access to "cat" encoding!\n\nThis is why attention works: No lossy compression, dynamic selection, direct information access.',
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      'Compare Bahdanau (additive) attention and Luong (multiplicative) attention. What are the architectural differences, parameter counts, and computational trade-offs? When would you choose one over the other?',
    sampleAnswer:
      'Bahdanau and Luong attention are two main variants with different score functions, introduced in landmark 2015 papers.\n\n**Bahdanau Attention (Additive) - 2015**\n\nPaper: "Neural Machine Translation by Jointly Learning to Align and Translate"\n\n**Score Function**:\n```\ne_{t,i} = v^T tanh(W_a h_i + U_a s_{t-1})\n```\n\nWhere:\n- h_i: Encoder hidden state (size d_h)\n- s_{t-1}: Decoder hidden state (size d_s)\n- W_a: (d_a × d_h) matrix\n- U_a: (d_a × d_s) matrix  \n- v: (d_a × 1) vector\n- d_a: Attention hidden size (typically 256-512)\n\n**Architecture**:\n1. Project h_i to d_a dimensions: W_a h_i\n2. Project s_{t-1} to d_a dimensions: U_a s_{t-1}\n3. Add projections and apply tanh\n4. Project to scalar with v\n\n**Parameters**:\n```\nW_a: d_a × d_h\nU_a: d_a × d_s\nv: d_a\nTotal: d_a × (d_h + d_s + 1)\n\nExample (d_h=512, d_s=512, d_a=256):\nTotal: 256 × (512 + 512 + 1) = 262,400 parameters\n```\n\n**Luong Attention (Multiplicative) - 2015**\n\nPaper: "Effective Approaches to Attention-based Neural Machine Translation"\n\nThree variants:\n\n**1. Dot Product**:\n```\ne_{t,i} = h_i^T s_t\n```\n- Simplest\n- Zero parameters!\n- Requires d_h = d_s\n\n**2. General (Most Common)**:\n```\ne_{t,i} = h_i^T W_a s_t\n```\n- W_a: (d_h × d_s) matrix\n- Can handle different dimensions\n\n**Parameters** (General):\n```\nW_a: d_h × d_s\n\nExample (d_h=512, d_s=512):\nTotal: 512 × 512 = 262,144 parameters\n```\n\n**3. Concat** (Similar to Bahdanau):\n```\ne_{t,i} = v^T tanh(W_a [h_i; s_t])\n```\n\n**Key Architectural Differences**:\n\n| Aspect | Bahdanau | Luong (General) |\n|--------|----------|----------------|\n| **Score Function** | Additive (tanh) | Multiplicative (dot) |\n| **Decoder State** | Previous (s_{t-1}) | Current (s_t) |\n| **Non-linearity** | tanh | None (linear) |\n| **Hidden Dimension** | Extra d_a | None |\n| **Complexity** | More complex | Simpler |\n\n**Timing Difference**:\n\n**Bahdanau**:\n```\n1. Compute attention with s_{t-1}\n2. Get context c_t\n3. Compute s_t = LSTM(s_{t-1}, [y_{t-1}; c_t])\n4. Compute output from s_t\n```\n\n**Luong**:\n```\n1. Compute s_t = LSTM(s_{t-1}, y_{t-1})\n2. Compute attention with s_t\n3. Get context c_t\n4. Compute output from [s_t; c_t]\n```\n\nBahdanau: Attention before LSTM update\nLuong: Attention after LSTM update\n\n**Computational Complexity**:\n\nFor batch size b, sequence lengths n (source) and m (target), hidden size d:\n\n**Bahdanau**:\n- Score computation: O(b × m × n × d_a)\n- More operations due to tanh and two projections\n\n**Luong (General)**:\n- Score computation: O(b × m × n × d)\n- Single matrix multiplication\n- ~30% faster typically\n\n**Luong (Dot)**:\n- Score computation: O(b × m × n × d)\n- No parameters, just dot product\n- ~40% faster than Bahdanau\n\n**Memory Usage**:\n\n**Bahdanau**:\n- Stores: W_a, U_a, v\n- Intermediate: tanh activations\n- Slightly higher memory\n\n**Luong**:\n- Stores: W_a only (General) or nothing (Dot)\n- Lower memory footprint\n\n**Performance Comparison** (Empirical):\n\n**Translation Quality** (BLEU scores, typical):\n```\nModel              | EN-DE | EN-FR | Parameters\n-------------------|-------|-------|------------\nNo Attention       | 20.5  | 28.3  | Base\nBahdanau          | 23.8  | 32.1  | +262K\nLuong (Dot)       | 23.2  | 31.5  | +0\nLuong (General)   | 23.9  | 32.3  | +262K\nLuong (Concat)    | 23.7  | 32.0  | +262K\n```\n\nVery similar performance!\n\n**When to Choose Bahdanau**:\n\n1. **Original/Historical**: First attention mechanism\n2. **Non-linear Scoring**: If you believe tanh helps match encoder/decoder states\n3. **Research/Replication**: Reproducing original results\n4. **Slightly Better** (sometimes): Marginal quality edge in some tasks\n\n**When to Choose Luong**:\n\n1. **Faster Training** (~30% faster):\n   - Production systems\n   - Limited compute budget\n   - Large-scale training\n\n2. **Simpler Implementation**:\n   - Easier to debug\n   - Cleaner code\n   - Fewer hyperparameters\n\n3. **Memory Constrained**:\n   - Dot product: Zero parameters\n   - Lower memory footprint\n\n4. **Modern Default**:\n   - More commonly used in recent work\n   - Better optimized in frameworks\n\n**Specific Recommendations**:\n\n**Use Dot Product Luong** when:\n- Encoder/decoder same dimension\n- Speed critical\n- Memory very limited\n- Simple baseline needed\n\n**Use General Luong** when:\n- Different encoder/decoder dimensions\n- Good speed/quality balance wanted\n- Modern production system\n- **Most common choice**\n\n**Use Bahdanau** when:\n- Reproducing specific results\n- Research comparing mechanisms\n- Suspect non-linearity helps your task\n- Not concerned about speed\n\n**Modern Context**:\n\nBoth largely superseded by **scaled dot-product attention** in Transformers:\n```\nAttention(Q, K, V) = softmax(QK^T / √d_k) V\n```\n\nWhich is like Luong dot product but:\n- Applied to all positions simultaneously (self-attention)\n- Scaled by √d_k for stability\n- Used in BERT, GPT, modern LLMs\n\n**Practical Advice**:\n\n1. **Start with Luong General**: Best default\n2. **Try Dot Product**: If dimensions match and want speed\n3. **Bahdanau** only if specific reason\n4. **Experiment**: Difference often small (<1% BLEU)\n5. **Move to Transformers**: If possible for your task\n\nIn practice, choice between Bahdanau/Luong matters less than:\n- Overall architecture quality\n- Training data size\n- Hyperparameter tuning\n- Regularization strategies\n\nBoth are powerful attention mechanisms that revolutionized sequence modeling!',
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'Explain how attention weights provide interpretability in neural models. How would you visualize and analyze attention patterns to debug or understand model behavior? Provide examples of what attention patterns might reveal about translation or other sequence tasks.',
    sampleAnswer:
      'Attention weights provide a window into what the model is \'focusing on\', making neural networks more interpretable.\n\n**Why Attention is Interpretable**:\n\nAttention weights α_{t,i} represent the importance of source position i when generating target position t.\n\n**Properties**:\n1. **Probabilistic**: α_{t,i} ∈ [0, 1]\n2. **Normalized**: Σ_i α_{t,i} = 1\n3. **Sparse**: Often focuses on few positions\n4. **Learnable**: Not hardcoded, model learns relevance\n\n**Visualization Methods**:\n\n**1. Heatmap (Most Common)**:\n```\n              Source (Input)\n            I  love  cats  .\nTarget  J\'  ■  □     □    □     ← focuses on "I"\n(Out)   aime □  ■     □    □     ← focuses on "love"\n        les  □  □     ■    □     ← focuses on "cats"\n        chats□  □     ■    □\n        .    □  □     □    ■     ← focuses on "."\n\n■ = high attention, □ = low attention\n```\n\nInterpretation: Model learned word-by-word alignment!\n\n**2. Line Plot**:\n```python\nplt.plot(attention_weights[t, :])\nplt.xlabel(\'Source Position\')\nplt.ylabel(\'Attention Weight\')\nplt.title(f\'Attention when generating: {target_word}\')\n```\n\nShows distribution over source at specific target position.\n\n**3. Attention Flow Diagram**:\n```\nSource: The    cat    sat    on    the    mat\n         ↓      ↓      ↓      ↓     ↓      ↓\n         └──────┬──────┘      │     │      │\n                ↓              ↓     ↓      ↓\nTarget:        Le             sur   le    tapis\n```\n\nArrows show attention connections.\n\n**Attention Patterns and Interpretations**:\n\n**Pattern 1: Diagonal (Monotonic Alignment)**\n\n```\nAttention:\n  s1  s2  s3  s4  s5\nt1 ■  □   □   □   □\nt2 □   ■  □   □   □  \nt3 □   □   ■  □   □\nt4 □   □   □   ■  □\nt5 □   □   □   □   ■\n```\n\n**Interpretation**: Word-by-word translation\n- Similar word order in source/target\n- Example: English → French, Spanish → Italian\n- **Insight**: Languages are structurally similar\n\n**Pattern 2: Horizontal Bands (One-to-Many)**\n\n```\nAttention:\n  s1  s2  s3  s4\nt1 ■  □   □   □\nt2 ■  □   □   □    \nt3 ■  □   □   □\nt4 □   ■  □   □\n```\n\n**Interpretation**: One source word generates multiple target words\n- Example: "not" → "ne ... pas" (French negation split)\n- "yesterday" → "gestern" + "Abend" (German compound)\n- **Insight**: Target language more verbose or splits concepts\n\n**Pattern 3: Vertical Bands (Many-to-One)**\n\n```\nAttention:\n  s1  s2  s3  s4\nt1 ■  ■   □   □\nt2 □   □   ■  ■\nt3 □   □   □   ■\n```\n\n**Interpretation**: Multiple source words compressed to one target\n- Example: "will go" → "ira" (French future tense)\n- "have been" → single character (Chinese aspect marker)\n- **Insight**: Target language more concise or uses different grammar\n\n**Pattern 4: Reverse Diagonal (Reordering)**\n\n```\nAttention:\n  s1  s2  s3  s4\nt1 □   □   □   ■\nt2 □   □   ■  □    \nt3 □   ■  □   □\nt4 ■  □   □   □\n```\n\n**Interpretation**: Complete word order reversal\n- Example: English "beautiful house" → German "schönes Haus" (adjective after noun in some constructions)\n- Japanese (SOV) → English (SVO)\n- **Insight**: Major structural difference between languages\n\n**Pattern 5: Diffuse (Uncertainty)**\n\n```\nAttention:\n  s1  s2  s3  s4\nt1 ▓  ▓   ▓   ▓\nt2 ▓  ▓   ▓   ▓    \nt3 ▓  ▓   ▓   ▓\n\n▓ = moderate attention everywhere\n```\n\n**Interpretation**: Model is uncertain\n- Ambiguous word or phrase\n- Out-of-vocabulary handling\n- Model hasn\'t learned clear alignment\n- **Problem indicator**: May need more training or data\n\n**Debugging with Attention**:\n\n**Issue 1: Wrong Translation**\n\nInput: "I don\'t like apples"\nOutput: "J\'aime les pommes" (I like apples - wrong!)\n\nCheck attention for "don\'t":\n```\nAttention when generating "aime":\n  I   don\'t  like  apples\n  □    □     ■     □\n```\n\n**Diagnosis**: Not attending to "don\'t"!\n**Solution**: Need more negation examples in training\n\n**Issue 2: Repeated Words**\n\nOutput: "Le le chat chat"\n\nCheck attention:\n```\n         The  cat\nLe       ■    □\nle       ■    □    ← Same attention as previous!\ncat      □    ■\nchat     □    ■    ← Repeating!\n```\n\n**Diagnosis**: Attention repeating same positions\n**Solution**: Add coverage mechanism (penalize repeated attention)\n\n**Issue 3: Dropped Words**\n\nInput: "The quick brown fox"\nOutput: "Le renard" (missing "quick brown")\n\nCheck attention coverage:\n```\nTotal attention to "quick": 0.05 (almost none!)\nTotal attention to "brown": 0.08 (almost none!)\n```\n\n**Diagnosis**: Some source words never attended\n**Solution**: Coverage loss, attention supervision\n\n**Advanced Analysis**:\n\n**1. Attention Entropy**\n\nMeasure how focused attention is:\n```\nH(α_t) = -Σ_i α_{t,i} log(α_{t,i})\n\nLow entropy: Focused (good)\nHigh entropy: Diffuse (uncertain)\n```\n\n**2. Attention Alignment Error**\n\nCompare to human word alignments:\n```\nAER = 1 - (|A ∩ A_gold| / |A ∪ A_gold|)\n\nwhere A = predicted alignments (high attention)\n      A_gold = gold alignments\n```\n\n**3. Coverage Statistics**\n\nEnsure all source words attended:\n```\nCoverage: c_i = Σ_t α_{t,i}\n\nIdeal: c_i ≈ 1 for all i (each source word attended exactly once)\n```\n\n**Real-World Insights from Attention**:\n\n**Translation**:\n- Reveals syntactic differences between languages\n- Shows which constructions model finds difficult\n- Identifies systematic errors (negation, tense, etc.)\n\n**Summarization**:\n- Highlights sentences model considers important\n- Shows which parts of document inform each summary sentence\n- Can identify if model just copies or truly understands\n\n**Question Answering**:\n- Shows which passage sentences used for answer\n- Reveals reasoning chain\n- Can catch when model uses irrelevant information\n\n**Limitations of Attention Interpretation**:\n\n1. **Not Explanation**: Correlation ≠ causation\n   - High attention doesn\'t prove that information is used\n   - Low attention doesn\'t mean information ignored (could be in hidden state)\n\n2. **Multiple Attention Heads**: In Transformers, many attention heads\n   - Which one to visualize?\n   - Different heads may serve different purposes\n\n3. **Attention as Only One Part**: \n   - Model also uses hidden states, feedforward layers\n   - Attention is partial view of model\'s reasoning\n\n**Best Practices**:\n\n1. **Visualize systematically**: Random samples + errors + edge cases\n2. **Compare patterns**: Good vs bad outputs\n3. **Aggregate statistics**: Not just individual examples\n4. **Domain knowledge**: Understand expected alignments\n5. **Iterate**: Use insights to improve training\n\nAttention visualization is one of the few interpretability tools for deep learning. While not perfect, it provides valuable insights into model behavior and helps debug systematic errors!',
    keyPoints: [],
  },
];
