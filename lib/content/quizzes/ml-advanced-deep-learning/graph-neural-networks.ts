/**
 * Graph Neural Networks (GNNs) Quiz
 */

export const graphNeuralNetworksQuiz = [
  {
    id: 'gnn-q1',
    question:
      'Explain the message passing framework in Graph Neural Networks. How do GNNs aggregate information from neighbors, and why is this different from CNNs on grids?',
    sampleAnswer:
      "Graph Neural Networks extend deep learning to **irregular graph-structured data** through **message passing**, where nodes iteratively aggregate information from their neighbors to learn representations.\n\n**The Graph Structure Challenge**:\n\n**Regular Grid (CNN can handle)**:\n```\nImage: 28×28 grid\n[pixel] - [pixel] - [pixel]\n   |         |         |\n[pixel] - [pixel] - [pixel]\n   |         |         |\n[pixel] - [pixel] - [pixel]\n\nProperties:\n- Fixed structure (everyone has 4/8 neighbors)\n- Regular connectivity\n- Spatial locality\n→ Convolution works: shared kernel slides across grid\n```\n\n**Irregular Graph (CNN fails)**:\n```\nSocial Network:\n[Alice] - [Bob]\n    |       /  \\\n[Charlie] - [David] - [Eve]\n            |     \\\n          [Frank]  [Grace]\n\nProperties:\n- Variable neighbors (Alice: 2, David: 5)\n- Irregular connectivity\n- No spatial arrangement\n→ Convolution doesn't work: no fixed grid structure!\n```\n\n**Message Passing Framework**:\n\n**Core Idea**: Each node aggregates information from neighbors\n\n**Three-Step Process**:\n\n**1. Message Generation**:\n```python\n# For each edge (u → v), create message\nm_{u→v} = MESSAGE(h_u, h_v, e_{u,v})\n\nwhere:\n- h_u: Node u's features\n- h_v: Node v's features (recipient)\n- e_{u,v}: Edge features (optional)\n- MESSAGE: Neural network\n```\n\n**2. Message Aggregation**:\n```python\n# Aggregate all messages coming to node v\nm_v = AGGREGATE({m_{u→v} : u ∈ N(v)})\n\nwhere:\n- N(v): Neighbors of v\n- AGGREGATE: Sum, mean, max, or attention\n```\n\n**3. Node Update**:\n```python\n# Update node v's representation\nh_v^{(k+1)} = UPDATE(h_v^{(k)}, m_v)\n\nwhere:\n- h_v^{(k)}: Node v at layer k\n- UPDATE: Neural network (often MLP + activation)\n```\n\n**Complete GNN Layer**:\n\n```python\nclass GNNLayer (nn.Module):\n    def __init__(self, in_dim, out_dim):\n        # Message generation\n        self.message_mlp = nn.Sequential(\n            nn.Linear(2 * in_dim, out_dim),  # Concat sender & receiver\n            nn.ReLU()\n        )\n        \n        # Node update\n        self.update_mlp = nn.Sequential(\n            nn.Linear (in_dim + out_dim, out_dim),  # Concat old + message\n            nn.ReLU()\n        )\n    \n    def forward (self, h, edge_index):\n        # h: (num_nodes, in_dim) - node features\n        # edge_index: (2, num_edges) - graph structure\n        \n        row, col = edge_index  # Source and target nodes\n        \n        # Step 1: Generate messages\n        # Concatenate features of connected nodes\n        messages = self.message_mlp(\n            torch.cat([h[row], h[col]], dim=1)\n        )  # (num_edges, out_dim)\n        \n        # Step 2: Aggregate messages (sum)\n        aggregated = torch.zeros (h.size(0), messages.size(1))\n        aggregated.index_add_(0, col, messages)  # Sum messages to each node\n        \n        # Step 3: Update node features\n        h_new = self.update_mlp(\n            torch.cat([h, aggregated], dim=1)\n        )  # (num_nodes, out_dim)\n        \n        return h_new\n```\n\n**Concrete Example - Social Network**:\n\n**Initial State**:\n```\nGraph:\nAlice(A) - Bob(B) - Charlie(C)\n   |         |         |\n David(D) - Eve(E) - Frank(F)\n\nNode Features (Layer 0):\nh_A^(0) = [1, 0, 1]  # Alice: [age_young, gender_F, active]\nh_B^(0) = [0, 1, 1]  # Bob: [age_old, gender_M, active]\nh_C^(0) = [1, 1, 0]  # Charlie: [age_young, gender_M, inactive]\n...\n```\n\n**Layer 1 - Message Passing**:\n\n**Alice\'s neighbors**: {Bob, David}\n\n```python\n# 1. Generate messages from neighbors to Alice\nm_{B→A} = MLP([h_B; h_A]) = MLP([[0,1,1]; [1,0,1]]) = [0.3, 0.7]\nm_{D→A} = MLP([h_D; h_A]) = MLP([[1,1,1]; [1,0,1]]) = [0.5, 0.4]\n\n# 2. Aggregate messages (sum)\nm_A = m_{B→A} + m_{D→A} = [0.3+0.5, 0.7+0.4] = [0.8, 1.1]\n\n# 3. Update Alice's features\nh_A^(1) = UPDATE([h_A^(0); m_A]) \n        = MLP([[1,0,1]; [0.8,1.1]]) \n        = [0.9, 0.3, 0.8]\n\n# Alice now has information from Bob and David!\n```\n\n**Bob's neighbors**: {Alice, Charlie, Eve}\n\n```python\n# Bob receives messages from 3 neighbors\nm_B = m_{A→B} + m_{C→B} + m_{E→B}\nh_B^(1) = UPDATE([h_B^(0); m_B])\n\n# Bob aggregates information from all his connections\n```\n\n**After k layers**: Each node has information from k-hop neighbors\n```\nLayer 0: Node knows only itself\nLayer 1: Node knows 1-hop neighbors (direct connections)\nLayer 2: Node knows 2-hop neighbors (friends of friends)\nLayer 3: Node knows 3-hop neighbors\n...\n\nk layers → k-hop receptive field\n```\n\n**GNN vs CNN**:\n\n**CNN (Grid)**:\n```python\n# Convolution on image\noutput[i,j] = Σ_m Σ_n kernel[m,n] * input[i+m, j+n]\n\nKey properties:\n1. Fixed neighborhood (3×3 or 5×5)\n2. Translation invariance (same kernel everywhere)\n3. Regular grid structure\n4. Efficient (matrix operations)\n\nExample:\n    [1] [2] [3]\n    [4] [5] [6]  ← Apply 3×3 kernel\n    [7] [8] [9]\n\nPixel [5] always has 8 neighbors at fixed positions\n```\n\n**GNN (Graph)**:\n```python\n# Message passing on graph\nh_v^{k+1} = UPDATE(h_v^k, AGGREGATE({h_u^k : u ∈ N(v)}))\n\nKey properties:\n1. Variable neighborhood (0 to hundreds of neighbors)\n2. Permutation invariance (order doesn't matter)\n3. Irregular structure (no grid)\n4. Flexible (handles any graph)\n\nExample:\n    [A] - [B] - [C]\n      \\    |    /\n       [D] - [E]\n\nNode D has 2 neighbors: {A, E}\nNode E has 3 neighbors: {B, C, D}\nNo fixed positions!\n```\n\n**Common Aggregation Functions**:\n\n**1. Sum (most common)**:\n```python\nagg_v = Σ_{u ∈ N(v)} m_{u→v}\n\n# Properties:\n- Permutation invariant (order doesn't matter)\n- Sensitive to degree (high-degree nodes get large sums)\n- Differentiable\n```\n\n**2. Mean (normalized)**:\n```python\nagg_v = (1/|N(v)|) Σ_{u ∈ N(v)} m_{u→v}\n\n# Properties:\n- Normalized by degree\n- Less sensitive to node degree\n- Better for graphs with varying degrees\n```\n\n**3. Max (pooling)**:\n```python\nagg_v = max_{u ∈ N(v)} m_{u→v}\n\n# Properties:\n- Selects most salient feature\n- Robust to outliers\n- Used in some GNN variants\n```\n\n**4. Attention (Graph Attention Networks)**:\n```python\n# Compute attention weights\nα_{u,v} = softmax(ATTENTION(h_u, h_v))\n\n# Weighted aggregation\nagg_v = Σ_{u ∈ N(v)} α_{u,v} * m_{u→v}\n\n# Properties:\n- Learns which neighbors are important\n- Different weights for different neighbors\n- More expressive but slower\n```\n\n**Why Message Passing Works**:\n\n**Inductive Bias**: Graphs exhibit **homophily**\n```\nHomophily: Connected nodes tend to be similar\n\nExamples:\n- Social networks: Friends have similar interests\n- Citation networks: Cited papers are related\n- Molecules: Bonded atoms have similar properties\n\n→ Aggregating neighbor information is useful!\n```\n\n**Representation Learning**:\n```python\n# After k GNN layers:\nh_v^(k) = f (h_v^(0), {h_u^(0) : u ∈ N_k (v)}, graph_structure)\n\nwhere N_k (v) = k-hop neighborhood\n\n# Node representation encodes:\n1. Node\'s own features\n2. Neighbors' features\n3. Graph structure (connectivity)\n\n→ Rich representation for downstream tasks!\n```\n\n**Graph-Level Tasks**:\n\n```python\n# After node-level GNN:\nh_nodes = GNN(graph)  # (num_nodes, hidden_dim)\n\n# Global pooling for graph classification\nh_graph = READOUT({h_v : v ∈ V})\n\n# Common readout functions:\n# 1. Sum: h_graph = Σ_v h_v\n# 2. Mean: h_graph = (1/|V|) Σ_v h_v\n# 3. Max: h_graph = max_v h_v\n# 4. Attention: h_graph = Σ_v α_v h_v\n\n# Classify graph\ny_graph = MLP(h_graph)\n```\n\n**Practical Implementation (PyTorch Geometric)**:\n\n```python\nimport torch_geometric as pyg\nfrom torch_geometric.nn import GCNConv, global_mean_pool\n\nclass GNN(nn.Module):\n    def __init__(self, num_features, hidden_dim, num_classes):\n        super().__init__()\n        # GNN layers\n        self.conv1 = GCNConv (num_features, hidden_dim)\n        self.conv2 = GCNConv (hidden_dim, hidden_dim)\n        self.conv3 = GCNConv (hidden_dim, hidden_dim)\n        \n        # Classifier\n        self.classifier = nn.Linear (hidden_dim, num_classes)\n    \n    def forward (self, data):\n        x, edge_index, batch = data.x, data.edge_index, data.batch\n        \n        # Message passing (3 layers)\n        x = self.conv1(x, edge_index).relu()\n        x = self.conv2(x, edge_index).relu()\n        x = self.conv3(x, edge_index).relu()\n        \n        # Global pooling (graph-level)\n        x = global_mean_pool (x, batch)\n        \n        # Classification\n        return self.classifier (x)\n\n# Usage:\nmodel = GNN(num_features=16, hidden_dim=64, num_classes=10)\nfor data in graph_loader:\n    output = model (data)\n    loss = F.cross_entropy (output, data.y)\n    loss.backward()\n```\n\n**Applications**:\n\n**1. Node Classification**:\n```\nSocial network: Predict user interests\nCitation network: Classify paper topics\nBiological: Predict protein functions\n```\n\n**2. Link Prediction**:\n```\nRecommendation: Will user A like item B?\nKnowledge graphs: Missing relations\nDrug discovery: Protein-protein interactions\n```\n\n**3. Graph Classification**:\n```\nMolecule property prediction (toxicity, solubility)\nProtein classification (function, structure)\nProgram analysis (bug detection)\n```\n\n**Key Insight**: GNNs generalize convolution to **irregular graphs** through **message passing** - nodes iteratively aggregate information from neighbors, building representations that capture both node features and graph structure. Unlike CNNs which rely on fixed grid structure, GNNs are **permutation invariant** and handle variable-size neighborhoods, making them suitable for social networks, molecules, knowledge graphs, and any graph-structured data.",
    keyPoints: [],
  },
  {
    id: 'gnn-q2',
    question:
      'Explain the over-smoothing problem in deep GNNs. Why do node representations become indistinguishable after many layers, and what techniques can mitigate this?',
    sampleAnswer:
      "Over-smoothing is a fundamental limitation of deep GNNs where **node representations converge to similar values** after many layers, losing their discriminative power.\n\n**The Over-Smoothing Problem**:\n\n**What Happens**:\n```python\n# Layer 0: Nodes have distinct features\nh^(0) = [[1,0,0], [0,1,0], [0,0,1], [1,1,0], ...]\n# Diversity: High\n\n# Layer 2: Features start blending\nh^(2) = [[0.4,0.3,0.2], [0.3,0.4,0.3], [0.2,0.3,0.5], ...]\n# Diversity: Medium\n\n# Layer 10: All features converge!\nh^(10) = [[0.33,0.33,0.34], [0.33,0.34,0.33], [0.34,0.33,0.33], ...]\n# Diversity: Very low - almost identical!\n\n→ Cannot distinguish nodes anymore!\n```\n\n**Why It Happens**:\n\n**Mathematical Explanation**:\n\n**Message Passing Recursion**:\n```\nh_v^{(k+1)} = σ(W^{(k)} · Σ_{u ∈ N(v) ∪ {v}} (1/√(d_v · d_u)) · h_u^{(k)})\n\nwhere:\n- d_v: Degree of node v\n- σ: Activation function\n- W^{(k)}: Weight matrix at layer k\n```\n\n**Repeated Averaging**:\n```\nEach layer: Node averages its neighbors' features\n\nLayer 1: v sees 1-hop neighbors\nLayer 2: v sees 2-hop neighbors (neighbors of neighbors)\nLayer k: v sees k-hop neighbors\n\nAs k increases:\n- Receptive field grows exponentially\n- Information flows from farther away\n- Local distinctions disappear\n```\n\n**Graph Laplacian Perspective**:\n```python\n# GNN can be viewed as applying graph Laplacian\nH^{(k+1)} = σ(D^{-1/2} A D^{-1/2} H^{(k)} W^{(k)})\n\nwhere:\n- A: Adjacency matrix\n- D: Degree matrix\n- D^{-1/2} A D^{-1/2}: Normalized adjacency (diffusion operator)\n\n# Repeated application:\nH^{(k)} ≈ (D^{-1/2} A D^{-1/2})^k H^{(0)} ...\n\n# As k → ∞:\nLim_{k→∞} (D^{-1/2} A D^{-1/2})^k = 1 · π^T\n\nwhere π = stationary distribution\n\n→ All nodes converge to same vector!\n```\n\n**Concrete Example - Triangle Graph**:\n\n```\nGraph:\n    A\n   / \\\n  B - C\n\nInitial features:\nh_A^(0) = [1, 0, 0]\nh_B^(0) = [0, 1, 0]\nh_C^(0) = [0, 0, 1]\n\n# Layer 1 (mean aggregation):\nh_A^(1) = mean([h_A^(0), h_B^(0), h_C^(0)]) = [1/3, 1/3, 1/3]\nh_B^(1) = mean([h_B^(0), h_A^(0), h_C^(0)]) = [1/3, 1/3, 1/3]\nh_C^(1) = mean([h_C^(0), h_A^(0), h_B^(0)]) = [1/3, 1/3, 1/3]\n\n# All nodes identical after just 1 layer!\n# (For complete graph or small diameter)\n```\n\n**For Larger Graphs**:\n```\nSocial network (1000 nodes)\n\nLayer 1: Nodes see direct friends (different for each node)\nLayer 2: Nodes see friends-of-friends (overlap increases)\nLayer 5: Receptive fields overlap significantly\nLayer 10: Most nodes see almost the entire graph\nLayer 20: All nodes see the entire graph → identical features\n\nSmall-world property: Most graphs have diameter ≈ 6-10\n→ Deep GNNs (>10 layers) cause over-smoothing\n```\n\n**Measuring Over-Smoothing**:\n\n```python\ndef measure_smoothness (h):\n    # Compute pairwise distances between node features\n    distances = torch.cdist (h, h)  # (n, n)\n    avg_distance = distances.mean()\n    return avg_distance\n\n# Track over training:\nfor layer in range(20):\n    h = gnn.forward_k_layers (x, edge_index, k=layer)\n    smoothness = measure_smoothness (h)\n    print(f\"Layer {layer}: {smoothness:.4f}\")\n\n# Output:\n# Layer 0: 1.2453 (high diversity)\n# Layer 2: 0.8921\n# Layer 5: 0.4123\n# Layer 10: 0.0891 (low diversity)\n# Layer 20: 0.0012 (almost identical)\n```\n\n**Mitigation Techniques**:\n\n**1. Residual Connections (Skip Connections)**:\n\n```python\nclass GNNLayerWithResidual (nn.Module):\n    def __init__(self, hidden_dim):\n        self.gnn = GCNConv (hidden_dim, hidden_dim)\n    \n    def forward (self, h, edge_index):\n        # Message passing\n        h_new = self.gnn (h, edge_index).relu()\n        \n        # Residual connection: h_out = h_new + h\n        h_out = h_new + h  # Skip connection\n        \n        return h_out\n\n# Why it helps:\n# - Preserves initial features\n# - Gradient flow easier (helps training)\n# - Prevents complete convergence\n\n# With residual:\nh^(k) = σ(message (h^(k-1))) + h^(k-1)\n       = new_info + old_info\n\n# Retains original node identity!\n```\n\n**2. Jumping Knowledge Networks**:\n\n```python\nclass JKNet (nn.Module):\n    def __init__(self, num_layers):\n        self.layers = nn.ModuleList([\n            GCNConv (hidden_dim, hidden_dim) \n            for _ in range (num_layers)\n        ])\n        \n        # Combine all layers\n        self.jump = JumpingKnowledge (mode='cat')  # or 'max', 'lstm'\n    \n    def forward (self, x, edge_index):\n        layer_outputs = [x]  # Include input\n        \n        h = x\n        for layer in self.layers:\n            h = layer (h, edge_index).relu()\n            layer_outputs.append (h)  # Save each layer\n        \n        # Aggregate all layers (not just last)\n        h_final = self.jump (layer_outputs)  # Concatenate all\n        \n        return h_final\n\n# Why it helps:\n# - Uses representations from all layers\n# - Layer 1: Local info (1-hop)\n# - Layer 5: Global info (5-hop)\n# - Concatenate: Both local and global!\n\n# Output dimension: num_layers × hidden_dim\n```\n\n**3. Initial Residual Connections**:\n\n```python\nclass InitialResidualGNN(nn.Module):\n    def forward (self, x, edge_index):\n        x_0 = x  # Save initial features\n        \n        for layer in self.layers:\n            x = layer (x, edge_index).relu()\n            \n            # Connect to initial features (not previous layer)\n            x = x + x_0  # Initial residual\n        \n        return x\n\n# Why it helps:\n# - Always preserves original node features\n# - Stronger than layer-to-layer residual\n# - Ensures node identity never lost\n```\n\n**4. DropEdge (Regularization)**:\n\n```python\nclass DropEdgeGNN(nn.Module):\n    def __init__(self, drop_rate=0.5):\n        self.drop_rate = drop_rate\n    \n    def forward (self, x, edge_index):\n        # Randomly drop edges during training\n        if self.training:\n            mask = torch.rand (edge_index.size(1)) > self.drop_rate\n            edge_index_dropped = edge_index[:, mask]\n        else:\n            edge_index_dropped = edge_index\n        \n        # Message passing on subsampled graph\n        h = self.gnn (x, edge_index_dropped)\n        return h\n\n# Why it helps:\n# - Prevents over-reliance on specific edges\n# - Reduces receptive field overlap\n# - Acts as regularization\n# - Different subgraphs each iteration\n```\n\n**5. Graph Normalization**:\n\n```python\nclass PairNorm (nn.Module):\n    # Normalize to maintain scale and center\n    def forward (self, h):\n        # Center: mean = 0\n        h_centered = h - h.mean (dim=0, keepdim=True)\n        \n        # Scale: maintain unit scale\n        scale = h.size(0) ** 0.5\n        h_normalized = scale * h_centered / (h_centered.norm() + 1e-5)\n        \n        return h_normalized\n\nclass GNNWithNorm (nn.Module):\n    def forward (self, x, edge_index):\n        for layer in self.layers:\n            x = layer (x, edge_index)\n            x = self.pairnorm (x)  # Normalize after each layer\n            x = x.relu()\n        return x\n\n# Why it helps:\n# - Prevents feature collapse to single point\n# - Maintains diversity across nodes\n# - Keeps features in reasonable range\n```\n\n**6. Limit Depth (Most Practical)**:\n\n```python\n# Instead of 20 layers, use 2-5 layers\n\n# Empirical findings:\n# - 2-3 layers: Sufficient for most tasks\n# - 4-6 layers: Good for large graphs\n# - >10 layers: Usually hurts performance\n\n# Why shallow works:\n# - Most graphs have small diameter\n# - Local structure most important\n# - Over-smoothing kicks in quickly\n```\n\n**7. Adaptive Depth (Graph-Dependent)**:\n\n```python\nclass AdaptiveGNN(nn.Module):\n    def __init__(self, max_layers=10):\n        self.layers = nn.ModuleList([...])  # max_layers\n        self.gates = nn.ModuleList([  # Learned stopping\n            nn.Linear (hidden_dim, 1) for _ in range (max_layers)\n        ])\n    \n    def forward (self, x, edge_index):\n        h = x\n        outputs = []\n        \n        for i, (layer, gate) in enumerate (zip (self.layers, self.gates)):\n            h_new = layer (h, edge_index)\n            \n            # Learned gate: Should we continue?\n            gate_val = torch.sigmoid (gate (h))\n            \n            # Weighted combination\n            h = gate_val * h_new + (1 - gate_val) * h\n            outputs.append (h)\n        \n        return h\n\n# Learns optimal depth per node/graph\n```\n\n**Performance Comparison**:\n\n```python\n# Citation network (Cora dataset)\n\n# Standard GNN:\n2 layers: 81.5% accuracy\n5 layers: 78.2% accuracy (slight over-smoothing)\n10 layers: 65.1% accuracy (severe over-smoothing)\n20 layers: 52.3% accuracy (worse than random!)\n\n# GNN with Residuals:\n2 layers: 81.8% accuracy\n5 layers: 82.1% accuracy (maintains performance)\n10 layers: 80.5% accuracy (degrades slowly)\n20 layers: 75.2% accuracy (still reasonable)\n\n# JumpingKnowledge:\n2 layers: 82.0% accuracy\n5 layers: 83.5% accuracy (best!)\n10 layers: 82.8% accuracy (robust)\n20 layers: 79.1% accuracy\n```\n\n**When Is Depth Needed?**\n\n**Small graphs / Large diameter**:\n```\nBiological networks: Diameter = 20-30\n→ May benefit from deeper GNNs\n\nMolecules: Diameter = 5-10\n→ 3-5 layers sufficient\n```\n\n**Heterogeneous information**:\n```\nKnowledge graphs: Multiple relation types\n→ Deeper GNNs capture complex patterns\n\nHomogeneous: Social network\n→ Shallow GNNs sufficient\n```\n\n**Key Insight**: Over-smoothing occurs because **repeated averaging** causes node features to converge to the graph's stationary distribution, losing local structure. Since most real-world graphs have small diameter (small-world property), deep GNNs (>6 layers) cause representations to collapse. Practical solutions include **residual connections** (preserve initial features), **jumping knowledge** (aggregate multiple scales), and simply **limiting depth** to 2-5 layers for most tasks.",
    keyPoints: [],
  },
  {
    id: 'gnn-q3',
    question:
      'Compare different GNN architectures: Graph Convolutional Networks (GCN), Graph Attention Networks (GAT), and GraphSAGE. What are their key differences in aggregation mechanisms, and when would you use each?',
    sampleAnswer:
      "Different GNN architectures vary in how they **aggregate neighbor information**, with tradeoffs between simplicity, expressiveness, and computational cost.\n\n**1. Graph Convolutional Networks (GCN)**:\n\n**Key Idea**: Symmetric normalized aggregation\n\n**Formulation**:\n```\nh_v^{(k+1)} = σ(Σ_{u ∈ N(v) ∪ {v}} (1/√(d_v · d_u)) · W^{(k)} h_u^{(k)})\n\nwhere:\n- d_v, d_u: Degrees of nodes v, u\n- 1/√(d_v · d_u): Symmetric normalization\n- W^{(k)}: Shared weight matrix\n- σ: Activation (ReLU)\n```\n\n**Matrix Form**:\n```python\nH^{(k+1)} = σ(D^{-1/2} Ã D^{-1/2} H^{(k)} W^{(k)})\n\nwhere:\n- Ã = A + I (adjacency + self-loops)\n- D: Degree matrix\n- H^{(k)}: Node features at layer k\n```\n\n**Implementation**:\n```python\nclass GCNConv (nn.Module):\n    def __init__(self, in_channels, out_channels):\n        self.lin = nn.Linear (in_channels, out_channels, bias=False)\n        self.bias = nn.Parameter (torch.zeros (out_channels))\n    \n    def forward (self, x, edge_index):\n        # Add self-loops\n        edge_index, _ = add_self_loops (edge_index, num_nodes=x.size(0))\n        \n        # Compute normalization\n        row, col = edge_index\n        deg = degree (col, x.size(0), dtype=x.dtype)\n        deg_inv_sqrt = deg.pow(-0.5)\n        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n        \n        # Linear transformation\n        x = self.lin (x)\n        \n        # Normalize and aggregate\n        out = torch.zeros_like (x)\n        for i in range (edge_index.size(1)):\n            out[col[i]] += norm[i] * x[row[i]]\n        \n        return out + self.bias\n\n# Usage:\ngcn = GCNConv(64, 128)\nh_new = gcn (h, edge_index)\n```\n\n**Normalization Intuition**:\n```\nWhy 1/√(d_v · d_u)?\n\nWithout normalization:\n- High-degree nodes receive large aggregations\n- Features explode or vanish\n- Training unstable\n\nWith symmetric normalization:\n- Balances contribution from each neighbor\n- Degree-invariant features\n- Stable training\n\nExample:\nNode A: degree 2\nNode B: degree 100\n\nWithout norm: A→B contributes 1.0\n               B→A contributes 1.0 (but B has 100 neighbors!)\n               → A overwhelmed by B\n\nWith norm: A→B contributes 1/√(2·100) ≈ 0.07\n           B→A contributes 1/√(100·2) ≈ 0.07\n           → Balanced!\n```\n\n**Pros**:\n```\n✓ Simple and efficient\n✓ Spectral foundation (graph Laplacian)\n✓ Fast (matrix operations)\n✓ Works well in practice\n✓ Few hyperparameters\n```\n\n**Cons**:\n```\n✗ Fixed aggregation weights (not learned per edge)\n✗ Treats all neighbors equally\n✗ Sensitive to graph structure\n✗ Assumes homophily (similar neighbors)\n```\n\n**2. Graph Attention Networks (GAT)**:\n\n**Key Idea**: Learn attention weights for neighbors\n\n**Formulation**:\n```\nh_v^{(k+1)} = σ(Σ_{u ∈ N(v)} α_{vu} W^{(k)} h_u^{(k)})\n\nwhere α_{vu} = attention weight (learned!)\n\nα_{vu} = softmax_u (e_{vu})\n       = exp (e_{vu}) / Σ_{u' ∈ N(v)} exp (e_{vu'})\n\ne_{vu} = LeakyReLU(a^T [Wh_v || Wh_u])\n       = attention score\n```\n\n**Implementation**:\n```python\nclass GATConv (nn.Module):\n    def __init__(self, in_channels, out_channels, heads=1):\n        self.heads = heads\n        self.out_per_head = out_channels // heads\n        \n        # Linear transformation\n        self.lin = nn.Linear (in_channels, heads * self.out_per_head, bias=False)\n        \n        # Attention mechanism\n        self.att = nn.Parameter (torch.zeros(1, heads, 2 * self.out_per_head))\n        \n        self.bias = nn.Parameter (torch.zeros (out_channels))\n        self.leaky_relu = nn.LeakyReLU(0.2)\n    \n    def forward (self, x, edge_index):\n        # Linear transformation\n        x = self.lin (x).view(-1, self.heads, self.out_per_head)\n        \n        row, col = edge_index\n        \n        # Compute attention scores\n        x_i = x[row]  # Source nodes\n        x_j = x[col]  # Target nodes\n        \n        # Concatenate [x_i || x_j]\n        x_cat = torch.cat([x_i, x_j], dim=-1)  # (num_edges, heads, 2*out_per_head)\n        \n        # Attention score: e_ij = a^T [Wh_i || Wh_j]\n        alpha = (x_cat * self.att).sum (dim=-1)  # (num_edges, heads)\n        alpha = self.leaky_relu (alpha)\n        \n        # Softmax normalization\n        alpha = softmax (alpha, col, num_nodes=x.size(0))\n        \n        # Apply dropout to attention\n        alpha = F.dropout (alpha, p=0.6, training=self.training)\n        \n        # Weighted aggregation\n        out = torch.zeros (x.size(0), self.heads, self.out_per_head)\n        for i in range (edge_index.size(1)):\n            out[col[i]] += alpha[i].unsqueeze(-1) * x_j[i]\n        \n        # Concatenate heads or average\n        out = out.view(-1, self.heads * self.out_per_head)\n        \n        return out + self.bias\n\n# Usage:\ngat = GATConv(64, 128, heads=8)  # Multi-head attention\nh_new = gat (h, edge_index)\n```\n\n**Multi-Head Attention**:\n```python\n# K attention heads (like Transformer)\nheads = 8\n\n# Each head learns different attention pattern:\n# Head 1: Attends to similar age\n# Head 2: Attends to same community\n# Head 3: Attends to high-degree nodes\n# ...\n\n# Concatenate or average heads\nh_out = Concat([h_1, h_2, ..., h_K])  # or Mean([...])\n```\n\n**Attention Visualization**:\n```python\n# Example: Social network\nNode Alice connects to: [Bob, Charlie, David]\n\nGCN (fixed weights):\nα_AB = 1/√(2·3) = 0.41\nα_AC = 1/√(2·5) = 0.32\nα_AD = 1/√(2·10) = 0.22\n# Based only on degree\n\nGAT (learned weights):\nα_AB = 0.15  # Low attention (different interests)\nα_AC = 0.70  # High attention (similar interests)\nα_AD = 0.15  # Low attention (different age group)\n# Learned from data!\n\n# GAT discovers: Charlie is most relevant to Alice\n# (despite having medium degree)\n```\n\n**Pros**:\n```\n✓ Adaptive attention (learns importance)\n✓ Different weights per neighbor\n✓ Interpretable (can visualize attention)\n✓ Robust to noisy edges\n✓ Works well with heterogeneous graphs\n```\n\n**Cons**:\n```\n✗ Slower than GCN (computes attention)\n✗ More parameters (attention mechanism)\n✗ Harder to train (unstable early on)\n✗ Overfitting on small graphs\n```\n\n**3. GraphSAGE (Graph Sample and AggregatE)**:\n\n**Key Idea**: Sample neighbors and aggregate with various functions\n\n**Formulation**:\n```\nh_v^{(k+1)} = σ(W^{(k)} · CONCAT(h_v^{(k)}, AGG({h_u^{(k)} : u ∈ S(N(v))})))\n\nwhere:\n- S(N(v)): Sampled neighbors (not all!)\n- AGG: Aggregation function (mean, pool, LSTM)\n- CONCAT: Preserve node's own features\n```\n\n**Sampling Strategy**:\n```python\n# Fixed-size neighbor sampling\ndef sample_neighbors (node, K):\n    neighbors = get_neighbors (node)\n    if len (neighbors) > K:\n        sampled = random.sample (neighbors, K)\n    else:\n        sampled = neighbors\n    return sampled\n\n# Example:\nNode A has 1000 neighbors\nSample K=25 neighbors uniformly\n→ Aggregate only 25 (constant complexity!)\n```\n\n**Aggregation Functions**:\n\n**Mean Aggregator**:\n```python\nclass MeanAggregator:\n    def aggregate (self, h_neighbors):\n        # Simple average\n        return h_neighbors.mean (dim=0)\n\n# Same as GCN but with sampling\n```\n\n**Pool Aggregator**:\n```python\nclass PoolAggregator (nn.Module):\n    def __init__(self, in_dim, out_dim):\n        self.mlp = nn.Linear (in_dim, out_dim)\n    \n    def aggregate (self, h_neighbors):\n        # Transform then max pool\n        transformed = self.mlp (h_neighbors).relu()\n        pooled = transformed.max (dim=0)[0]\n        return pooled\n\n# More expressive than mean\n```\n\n**LSTM Aggregator**:\n```python\nclass LSTMAggregator (nn.Module):\n    def __init__(self, in_dim, hidden_dim):\n        self.lstm = nn.LSTM(in_dim, hidden_dim)\n    \n    def aggregate (self, h_neighbors):\n        # Random permutation (LSTM sees sequence)\n        perm = torch.randperm (h_neighbors.size(0))\n        h_perm = h_neighbors[perm]\n        \n        # LSTM processes neighbors\n        _, (h_final, _) = self.lstm (h_perm.unsqueeze(0))\n        return h_final.squeeze(0)\n\n# Most expressive but slowest\n```\n\n**Full Implementation**:\n```python\nclass SAGEConv (nn.Module):\n    def __init__(self, in_channels, out_channels, aggr='mean'):\n        self.aggr = aggr\n        \n        # Weights\n        self.lin_neighbor = nn.Linear (in_channels, out_channels)\n        self.lin_self = nn.Linear (in_channels, out_channels)\n        \n        if aggr == 'pool':\n            self.pool_mlp = nn.Linear (in_channels, in_channels)\n    \n    def forward (self, x, edge_index, num_samples=25):\n        row, col = edge_index\n        \n        # Sample neighbors (fixed size per node)\n        sampled_neighbors = self.sample_neighbors (edge_index, num_samples)\n        \n        # Aggregate neighbors\n        if self.aggr == 'mean':\n            h_neigh = scatter_mean (x[row], col, dim=0, dim_size=x.size(0))\n        elif self.aggr == 'pool':\n            h_transformed = self.pool_mlp (x[row]).relu()\n            h_neigh = scatter_max (h_transformed, col, dim=0, dim_size=x.size(0))[0]\n        \n        # Combine self and neighbors\n        h_self = self.lin_self (x)\n        h_neighbor = self.lin_neighbor (h_neigh)\n        \n        # L2 normalize (optional)\n        h_out = F.normalize (h_self + h_neighbor, p=2, dim=1)\n        \n        return h_out\n\n# Usage:\nsage = SAGEConv(64, 128, aggr='mean')\nh_new = sage (h, edge_index, num_samples=25)\n```\n\n**Inductive Learning**:\n```python\n# Key feature: Can generalize to new nodes!\n\n# Training:\ntrain_nodes = [1, 2, 3, ..., 1000]\nmodel.train (train_nodes)\n\n# Inference on new nodes (not seen during training):\nnew_nodes = [1001, 1002, 1003]\nfor node in new_nodes:\n    neighbors = sample_neighbors (node, K=25)\n    h_new = aggregate (neighbors)  # Uses trained aggregator\n    # Works! Didn't need to retrain\n\n# GCN/GAT: Need entire graph adjacency (transductive)\n# GraphSAGE: Only needs local neighbors (inductive)\n```\n\n**Pros**:\n```\n✓ Scalable (samples fixed-size neighborhood)\n✓ Inductive (generalizes to new nodes)\n✓ Flexible aggregators (mean, pool, LSTM)\n✓ Constant memory per node\n✓ Handles large graphs (millions of nodes)\n```\n\n**Cons**:\n```\n✗ Sampling introduces randomness\n✗ May miss important neighbors\n✗ Requires tuning K (sample size)\n✗ More complex implementation\n```\n\n**Comparison Table**:\n\n| Aspect | GCN | GAT | GraphSAGE |\n|--------|-----|-----|----------|\n| **Aggregation** | Fixed (symmetric norm) | Learned (attention) | Sampled (various) |\n| **Complexity** | O(E) | O(E·H) | O(N·K) |\n| **Parameters** | Low | High (attention) | Medium |\n| **Interpretability** | Low | High (attention viz) | Medium |\n| **Scalability** | Medium | Low | High |\n| **Inductive** | No | No | Yes |\n| **Best For** | Homogeneous graphs | Heterogeneous graphs | Large-scale graphs |\n\nwhere:\n- E: Number of edges\n- H: Number of attention heads\n- N: Number of nodes\n- K: Sample size\n\n**When to Use Each**:\n\n**GCN**:\n```\n✓ Small to medium graphs (<100K nodes)\n✓ Homogeneous data (nodes similar)\n✓ Transductive setting (fixed graph)\n✓ Need simplicity and speed\n\nExamples:\n- Citation networks (papers cite similar papers)\n- Molecular property prediction (atoms have consistent bonds)\n```\n\n**GAT**:\n```\n✓ Heterogeneous graphs (diverse node types)\n✓ Need interpretability (attention visualization)\n✓ Noisy edges (attention filters noise)\n✓ Heterophilous graphs (dissimilar neighbors)\n\nExamples:\n- Knowledge graphs (multiple relation types)\n- Social networks (diverse connections)\n- Biological networks (complex interactions)\n```\n\n**GraphSAGE**:\n```\n✓ Large graphs (>1M nodes)\n✓ Inductive setting (new nodes arrive)\n✓ Need scalability\n✓ Streaming/dynamic graphs\n\nExamples:\n- Recommendation systems (new users/items)\n- Pinterest graph (billions of nodes)\n- Protein-protein interaction (new proteins)\n```\n\n**Key Insight**: GCN uses **fixed symmetric normalization** (simple, efficient), GAT learns **adaptive attention weights** (expressive, interpretable), and GraphSAGE performs **fixed-size sampling** (scalable, inductive). Choose based on graph size, heterogeneity, and whether you need to generalize to unseen nodes.",
    keyPoints: [],
  },
];
