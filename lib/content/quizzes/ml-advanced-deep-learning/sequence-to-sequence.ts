/**
 * Sequence-to-Sequence Models Quiz
 */
export const sequenceToSequenceQuiz = [
  {
    id: 'cnn-q1',
    question:
      "Explain the 'bottleneck problem' in Seq2Seq models. Why does compressing an entire input sequence into a fixed-size context vector limit performance, especially for long sequences? How does this problem manifest in practice?",
    sampleAnswer:
      "The bottleneck problem is a fundamental limitation of basic Seq2Seq architecture where all input information must flow through a fixed-size context vector.\n\n**The Problem**:\n\nIn standard Seq2Seq:\n```\nInput (variable length) → Encoder → Context (fixed size) → Decoder → Output\n  x_1,...,x_n (n varies)     LSTM      c (e.g., 512-dim)    LSTM    y_1,...,y_m\n```\n\nThe context vector c is typically 512-1024 dimensions, regardless of input length.\n\n**Why It's Problematic**:\n\n1. **Information Compression Loss**:\n   - 5-word sentence: ~50 dimensions/word\n   - 100-word sentence: ~5 dimensions/word\n   - Long sequences can't fit all information into fixed size\n   - Early tokens may be forgotten to make room for later ones\n\n2. **Fixed Capacity**:\n   - Same 512-dim vector for 'Hello' vs entire paragraph\n   - No scalability with input complexity\n   - Model must choose what to remember/forget\n\n3. **Distant Dependencies**:\n   - Encoder processes tokens sequentially\n   - By the time encoder reaches token 100, information from token 1 has degraded\n   - Even with LSTM, very long sequences lose early information\n\n**Practical Manifestations**:\n\n**1. Translation Quality Degradation**:\n```\nShort sentence (5 words):\nInput:  'I love machine learning'\nOutput: 'J'aime l'apprentissage automatique' ✓ (correct)\n\nLong sentence (30 words):\nInput:  'The researcher, who works at a prestigious university...(30 words)'\nOutput: 'Le chercheur, qui....[missing/incorrect later parts]' ✗\n```\nTranslation quality drops significantly after ~20-30 words.\n\n**2. Loss of Early Context**:\n```\nInput: 'The cat, which was sitting on the mat and looking very comfortable, was hungry'\nContext vector: [0.2, -0.5, 0.8, ...] (512 dims)\n\n→ 'cat' information compressed and mixed with all other words\n→ When generating 'was hungry', model struggles to recall subject 'cat'\n→ Might generate 'were hungry' (plural verb error)\n```\n\n**3. Forgetting Important Details**:\n```\nSummarization task:\nInput: Article with key detail at word 10 and word 500\n\nContext vector: Tends to remember recent information (word 500) better\n                Forgets earlier important detail (word 10)\n                \nSummary: Missing early key information ✗\n```\n\n**4. Inability to Focus**:\n```\nTranslating word 'bank':\n- Context early: 'river' → should translate as 'rive' (riverbank)\n- Context late: 'money' → should translate as 'banque' (financial bank)\n\nFixed context: Averages all context → ambiguous → wrong translation\n```\n\n**Mathematical Perspective**:\n\nContext vector c is final hidden state h_n after processing n tokens:\nc = h_n = f(h_{n-1}, x_n)\n\nInformation from x_1 must flow through n state updates:\nx_1 → h_1 → h_2 → ... → h_n = c\n\nEven with LSTM, this path has limited capacity. Information from x_1 is:\n1. Transformed through n updates\n2. Mixed with information from x_2...x_n\n3. Compressed into fixed size\n\n**Empirical Evidence**:\n\nResearch showed translation quality vs sentence length:\n- <10 words: ~40 BLEU score\n- 20-30 words: ~30 BLEU score\n- >40 words: ~20 BLEU score (significant degradation!)\n\n**Why Gradient Flow Isn't Enough**:\n\nLSTM solves vanishing gradients (gradients can flow), but NOT the capacity problem (information must still compress).\n\nAnalogy:\n- Vanishing gradients = pipe is clogged (fixed by LSTM)\n- Bottleneck = pipe is too narrow (NOT fixed by LSTM)\n\n**Solutions**:\n\n1. **Attention Mechanism** (next section):\n   - Decoder can 'look back' at all encoder states\n   - No compression through single vector\n   - Direct access to relevant input positions\n\n2. **Larger Context** (partial fix):\n   - Use 2048-dim instead of 512-dim\n   - Helps but doesn't solve fundamental issue\n   - Still fixed size\n\n3. **Multiple Context Vectors**:\n   - Use multiple encoder outputs\n   - Still not as good as attention\n\nThe bottleneck problem was a major blocker until attention mechanisms (2015) provided the breakthrough solution, leading to the Transformer revolution.",
    keyPoints: [],
  },
  {
    id: 'cnn-q2',
    question:
      "Explain teacher forcing vs free running in Seq2Seq training. What is 'exposure bias', and how can scheduled sampling help? Provide a concrete example showing the problem and solution.",
    sampleAnswer:
      "Teacher forcing is a training technique where we feed the true previous output (not the model's prediction) as the next input during decoder training. This creates a mismatch between training and inference that leads to exposure bias.\n\n**Teacher Forcing (Training)**:\n\n```\nTrue sequence: ['<START>', 'The', 'cat', 'sat', '<END>']\n\nStep 1: Input: '<START>' → Predict: 'The'\nStep 2: Input: 'The' (TRUE) → Predict: 'cat'  \nStep 3: Input: 'cat' (TRUE) → Predict: 'sat'\nStep 4: Input: 'sat' (TRUE) → Predict: '<END>'\n```\n\nKey: Even if model predicted 'Dog' at step 1, we still feed 'The' at step 2.\n\n**Free Running (Inference)**:\n\n```\nStep 1: Input: '<START>' → Predict: 'The'\nStep 2: Input: 'The' (PREDICTED) → Predict: 'cat'\nStep 3: Input: 'cat' (PREDICTED) → Predict: 'sat'  \nStep 4: Input: 'sat' (PREDICTED) → Predict: '<END>'\n```\n\nKey: Model uses its own predictions as next inputs.\n\n**Advantages of Teacher Forcing**:\n\n1. **Faster Convergence**:\n   - Each step learns from correct context\n   - No error accumulation during training\n   - Can learn all positions in parallel\n\n2. **Stable Gradients**:\n   - Consistent input distribution\n   - No cascading errors\n\n3. **Parallel Training**:\n   - All decoder steps can be computed together\n   - Much faster than autoregressive training\n\n**The Exposure Bias Problem**:\n\n**Definition**: Model never sees its own errors during training, so it doesn't learn to recover from mistakes.\n\n**Concrete Example - Machine Translation**:\n\nTraining with teacher forcing:\n```\nInput: 'I love cats'\nTarget: 'J'aime les chats'\n\nStep 1: Input '<START>' → Predict 'J'aime' ✓\nStep 2: Input 'J'aime' (TRUE) → Predict 'les' ✓  \nStep 3: Input 'les' (TRUE) → Predict 'chats' ✓\n\n→ Model learns assuming perfect previous predictions\n```\n\nInference without teacher forcing:\n```\nStep 1: Input '<START>' → Predict 'Je' ✗ (should be 'J'aime')\nStep 2: Input 'Je' (WRONG!) → Model confused → Predict 'suis' ✗\nStep 3: Input 'suis' (WRONG!) → Predict 'un' ✗\nOutput: 'Je suis un' (complete nonsense!)\n\n→ Single error cascades into complete failure\n```\n\n**Why This Happens**:\n\n1. **Distribution Mismatch**:\n   - Training: Always correct context\n   - Inference: Often wrong context\n   - Model never learned to handle wrong context\n\n2. **Error Accumulation**:\n   - One wrong prediction → next context is wrong\n   - Next prediction more likely wrong\n   - Errors compound exponentially\n\n3. **Lack of Robustness**:\n   - Model overfits to perfect sequences\n   - Can't recover from mistakes\n   - Fragile at inference time\n\n**Scheduled Sampling - The Solution**:\n\nGradually transition from teacher forcing to free running during training:\n\n```python\n# Scheduled sampling implementation\ndef scheduled_sampling_probability(epoch, total_epochs, schedule='linear'):\n    \"\"\"Calculate probability of teacher forcing.\"\"\"\n    if schedule == 'linear':\n        # Linearly decrease from 1.0 to 0.0\n        return 1.0 - (epoch / total_epochs)\n    elif schedule == 'exponential':\n        # Exponentially decrease\n        return 0.99 ** epoch\n    elif schedule == 'inverse_sigmoid':\n        # Smooth transition\n        k = 10 / total_epochs\n        return k / (k + np.exp(epoch / total_epochs))\n\n# During training\nfor epoch in range(total_epochs):\n    teacher_forcing_ratio = scheduled_sampling_probability(epoch, total_epochs)\n    \n    # For each step\n    use_teacher_forcing = random.random() < teacher_forcing_ratio\n    \n    if use_teacher_forcing:\n        decoder_input = target_token  # True token\n    else:\n        decoder_input = predicted_token  # Model's prediction\n```\n\n**Scheduled Sampling in Practice**:\n\nEpoch-by-epoch progression:\n```\nEpoch 1: teacher_forcing_ratio = 1.0 (100% teacher forcing)\n  → Always use true tokens\n  → Fast initial learning\n\nEpoch 10: teacher_forcing_ratio = 0.75 (75% teacher forcing)\n  → 75% true tokens, 25% predictions\n  → Start seeing own errors\n\nEpoch 20: teacher_forcing_ratio = 0.5 (50% teacher forcing)\n  → Half and half\n  → Balanced exposure\n\nEpoch 30: teacher_forcing_ratio = 0.25 (25% teacher forcing)\n  → Mostly predictions\n  → Learning to recover\n\nEpoch 40: teacher_forcing_ratio = 0.0 (0% teacher forcing)\n  → All predictions (like inference)\n  → Full robustness\n```\n\n**Example with Scheduled Sampling**:\n\nEpoch 20 (50% teacher forcing):\n```\nStep 1: Predict 'Je' ✗\nStep 2: Use teacher forcing (50% chance) → Input 'J'aime' (TRUE)\n        → Predict 'les' ✓ (back on track!)\nStep 3: Use own prediction (50% chance) → Input 'les' (PREDICTED)\n        → Predict 'chats' ✓\n\n→ Model learns to continue from both correct and incorrect contexts\n→ More robust to errors\n```\n\n**Benefits of Scheduled Sampling**:\n\n1. **Early Training**: Fast convergence with teacher forcing\n2. **Late Training**: Robustness from seeing own errors\n3. **Smooth Transition**: Gradual shift prevents instability\n4. **Better Generalization**: Model handles imperfect contexts\n\n**Empirical Results**:\n\nWithout scheduled sampling:\n- Training loss: Low ✓\n- Validation loss: High ✗\n- BLEU score: 28.5\n\nWith scheduled sampling:\n- Training loss: Higher initially\n- Validation loss: Lower overall ✓\n- BLEU score: 31.2 (+2.7 improvement!)\n\n**Modern Approaches**:\n\n1. **Curriculum Learning**: Start with short sequences, gradually increase length\n2. **Minimum Risk Training**: Optimize for sequence-level loss\n3. **Reinforcement Learning**: Reward final output quality\n4. **Transformers**: Self-attention reduces error accumulation\n\n**Practical Recommendations**:\n\n1. **Start with pure teacher forcing** (epochs 1-10)\n2. **Gradually reduce** (epochs 10-30)\n3. **End with 10-20% teacher forcing** (not 0%)\n   - Complete free running can be too hard\n   - Small amount of teacher forcing helps\n4. **Use smooth schedule** (inverse sigmoid > linear > exponential)\n\nScheduled sampling is now standard practice for training Seq2Seq models, providing significant improvements in generation quality and robustness.",
    keyPoints: [],
  },
  {
    id: 'cnn-q3',
    question:
      'Compare greedy decoding and beam search for Seq2Seq models. Explain with a concrete example how beam search can find better sequences than greedy. What are the trade-offs, and how would you choose the beam width?',
    sampleAnswer:
      "Greedy decoding and beam search are inference strategies for generating sequences from trained Seq2Seq models, with different trade-offs between speed and quality.\n\n**Greedy Decoding**:\n\nAt each step, choose the single highest probability token:\n\n```python\nfor t in range(max_length):\n    probs = model.predict(current_input)\n    next_token = argmax(probs)  # Choose best\n    output.append(next_token)\n    current_input = next_token\n```\n\n**Beam Search**:\n\nMaintain top-k (beam width) most likely sequences:\n\n```python\nbeams = [initial_sequence]\n\nfor t in range(max_length):\n    candidates = []\n    for beam in beams:\n        probs = model.predict(beam)\n        top_k = get_top_k(probs, k=beam_width)\n        for token in top_k:\n            new_beam = beam + [token]\n            new_score = beam.score + log_prob(token)\n            candidates.append((new_beam, new_score))\n    \n    # Keep top beam_width candidates\n    beams = sorted(candidates, by=score)[:beam_width]\n```\n\n**Concrete Example - Translation**:\n\nInput: \"I am happy\"\nTarget: \"Je suis heureux\"\n\n**Greedy Decoding** (beam width = 1):\n\n```\nStep 1: P('Je'|<START>) = 0.6  ← Choose\n        P('J') = 0.3\n        P('Moi') = 0.1\n→ Choose 'Je'\n\nStep 2: P('suis'|'Je') = 0.5  ← Choose  \n        P('me') = 0.3\n        P('ai') = 0.2\n→ Choose 'suis'\n\nStep 3: P('content'|'Je','suis') = 0.55  ← Choose\n        P('heureux'|'Je','suis') = 0.45\n→ Choose 'content'\n\nFinal: 'Je suis content' (score = log(0.6) + log(0.5) + log(0.55) = -1.48)\n```\n\nProblem: Chose 'content' (0.55) over 'heureux' (0.45) greedily.\n\n**Beam Search** (beam width = 2):\n\n```\nStep 1: Keep top 2\n  Beam 1: ['Je'] (score = log(0.6) = -0.51)\n  Beam 2: ['J'] (score = log(0.3) = -1.20)\n\nStep 2: Expand both beams, keep top 2 overall\n  From Beam 1 'Je':\n    'Je','suis' (score = -0.51 + log(0.5) = -1.20)\n    'Je','me' (score = -0.51 + log(0.3) = -1.71)\n  From Beam 2 'J':\n    'J','suis' (score = -1.20 + log(0.4) = -2.11)\n    'J','ai' (score = -1.20 + log(0.3) = -2.40)\n  \n  Keep top 2:\n  Beam 1: ['Je','suis'] (score = -1.20) ✓\n  Beam 2: ['Je','me'] (score = -1.71)\n\nStep 3: Expand again\n  From 'Je','suis':\n    'Je','suis','heureux' (score = -1.20 + log(0.45) = -2.00) ✓✓\n    'Je','suis','content' (score = -1.20 + log(0.55) = -1.80)\n  From 'Je','me':\n    'Je','me','sens' (score = -1.71 + log(0.6) = -2.22)\n    'Je','me','trouve' (score = -1.71 + log(0.3) = -2.91)\n  \n  Keep top 2:\n  Beam 1: ['Je','suis','content'] (score = -1.80)\n  Beam 2: ['Je','suis','heureux'] (score = -2.00)\n\nStep 4: Both beams add '<END>'\n  'Je suis content' (total score = -1.80)\n  'Je suis heureux' (total score = -2.00)\n\nBest: 'Je suis content' BUT with length normalization:\n  'Je suis content': -1.80 / 3 = -0.60\n  'Je suis heureux': -2.00 / 3 = -0.67\n  \nActually 'Je suis heureux' might be chosen with proper scoring!\n```\n\n**Why Beam Search Finds Better Sequences**:\n\n1. **Exploration**: Keeps multiple hypotheses alive\n   - Greedy might pick 'content' (0.55) early\n   - Beam search explores 'heureux' (0.45) path\n   - Later evidence might favor 'heureux' overall\n\n2. **Global vs Local Optimality**:\n   - Greedy: Locally optimal at each step\n   - Beam: More likely to find globally optimal sequence\n\n3. **Recovers from Early Mistakes**:\n   ```\n   Greedy: Makes bad choice at step 2 → stuck with it\n   Beam: Bad choice at step 2 → still exploring better alternatives\n   ```\n\n**Trade-offs**:\n\n| Aspect | Greedy (k=1) | Beam (k=5) | Beam (k=50) |\n|--------|--------------|------------|-------------|\n| Speed | Fastest | 5× slower | 50× slower |\n| Memory | Minimal | 5× more | 50× more |\n| Quality | Lowest | Good | Slightly better |\n| Deterministic | Yes | Yes | Yes |\n\n**Computational Complexity**:\n\n- Greedy: O(T × V) where T = length, V = vocabulary\n  - One forward pass per timestep\n\n- Beam (width k): O(T × k × V)\n  - k forward passes per timestep\n  - k× slower than greedy\n\n**Empirical Quality**:\n\nTypical BLEU scores on translation:\n```\nBeam Width | BLEU Score | Speed\n-----------+------------+-------\n1 (greedy) | 28.5       | 1.0×\n2          | 31.2       | 2.0×\n5          | 32.8       | 5.0×\n10         | 33.1       | 10.0×\n50         | 33.3       | 50.0×\n100        | 33.3       | 100.0×\n```\n\nDiminishing returns! Beyond k=10, marginal improvement.\n\n**Choosing Beam Width**:\n\n**Small Beam Width (k=1-3)**:\n- Use for: Real-time applications, chatbots, interactive systems\n- Example: Voice assistants need <100ms latency\n- Trade-off: Accept slightly lower quality for speed\n\n**Medium Beam Width (k=5-10)** ← Most Common:\n- Use for: General translation, most NLP tasks\n- Good balance of quality and speed\n- Recommended default\n\n**Large Beam Width (k=20-100)**:\n- Use for: Offline processing, research, benchmarking\n- When quality is critical and time doesn't matter\n- Example: Professional translation services\n\n**Very Large Beam Width (k>100)**:\n- Usually not worth it (diminishing returns)\n- Can actually hurt due to:\n  - Over-confidence in model\n  - Preference for shorter sequences\n  - Exploration of too many unlikely paths\n\n**Length Normalization**:\n\nProblem: Longer sequences have lower scores (more multiplications).\n\nSolution: Normalize by length:\n```\nscore_normalized = score / length^α\n\nwhere α ∈ [0.5, 1.0]\n```\n\nWithout normalization: Beam search prefers short sequences\nWith normalization: Fair comparison across lengths\n\n**Practical Recommendations**:\n\n1. **Start with k=5** for most applications\n2. **Increase to k=10** if quality matters more than speed\n3. **Use k=1 (greedy)** only if speed is critical\n4. **Apply length normalization** always\n5. **Add coverage penalty** to prevent repetition\n6. **Monitor beam search diversity** - all beams very similar means k too large\n\n**Modern Alternatives**:\n\n1. **Top-k Sampling**: Probabilistic, more diverse\n2. **Nucleus (Top-p) Sampling**: Dynamic cutoff\n3. **Temperature Scaling**: Control randomness\n4. **Diverse Beam Search**: Encourage different beams\n\nBeam search remains the standard for tasks requiring deterministic, high-quality outputs (translation, summarization), while sampling methods are preferred for creative generation (storytelling, dialogue).",
    keyPoints: [],
  },
];
