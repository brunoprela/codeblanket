/**
 * Autoencoders Quiz
 */

export const autoencodersQuiz = [
  {
    id: 'autoencoder-q1',
    question:
      'Explain how autoencoders learn compressed representations without explicit supervision. Walk through the architecture, loss function, and what makes the bottleneck layer learn meaningful features.',
    sampleAnswer:
      'Autoencoders learn compressed representations through **reconstruction loss** - forcing the network to encode input information efficiently to reconstruct it accurately.\\n\\n**Autoencoder Architecture**:\\n\\n```python\\nclass Autoencoder(nn.Module):\\n    def __init__(self, input_dim=784, latent_dim=32):\\n        # Encoder: Compress input to latent representation\\n        self.encoder = nn.Sequential(\\n            nn.Linear(input_dim, 256),  # 784 → 256\\n            nn.ReLU(),\\n            nn.Linear(256, 128),         # 256 → 128\\n            nn.ReLU(),\\n            nn.Linear(128, latent_dim)   # 128 → 32 (bottleneck)\\n        )\\n        \\n        # Decoder: Reconstruct input from latent\\n        self.decoder = nn.Sequential(\\n            nn.Linear(latent_dim, 128),  # 32 → 128\\n            nn.ReLU(),\\n            nn.Linear(128, 256),         # 128 → 256\\n            nn.ReLU(),\\n            nn.Linear(256, input_dim),   # 256 → 784\\n            nn.Sigmoid()                 # Output in [0,1]\\n        )\\n    \\n    def forward(self, x):\\n        # Encode\\n        z = self.encoder(x)  # Latent representation\\n        # Decode\\n        x_recon = self.decoder(z)  # Reconstruction\\n        return x_recon, z\\n```\\n\\n**Training Process**:\\n\\n**Loss Function** - Mean Squared Error (MSE):\\n```python\\ndef train_step(model, x):\\n    # Forward pass\\n    x_recon, z = model(x)\\n    \\n    # Reconstruction loss\\n    loss = nn.MSELoss()(x_recon, x)\\n    # Or Binary Cross-Entropy for [0,1] inputs:\\n    # loss = nn.BCELoss()(x_recon, x)\\n    \\n    # Backprop\\n    loss.backward()\\n    optimizer.step()\\n    \\n    return loss.item()\\n```\\n\\n**Why the Bottleneck Learns Meaningful Features**:\\n\\n**1. Information Bottleneck Principle**:\\n```\\nInput: 784 dimensions (28×28 MNIST image)\\n   ↓\\nEncoder compresses to 32 dimensions\\n   ↓\\nBottleneck: Must retain essential information\\n   ↓\\nDecoder reconstructs 784 dimensions\\n\\nChallenge: Fit 784 dimensions of info into 32 dimensions!\\n→ Forces encoder to learn compact, efficient representation\\n```\\n\\n**What Happens During Training**:\\n\\n**Epoch 1-5** (Learning basic structure):\\n```\\nInput: Digit "3"\\nLatent (random): [0.1, -0.3, 0.8, ...] (32 dims)\\nReconstruction: Blurry blob\\nLoss: 0.42 (high)\\n\\n→ Backprop updates encoder/decoder\\n→ Encoder learns: Capture overall shape\\n```\\n\\n**Epoch 10-20** (Learning details):\\n```\\nInput: Digit "3"\\nLatent (structured): [1.2, -0.5, 0.1, ...]\\nReconstruction: Recognizable "3" but blurry\\nLoss: 0.15 (improving)\\n\\n→ Encoder learns: Capture curves, loops\\n→ Different digits → different latent representations\\n```\\n\\n**Epoch 50+** (Converged):\\n```\\nInput: Digit "3"\\nLatent (meaningful): [1.8, -0.9, 0.0, ...]\\nReconstruction: Clear "3"\\nLoss: 0.03 (low)\\n\\n→ Encoder learned: Semantic features (digit identity, style)\\n→ Latent space organized: Similar digits → nearby points\\n```\\n\\n**Why Features Are Meaningful**:\\n\\n**Dimensionality Reduction Forces Structure**:\\n```\\nWithout bottleneck (784 → 784):\\n- Model can just copy input through (identity function)\\n- Learns nothing useful\\n- Loss → 0 trivially\\n\\nWith bottleneck (784 → 32 → 784):\\n- Cannot memorize pixel-by-pixel\\n- Must learn abstract features:\\n  * Dim 0: "Has vertical line?" → 0.9 for "1", -0.5 for "0"\\n  * Dim 1: "Has top loop?" → 1.2 for "9", -0.8 for "1"\\n  * Dim 2: "Curve openness" → 0.7 for "C", -0.6 for "O"\\n- Learns generalizable patterns\\n```\\n\\n**2. Reconstruction Pressure**:\\n```python\\n# To minimize reconstruction error:\\nloss = ||x - decoder(encoder(x))||²\\n\\n# Encoder must preserve information needed for reconstruction\\n# Cannot discard important features (would increase loss)\\n# Must prioritize most informative features (limited capacity)\\n```\\n\\n**Example: What Gets Encoded**:\\n\\n**High-Variance Features** (preserved):\\n```\\nDigit identity (0-9): High variance across dataset\\n→ Critical for reconstruction\\n→ Encoded in latent space (dim 0-10)\\n\\nStroke thickness: Medium variance\\n→ Moderately important\\n→ Encoded (dim 11-15)\\n```\\n\\n**Low-Variance Features** (discarded):\\n```\\nBackground noise: Low variance (always ~0)\\n→ Not critical for reconstruction\\n→ Discarded by encoder\\n\\nExact pixel position: Low impact on overall shape\\n→ Smoothed out\\n→ Reconstruction slightly blurry but recognizable\\n```\\n\\n**3. Unsupervised Learning**:\\n```\\nNo labels needed!\\n- Input: Image of "3"\\n- Target: Same image of "3"\\n- Model learns by trying to copy input through bottleneck\\n\\nContras with supervised:\\n- Would need labels: "This is digit 3"\\n- Autoencoder discovers digit structure automatically\\n```\\n\\n**Latent Space Visualization**:\\n\\n**t-SNE of Latent Representations**:\\n```python\\n# Extract latent codes for all MNIST digits\\nlatents = []\\nlabels = []\\nfor x, y in test_loader:\\n    _, z = model(x)\\n    latents.append(z.detach())\\n    labels.append(y)\\n\\n# Visualize in 2D\\nfrom sklearn.manifold import TSNE\\nz_2d = TSNE(n_components=2).fit_transform(latents)\\nplt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap=\'tab10\')\\n\\n# Observation: Digits cluster by class!\\n# - All "0"s in one region\\n# - All "1"s in another region\\n# - Smooth transitions between similar digits (8 ↔ 3)\\n```\\n\\n**Why This Works Without Labels**:\\n```\\nReconstruction loss implicitly encourages:\\n1. Similar inputs → similar latents (to reconstruct similarly)\\n2. Different inputs → different latents (to distinguish in reconstruction)\\n3. Smooth latent space (interpolation should give valid outputs)\\n\\n→ Emergent clustering by semantic similarity!\\n```\\n\\n**Mathematical Intuition**:\\n\\n**Optimization Objective**:\\n```\\nmin_encoder,decoder E[||x - decoder(encoder(x))||²]\\n\\nSubject to: dim(encoder(x)) << dim(x)\\n```\\n\\n**Solution**: Principal Component Analysis (PCA) approximation\\n```\\nLinear autoencoder → learns PCA subspace\\nNonlinear autoencoder → learns nonlinear manifold\\n\\nBoth capture maximum variance directions\\n→ Most informative features for reconstruction\\n```\\n\\n**Applications Leveraging Learned Representations**:\\n\\n**1. Dimensionality Reduction**:\\n```python\\n# Compress 784-dim images to 32-dim codes\\nz = model.encoder(x)  # 784 → 32\\n# Use z for downstream tasks (faster, less memory)\\n```\\n\\n**2. Denoising**:\\n```python\\n# Train on noisy → clean pairs\\nx_noisy = x + noise\\nx_clean_recon = model(x_noisy)\\n# Bottleneck filters out noise (low-variance)\\n```\\n\\n**3. Anomaly Detection**:\\n```python\\n# Normal samples: Low reconstruction error\\n# Anomalies: High reconstruction error (not seen during training)\\nerror = ||x - model(x)||²\\nis_anomaly = error > threshold\\n```\\n\\n**Key Insight**: The autoencoder bottleneck acts as an **information funnel** - reconstruction loss forces the network to preserve only the most essential, high-variance features, automatically discovering meaningful representations without supervision.',
    keyPoints: [],
  },
  {
    id: 'autoencoder-q2',
    question:
      'Explain Variational Autoencoders (VAEs) and how they differ from standard autoencoders. What is the reparameterization trick, and why is it necessary?',
    sampleAnswer:
      'Variational Autoencoders (VAEs) extend autoencoders by learning a **probabilistic latent space** with well-defined structure, enabling controlled generation of new samples.\\n\\n**Standard Autoencoder vs VAE**:\\n\\n**Standard Autoencoder**:\\n```python\\n# Encoder: Deterministic mapping\\nz = encoder(x)  # Fixed vector for each input\\n\\n# Latent space: Arbitrary structure\\n# - Gaps between clusters\\n# - Irregular distribution\\n# - Interpolation may give invalid outputs\\n```\\n\\n**Variational Autoencoder**:\\n```python\\n# Encoder: Probabilistic mapping\\nμ, log_σ² = encoder(x)  # Mean and log-variance\\nz ~ N(μ, σ²)             # Sample from distribution\\n\\n# Latent space: Structured distribution\\n# - Continuous (no gaps)\\n# - Smooth (interpolation valid)\\n# - Regular (follows prior distribution)\\n```\\n\\n**VAE Architecture**:\\n\\n```python\\nclass VAE(nn.Module):\\n    def __init__(self, input_dim=784, latent_dim=20):\\n        # Encoder: Output mean and log-variance\\n        self.encoder = nn.Sequential(\\n            nn.Linear(input_dim, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, 128),\\n            nn.ReLU()\\n        )\\n        self.fc_mu = nn.Linear(128, latent_dim)          # Mean\\n        self.fc_logvar = nn.Linear(128, latent_dim)      # Log-variance\\n        \\n        # Decoder: Reconstruct from sampled latent\\n        self.decoder = nn.Sequential(\\n            nn.Linear(latent_dim, 128),\\n            nn.ReLU(),\\n            nn.Linear(128, 256),\\n            nn.ReLU(),\\n            nn.Linear(256, input_dim),\\n            nn.Sigmoid()\\n        )\\n    \\n    def encode(self, x):\\n        h = self.encoder(x)\\n        mu = self.fc_mu(h)\\n        logvar = self.fc_logvar(h)\\n        return mu, logvar\\n    \\n    def reparameterize(self, mu, logvar):\\n        # Reparameterization trick (explained below)\\n        std = torch.exp(0.5 * logvar)  # σ = exp(log(σ²)/2)\\n        eps = torch.randn_like(std)     # Sample ε ~ N(0,1)\\n        z = mu + eps * std               # z = μ + ε·σ\\n        return z\\n    \\n    def decode(self, z):\\n        return self.decoder(z)\\n    \\n    def forward(self, x):\\n        mu, logvar = self.encode(x)\\n        z = self.reparameterize(mu, logvar)\\n        x_recon = self.decode(z)\\n        return x_recon, mu, logvar\\n```\\n\\n**VAE Loss Function**:\\n\\n**Two Components**:\\n```python\\ndef vae_loss(x, x_recon, mu, logvar):\\n    # 1. Reconstruction loss (same as standard autoencoder)\\n    recon_loss = nn.BCELoss(reduction=\'sum\')(x_recon, x)\\n    \\n    # 2. KL divergence loss (regularization)\\n    # KL(q(z|x) || p(z)) where p(z) = N(0, I)\\n    kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\\n    \\n    return recon_loss + kl_loss\\n```\\n\\n**Understanding KL Divergence Term**:\\n\\n**Role**: Regularizes latent space to match prior N(0,I)\\n\\n**Without KL Loss** (standard autoencoder behavior):\\n```\\nEncoder can encode each digit to arbitrary location:\\n- "0" → z = [100, 200, ...]\\n- "1" → z = [-50, 80, ...]\\n- Gaps between clusters\\n- Cannot sample new digits (where to sample from?)\\n```\\n\\n**With KL Loss** (VAE):\\n```\\nEncoder encouraged to:\\n- Center distributions around origin (μ → 0)\\n- Spread out to fill space (σ² → 1)\\n- Overlap between classes (smooth transitions)\\n\\nResult:\\n- "0" → μ=[0.5, -0.3], σ²=[0.8, 0.9]\\n- "1" → μ=[-0.2, 0.7], σ²=[1.1, 0.7]\\n- Can sample from N(0,I) → get valid digits!\\n```\\n\\n**The Reparameterization Trick**:\\n\\n**The Problem**:\\n\\n**Naive Sampling** (doesn\'t work):\\n```python\\nz = sample_from_N(mu, sigma²)  # Random sampling\\nx_recon = decoder(z)\\nloss = reconstruction_loss(x, x_recon)\\nloss.backward()  # ❌ Cannot backprop through random sampling!\\n\\n# Sampling is non-differentiable:\\n# ∂z/∂μ = ? (undefined - random operation)\\n# Cannot compute gradients → cannot train!\\n```\\n\\n**Why Sampling Breaks Gradient**:\\n```\\nComputation graph:\\nx → encoder → μ, σ² → [RANDOM SAMPLE] → z → decoder → x_recon → loss\\n                       ↑\\n                    Breaks gradient flow!\\n```\\n\\n**Reparameterization Trick** (solution):\\n\\n**Key Idea**: Move randomness outside gradient path\\n\\n**Instead of**: `z ~ N(μ, σ²)` (non-differentiable)\\n\\n**Do**: `z = μ + ε·σ where ε ~ N(0,1)` (differentiable w.r.t. μ, σ)\\n\\n**Implementation**:\\n```python\\ndef reparameterize(mu, logvar):\\n    # Compute standard deviation\\n    std = torch.exp(0.5 * logvar)  # σ = exp(log(σ²)/2)\\n    \\n    # Sample random noise (independent of parameters)\\n    eps = torch.randn_like(std)  # ε ~ N(0,1)\\n    \\n    # Reparameterize: z = μ + ε·σ\\n    z = mu + eps * std\\n    \\n    return z  # Differentiable w.r.t. mu and logvar!\\n```\\n\\n**Why This Works**:\\n\\n**Gradients Flow Through**:\\n```python\\n# Forward:\\nz = mu + eps * std\\nx_recon = decoder(z)\\nloss = L(x, x_recon)\\n\\n# Backward (gradients):\\n∂loss/∂mu = ∂loss/∂z · ∂z/∂mu = ∂loss/∂z · 1  ✓\\n∂loss/∂std = ∂loss/∂z · ∂z/∂std = ∂loss/∂z · eps  ✓\\n\\n# Randomness (eps) is constant during backward pass\\n# Treated as data, not parameters\\n```\\n\\n**Mathematical Equivalence**:\\n```\\nBoth give same distribution:\\n- Direct: z ~ N(μ, σ²)\\n- Reparam: z = μ + ε·σ where ε ~ N(0,1)\\n\\nProof:\\nIf ε ~ N(0,1), then μ + σ·ε ~ N(μ, σ²)\\n(Linear transformation of Gaussian is Gaussian)\\n```\\n\\n**Training Process**:\\n\\n```python\\nfor epoch in range(num_epochs):\\n    for x_batch in dataloader:\\n        # Encode: Get distribution parameters\\n        mu, logvar = model.encode(x_batch)\\n        \\n        # Reparameterize: Sample latent (differentiable)\\n        z = model.reparameterize(mu, logvar)\\n        \\n        # Decode: Reconstruct\\n        x_recon = model.decode(z)\\n        \\n        # Compute loss\\n        recon_loss = BCE(x_recon, x_batch)\\n        kl_loss = -0.5 * torch.sum(1 + logvar - mu² - exp(logvar))\\n        loss = recon_loss + kl_loss\\n        \\n        # Backprop (gradients flow through reparameterization)\\n        loss.backward()\\n        optimizer.step()\\n```\\n\\n**Generation with VAE**:\\n\\n**1. Sample New Images**:\\n```python\\n# Sample from prior N(0,I)\\nz = torch.randn(batch_size, latent_dim)  # Random latent codes\\nx_generated = model.decode(z)            # Generate images\\n\\n# Result: Novel images (not in training set) that look realistic\\n```\\n\\n**2. Interpolation**:\\n```python\\n# Encode two images\\nz1 = model.encode(x1)  # Digit "3"\\nz2 = model.encode(x2)  # Digit "8"\\n\\n# Interpolate in latent space\\nfor alpha in np.linspace(0, 1, 10):\\n    z_interp = (1 - alpha) * z1 + alpha * z2\\n    x_interp = model.decode(z_interp)\\n    # Result: Smooth morph from "3" to "8"\\n```\\n\\n**3. Latent Space Arithmetic**:\\n```python\\n# "king" - "man" + "woman" = "queen" (like Word2Vec)\\nz_smiling = encode("smiling face")\\nz_neutral = encode("neutral face")\\nz_frowning = encode("frowning face")\\n\\n# "smile vector"\\nsmile_vec = z_smiling - z_neutral\\n\\n# Apply to new face\\nz_new_neutral = encode("new person neutral")\\nz_new_smiling = z_new_neutral + smile_vec\\nx_new_smiling = decode(z_new_smiling)  # New person smiling!\\n```\\n\\n**Key Differences Summary**:\\n\\n**Standard Autoencoder**:\\n- Deterministic encoding: `z = encoder(x)`\\n- Arbitrary latent space\\n- Cannot generate new samples (where to sample?)\\n- Interpolation may give invalid outputs\\n\\n**VAE**:\\n- Probabilistic encoding: `z ~ q(z|x) = N(μ(x), σ²(x))`\\n- Structured latent space (forced to match N(0,I))\\n- Can generate new samples: `z ~ N(0,I) → x = decoder(z)`\\n- Smooth interpolation (continuous latent space)\\n- Reparameterization trick enables training\\n\\n**Key Insight**: The reparameterization trick is a clever mathematical transformation that makes **random sampling differentiable** by separating the stochastic part (ε) from the learned parameters (μ, σ), enabling backpropagation through probabilistic layers.',
    keyPoints: [],
  },
  {
    id: 'autoencoder-q3',
    question:
      'Compare autoencoders with PCA (Principal Component Analysis) for dimensionality reduction. What are the advantages and limitations of each?',
    sampleAnswer:
      "Autoencoders and PCA both perform dimensionality reduction, but differ fundamentally in their expressiveness: PCA learns **linear** transformations, while autoencoders can learn **nonlinear** manifolds.\n\n**Principal Component Analysis (PCA)**:\n\n**Method**: Find orthogonal directions of maximum variance\n\n**Mathematical Formulation**:\n```python\n# Given data X (n_samples × n_features)\n# 1. Center data\nX_centered = X - X.mean(axis=0)\n\n# 2. Compute covariance matrix\nC = X_centered.T @ X_centered / n_samples  # (n_features × n_features)\n\n# 3. Eigendecomposition\neigenvalues, eigenvectors = np.linalg.eig(C)\n\n# 4. Select top k eigenvectors (principal components)\nW = eigenvectors[:, :k]  # (n_features × k)\n\n# 5. Project data\nZ = X_centered @ W  # (n_samples × k)\n\n# 6. Reconstruct\nX_recon = Z @ W.T + X.mean(axis=0)\n```\n\n**Linear Transformation**:\n```\nEncoding: z = W^T x (linear projection)\nDecoding: x_recon = W z + μ (linear reconstruction)\n\nW: Linear transformation matrix\nNo nonlinearity!\n```\n\n**Linear Autoencoder (Equivalent to PCA)**:\n\n```python\nclass LinearAutoencoder(nn.Module):\n    def __init__(self, input_dim=784, latent_dim=50):\n        self.encoder = nn.Linear(input_dim, latent_dim, bias=False)\n        self.decoder = nn.Linear(latent_dim, input_dim, bias=False)\n    \n    def forward(self, x):\n        z = self.encoder(x)  # Linear projection\n        x_recon = self.decoder(z)  # Linear reconstruction\n        return x_recon\n\n# Trained with MSE loss → learns PCA subspace!\n```\n\n**Theorem**: Linear autoencoder trained with MSE loss learns the same subspace as PCA (up to rotation).\n\n**Nonlinear Autoencoder**:\n\n```python\nclass NonlinearAutoencoder(nn.Module):\n    def __init__(self, input_dim=784, latent_dim=50):\n        self.encoder = nn.Sequential(\n            nn.Linear(input_dim, 256),\n            nn.ReLU(),                    # Nonlinearity!\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, latent_dim)\n        )\n        self.decoder = nn.Sequential(\n            nn.Linear(latent_dim, 128),\n            nn.ReLU(),                    # Nonlinearity!\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_dim)\n        )\n    \n    def forward(self, x):\n        z = self.encoder(x)  # Nonlinear transformation\n        x_recon = self.decoder(z)  # Nonlinear reconstruction\n        return x_recon\n\n# Can learn curved manifolds that PCA cannot!\n```\n\n**Key Differences**:\n\n**1. Expressiveness**:\n\n**PCA (Linear)**:\n```\nData lies on curved manifold:\n🌙 C-shaped clusters\n\nPCA finds best linear subspace:\n│ (straight line/plane)\n│\n│  🌙  ← Projects onto line\n│\n\nResult: Loses curvature, clusters overlap\n```\n\n**Nonlinear Autoencoder**:\n```\nData lies on curved manifold:\n🌙 C-shaped clusters\n\nAutoencoder learns curved mapping:\n↗↗↗\n   🌙  ← Follows curve\n↘↘↘\n\nResult: Preserves curvature, clusters separate\n```\n\n**Concrete Example - Swiss Roll Dataset**:\n\n```python\n# 3D Swiss roll → 2D\nfrom sklearn.datasets import make_swiss_roll\nX, color = make_swiss_roll(n_samples=1000)\n\n# PCA (linear projection)\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nZ_pca = pca.fit_transform(X)\n\n# Result: Folds of swiss roll collapse\n# Points from different layers overlap!\n# Reconstruction error: High\n\n# Autoencoder (nonlinear)\nautoencoder = NonlinearAutoencoder(input_dim=3, latent_dim=2)\nautoencoder.train(X)\nZ_ae = autoencoder.encode(X)\n\n# Result: Unrolls swiss roll\n# Points from different layers separated\n# Reconstruction error: Low\n```\n\n**2. Training**:\n\n**PCA**:\n```python\n# Closed-form solution (eigendecomposition)\nC = X.T @ X / n\neigenvalues, W = np.linalg.eig(C)\n\n# Pros:\n- No hyperparameters (learning rate, etc.)\n- Deterministic (same result every time)\n- Fast (O(d³) for covariance, O(d²k) for projection)\n- No local minima\n\n# Cons:\n- Doesn't scale to very high dimensions (d > 10,000)\n- Memory: O(d²) for covariance matrix\n```\n\n**Autoencoder**:\n```python\n# Iterative optimization (gradient descent)\nfor epoch in range(num_epochs):\n    for x_batch in dataloader:\n        x_recon = model(x_batch)\n        loss = MSELoss(x_recon, x_batch)\n        loss.backward()\n        optimizer.step()\n\n# Pros:\n- Scales to high dimensions (d = millions)\n- Memory: O(d·k) for weight matrices\n- Mini-batch training (constant memory)\n\n# Cons:\n- Hyperparameters: LR, architecture, epochs\n- Non-deterministic (different runs → different results)\n- Slower training (minutes to hours vs seconds)\n- Local minima (may not find global optimum)\n```\n\n**3. Interpretability**:\n\n**PCA**:\n```\nPrincipal components = eigenvectors of covariance\n\n# Can visualize and interpret\nPC1: 40% variance (dominant pattern)\nPC2: 25% variance (second pattern)\nPC3: 15% variance (third pattern)\n\n# Each PC is a weighted combination of original features\nPC1 = 0.5·feature1 + 0.3·feature2 - 0.2·feature3 + ...\n→ Can understand what each PC represents\n```\n\n**Autoencoder**:\n```\nLatent dimensions = learned nonlinear features\n\n# Black box (hard to interpret)\nLatent dim 0: ??? (complex combination)\nLatent dim 1: ??? (nonlinear transform)\n\n# Cannot easily describe what each dimension captures\n→ Requires visualization/probing to understand\n```\n\n**4. Dimensionality Reduction Performance**:\n\n**Reconstruction Error Comparison** (MNIST):\n\n```python\n# Reduce 784D → 50D → 784D\n\n# PCA\npca = PCA(n_components=50)\nZ_pca = pca.fit_transform(X)\nX_recon_pca = pca.inverse_transform(Z_pca)\nrecon_error_pca = np.mean((X - X_recon_pca)**2)\n# Result: 0.012 (moderate error, loses fine details)\n\n# Linear Autoencoder\nae_linear = LinearAutoencoder(784, 50)\nae_linear.train(X)\nX_recon_ae_linear = ae_linear(X)\nrecon_error_ae_linear = np.mean((X - X_recon_ae_linear)**2)\n# Result: 0.012 (same as PCA - learns same subspace!)\n\n# Nonlinear Autoencoder\nae_nonlinear = NonlinearAutoencoder(784, 50)\nae_nonlinear.train(X)\nX_recon_ae_nonlinear = ae_nonlinear(X)\nrecon_error_ae_nonlinear = np.mean((X - X_recon_ae_nonlinear)**2)\n# Result: 0.006 (lower error - captures nonlinear structure!)\n```\n\n**5. When to Use Each**:\n\n**Use PCA When**:\n```\n✓ Data is approximately linear\n✓ Need fast, deterministic solution\n✓ Want interpretable components\n✓ Small to medium dimensionality (d < 10,000)\n✓ Limited computational resources\n✓ Need mathematical guarantees (orthogonal, max variance)\n\nExamples:\n- Financial data (stock prices correlate linearly)\n- Simple image compression (natural images have linear structure)\n- Exploratory data analysis (quick first pass)\n```\n\n**Use Autoencoder When**:\n```\n✓ Data lies on nonlinear manifold\n✓ High dimensionality (d > 10,000)\n✓ Need better reconstruction (worth extra compute)\n✓ Want to learn task-specific features\n✓ Have computational resources (GPU)\n✓ Can tolerate longer training time\n\nExamples:\n- Complex images (faces, scenes - highly nonlinear)\n- Anomaly detection (learn normal manifold)\n- Generative modeling (VAE, etc.)\n- Pretraining for downstream tasks\n```\n\n**Hybrid Approach**:\n```python\n# Use PCA for initial dimensionality reduction\n# Then autoencoder for fine-grained learning\n\n# Step 1: PCA (10,000D → 500D)\npca = PCA(n_components=500)\nZ_pca = pca.fit_transform(X)  # Fast, removes noise\n\n# Step 2: Autoencoder (500D → 50D)\nautoencoder = NonlinearAutoencoder(500, 50)\nautoencoder.train(Z_pca)  # Smaller input, faster training\n\n# Best of both worlds:\n- Fast initial reduction (PCA)\n- Expressive final embedding (autoencoder)\n```\n\n**Advantages Summary**:\n\n**PCA**:\n+ Fast (closed-form solution)\n+ Deterministic\n+ Interpretable\n+ No hyperparameters\n+ Theoretical guarantees\n- Only linear\n- Doesn't scale to very high dimensions\n\n**Autoencoder**:\n+ Nonlinear (expressive)\n+ Scales to high dimensions\n+ Better reconstruction\n+ Flexible architecture\n+ Can be task-specific\n- Slower training\n- Hyperparameter tuning\n- Less interpretable\n- Local minima\n\n**Key Insight**: PCA is a special case of linear autoencoders. Nonlinear autoencoders generalize PCA to learn curved manifolds, achieving better reconstruction at the cost of interpretability and computational expense.",
    keyPoints: [],
  },
];
