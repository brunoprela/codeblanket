/**
 * Quiz questions for Message Queues & Async Processing section
 */

export const messagequeuesQuiz = [
  {
    id: 'q1',
    question:
      'Your e-commerce site receives 10K orders/sec during Black Friday, but your order processing service can only handle 1K orders/sec. Orders are currently failing. Design a solution using message queues.',
    sampleAnswer:
      'SOLUTION: MESSAGE QUEUE WITH LOAD LEVELING. PROBLEM ANALYSIS: Without queue: 10K orders/sec → Order processing service (capacity 1K/sec) → 9K orders/sec FAIL (90% failure rate). Users see errors, orders lost. PROPOSED ARCHITECTURE: Clients → Web servers → Message Queue (SQS/Kafka) → Order processing workers. IMPLEMENTATION: STEP 1: INTRODUCE MESSAGE QUEUE. When order received: Web server validates order (fast, <10ms). Web server sends message to queue: {"orderId": 12345, "userId": 789, ...}. Web server responds to user immediately: "Order received! Processing...". No waiting for order processing. STEP 2: CONFIGURE QUEUE. Queue type: AWS SQS (managed, no ops overhead). Queue capacity: Unlimited (can buffer millions of messages). Message retention: 4 days (if processing delayed). Delivery: At-least-once (no orders lost). STEP 3: ORDER PROCESSING WORKERS. Deploy workers to consume from queue. Workers: 10 instances, each handles 100 orders/sec = 1K orders/sec total. Workers: Pull messages from queue at their own pace. Workers: Process order (validate payment, update inventory, send confirmation email). Workers: Acknowledge message after successful processing. STEP 4: HANDLE PROCESSING. Flow: (1) Queue receives 10K orders/sec (no problem, just buffers). (2) Queue depth grows to 100K messages (10K/sec incoming - 1K/sec outgoing = 9K/sec accumulation). (3) Workers consume at 1K/sec. (4) After spike ends (e.g., 1 minute of 10K/sec), queue has 540K messages (9K/sec × 60sec = 540K). (5) Workers drain queue at 1K/sec: 540K messages / 1K per sec = 540 seconds = 9 minutes. Result: All orders processed within 9 minutes (acceptable for most e-commerce). BENEFITS: User experience: Instant response ("Order received"). No failures: Queue buffers all orders. Resilience: If workers crash, messages stay in queue (retry). Scalability: Can add more workers if queue depth too high. SCALING STRATEGY: Monitor queue depth in real-time. If queue depth > 100K: Auto-scale workers (add 10 more instances → 2K orders/sec). If queue depth < 10K: Scale down workers (save costs). Target: Keep processing within 5 minutes. FAILURE HANDLING: Worker crashes: Message not acknowledged → Returns to queue → Retried by another worker. Payment fails: Retry 3 times with exponential backoff (1s, 2s, 4s). After 3 failures: Move to Dead Letter Queue (DLQ). DLQ: Operations team reviews failed orders manually. IDEMPOTENCY: Problem: Message redelivered if worker crashes after processing but before acknowledging. Solution: Track processed order IDs in database. Before processing order: Check if order ID already processed. If yes, skip (already done). If no, process and mark as done. Result: Duplicate messages don\'t result in duplicate orders. MONITORING: Metrics: (1) Queue depth (messages in queue). (2) Message age (oldest message in queue). (3) Consumer lag (how far behind are workers). (4) Processing rate (orders/sec). (5) DLQ size (failed orders). Alerts: Queue depth > 500K (workers too slow). Message age > 1 hour (processing delayed). DLQ size > 100 (many failures). COST ANALYSIS: AWS SQS: $0.40 per million messages. Black Friday: 10K orders/sec × 3600 sec (1 hour) = 36M messages. Cost: 36M × $0.40 / 1M = $14.40 (cheap!). Workers: 10 × $0.10/hour (spot instances) = $1/hour. Total: $15.40 to handle 36M orders (negligible vs revenue). ALTERNATIVE CONSIDERED: Synchronous processing with more servers: Would need 10K orders/sec capacity = 100 instances running 24/7. Cost: 100 × $0.10/hour × 24 hours × 30 days = $7,200/month. Verdict: Queue approach much cheaper (workers scale down after spike). REAL-WORLD EXAMPLE: Amazon uses SQS to buffer orders during Prime Day. Queue absorbs traffic spikes (millions of orders). Workers process orders asynchronously. Users get instant confirmation, orders processed within minutes. FINAL RECOMMENDATION: Use AWS SQS (managed, reliable, cheap). Buffer orders in queue during spike. Workers consume at sustainable rate (1K/sec). Auto-scale workers based on queue depth. Implement idempotency (prevent duplicate orders). Monitor queue metrics and alert. Expected outcome: 100% orders processed, Zero failures, 9-minute processing time (acceptable).',
    keyPoints: [
      'Message queue buffers traffic spikes (10K orders/sec → queue → 1K orders/sec processing)',
      'Instant user response ("Order received"), processing happens asynchronously',
      'Auto-scale workers based on queue depth to reduce processing time',
      'Implement idempotency to handle message redelivery (track processed order IDs)',
      'Monitor queue depth and message age to detect processing delays',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the difference between a Queue (point-to-point) and a Topic (pub/sub). Give a real-world scenario where you would use each.',
    sampleAnswer:
      'QUEUE (POINT-TO-POINT) VS TOPIC (PUB/SUB): QUEUE (POINT-TO-POINT): Model: One producer → Queue → One consumer (or consumer group). Each message consumed by exactly ONE consumer. Once consumed, message deleted from queue. Multiple consumers compete for messages (load distribution). EXAMPLE ARCHITECTURE: Producer: Web server sends messages. Queue: "email_tasks". Consumers: 5 email worker instances. Message: "Send welcome email to user@example.com". Flow: (1) Producer sends message to queue. (2) Consumer 1 pulls message from queue. (3) Consumer 1 processes message (sends email). (4) Consumer 1 acknowledges → Message deleted from queue. (5) Other consumers (2-5) never see this message. Result: Email sent exactly once (by one worker). USE CASE: TASK DISTRIBUTION (BACKGROUND JOBS). REAL-WORLD SCENARIO 1: IMAGE PROCESSING PIPELINE. Problem: Users upload images. Need to: resize image, generate thumbnails, optimize quality. Solution: Queue-based task distribution. Architecture: User uploads image → API server → Queue: "image_processing_tasks". Queue contains message: {"imageId": 123, "action": "process"}. 10 worker instances pull messages from queue. Each worker: Downloads image, processes it, uploads result, acknowledges message. Why queue (not topic)? Each image processed by ONE worker (avoid duplicate work). Load distributed evenly across 10 workers. Simple task distribution. TOPIC (PUBLISH/SUBSCRIBE): Model: One producer → Topic → Multiple subscribers. Each message delivered to ALL subscribers. Message not deleted until all subscribers consume it. Subscribers process message independently. EXAMPLE ARCHITECTURE: Producer: Payment service publishes messages. Topic: "payment_completed". Subscribers: (1) Email service (sends receipt). (2) Analytics service (tracks revenue). (3) Inventory service (updates stock). (4) Fraud detection service (analyzes transaction). Message: {"orderId": 456, "amount": 99.99, "userId": 789}. Flow: (1) Producer publishes message to topic. (2) Message delivered to ALL 4 subscribers simultaneously. (3) Each subscriber processes message independently. (4) Email service sends receipt. (5) Analytics service logs revenue. (6) Inventory service decrements stock. (7) Fraud detection analyzes transaction. Result: All 4 actions happen (fan-out). USE CASE: EVENT BROADCASTING. REAL-WORLD SCENARIO 2: ORDER COMPLETION EVENT. Problem: When order completed, need to: (1) Send confirmation email to customer. (2) Send notification to seller. (3) Update analytics dashboard. (4) Trigger inventory restock if low. (5) Log to audit system. Solution: Topic-based pub/sub. Architecture: Order service publishes "OrderCompleted" event to topic. 5 services subscribe to topic. Each service receives event and acts independently. Why topic (not queue)? Multiple actions needed for same event. Services independent (email service down doesn\'t affect analytics). Easy to add new subscribers later (no code change to producer). COMPARISON TABLE: Aspect / Queue / Topic: Message delivery: One consumer / All subscribers. Message deletion: After consumed / After all consume. Use case: Task distribution / Event broadcasting. Consumers: Compete for messages / Each gets copy. Example: Background jobs / Notifications. WHEN TO USE QUEUE: Task distribution: Process 1000 emails (each sent once). Load balancing: 10 workers share workload. No duplication: Each task done exactly once. Example: Video encoding, data processing, email sending. WHEN TO USE TOPIC: Event notification: Payment completed → Notify multiple services. Fan-out: One event, multiple actions. Loosely coupled services: Subscribers independent. Example: User registration, order events, system alerts. HYBRID ARCHITECTURE (QUEUE + TOPIC): Many systems use both. Example: E-commerce order flow. STEP 1: TOPIC for order created event. Producer: Order service publishes "OrderCreated" to topic. Subscribers: Email service, Analytics service, Fraud detection service. Each service receives event independently. STEP 2: QUEUE for order processing tasks. After fraud check passes: Fraud service sends message to "order_fulfillment_queue". 10 fulfillment workers compete for messages. Each order fulfilled by one worker. Why hybrid? Order creation is an EVENT (multiple services need to know) → Topic. Order fulfillment is a TASK (done once by one worker) → Queue. KAFKA EXAMPLE: Kafka Topics: Act like pub/sub (multiple consumer groups). Kafka Consumer Groups: Act like queue (within group, one consumer per message). Kafka combines both models! Topic: "user_signups". Consumer Group 1 (Email service): One consumer in group processes message. Consumer Group 2 (Analytics service): One consumer in group processes same message. Result: Message delivered to both groups, but only one consumer per group. REAL-WORLD EXAMPLES: Queue: Netflix: Video encoding tasks (one worker per video). Uber: Ride matching tasks (one matcher per ride). Topic: Slack: Message events (multiple services notified). Twitter: Tweet events (timeline, notifications, analytics). MISTAKES TO AVOID: Using queue for events: If multiple actions needed, use topic (not queue). Example: Don\'t send order event to email queue and analytics queue separately (use one topic with 2 subscribers). Using topic for tasks: If only one action needed, use queue (not topic). Example: Don\'t use topic for "process image" task (wastes resources if multiple services process same image). FINAL RECOMMENDATION: Queue: For task distribution, background jobs, load balancing. Topic: For event broadcasting, notifications, loosely coupled services. Hybrid: Use both when system has events (topic) and tasks (queue).',
    keyPoints: [
      'Queue: each message consumed by ONE consumer (task distribution, background jobs)',
      'Topic: each message delivered to ALL subscribers (event broadcasting, notifications)',
      'Use queue for: image processing, email sending (task done once)',
      'Use topic for: order events, payment completed (multiple services notified)',
      'Hybrid: Use both (topic for events, queue for tasks) in same system',
    ],
  },
  {
    id: 'q3',
    question:
      'Your message consumer processes payments. Due to a bug, processing fails for all messages. They keep retrying, filling the queue. How would you handle this situation?',
    sampleAnswer:
      'HANDLING FAILING MESSAGE CONSUMER (POISON PILL SCENARIO): SCENARIO: Bug in payment consumer code (e.g., NullPointerException). Consumer tries to process message → Fails → Message returned to queue → Retry → Fails → Retry... Loop continues forever. Queue fills up with retried messages. New messages can\'t be processed (queue full). System effectively down. IMMEDIATE ACTIONS (INCIDENT RESPONSE): ACTION 1: STOP THE CONSUMERS (CIRCUIT BREAKER). Immediately stop all consumer instances. Why: Prevent infinite retry loop, stop queue from filling further. How: Kill consumer processes or set consumer count to 0 in auto-scaling group. Result: Messages stay in queue (safe), no more retries (queue stabilizes). ACTION 2: INVESTIGATE THE ISSUE. Examine logs: Find error message/stack trace. Identify root cause: NullPointerException in payment processing code. Example: Code assumed field always present, but message missing field. Reproduce: Test with sample message locally. ACTION 3: APPLY IMMEDIATE FIX. If simple fix: Deploy hotfix (e.g., add null check). If complex: Implement temporary workaround (e.g., skip invalid messages). Deploy fix to staging, test thoroughly. Deploy fix to production. ACTION 4: RESTART CONSUMERS. Start consumer instances again. Monitor closely: Check if messages processing successfully. Watch queue depth: Should decrease steadily. Watch error rate: Should be 0% (or very low). LONG-TERM SOLUTIONS (PREVENTING RECURRENCE): SOLUTION 1: DEAD LETTER QUEUE (DLQ). Configure DLQ for failed messages. Max retries: 3 attempts. Backoff: Exponential (1s, 2s, 4s). After 3 failures: Move message to DLQ (stop retrying). Benefits: Poison pill messages don\'t block queue. DLQ for manual inspection and fixing. Configuration: Main Queue: "payments_queue" (max retries: 3). DLQ: "payments_dlq" (holds failed messages). Workflow: (1) Consumer fails to process message. (2) Message returned to queue (attempt 1). (3) Consumer fails again (attempt 2). (4) Consumer fails again (attempt 3). (5) Message moved to DLQ (stop retrying). (6) Alert operations team: "DLQ size increased". (7) Team inspects DLQ, identifies issue, fixes it. (8) Reprocess DLQ messages after fix. SOLUTION 2: MESSAGE VALIDATION. Validate message before processing. If invalid: Log error, move to DLQ immediately (don\'t retry). If valid: Process normally. Code example: def process_payment(message): # Validate message structure. if not message.get("amount") or not message.get("userId"): logger.error(f"Invalid message: {message}"). move_to_dlq(message). return. # Process payment. process_payment_logic(message). Benefits: Invalid messages detected early (don\'t retry). Clear error logging (easier to debug). SOLUTION 3: CIRCUIT BREAKER. If consumer fails N times in a row, stop processing temporarily. Wait T seconds, then retry. If still failing, alert and stop. Code example: consecutive_failures = 0. MAX_FAILURES = 10. def process_message(message): global consecutive_failures. try: process_payment(message). consecutive_failures = 0  # Reset on success. except Exception as e: consecutive_failures += 1. if consecutive_failures >= MAX_FAILURES: logger.critical("Circuit breaker opened! Too many failures."). stop_consumer()  # Stop processing. alert_ops_team(). raise. Benefits: Prevents infinite retry loop. Automatic stop on repeated failures. Operations team alerted quickly. SOLUTION 4: MONITORING & ALERTING. Track consumer metrics: Messages processed/sec (should be steady). Error rate (should be <1%). Queue depth (should be low). DLQ size (should be 0 or small). Consumer lag (time between message sent and processed). Alerts: Error rate > 5% for 5 minutes → Page on-call engineer. DLQ size > 100 → Alert (many failed messages). Queue depth > 10,000 → Warning (consumers too slow or failing). Consumer lag > 1 hour → Warning (processing delayed). Benefits: Early detection of issues. Fast response time (reduce downtime). SOLUTION 5: IDEMPOTENCY. Make consumer idempotent (handle duplicate messages). Track processed message IDs in database. Before processing: Check if message already processed (skip if yes). After processing: Mark message as processed. Why important for retries: If consumer processes payment, then crashes before acknowledging message, message redelivered. Without idempotency: User charged twice (bad!). With idempotency: Second processing skipped (user charged once). SOLUTION 6: GRADUAL ROLLOUT (CANARY DEPLOYMENT). When deploying new consumer code: Deploy to 1 instance first (canary). Monitor canary for 10 minutes (check error rate, queue depth). If healthy: Deploy to 10% of instances, monitor. If still healthy: Deploy to 100%. If any issues: Rollback canary, investigate. Benefits: Catch bugs early (affect small % of traffic). Fast rollback (only 1 instance affected). Prevent large-scale outages. SOLUTION 7: MESSAGE TTL (TIME TO LIVE). Set message expiration: Messages older than 24 hours deleted from queue. Why: If processing delayed significantly (e.g., consumer down for days), old messages may be obsolete. Example: "Send flash sale email" message from 3 days ago (sale over, don\'t send). Configuration: Message TTL: 24 hours. Result: Old messages auto-deleted (prevent processing stale data). RUNBOOK FOR FUTURE INCIDENTS: STEP 1: Detect issue (alert: high error rate or queue depth). STEP 2: Stop consumers (prevent further damage). STEP 3: Investigate logs (find root cause). STEP 4: Apply fix (hotfix or workaround). STEP 5: Test fix in staging. STEP 6: Restart consumers (1 instance first, then all). STEP 7: Monitor closely (error rate, queue depth). STEP 8: Inspect DLQ (reprocess valid messages after fix). STEP 9: Post-mortem (document incident, improve prevention). REAL-WORLD EXAMPLE: AWS: Customer had payment processor bug. Bug caused infinite retries (queue filled up). No DLQ configured (messages retried forever). Solution: Implemented DLQ, max retries = 3. Deployed circuit breaker (stop after 10 consecutive failures). Incident avoided in future deployments. FINAL RECOMMENDATION: Implement DLQ with max retries (3 attempts). Add circuit breaker (stop after 10 consecutive failures). Validate messages before processing (catch invalid data early). Monitor error rate and queue depth (alert on anomalies). Make consumer idempotent (handle duplicate messages). Test consumer code thoroughly before deployment (staging environment). Document runbook for handling consumer failures. Expected outcome: Future bugs caught early (circuit breaker stops processing). Failed messages isolated (DLQ), don\'t block queue. Fast incident resolution (runbook).',
    keyPoints: [
      'Immediate: Stop consumers to prevent infinite retry loop',
      'Dead Letter Queue (DLQ): Move failed messages after N retries (prevent blocking)',
      'Circuit breaker: Stop processing after N consecutive failures (alert ops team)',
      'Validation: Check message structure before processing (catch invalid data early)',
      'Monitoring: Track error rate, queue depth, DLQ size (alert on anomalies)',
    ],
  },
];
