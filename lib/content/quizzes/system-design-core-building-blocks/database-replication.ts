/**
 * Quiz questions for Database Replication section
 */

export const databasereplicationQuiz = [
  {
    id: 'q1',
    question:
      'Your application has high read traffic (100K reads/sec) but low write traffic (1K writes/sec). Users complain about slow response times. You have a single PostgreSQL database. Propose a solution and explain the trade-offs.',
    sampleAnswer:
      'SOLUTION: ADD READ REPLICAS. DIAGNOSIS: High read traffic overwhelming single database: 100K reads/sec is too much for one PostgreSQL instance (typically handles 10-20K reads/sec efficiently). Writes are low (1K writes/sec), so primary can handle write load easily. Bottleneck is reads, not writes. PROPOSED ARCHITECTURE: Primary-Replica Replication: 1 Primary database: Handles ALL writes (1K writes/sec). 10 Read Replicas: Each handles 10K reads/sec. Load balancer: Distributes reads across 10 replicas. Total read capacity: 10 replicas × 10K = 100K reads/sec. IMPLEMENTATION DETAILS: Replication Setup: Configure async replication from Primary to Replicas. Replication lag: Acceptable 1-5 seconds for most apps. Load Balancer: Round-robin or least-connections algorithm. Health checks: Remove lagging replicas if lag >10 seconds. Application Changes: Write queries: Route to Primary. Read queries: Route to Load Balancer → Replicas. BENEFITS: Read scalability: Handles 100K reads/sec (vs 10K before). Response time: Reduced from seconds to milliseconds. High availability: If one replica fails, other 9 still handle traffic. Fault tolerance: If Primary fails, promote replica to Primary. TRADE-OFFS ACCEPTED: TRADE-OFF 1: EVENTUAL CONSISTENCY. Problem: User writes data, immediately reads from replica, data not yet replicated → sees stale data. Impact: User posts comment, refreshes page, comment missing (appears for 1-5 seconds later). Severity: Minor for social media, critical for banking. Mitigation: (1) Read-your-own-writes: After user writes, route their reads to Primary for 5 seconds. (2) Sticky sessions: Route user to same replica (lag affects all users equally). (3) Show "saving..." indicator to set expectations. Decision: Acceptable for most web apps. TRADE-OFF 2: REPLICATION LAG. Problem: Under high load, replicas may lag 10-60 seconds behind Primary. Impact: Users see outdated data (e.g., old inventory counts, stale comments). Monitoring: Track replication lag: lag = Primary_log_position - Replica_log_position. Alert if lag >5 seconds consistently. Mitigation: (1) Remove lagging replicas from load balancer temporarily. (2) Add more replicas (reduce per-replica load). (3) Upgrade replica hardware (faster disk I/O). Decision: Monitor actively, accept occasional lag. TRADE-OFF 3: COMPLEXITY. Added complexity: Multiple databases to maintain. Monitoring: Need to monitor Primary + 10 Replicas. Failover: Need automated failover process if Primary fails. Costs: 11 database instances (Primary + 10 Replicas) vs 1 → 11× cost. Decision: Worth it for 10× read performance improvement. TRADE-OFF 4: WRITE SCALABILITY NOT IMPROVED. Replication helps reads, NOT writes. All writes still go to single Primary (1K writes/sec). If writes increase to 10K writes/sec, Primary becomes bottleneck. Solution for future: Shard database (split by user_id) if writes increase. Current: Not needed (writes are low). COST ANALYSIS: Before: 1 database at $500/month. After: 1 Primary + 10 Replicas at $5,500/month. Increase: 11× cost for 10× read capacity. Per-read cost: Actually cheaper (spreading load). Business value: Fast response times → better user experience → more users. Decision: ROI positive if users value speed. ALTERNATIVE CONSIDERED: CACHING (REDIS). Could we use Redis instead of replicas? Yes, but: Redis: Cache hot data (e.g., top 1000 products). Good for: 80% of reads hit cache → Reduce DB load 80%. Bad for: Remaining 20% still hits database → May still be overloaded. Cache invalidation: Complexity when data changes. Verdict: Use BOTH: Redis cache (reduce load), Read replicas (handle remaining load). REAL-WORLD EXAMPLE: REDDIT. Before: Single PostgreSQL database, slow during peak hours. After: 1 Primary + 50 Read Replicas, Redis cache. Result: Handles 50M+ users, fast response times. FINAL RECOMMENDATION: Add 10 Read Replicas with async replication. Route reads to replicas via load balancer. Implement read-your-own-writes for consistency. Monitor replication lag actively. Add Redis cache for hot data (future optimization). Expected outcome: Response times drop from 2-3 seconds to 50-200ms, Handle 100K reads/sec easily, 10× capacity for growth.',
    keyPoints: [
      'Read-heavy workload: add read replicas (10 replicas for 100K reads/sec)',
      'Primary handles writes, replicas handle reads via load balancer',
      'Trade-off: eventual consistency (replication lag 1-5 seconds)',
      'Mitigation: read-your-own-writes, monitor lag, remove lagging replicas',
      'Cost: 11× infrastructure cost, but 10× read capacity (worth it for UX)',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the difference between synchronous and asynchronous replication. When would you use each? What are the risks of asynchronous replication?',
    sampleAnswer:
      'SYNCHRONOUS VS ASYNCHRONOUS REPLICATION: SYNCHRONOUS REPLICATION: How it works: Write confirmed ONLY after data written to Primary AND at least one Replica. Flow: (1) App writes to Primary. (2) Primary writes to disk. (3) Primary sends data to Replica. (4) Replica writes to disk. (5) Replica sends ACK to Primary. (6) Primary confirms write to App. Timing: Write latency = Primary write + Network round-trip + Replica write. Example: Primary write 5ms + Network 2ms + Replica write 5ms = 12ms total. Guarantees: STRONG CONSISTENCY: Replica always has same data as Primary. NO DATA LOSS: If Primary crashes after confirming write, data exists on Replica. DURABILITY: Data persisted on multiple machines before confirming. WHEN TO USE SYNCHRONOUS: Banking/Financial: Money transfers (data loss = customer money lost). Regulatory compliance: Healthcare records (HIPAA), financial records (SOX). Critical business data: Orders, transactions, user accounts. When: Correctness > Performance. PROS OF SYNCHRONOUS: Data safety: Zero data loss guarantee. Immediate failover: Replica ready to take over instantly. Strong consistency: Reads from replica always return latest data. Predictable: No replication lag. CONS OF SYNCHRONOUS: SLOWER WRITES: 2-3× slower than async (network latency). If Primary → Replica latency is 50ms: Each write takes extra 50ms. At 1000 writes/sec: 50ms per write is noticeable. AVAILABILITY RISK: If Replica is down or slow, writes block or fail. Network partition between Primary and Replica: Writes cannot complete. High availability hurt by synchronous replication. GEOGRAPHIC LIMITATIONS: Cross-region replication (US → EU): 100ms latency. Every write takes extra 100ms (unacceptable for most apps). Synchronous replication works only within same datacenter. ASYNCHRONOUS REPLICATION: How it works: Write confirmed IMMEDIATELY after data written to Primary. Replica updated later (in background). Flow: (1) App writes to Primary. (2) Primary writes to disk. (3) Primary confirms write to App IMMEDIATELY. (4) Background process sends data to Replica (async). (5) Replica eventually writes to disk. Timing: Write latency = Primary write only (5ms). Replication happens in background (user doesn\'t wait). Example: Primary write 5ms → User gets confirmation. Replica catches up 1-5 seconds later (user doesn\'t notice). Guarantees: EVENTUAL CONSISTENCY: Replica will eventually have data (not immediately). DATA LOSS POSSIBLE: If Primary crashes before replicating, recent writes lost. FAST WRITES: No wait for Replica. WHEN TO USE ASYNCHRONOUS: Most web applications: Social media (Twitter, Facebook, Instagram). E-commerce (Amazon, eBay). SaaS applications. When: Performance > Absolute consistency. Non-critical data: User preferences, caching, analytics. Read-heavy workloads: Replicas handle reads (slight staleness OK). Cross-region replication: Geographic distribution (async required for reasonable performance). PROS OF ASYNCHRONOUS: FAST WRITES: No replication overhead (2-3× faster than sync). PRIMARY AVAILABILITY: Writes succeed even if Replica down. CROSS-REGION: Works globally (US → EU replication feasible). SIMPLE: No complex coordination between nodes. CONS OF ASYNCHRONOUS: DATA LOSS RISK: If Primary fails, recent writes (last 1-5 seconds) lost. Example: User posts comment, Primary crashes 2 seconds later (before replicating) → Comment lost. Mitigation: Use battery-backed write cache, frequent checkpoints. REPLICATION LAG: Replica may be 1-5 seconds behind (or minutes under high load). User posts comment, refreshes, comment missing (lag) → Confusing UX. Mitigation: Read-your-own-writes pattern. STALE READS: Reading from Replica returns old data. Example: User updates profile, reads from Replica, sees old profile. Mitigation: Route reads to Primary for short time after write. UNPREDICTABLE: Replication lag varies with load. Normal: 1-5 seconds. High load: 10-60 seconds. Mitigation: Monitor lag, remove lagging Replicas from rotation. COMPARISON TABLE: Aspect / Synchronous / Asynchronous: Write Latency: Slow (2-3× baseline) / Fast (baseline). Data Loss Risk: None (guaranteed safe) / Possible (last few seconds). Consistency: Strong (Replica = Primary) / Eventual (Replica lags). Availability: Lower (Replica failure affects writes) / Higher (writes succeed even if Replica down). Geographic: Same datacenter only / Works cross-region. Use Case: Banking, critical data / Most web apps. RISKS OF ASYNCHRONOUS REPLICATION: RISK 1: DATA LOSS. Scenario: User submits form, Primary confirms, crashes before replicating. Result: Form data lost (user needs to resubmit). Frequency: Rare (Primary failures uncommon), but impactful. Mitigation: (1) Use semi-sync for critical tables (wait for 1 Replica). (2) Frequent checkpoints (reduce window of data loss). (3) Accept risk for non-critical data. RISK 2: USER CONFUSION FROM LAG. Scenario: User posts comment, refreshes page, comment missing (lag 3 seconds). Result: User thinks comment failed, posts again (duplicate). Mitigation: (1) Read-your-own-writes: Route user\'s reads to Primary for 5-10 seconds after write. (2) Show "saving..." then "saved" indicator (set expectations). (3) Sticky sessions: Route user to same Replica (lag affects everyone equally). RISK 3: REPLICATION LAG SPIKE. Scenario: Traffic spike → Primary overloaded → Replication slows → Lag increases to 60 seconds. Result: Reads return very stale data (1 minute old). Mitigation: (1) Monitor lag: Alert if >10 seconds. (2) Remove lagging Replicas from load balancer. (3) Auto-scaling: Add more Replicas when lag increases. RISK 4: FAILOVER DATA LOSS. Scenario: Primary crashes, Replica has not caught up → Lost writes. Example: Primary fails, Replica is 5 seconds behind → Last 5 seconds of writes lost (could be hundreds or thousands of transactions). Mitigation: (1) Use semi-sync replication (at least 1 Replica always up-to-date). (2) Accept data loss for non-critical systems. (3) Application-level tracking: Retry failed writes. SEMI-SYNCHRONOUS REPLICATION (MIDDLE GROUND): How it works: Wait for at least ONE Replica to confirm (not all). Flow: Primary → Replica1 (wait for ACK) + Replica2, 3, 4... (async). Benefits: Faster than full sync (wait for 1 Replica, not all). Safer than async (at least 1 Replica has data). Trade-off: 1 Replica may still lag (eventual consistency for reads from lagging Replicas). Use case: Production systems needing balance of speed and safety. REAL-WORLD EXAMPLES: SYNCHRONOUS: Banking: CitiBank, Chase (financial transactions). Healthcare: Epic, Cerner (patient records). ASYNCHRONOUS: Social Media: Facebook, Twitter, Instagram (posts, likes, comments). E-commerce: Amazon (product browsing, reviews). SaaS: Slack, Salesforce (messages, CRM data). SEMI-SYNC: MySQL semi-sync: Wait for 1 Replica in same datacenter, async to cross-region Replicas. PostgreSQL synchronous_commit = remote_apply: Similar to semi-sync. FINAL RECOMMENDATION: DEFAULT: Asynchronous replication (fast, works globally, suitable for most apps). UPGRADE: Semi-sync for critical data (balance speed and safety). SPECIAL: Sync only for absolutely critical data (banking, financial, compliance). Trade-off awareness: Understand consistency vs performance trade-off. Monitor always: Track replication lag regardless of mode.',
    keyPoints: [
      'Synchronous: wait for replica ACK (slow, strong consistency, no data loss)',
      'Asynchronous: confirm immediately (fast, eventual consistency, data loss risk)',
      'Use sync for: banking, financial, critical data (correctness > performance)',
      'Use async for: most web apps, social media, e-commerce (performance > immediate consistency)',
      'Risk mitigation: monitor lag, read-your-own-writes, semi-sync for balance',
    ],
  },
  {
    id: 'q3',
    question:
      'Your primary database just crashed. Walk through the failover process step-by-step. What challenges might you encounter? How do you prevent split-brain?',
    sampleAnswer:
      "FAILOVER PROCESS - STEP BY STEP: STEP 1: DETECTION (10-30 seconds). What happens: Monitoring system detects Primary is down. Detection methods: (1) Heartbeat failure: Primary stops sending heartbeat (every 1-2 seconds). (2) Health check timeout: HTTP /health endpoint not responding. (3) Connection failure: Cannot establish TCP connection to Primary. (4) Quorum: Majority of nodes agree Primary is down. Timing: Detection threshold: 3 consecutive failures (to avoid false positives). If heartbeat every 2 seconds, 3 failures = 6 seconds minimum. Add network delays, consensus: 10-30 seconds total. Challenges: False positive: Network glitch makes Primary appear down (but it's not). Solution: Require multiple failed health checks before declaring Primary down. Slow detection: Longer detection = longer downtime. Solution: Reduce heartbeat interval (but increases network overhead). STEP 2: CHOOSE NEW PRIMARY (ELECTION) (5-10 seconds). What happens: Select which Replica to promote to new Primary. Selection criteria: (1) Most up-to-date Replica (highest log position). (2) Lowest replication lag. (3) Geographic location (same datacenter as old Primary preferred). (4) Manual preference (priority weights). Algorithm: Raft consensus or Paxos for distributed election. MongoDB: Replica set election (highest priority, most recent oplog). MySQL: Manual selection or automated (Orchestrator, ProxySQL). Timing: Consensus algorithm: 5-10 seconds for cluster to agree. Challenges: No clear winner: Multiple Replicas have same log position. Solution: Use tie-breaker (node ID, datacenter, priority). Network partition: Split-brain risk (see below). STEP 3: PROMOTE REPLICA TO PRIMARY (10-20 seconds). What happens: Selected Replica transitions from read-only to read-write mode. Actions: (1) Stop replication: Replica stops receiving updates from old Primary. (2) Enable writes: Change database configuration to accept writes. (3) Update metadata: Mark node as Primary in cluster state. (4) Start replication: New Primary starts sending updates to other Replicas. Timing: Configuration change + restart: 10-20 seconds. Challenges: Incomplete replication: New Primary may be missing recent writes (if async replication). Example: Old Primary had 1000 writes, Replica only received 990 → 10 writes lost. Mitigation: Use semi-sync replication (at least 1 Replica always up-to-date). Accept data loss for async replication. STEP 4: RECONFIGURE APPLICATION (10-30 seconds). What happens: Update application to write to new Primary instead of old Primary. Methods: (1) DNS update: Change DNS entry to point to new Primary IP. (2) Load balancer: Update load balancer config to route to new Primary. (3) Service discovery: Update service registry (Consul, etcd, ZooKeeper). (4) Connection pool refresh: Application refreshes database connections. Timing: DNS: 30-60 seconds (DNS TTL). Load balancer: 5-10 seconds. Service discovery: 5-10 seconds (fastest). Challenges: DNS caching: Clients may cache old DNS (stale) for minutes. Solution: Low DNS TTL (10-30 seconds), but increases DNS query load. Connection pool: Existing connections to old Primary need to close/reconnect. Solution: Connection pool health checks detect failed connections quickly. Split writes: Some app instances write to old Primary, others to new Primary → Data divergence! Solution: Fence old Primary (make it unable to accept writes). STEP 5: FENCE OLD PRIMARY (PREVENT SPLIT-BRAIN) (immediate). What happens: Ensure old Primary cannot accept writes after being declared dead. Fencing methods: (1) STONITH (Shoot The Other Node In The Head): Physically power off old Primary via remote management (IPMI). (2) Network isolation: Block old Primary\'s network access. (3) Kill process: Force-stop database process on old Primary. (4) Revoke access: Remove old Primary's write permissions at storage level. (5) Epoch numbers: Use epoch/term numbers to reject writes from old Primary. Why critical: Prevents split-brain scenario (see below). Timing: Should happen BEFORE or simultaneously with promotion. Challenges: Old Primary may be network-isolated (can't reach it to fence). Solution: Fencing must be enforceable without network access (STONITH). STEP 6: OLD PRIMARY RECOVERY (when it comes back). What happens: Old Primary recovers, needs to rejoin as Replica. Actions: (1) Detect state: Old Primary discovers it's no longer Primary. (2) Discard divergent writes: Rollback any writes accepted during isolation (if split-brain occurred). (3) Resync data: Catch up with new Primary. (4) Join as Replica: Start replicating from new Primary. Timing: Data resync: Depends on divergence (minutes to hours for large divergence). Challenges: Data conflict: Old Primary accepted writes during split-brain. Solution: Discard old Primary\'s writes (data loss), or manual reconciliation. TOTAL FAILOVER TIME: Detection: 10-30 seconds. Election: 5-10 seconds. Promotion: 10-20 seconds. Reconfiguration: 10-30 seconds. Total: 35-90 seconds downtime (automated failover). Manual failover: Minutes to hours (human intervention). CHALLENGES ENCOUNTERED: CHALLENGE 1: SPLIT-BRAIN. Problem: Network partition isolates old Primary from cluster, but old Primary is still running and accepting writes. Scenario: (1) Network partition: Old Primary can't reach cluster. (2) Cluster thinks Primary is down, promotes Replica to new Primary. (3) Old Primary doesn't know it was demoted, keeps accepting writes. (4) Now TWO Primaries accepting writes → DATA DIVERGENCE! Impact: Write 1 goes to old Primary: User A updates account balance to $100. Write 2 goes to new Primary: User B updates account balance to $200. When network heals: Which is correct? $100 or $200? Both? Neither? Data integrity compromised. SOLUTION: FENCING (Prevent old Primary from accepting writes). PREVENTING SPLIT-BRAIN: SOLUTION 1: FENCING WITH STONITH. How: Use remote management to physically power off old Primary. Example: IPMI, iLO, iDRAC (out-of-band management). Pros: Guaranteed to work (physical power off). Cons: Requires special hardware support. SOLUTION 2: QUORUM WITH ODD NUMBER OF NODES. How: Require majority (quorum) to accept writes. Example: 5-node cluster, need 3 nodes to agree (majority). If network partition: 3 nodes on one side, 2 on other. Side with 3 nodes can accept writes (has quorum). Side with 2 nodes cannot accept writes (no quorum). Result: Only one side can be Primary. Pros: Mathematically prevents split-brain. Cons: Requires odd number of nodes (3, 5, 7). SOLUTION 3: EPOCH NUMBERS (FENCING TOKENS). How: Each Primary term has an epoch number (increments on each failover). New Primary: epoch = N + 1. Old Primary: epoch = N (stale). Storage layer: Only accept writes from highest epoch. Result: Old Primary\'s writes rejected (epoch too old). Pros: Elegant, no physical fencing needed. Cons: Requires storage layer support. SOLUTION 4: WITNESS NODE (TIE-BREAKER). How: Deploy lightweight witness node (doesn't store data, just participates in quorum). Example: 2 datacenters with 1 database each + 1 witness. DC1: Primary + Witness (2 nodes). DC2: Replica (1 node). If DC1 Primary fails: DC2 Replica can't get quorum (1 out of 3) → Can't promote. If DC2 Replica fails: DC1 Primary keeps working (2 out of 3 quorum). Result: Prevents split-brain with even number of databases. REAL-WORLD EXAMPLES: MONGODB REPLICA SETS: Automatic failover: Election takes 10-12 seconds. Fencing: Quorum-based (majority must agree). Split-brain prevention: Odd number of nodes required (3, 5, 7). MYSQL WITH ORCHESTRATOR: Automatic failover: Detection + promotion + reconfiguration = 30-60 seconds. Fencing: Optional STONITH via API. Split-brain prevention: Manual (administrator must ensure old Primary stopped). POSTGRESQL WITH PATRONI: Automatic failover: Uses etcd/Consul for consensus, ~30 seconds. Fencing: Watchdog timer (old Primary auto-terminates if can't reach etcd). Split-brain prevention: Distributed lock in etcd. CASSANDRA: No failover needed: Multi-master (no single Primary). Every node can accept writes. Split-brain: Not applicable (eventual consistency, no Primary). BEST PRACTICES FOR FAILOVER: (1) Automate failover: Human reaction time too slow (minutes). (2) Test regularly: Chaos engineering (simulate failures monthly). (3) Monitor actively: Track replication lag, health checks. (4) Use quorum: Odd number of nodes prevents split-brain. (5) Implement fencing: STONITH or epoch numbers mandatory. (6) Document runbooks: Manual failover procedure if automation fails. (7) Accept data loss: If using async replication, last few seconds may be lost. FINAL RECOMMENDATION: Use automated failover with quorum-based consensus. Implement fencing (STONITH or epoch numbers) to prevent split-brain. Monitor failover time (alert if >90 seconds). Test failover monthly (chaos engineering). Accept 35-90 seconds downtime during failover (better than manual hours). Document manual failover procedure as backup.",
    keyPoints: [
      'Failover steps: Detect (10-30s) → Elect (5-10s) → Promote (10-20s) → Reconfigure (10-30s)',
      'Total downtime: 35-90 seconds for automated failover',
      'Split-brain: Two primaries accepting writes after network partition (data divergence)',
      'Prevention: Fencing (STONITH, quorum, epoch numbers) to ensure only one primary',
      'Best practice: Automated failover + quorum + fencing + regular testing',
    ],
  },
];
