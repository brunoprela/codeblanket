/**
 * Quiz questions for Load Balancing section
 */

export const loadbalancingQuiz = [
  {
    id: 'q1',
    question:
      "You're designing a system where user sessions contain large amounts of state stored in server memory. The load balancer is currently using Round Robin. Users complain they're randomly logged out. Explain what's happening and propose two solutions with trade-offs.",
    sampleAnswer:
      "PROBLEM DIAGNOSIS: Users are being logged out because Round Robin distributes each request randomly across servers. When User A logs in on Server 1, their session is stored in Server 1's memory. The next request from User A might go to Server 2, which doesn't have their session data, making it appear they're logged out. This is the classic session persistence problem. SOLUTION 1: STICKY SESSIONS (Quick Fix): Configure load balancer to use IP Hash or cookie-based stickiness. Each user consistently routed to same server. Implementation: Set cookie on first request, use that cookie to route all subsequent requests to same server. PROS: (1) Quick to implement, no architecture changes. (2) Session data stays in memory (fast). CONS: (1) If server crashes, all sessions on that server are lost (users logged out). (2) Uneven load distribution (popular users might all hash to same server). (3) Doesn't scale well (can't easily add/remove servers). (4) Prevents true stateless design. When to use: Temporary fix, legacy systems, low-traffic applications. SOLUTION 2: SHARED SESSION STORAGE (Recommended): Store sessions in Redis or database instead of server memory. Any server can retrieve any session. Implementation: (1) User logs in → Server generates session ID, stores session data in Redis. (2) Server sets cookie with session ID. (3) Next request → Any server retrieves session from Redis using session ID. PROS: (1) Servers truly stateless (can route to any server). (2) No data loss if server crashes. (3) Easy to add/remove servers. (4) Better load distribution. (5) Scalable horizontally. CONS: (1) Extra network hop to Redis (slight latency increase). (2) Redis becomes dependency (single point of failure - mitigate with Redis cluster). (3) Requires architecture change. When to use: Production systems, high-availability requirements, any system that needs to scale. PERFORMANCE COMPARISON: Sticky sessions: Session retrieval ~0ms (in memory). Shared storage: Session retrieval ~1-2ms (Redis). For most applications, 1-2ms is acceptable trade-off for much better reliability and scalability. RECOMMENDED APPROACH: Implement Solution 2 (shared session storage). This is standard practice in distributed systems. Make Redis highly available (Redis Sentinel or Redis Cluster). If latency critical, cache frequently-accessed session data in server memory but always verify with Redis (cache-aside pattern). WHY STATELESS IS BETTER: (1) Servers interchangeable (no special handling). (2) Auto-scaling works properly (can add/remove servers freely). (3) Deployment easier (can update servers one at a time). (4) Better fault tolerance. MIGRATION PATH: If currently using sticky sessions: (1) Deploy Redis cluster. (2) Write session data to BOTH memory and Redis (dual write). (3) Read from memory first (for performance), fall back to Redis if missing. (4) After verification period, remove memory storage. (5) Remove sticky session configuration.",
    keyPoints: [
      'Round Robin without session persistence causes random logouts',
      'Sticky sessions (quick fix): keeps user on same server, but not scalable',
      'Shared session storage (recommended): Redis/database for sessions, servers stateless',
      'Stateless design enables horizontal scaling, better fault tolerance',
      'Trade-off: slight latency increase (1-2ms) vs much better scalability and reliability',
    ],
  },
  {
    id: 'q2',
    question:
      'Your application has 3 servers: Server A (32 cores, 64GB RAM), Server B (16 cores, 32GB RAM), Server C (16 cores, 32GB RAM). Which load balancing algorithm would you choose and why? What configuration parameters would you set?',
    sampleAnswer:
      "ANALYSIS: Servers have different capacities. Server A is 2× as powerful as Servers B and C. This rules out simple Round Robin (would overload B and C, underutilize A). RECOMMENDED ALGORITHM: WEIGHTED ROUND ROBIN: Why: (1) Accounts for different server capacities. (2) More powerful Server A gets more requests. (3) Simple to implement and configure. (4) Predictable behavior. CONFIGURATION: Server A: Weight = 4 (32 cores). Server B: Weight = 2 (16 cores). Server C: Weight = 2 (16 cores). Ratio: 4:2:2 means for every 8 requests: 4 → Server A, 2 → Server B, 2 → Server C. Result: Server A handles 50% of traffic, B and C each handle 25%. This matches their relative capacities. ALTERNATIVE ALGORITHM: LEAST RESPONSE TIME: If servers have variable performance or different hardware beyond CPU (e.g., SSD vs HDD), Least Response Time adapts dynamically. Configuration: Monitor response times every 5 seconds, route to server with best (response time + active connections). Why this might be better: (1) Adapts to actual performance. (2) If Server A is overloaded, automatically routes less traffic to it. (3) Handles heterogeneous environments better. Trade-off: More complex, requires active monitoring. IMPLEMENTATION DETAILS: Health Checks: Interval: 5 seconds, Timeout: 2 seconds, Unhealthy threshold: 3 consecutive failures, Healthy threshold: 2 consecutive successes. Connection Limits: Set max connections per server based on capacity: Server A: max 1000 connections, Server B: max 500 connections, Server C: max 500 connections. Prevents any server from being overwhelmed. Timeout Configuration: Request timeout: 30 seconds (prevent slow requests from tying up resources), Idle timeout: 60 seconds (close inactive connections). NGINX CONFIGURATION EXAMPLE: upstream backend { least_conn; # Or use weighted round robin server server-a.example.com:8080 weight=4 max_conns=1000; server server-b.example.com:8080 weight=2 max_conns=500; server server-c.example.com:8080 weight=2 max_conns=500; } MONITORING: Track per-server metrics: (1) Request count (verify weights working correctly). (2) CPU utilization (should be similar across all servers ~80%). (3) Response times (identify bottlenecks). (4) Error rates (detect failing servers). Alert if: Any server consistently at 100% CPU, Response time > 1 second at p95, Error rate > 1%. SCALING STRATEGY: If all servers reaching capacity: Add more servers with appropriate weights. Scale horizontally (add more 16-core servers) rather than vertically (less cost-effective). If Server A is bottleneck: Could split into 2× 16-core servers (easier to scale horizontally). COST CONSIDERATION: Running one 32-core server might be more cost-effective than two 16-core servers (depends on cloud provider pricing). But multiple smaller servers give better fault tolerance. REAL-WORLD EXAMPLE: At scale (e.g., Netflix, AWS): Don't mix server sizes - use homogeneous server types for simplicity. Easier operations: Any server can handle any load, simpler configuration, easier capacity planning. Auto-scaling groups use identical instance types. But in your scenario with existing heterogeneous hardware: Weighted Round Robin is the pragmatic solution. FINAL RECOMMENDATION: Start with Weighted Round Robin (4:2:2 ratio). Monitor closely for 1 week. If seeing uneven load despite weights: Switch to Least Response Time (more dynamic). If working well: Keep it simple.",
    keyPoints: [
      'Heterogeneous servers require Weighted Round Robin or Least Response Time',
      'Weight based on server capacity: 32-core gets 2× weight of 16-core',
      'Set max connection limits per server to prevent overload',
      'Monitor CPU utilization to verify weights are correct (all ~80%)',
      'Ideal: Use homogeneous servers for simplicity (same hardware for all)',
    ],
  },
  {
    id: 'q3',
    question:
      'Explain the difference between Layer 4 and Layer 7 load balancing. Give a real-world scenario where you would choose each, and explain why.',
    sampleAnswer:
      "LAYER 4 (TRANSPORT LAYER) LOAD BALANCING: What it examines: IP addresses, TCP/UDP ports, Network layer information only. What it CANNOT see: HTTP headers, URL paths, cookies, Request body. How it routes: Based on IP:port only. All requests from same client/port go to same backend server. Speed: VERY FAST (minimal processing, just forwards packets). Example: AWS Network Load Balancer (NLB), HAProxy in TCP mode. LAYER 7 (APPLICATION LAYER) LOAD BALANCING: What it examines: Full HTTP request: URL path, headers, cookies, query parameters. What it CAN do: Content-based routing, URL rewriting, SSL termination, Request/response modification. How it routes: Based on application-level data: /api/users → User Service, /api/posts → Post Service. Speed: SLOWER (must parse HTTP, more processing). Example: AWS Application Load Balancer (ALB), NGINX, HAProxy in HTTP mode. KEY DIFFERENCES COMPARISON: Processing: L4: Fast (just IP routing), L7: Slower (HTTP parsing). Routing Intelligence: L4: Basic (IP/port), L7: Advanced (content-based). Protocol: L4: Any TCP/UDP, L7: HTTP/HTTPS only. SSL Termination: L4: No (passes through), L7: Yes (can decrypt/encrypt). Use Cases: L4: High throughput, L7: Microservices, intelligent routing. SCENARIO 1: CHOOSE LAYER 4: Real-world: Database load balancing. Requirements: (1) Need to load balance PostgreSQL connections (not HTTP). (2) High throughput (100K connections/sec). (3) Minimal latency (every millisecond matters). (4) No need for intelligent routing (all requests go to read replicas). Setup: Layer 4 load balancer distributes PostgreSQL connections (port 5432) across 10 read replicas using Least Connections algorithm. Why Layer 4: (1) PostgreSQL is not HTTP (Layer 7 wouldn't work). (2) Need maximum performance (Layer 4 faster). (3) Simple round-robin to replicas sufficient. (4) No need to inspect query content. Configuration: Protocol: TCP, Port: 5432, Algorithm: Least Connections (long-lived DB connections), Health Check: TCP connect to port 5432. SCENARIO 2: CHOOSE LAYER 7: Real-world: Microservices architecture. Requirements: (1) Route different API paths to different services: /api/users/* → User Service, /api/posts/* → Post Service, /api/comments/* → Comment Service. (2) Need SSL termination (decrypt HTTPS at LB, HTTP to backends). (3) A/B testing (route 10% traffic to new service version). (4) Header-based routing (mobile app gets different backend). Setup: Layer 7 load balancer examines URL path and routes accordingly. Why Layer 7: (1) Need content-based routing (path inspection). (2) SSL termination (Layer 4 can't decrypt). (3) A/B testing requires cookie/header inspection. (4) Can't do this with Layer 4 (doesn't see URL). Configuration: Route by path: /api/users/* → User Service (3 instances), /api/posts/* → Post Service (5 instances), /api/comments/* → Comment Service (2 instances). SSL termination: HTTPS from client → HTTP to backends. Health Check: HTTP GET /health per service. A/B Testing Rule: If cookie: experiment=new → Route to new version, Else → Route to stable version. SCENARIO 3: HYBRID APPROACH: Real-world: High-traffic e-commerce (Amazon-scale). Architecture: Layer 4 LB (entry point) → Layer 7 LBs (routing) → Services. Why hybrid: (1) Layer 4 handles massive initial traffic (millions of req/sec). (2) Layer 4 distributes load across multiple Layer 7 LBs. (3) Layer 7 LBs do intelligent routing to microservices. Benefits: Best of both: L4 speed + L7 intelligence. Scaling: Can add more L7 LBs behind L4 as traffic grows. Flow: Client → L4 LB (based on IP) → One of 10 L7 LBs → L7 routes by path to appropriate microservice. PERFORMANCE COMPARISON: Throughput: L4: Millions of req/sec, L7: Hundreds of thousands req/sec. Latency: L4: <1ms overhead, L7: 1-5ms overhead (HTTP parsing). For most applications: 1-5ms is acceptable for the intelligence gained. Only use L4 when: (1) Non-HTTP protocol (databases, custom TCP), (2) Extreme performance needs (> 500K req/sec), (3) No need for content-based routing. COST CONSIDERATION: AWS pricing: NLB (L4): $0.0225/hour + $0.006/GB processed. ALB (L7): $0.0225/hour + $0.008/GB processed + $0.008 per LCU (Load Balancer Capacity Unit). L7 slightly more expensive but worth it for microservices. RECOMMENDATION: Default to Layer 7 (ALB/NGINX) for: Web applications, APIs, Microservices, When you need SSL termination, Content-based routing. Use Layer 4 (NLB/HAProxy TCP) for: Non-HTTP protocols, Extreme performance requirements, Simple IP-based distribution, Database load balancing. REAL-WORLD EXAMPLE: Company: Netflix. Layer 4 (Zuul 1): Initially used for API gateway. Layer 7 (Zuul 2): Migrated to for: Path-based routing to microservices, Request authentication, Rate limiting, Much more flexibility despite slight performance cost.",
    keyPoints: [
      'Layer 4: Fast, TCP/UDP routing, protocol-agnostic, use for databases or extreme performance',
      'Layer 7: Intelligent, HTTP-based routing, SSL termination, use for microservices',
      'Layer 7 can route by URL path (/api/users vs /api/posts)',
      'Layer 4 throughput: millions req/sec, Layer 7: hundreds of thousands',
      'Default to Layer 7 for web apps/APIs, use Layer 4 for non-HTTP or extreme scale',
    ],
  },
];
