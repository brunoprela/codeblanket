/**
 * Quiz questions for API Gateway section
 */

export const apigatewayQuiz = [
  {
    id: 'q1',
    question:
      'Your microservices architecture has 10 services. Each client (web, mobile) calls services directly. This causes: (1) Clients need to know all 10 endpoints, (2) Each service implements authentication separately. Propose a solution.',
    sampleAnswer:
      'SOLUTION: IMPLEMENT API GATEWAY. PROBLEM ANALYSIS: Current architecture issues: (1) Tight coupling: Clients know all service endpoints. (2) Duplicate auth: Each service validates tokens (10× duplication). (3) Complex clients: Web/mobile apps have complex service discovery logic. (4) Security: Services exposed directly (attack surface). (5) No rate limiting: Services vulnerable to abuse. PROPOSED SOLUTION: API Gateway as single entry point. ARCHITECTURE: Before: Client → Service 1, Service 2, ..., Service 10 (10 connections). After: Client → API Gateway → Service 1, Service 2, ..., Service 10 (1 connection). IMPLEMENTATION: Deploy API Gateway (Kong, AWS API Gateway, NGINX). Configure routing: /api/users/* → User Service. /api/orders/* → Order Service. /api/products/* → Product Service. Configure authentication: Gateway validates JWT tokens (once). Gateway forwards requests with user context. Configure rate limiting: 1000 requests/hour per user. Gateway returns 429 if exceeded. Configure logging: Centralized request logs. BENEFITS: Simplified clients: Only one endpoint (gateway.example.com). No auth duplication: Authentication at gateway only (1× not 10×). Rate limiting: Centralized (protect all services). Monitoring: Single place to log/monitor all traffic. Security: Services not exposed directly (internal network only). DETAILED FLOW: Example: Client requests GET /api/orders/123. Step 1: Client → Gateway (gateway.example.com/api/orders/123). Step 2: Gateway validates JWT token. If invalid: Return 401 Unauthorized. If valid: Extract user ID from token. Step 3: Gateway checks rate limit. If exceeded: Return 429 Too Many Requests. Step 4: Gateway routes request to Order Service (internal network). Request: GET http://order-service-internal:8080/orders/123. Headers: X-User-ID: 456 (from token). Step 5: Order Service processes request (no auth needed, trusts gateway). Step 6: Order Service returns response to Gateway. Step 7: Gateway logs request and returns response to Client. BENEFITS QUANTIFIED: Auth logic: Before: 10 services × 100 lines auth code = 1000 lines. After: Gateway: 100 lines. Services: 0 lines. Reduction: 90% less code. Performance: Before: Client makes 10 requests (one per service). Latency: 10 × 200ms = 2000ms (serial). After: Client makes 1 request to Gateway. Gateway aggregates if needed. Latency: 200ms (gateway) + parallel service calls. Security: Before: 10 attack surfaces (services exposed). After: 1 attack surface (only gateway exposed). TRADE-OFFS: Single point of failure: If gateway down, all services unavailable. Mitigation: Deploy multiple gateway instances behind load balancer. Added latency: Extra hop through gateway (~5-10ms). Mitigation: Deploy gateway close to services (same datacenter). Gateway becomes bottleneck: High traffic overloads gateway. Mitigation: Scale gateway horizontally (add instances). COST: API Gateway cost (AWS): ~$3.50 per million requests. 10M requests/month: $35. Engineering savings: Reduced auth code, easier monitoring. Net: Positive ROI (simplicity + security > cost). MIGRATION STRATEGY: Phase 1: Deploy gateway alongside existing architecture. Phase 2: Route 10% traffic through gateway (canary). Phase 3: Monitor for issues (latency, errors). Phase 4: Gradually increase to 100%. Phase 5: Remove direct client→service connections. MONITORING: Track metrics: Request count per endpoint. Response time (p50, p95, p99). Error rate (4xx, 5xx). Auth failures (invalid tokens). Rate limit hits. Alert if: Error rate >1%. Latency p95 >500ms. FINAL RECOMMENDATION: Implement API Gateway immediately. Centralize authentication (remove from services). Add rate limiting (protect from abuse). Monitor closely (single point of failure). Expected outcome: Simpler clients, 90% less auth code, better security.',
    keyPoints: [
      'API Gateway = single entry point for all clients (simplified client code)',
      'Centralized auth: validate JWT once at gateway (not in every service)',
      'Rate limiting: protect all services with centralized throttling',
      'Trade-off: Single point of failure (mitigate with multiple gateway instances)',
      'Result: 90% less auth code, better security, easier monitoring',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the difference between an API Gateway and a Load Balancer. When would you use each, and can you use both together?',
    sampleAnswer:
      'API GATEWAY VS LOAD BALANCER: LOAD BALANCER: Purpose: Distribute traffic across multiple instances of SAME service. Layer: Layer 4 (TCP/UDP) or Layer 7 (HTTP). Intelligence: Minimal (just route to healthy instances). Business logic: None. Example: 5 instances of User Service. Load balancer distributes requests evenly across all 5. Use case: Scale single service horizontally. API GATEWAY: Purpose: Route requests to DIFFERENT services based on URL path. Layer: Layer 7 (HTTP/HTTPS) only. Intelligence: High (routing, auth, rate limiting, transformation). Business logic: Yes (authentication, rate limiting, request transformation). Example: /api/users → User Service. /api/orders → Order Service. /api/products → Product Service. Use case: Microservices architecture (single entry point). COMPARISON TABLE: Aspect / Load Balancer / API Gateway: Routing: Same service (multiple instances) / Different services (by path). Layer: L4 or L7 / L7 only. Auth: No / Yes. Rate limiting: No / Yes. Transformation: No / Yes. Caching: Minimal / Yes. Monitoring: Basic / Advanced. Complexity: Low / High. WHEN TO USE LOAD BALANCER: Scaling single service: 10 instances of Order Service. High availability: If one instance fails, route to others. Simple traffic distribution: No business logic needed. Example: Single monolithic app with 5 replicas. WHEN TO USE API GATEWAY: Microservices: Multiple services behind single endpoint. Cross-cutting concerns: Auth, rate limiting, logging. Request routing: Different paths → different services. Client simplification: One endpoint instead of many. Example: E-commerce with User, Order, Product, Payment services. USING BOTH TOGETHER (COMMON): Architecture: Client → API Gateway → Load Balancer → Service Instances. Flow: (1) Client sends request to API Gateway. (2) Gateway: Authenticates, routes by path. (3) Gateway forwards to Order Service Load Balancer. (4) Load Balancer distributes across 5 Order Service instances. (5) One instance handles request. (6) Response back through Load Balancer → Gateway → Client. Why use both? Gateway: Handles application-level concerns (auth, routing). Load Balancer: Handles traffic distribution (scaling, availability). Result: Best of both worlds. EXAMPLE: NETFLIX ARCHITECTURE: Layer 1: AWS ELB (Load Balancer) → Distributes to Zuul instances. Layer 2: Zuul (API Gateway) → Routes by path, auth, rate limiting. Layer 3: Service Load Balancers → Distribute to service instances. Layer 4: Service Instances → Process requests. Benefits: Gateway scales independently (multiple Zuul instances). Services scale independently (multiple instances each). Gateway provides intelligence, Load Balancers provide distribution. DETAILED EXAMPLE: UBER: Client request: GET /api/rides/123. Architecture: (1) Client → Cloudflare (Global Load Balancer, CDN). (2) Cloudflare → AWS ELB (Regional Load Balancer). (3) ELB → API Gateway Instance (1 of 10). (4) API Gateway: Validates JWT, checks rate limit. (5) API Gateway routes: /api/rides → Ride Service Load Balancer. (6) Ride Service LB → Ride Service Instance (1 of 20). (7) Ride Service processes request. Result: Client makes 1 request. 4 layers of routing/distribution. Fast, reliable, scalable. CAN YOU SKIP ONE? Skip Load Balancer? If only 1 instance per service: Yes, go directly to service. If multiple instances: No, need load balancer. Skip API Gateway? If single monolithic app: Yes, just use load balancer. If microservices: No, need gateway for routing. RECOMMENDATION FOR DIFFERENT SCENARIOS: Scenario 1: Monolithic App (1 service, 10 instances). Use: Load Balancer only. Why: No need for routing (same service), just distribution. Scenario 2: Microservices (10 services, 1 instance each). Use: API Gateway only. Why: Need routing (different services), no need for distribution (1 instance each). Scenario 3: Microservices (10 services, 10 instances each). Use: API Gateway + Load Balancers. Why: Need routing (different services) AND distribution (multiple instances). This is most common in production. COST COMPARISON: Load Balancer only: AWS ELB: $20/month + data transfer. Simple, cheap. API Gateway only: AWS API Gateway: $3.50 per million requests. More expensive at scale. Both together: ELB: $20/month. API Gateway: $3.50 per million requests. Service LBs: $20/month each × 10 services = $200/month. Total: $220/month + $3.50/million requests. Worth it? Yes, for microservices (complexity reduction, security). TRADE-OFFS: Using both: Pros: Clean separation, scales independently, best practices. Cons: Added cost, more complexity, extra latency hop. Using only gateway: Pros: Simpler architecture, lower cost. Cons: Gateway must handle load balancing (added responsibility). Using only load balancer: Pros: Simple, cheap. Cons: No microservices support, no auth/rate limiting. FINAL RECOMMENDATION: Microservices: Use API Gateway + Load Balancers (industry standard). Monolith: Use Load Balancer only (simpler). Hybrid: Start with gateway only (1 instance per service), add load balancers when scaling (>3 instances per service).',
    keyPoints: [
      'Load Balancer: distributes traffic across instances of SAME service',
      'API Gateway: routes requests to DIFFERENT services by URL path',
      'Load Balancer: no business logic, just distribution',
      'API Gateway: auth, rate limiting, transformation',
      'Use both: Gateway for routing/auth, Load Balancer for scaling (industry standard)',
    ],
  },
  {
    id: 'q3',
    question:
      'Your API Gateway is becoming a performance bottleneck (high latency, 500ms+ response times). What could be the causes and how would you optimize?',
    sampleAnswer:
      "API GATEWAY PERFORMANCE OPTIMIZATION: SYMPTOMS: High latency: p95 = 500ms (should be <50ms). Slow requests: Users complain about lag. Gateway CPU: 80-90% utilization (bottleneck). DIAGNOSIS: Step 1: Measure where time is spent. Use APM tool (New Relic, Datadog) to trace requests. Break down latency: Gateway processing: ?ms. Backend service call: ?ms. Network: ?ms. COMMON CAUSES & SOLUTIONS: CAUSE 1: SYNCHRONOUS BLOCKING I/O. Problem: Gateway makes sequential backend calls. Example: Request to /api/user-profile. Gateway calls: (1) User Service: 100ms. (2) Order Service: 150ms. (3) Recommendation Service: 100ms. Total: 350ms (sequential). Solution: Parallel backend calls (async I/O). Implementation: Use async/await (Node.js) or CompletableFuture (Java). Make calls in parallel: Promise.all([userService(), orderService(), recommendationService()]). Total: max(100ms, 150ms, 100ms) = 150ms. Improvement: 350ms → 150ms (2.3× faster). CAUSE 2: NO CACHING. Problem: Every request hits backend services. Solution: Cache responses at gateway. Implementation: Cache GET requests: GET /api/products/123 → Cache for 60 seconds. Use Redis for cache (fast, distributed). Cache hit: 1-2ms (vs 100ms backend). Cache hit rate: 80-90% (10× load reduction). Example: Before: 100% requests hit backend (100ms each). After: 90% cached (2ms), 10% backend (100ms). Average: 0.9 × 2ms + 0.1 × 100ms = 11.8ms. Improvement: 100ms → 11.8ms (8.5× faster). CAUSE 3: INEFFICIENT AUTH VALIDATION. Problem: Gateway validates JWT on every request (expensive). Example: JWT validation: Parse token, verify signature, check expiration: 50ms. Solution: Cache auth results. Implementation: Cache user permissions in Redis. Key: token_hash → user_id + permissions. TTL: 5 minutes. Auth flow: (1) Hash token. (2) Check Redis cache. (3) If hit: Use cached user info (1ms). (4) If miss: Validate JWT, cache result (50ms). Cache hit rate: 95% (most users make multiple requests). Improvement: 50ms → 2.5ms (20× faster). CAUSE 4: TOO MANY GATEWAY RESPONSIBILITIES. Problem: Gateway does too much: Auth, rate limiting, logging, transformation, aggregation. Each adds latency. Solution: Offload non-critical logic. Move to backends: Complex transformations. Business logic. Aggregation (use GraphQL). Keep in gateway: Auth (critical for security). Rate limiting (protect backends). Simple routing. Improvement: Reduce gateway processing from 100ms to 10ms. CAUSE 5: INSUFFICIENT GATEWAY INSTANCES. Problem: Single gateway instance overloaded. CPU: 90% (can't handle more requests). Solution: Horizontal scaling (add more instances). Implementation: Deploy 10 gateway instances (instead of 1). Load balancer distributes traffic: Each instance handles 10% of traffic. CPU per instance: 9% (plenty of headroom). Improvement: Can handle 10× more traffic. CAUSE 6: SLOW BACKEND SERVICES. Problem: Gateway fast, but backend services slow. Example: Order Service takes 500ms to respond. Solution: Not a gateway problem, optimize backends. But gateway can help: (1) Circuit breaker: If backend slow/down, fail fast (don't wait). (2) Timeout: Set aggressive timeout (e.g., 200ms). (3) Retry: Retry failed requests (with backoff). (4) Fallback: Return cached/default response if backend unavailable. OPTIMIZATION CHECKLIST: 1. Enable caching: Redis cache for GET requests. Cache hit rate target: 90%. TTL: 60 seconds for static data. 2. Parallel backend calls: Use async I/O for multiple services. Reduce sequential latency. 3. Cache auth results: JWT validation result cached. TTL: 5 minutes. 4. Horizontal scaling: Add more gateway instances. Target: CPU <50% per instance. 5. Reduce responsibilities: Move complex logic to backends. Keep gateway thin. 6. Optimize logging: Async logging (don't block requests). Sample logs (e.g., 10% of requests). 7. Use HTTP/2: Multiplexing reduces connection overhead. 8. Connection pooling: Reuse backend connections (avoid TCP handshake). MONITORING: Track metrics: Latency: p50, p95, p99 (target: p95 <50ms). Throughput: Requests/sec (capacity planning). Cache hit rate: >90% (optimize caching). Error rate: <0.1% (reliability). CPU utilization: <50% (headroom). Alert if: p95 latency >100ms. Error rate >1%. CPU >80%. REAL-WORLD EXAMPLE: NETFLIX (ZUUL): Optimization: Async I/O (Netty framework, non-blocking). Caching (aggressive caching of metadata). Horizontal scaling (thousands of Zuul instances). Circuit breaker (fail fast if backend slow). Result: p99 latency <100ms. Handles billions of requests/day. EXPECTED IMPROVEMENTS: Before: p95 latency: 500ms. After optimizations: Caching: 500ms → 50ms (10× cache hit rate). Parallel calls: 50ms → 20ms (parallel backend calls). Auth caching: 20ms → 10ms (cached auth). Result: p95 latency: 10ms (50× improvement). COST: Horizontal scaling: 10 instances × $100/month = $1000/month. Redis cache: $200/month. Total: $1200/month. Benefit: Handle 50× more traffic, better UX, fewer customer complaints. ROI: Positive (avoid losing customers due to slow site). FINAL RECOMMENDATION: Profile gateway to identify bottleneck (use APM). Enable caching (biggest impact: 10× latency reduction). Parallelize backend calls (2-3× improvement). Scale horizontally (add instances as needed). Keep gateway thin (push logic to backends). Monitor continuously (p95 latency, cache hit rate).",
    keyPoints: [
      'Caching: Biggest impact (10× latency reduction, 90% cache hit rate)',
      'Parallel backend calls: 2-3× faster (async I/O instead of sequential)',
      'Horizontal scaling: Add more gateway instances (reduce CPU load)',
      'Cache auth results: 20× faster JWT validation',
      'Keep gateway thin: Move complex logic to backend services',
    ],
  },
];
