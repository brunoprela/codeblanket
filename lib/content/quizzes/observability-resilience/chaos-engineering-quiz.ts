/**
 * Quiz questions for Chaos Engineering section
 */

export const chaosEngineeringQuiz = [
  {
    id: 'q1',
    question:
      'What is Chaos Engineering, and how is it different from traditional testing? Why is it important for modern distributed systems?',
    sampleAnswer:
      'Chaos Engineering is the practice of intentionally introducing failures into systems to build confidence in their ability to withstand turbulent conditions. Unlike traditional testing which validates known scenarios, chaos engineering discovers unknown failure modes. **Traditional Testing**: Unit Tests: Test individual functions. Integration Tests: Test component interactions. Load Tests: Test performance under load. Limitations: Only test anticipated scenarios. Miss emergent behaviors in production. Don\'t test failure scenarios. Example: Tests pass, but in production when database slows down, entire system crashes because no timeouts configured. **Chaos Engineering**: Deliberately injects failures: Kill random instances, add network latency, fill disk to 100%. Runs in production (with safeguards). Discovers unexpected failure modes. Validates resilience mechanisms actually work. Example: Kill database → Discover circuit breakers not configured → Fix before real outage. **Key Differences**: Traditional: Known scenarios, pre-production, proves things work. Chaos: Unknown scenarios, production, finds where things break. **Why Important for Distributed Systems**: (1) **Complexity**: 100+ microservices create emergent behaviors impossible to anticipate. (2) **Failure is Guaranteed**: In distributed systems, something is always failing. (3) **Production is Different**: Different from staging in traffic, data, dependencies. (4) **Validation**: Proves resilience patterns (circuit breakers, retries) actually work. (5) **Practice**: Team gets practice handling failures before 2am pages. **Real Example**: Netflix runs Chaos Monkey continuously in production → Randomly terminates instances → Forces engineers to build resilient services → When AWS has actual outages, Netflix stays up while others fail. **Process**: (1) Define steady state (e.g., "99.9% success rate, p99 < 500ms"). (2) Hypothesis (e.g., "Losing 10% of instances won\'t affect users"). (3) Inject failure (kill 10% of instances). (4) Observe (did metrics stay within steady state?). (5) Learn and improve. **Not About**: Breaking things randomly, chaos for chaos sake. **About**: Scientific experiments to build confidence and find weaknesses proactively.',
    keyPoints: [
      'Chaos engineering: Deliberately inject failures to test resilience',
      'Different from testing: Finds unknown failure modes, runs in production',
      'Important because: Distributed systems have emergent, unpredictable failures',
      'Process: Define steady state → hypothesis → inject failure → observe → learn',
      'Example: Netflix Chaos Monkey randomly kills instances continuously',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the concept of "blast radius" in chaos engineering. How do you minimize it when running experiments in production?',
    sampleAnswer:
      'Blast radius is the maximum potential impact of a chaos experiment—how many users, services, or systems could be affected if the experiment goes wrong. Minimizing blast radius is critical for safe production chaos engineering. **What is Blast Radius**: Percentage of users affected, number of services impacted, potential revenue loss, geographic scope. Example: "10% of production traffic in us-east-1 region for checkout service." **Why It Matters**: Chaos experiments can go wrong (unexpected failure modes). Large blast radius = Many users affected. Small blast radius = Learning opportunity with minimal risk. **Minimizing Blast Radius - Gradual Expansion**: **Phase 1: Canary Environment** (0.1% traffic): Isolated environment with copy of production. Minimal real user impact. Example: Kill 1 instance in canary → Observe. **Phase 2: Single Instance** (1% traffic): Target single pod/instance in production. Affects small percentage of requests. Example: Add 5s latency to 1 of 100 instances → 1% of requests slow. **Phase 3: Single Availability Zone** (10% traffic): Limit to one AZ or region. Geographic isolation. Example: Network partition us-east-1a → Other AZs unaffected. **Phase 4: Percentage-Based** (25%, 50%, 75%): Gradually increase percentage of traffic. Monitor metrics at each step. Stop if metrics degrade. **Phase 5: Full Production** (100%): Only after succeeding at all previous phases. **Control Mechanisms**: **1. Feature Flags**: Enable chaos for specific users or traffic segments. Example: chaos_enabled_for_user_ids=[test_users]. **2. Time Windows**: Run during business hours when team is watching. Example: Monday-Friday 9am-5pm. **3. Kill Switch**: Manual override to stop experiment immediately. Example: Emergency stop button in UI. **4. Automatic Rollback**: If metrics degrade beyond threshold, auto-stop. Example: If error_rate > 1%, stop experiment. **5. Gradual Rollout**: Increase blast radius only if metrics stable. Example: 1% for 10 minutes → 10% for 10 minutes → etc. **Example Implementation**: Start: 1% traffic, us-east-1, business hours, team watching. Observation: Success rate 99.9%, latency normal. Expand: 10% traffic, same region. Observation: Still healthy. Expand: 10% traffic, all regions. Observation: Still healthy. Result: Validated resilience at scale. **If Things Go Wrong**: Metrics degrade (error rate spikes) → Kill switch activated → Experiment stops → Blast radius: Only 10% of users affected for 5 minutes → Acceptable trade-off for learning.',
    keyPoints: [
      'Blast radius: Maximum potential impact of chaos experiment',
      'Minimize by: Start small (canary → 1% → 10% → 100%)',
      'Control mechanisms: Feature flags, time windows, kill switch, auto-rollback',
      'Only expand if metrics remain stable',
      'Example: 1% traffic for 10 min → 10% → 50% → 100%',
    ],
  },
  {
    id: 'q3',
    question:
      'What are Game Days, and how do they complement automated chaos engineering? Walk through planning and executing a game day.',
    sampleAnswer:
      'Game Days are scheduled chaos engineering exercises where the entire team participates in simulated incident response. They complement automated chaos by practicing coordination, communication, and complex multi-service failure scenarios. **What are Game Days**: Scheduled event (e.g., first Thursday of month). Team practices incident response. Real production environment (with safeguards). Complex failure scenarios (not just single instance failure). Tests people and processes, not just technology. **Why Game Days**: (1) **Practice**: Team gets experience handling failures before real incidents. (2) **Test Runbooks**: Validates runbooks actually work. (3) **Coordination**: Tests cross-team communication. (4) **Confidence**: Reduces fear of production incidents. (5) **Find Gaps**: Discover missing tools, unclear ownership, inadequate documentation. **Planning Game Day (4 weeks out)**: **Week 1: Choose Scenario**: Database Failure (primary DB goes down). Region Outage (entire AWS region unavailable). DDoS Attack (10x traffic spike). Deployment Failure (bad code deployed). Dependency Failure (payment gateway down). **Week 2: Prepare**: Set date/time (business hours, avoid busy periods). Notify all stakeholders (don\'t surprise people). Assign roles: Incident Commander, Engineers (on-call), Observers, Scribe. Review runbooks. Test tools (can we actually kill the database in staging?). Set success criteria (service stays within SLO). **Week 3: Dry Run**: Run scenario in staging. Fix any tooling issues. Refine timing and scope. **Week 4: Game Day Execution**: **9:00 AM: Kickoff**: Review scenario (but don\'t reveal exact failure). Assign roles (IC, responders, observers). Set expectations (treat as real incident). **9:15 AM: Inject Failure**: Game master simulates failure (kill database). Team doesn\'t know what failure is (realistic!). **9:20 AM: Team Responds**: Incident Commander takes charge. Engineers debug (check metrics, logs, traces). Follow runbook (or discover runbook is wrong!). Communicate (update status, escalate if needed). **9:45 AM: Resolution**: Team identifies issue (database down). Team follows failover procedure. Validates system recovery. **10:00 AM: Debrief** (Most Important Part): **What Went Well**: "Failover worked as expected", "Team collaborated effectively", "Incident Commander kept everyone organized". **What Went Wrong**: "Runbook was outdated", "Took 10 minutes to find who owned database", "No way to contact DBA on-call", "Monitoring didn\'t show which database failed". **Action Items**: Update runbook with current procedures [Owner: Alice, Due: Next week]. Add database ownership to on-call rotation [Owner: Bob]. Improve monitoring to show database instance details [Owner: Charlie]. **Outcomes**: Team now confident handling database failures. Identified 3 gaps in runbooks and tooling. Next game day scheduled (different scenario). **Game Days vs Automated Chaos**: Automated: Runs continuously, tests technology (circuit breakers work), simple failures (kill instance). Game Days: Scheduled, tests people and processes, complex failures (multi-service), practice communication.',
    keyPoints: [
      'Game Days: Scheduled chaos exercises with entire team',
      'Tests: People, processes, communication (not just technology)',
      'Planning: Choose scenario, assign roles, prepare runbooks, dry run',
      'Execution: Inject failure, team responds, resolve, debrief',
      'Complement automated chaos: Complex scenarios, practice coordination',
    ],
  },
];
