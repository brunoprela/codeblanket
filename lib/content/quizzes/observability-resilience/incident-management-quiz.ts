/**
 * Quiz questions for Incident Management section
 */

export const incidentManagementQuiz = [
  {
    id: 'q1',
    question:
      'Walk through the complete incident lifecycle from detection to post-mortem. What are the key roles, and what should happen at each stage?',
    sampleAnswer:
      'Incident management follows a structured lifecycle to minimize impact and enable learning. **1. Detection (MTTD - Mean Time To Detect)**: Alert fires: "API error rate > 5% for 5 minutes." Goal: Detect before users notice. Automated monitoring should catch issues in < 5 minutes. Methods: Automated alerts (best), user reports, manual discovery. **2. Response (Initial)**: On-call engineer acknowledges alert (< 5 minutes). Assess severity: SEV-1 (critical - service down), SEV-2 (high - degraded), SEV-3 (medium - minor impact). Declare incident and create incident channel (#incident-2024-01-15). Notify stakeholders. If SEV-1: Page additional engineers, mobilize Incident Commander. **3. Investigation (MTTI - Mean Time To Investigate)**: Incident Commander coordinates response. Engineers gather data: Check metrics, logs, traces. Recent changes: Deployments, config changes in last hour? Form hypothesis: "Database connection pool exhausted?" Test hypothesis: Check connection pool metrics → Confirmed at 100%. Identify root cause: Traffic spike + fixed pool size = exhaustion. **4. Mitigation (MTTR - Mean Time To Recovery)**: Implement fix: Increase connection pool from 50 to 200. Deploy change (10 minutes). Alternative strategies: Rollback (fastest), Hotfix, Failover, Scale up. Monitor recovery: Error rate drops from 15% to 0.5%, Latency returns to normal. **5. Resolution**: Verify: All metrics normal, No ongoing impact, Users can access service. Communicate: Update status page: "RESOLVED", Email affected customers, Internal all-clear message. Close incident channel. **6. Post-Mortem (Within 48 hours)**: Write blameless post-mortem: What happened, Impact (users, duration, revenue), Root cause, Timeline, What went well, What went wrong, Action items with owners and deadlines. Post-mortem meeting: Review incident, Discuss action items, No blame! **Key Roles**: **Incident Commander**: Coordinates response, makes decisions, communicates with stakeholders. Does NOT fix issue (delegates to engineers). **On-Call Engineer**: Initial responder, investigates, implements fixes. Technical owner. **Communications Lead**: Updates status page, customer emails, social media. Keeps users informed. **Scribe**: Documents timeline, decisions, actions. Critical for post-mortem. **Subject Matter Experts**: Provide domain expertise when needed (database expert, security expert). **Metrics**: MTTD: Time from incident start to detection (target: < 5 min). MTTR: Time from detection to resolution (target: < 30 min for SEV-1). MTTA: Time from alert to acknowledgment (target: < 5 min).',
    keyPoints: [
      'Lifecycle: Detect → Respond → Investigate → Mitigate → Resolve → Post-mortem',
      'Key roles: Incident Commander (coordinates), On-Call (fixes), Comms Lead (updates users)',
      'MTTD < 5 min (detect), MTTR < 30 min for SEV-1 (resolve)',
      'Post-mortem within 48 hours, blameless, with action items',
      'Document everything: Timeline, decisions, learnings',
    ],
  },
  {
    id: 'q2',
    question:
      "What is a blameless post-mortem, why is it important, and how do you conduct one effectively? What should and shouldn't be included?",
    sampleAnswer:
      'Blameless post-mortems focus on systems and processes that failed, not individuals who made mistakes. They enable learning and prevent future incidents. **Why Blameless**: (1) **Psychological Safety**: If people are blamed, they hide mistakes. Hiding mistakes prevents learning and improvement. (2) **Systemic Issues**: Most incidents result from system design, not individual error. Blaming individuals ignores root causes. (3) **Learning Culture**: Organizations that blame don\'t learn. Organizations that learn iterate faster. **Blame Culture**: "Bob deployed bad code" → Bob is scared to deploy → Team deploys less frequently → More bugs accumulate → Worse problem. **Blameless Culture**: "Deployment lacked sufficient testing, monitoring didn\'t catch issue early" → Add integration tests, improve monitoring → System improves → Future incidents prevented. **What to Include**: **Factual Timeline**: 10:00: Deployment v2.5.0 started. 10:05: Error rate spiked to 15%. 10:07: Alert fired. 10:10: Engineer acknowledged. No: "Bob carelessly deployed buggy code." **Root Cause Analysis**: Connection pool fixed at 50 connections. Traffic spike (3x normal) exhausted pool. New requests queued and timed out. Yes: Focus on system design. No: "Bob should have known to increase pool size." **What Went Well**: Alert fired within 2 minutes. Engineer responded immediately. Root cause identified quickly (5 minutes). Fix deployed fast (10 minutes). Yes: Celebrate good processes. **What Went Wrong**: Connection pool size was static (no auto-scaling). No monitoring for connection pool utilization. Load testing didn\'t include traffic spike scenarios. Runbook didn\'t cover connection pool issues. Yes: System and process failures. **Action Items**: Implement dynamic connection pool sizing [Alice, Jan 22, P0]. Add connection pool metrics and alerts [Bob, Jan 20, P0]. Load test for 5x traffic [Charlie, Jan 29, P1]. Update runbook [David, Jan 18, P2]. Yes: Concrete, assignable, deadline. No: "Try to be more careful next time." **What NOT to Include**: Individual blame: "If Bob had tested better..." Punishment: "Bob will be on performance review." Vague lessons: "We need to be more careful." Names (except in action items): Use roles instead ("engineer", "on-call"). **Conducting Effective Post-Mortem**: **1. Write Document** (Within 48 hours while fresh). **2. Post-Mortem Meeting** (60 minutes max): Attendees: Incident responders, engineering leads, anyone interested (open invite). Review timeline (10 min). Discuss what went well (10 min). Discuss what went wrong (20 min). Define action items (15 min). Q&A (5 min). **3. Follow-Up**: Track action items to completion (critical!). Weekly check-ins until all complete. If 50% of action items never complete, post-mortems lose value. **Real Example**: Bad: "Sarah deleted production database because she wasn\'t careful." Good: "Production database was deleted because: (1) No confirmation prompt for destructive operations. (2) Production and staging had same naming. (3) Engineer had write access to prod (should be read-only)." Action items: Add confirmation for deletes, rename databases clearly, restrict prod access.',
    keyPoints: [
      'Blameless: Focus on systems/processes, not individuals',
      'Why: Enables psychological safety, learning, continuous improvement',
      'Include: Factual timeline, root cause, what went well/wrong, action items',
      "Don't include: Individual blame, punishment, vague lessons",
      'Critical: Track action items to completion (50%+ completion rate)',
    ],
  },
  {
    id: 'q3',
    question:
      "How do you prevent the same incident from happening twice? What processes and practices ensure action items are completed and incidents don't recur?",
    sampleAnswer:
      'Preventing incident recurrence requires systematic follow-through on action items and cultural practices. Many organizations write post-mortems but never fix root causes. **The Problem**: Post-mortem written → Action items defined → Everyone goes back to feature work → Action items forgotten → Same incident 3 months later → Frustrated team. **Solution Framework**: **1. Prioritize Action Items**: Classify by impact: P0 (Critical): Would prevent SEV-1, complete within 1 week. P1 (High): Would prevent SEV-2, complete within 1 month. P2 (Medium): Would prevent SEV-3 or improve response, complete within quarter. Reserve engineering capacity: 20% of each sprint for reliability work (action items, tech debt). Make action items real work: Add to sprint/roadmap like features. **2. Assign Clear Ownership**: Each action item needs: Owner (single person, not team), Deadline (specific date), Priority (P0/P1/P2). Bad: "Improve monitoring [Engineering Team, Q2]." Good: "Add database connection pool metrics and alert when > 80% [Alice, Jan 20, P0]." **3. Track to Completion**: Weekly review in team meeting: Review all open action items. Status update from owners. Identify blockers. Leadership dashboard: Open action items by age. Completion rate by team. Overdue items highlighted. Executive buy-in: If leadership doesn\'t care about action items, team won\'t either. **4. Make Action Items Visible**: Post-mortem board: "5 open action items from last 3 incidents." Team dashboard: "Team X: 85% action item completion rate (good!). Team Y: 40% (needs improvement)." **5. Celebrate Completion**: When action items prevent future incidents: "Last month\'s database monitoring action item caught today\'s issue before users noticed!" Recognition in team meetings. **6. Incident Recurrence Review**: If same/similar incident occurs: Emergency review: Why did action items not prevent this? Escalate to leadership: Action items clearly weren\'t prioritized. Re-evaluate process: Are we writing right action items? Are we completing them? **7. Automated Prevention**: Convert action items to automated checks where possible: "Add confirmation for destructive operations" → Enforce in CI/CD. "Increase database pool size" → Auto-scaling rule. "Add monitoring" → Infrastructure as code. **Real Examples**: **Google SRE**: Error budgets enforce reliability work. When budget low, feature freeze to complete action items. **Netflix**: Post-incident review program. Quarterly review of all incidents and action items. Executive visibility into completion rates. **Amazon**: COE (Correction of Error) process. Action items are requirements, not suggestions. Leadership reviews overdue items weekly. **Metrics to Track**: Action item completion rate (target: > 80%). Incident recurrence rate (target: < 10%). Time to complete P0 items (target: < 1 week). **Cultural Shift**: Treat action items like features: Sprint planned, progress tracked, demos given. Leadership values reliability: "We shipped 2 features and completed 5 incident action items this sprint" (both celebrated).',
    keyPoints: [
      'Reserve 20% capacity for reliability work (action items, tech debt)',
      'Assign clear ownership: Single person, deadline, priority',
      'Track weekly: Status updates, blockers, executive visibility',
      'Celebrate completion: Recognition when action items prevent incidents',
      'If incident recurs: Emergency review, escalate, re-evaluate process',
    ],
  },
];
