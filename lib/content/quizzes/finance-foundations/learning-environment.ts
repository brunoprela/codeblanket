export const learningEnvironmentQuiz = [
  {
    id: 'le-q-1',
    question:
      'Design a cloud-based development environment for a small quant trading team (5 engineers): (1) Infrastructure (AWS/GCP instances for backtesting, database for market data, caching layer), (2) CI/CD pipeline (automated testing, deployment to production), (3) Monitoring (strategy performance, system health, cost tracking), (4) Collaboration (shared code repository, research notebooks, documentation). Include cost optimization strategies. Estimate monthly AWS bill for: backtesting server (running 8hrs/day), PostgreSQL database (1TB storage), Redis cache (16GB), S3 storage (5TB historical data). How do you handle: security (API keys, production credentials)? Disaster recovery? Scaling for larger team?',
    sampleAnswer: `Cloud infrastructure design: Compute resources: backtesting server (EC2 c6i.4xlarge: 16 vCPUs, 32GB RAM, $0.68/hr, 8hrs/day × 30 days = $163/month), spot instances for cost savings (same instance ~$0.20/hr = $48/month, 70% savings but may be terminated), development instances (5× t3.medium: 2 vCPUs, 4GB RAM, $0.04/hr × 24hrs × 30 = $29/month each = $145 total), production strategy server (t3.xlarge: 4 vCPUs, 16GB RAM, always-on = $150/month), Database: RDS PostgreSQL (db.r6g.xlarge: 4 vCPUs, 32GB RAM, 1TB storage = $350/month), automated backups (7-day retention included), read replicas for analytics queries (+$350 if needed), TimescaleDB extension for timeseries data (free), Caching: ElastiCache Redis (cache.r6g.large: 2 vCPUs, 16GB RAM = $210/month), cluster mode for high availability (+100/month), Storage: S3 for historical data (5TB = $115/month at $0.023/GB), S3 Glacier for old data (>2 years, $0.004/GB = $20/month for 5TB), EBS volumes for EC2 (500GB SSD each × 6 instances = 3TB = $300/month), Network: data transfer out (1TB/month = $90, first 100GB free), VPN for team access (AWS Client VPN = $70/month), Total estimated cost: $163 (backtest) + $145 (dev) + $150 (prod) + $350 (DB) + $210 (cache) + $115 (S3) + $300 (EBS) + $90 (network) + $70 (VPN) = ~$1,600/month, Cost optimizations: use spot instances (save 70% on backtesting), reserved instances (1-year commit = 40% savings on always-on servers), S3 Glacier for archival (save 80%), scheduled start/stop for dev instances (only run 9am-6pm = save 60%), monitoring with AWS Cost Explorer (set budget alerts), CI/CD pipeline: GitHub (private repos, $4/user/month = $20 total), GitHub Actions for CI (test on every commit, 2000 minutes/month free, then $0.008/minute), testing: pytest for unit tests, backtesting validation (historical performance tests), linting (black, ruff, mypy), deployment: Docker containers (ECR for registry = $1/month), ECS Fargate for orchestration (pay per task), blue-green deployment (zero downtime), Monitoring: Grafana dashboard (self-hosted on t3.small = $15/month), Prometheus for metrics (scrape from strategies every 60s), CloudWatch logs ($5/month for log storage), PagerDuty alerts ($29/user/month = $145 total), strategy performance: Sharpe ratio, daily P&L, drawdown, win rate, cost tracking: AWS Cost Explorer, budget alerts (email if >$2000/month), tag resources by team/project, Collaboration: Git repository (GitHub, feature branches, PR reviews), Jupyter notebooks (JupyterHub on EC2 t3.large = $70/month), shared notebooks in S3, documentation (Confluence = $5/user/month = $25 total, or Notion free tier), Slack for communication (free tier), Security: secrets management (AWS Secrets Manager = $0.40/secret/month, store API keys, database passwords), IAM roles (least privilege, no root access), MFA required for all users, production credentials (separate AWS account, require approval for access), rotate keys quarterly, VPN for database access (no public internet exposure), audit logs (CloudTrail = $2/month), Disaster recovery: automated backups (RDS daily snapshots, S3 versioning), cross-region replication (replicate to us-west-2, +50% cost), runbook for recovery procedures (documented in Confluence), test recovery quarterly (restore from backup to staging), Scaling: 10-person team: +$290 dev instances, +$20 GitHub, +$290 PagerDuty = +$600/month, 50-person team: Kubernetes (EKS = $70/month + EC2 costs), shared compute pool, centralized backtesting queue.`,
    keyPoints: [
      'Compute: EC2 c6i.4xlarge for backtesting ($163/mo with spot ~$48), t3.medium for dev ($29 each), t3.xlarge prod ($150)',
      'Database: RDS PostgreSQL 1TB ($350/mo), Redis cache 16GB ($210/mo), S3 5TB historical ($115/mo, Glacier for archival $20)',
      'Total cost: ~$1,600/month for 5-person team, optimize with spot instances (70% savings), reserved instances (40% savings), scheduled stop/start',
      'CI/CD: GitHub Actions (2K free minutes), pytest + backtesting tests, Docker + ECS Fargate for deployment (blue-green, zero downtime)',
      'Security: AWS Secrets Manager for API keys, IAM roles (least privilege), MFA required, VPN for DB access, audit logs with CloudTrail',
    ],
  },
  {
    id: 'le-q-2',
    question:
      'Build a market data management system for research and production: (1) Ingestion pipeline (Yahoo Finance, Alpha Vantage, FRED, SEC EDGAR), (2) Storage architecture (hot/warm/cold tiers), (3) Data quality monitoring (missing values, outliers, schema validation), (4) Access layer (REST API, Python library, SQL views), (5) Versioning (track data corrections, schema changes). Include: deduplication logic, incremental updates (only fetch new data), compression (reduce storage costs), caching strategy. How do you ensure data consistency across environments (dev, staging, prod)? Handle restatements (company revises historical financials)?',
    sampleAnswer: `Market data management system: Ingestion pipeline architecture: orchestrator (Apache Airflow, schedule DAGs for each source), Yahoo Finance DAG (runs daily at 5pm ET after market close, fetch day's OHLCV data for 500 tickers, upsert into prices table), Alpha Vantage DAG (runs hourly during market hours 9:30am-4pm, fetch 1-min bars for watchlist 50 tickers, throttle to stay under 500 calls/day limit), FRED DAG (runs daily at 9am ET, check for new releases of tracked indicators GDP/UNRATE/DGS10, fetch if new data available), SEC EDGAR DAG (runs every 5 minutes, poll RSS feed for new 8-K/10-K/10-Q filings, download and parse), data validation on ingestion (check: no null values in price/volume, price change <50% day-over-day unless stock split, volume >0, timestamp within expected range), Storage architecture: Hot tier (last 30 days): PostgreSQL TimescaleDB (compressed chunks, indexes on ticker+timestamp, query latency <100ms), Warm tier (1-5 years): PostgreSQL with partitioning by month (monthly partitions archived to slower storage, query latency <1s), Cold tier (5+ years): S3 Parquet files (partitioned by year/month, compressed with Snappy, query via Athena, latency <5s acceptable), schema: prices table (id, ticker, date, open, high, low, close, adj_close, volume, source, ingested_at, version), metadata table (ticker, exchange, asset_type, first_date, last_date, update_frequency), Data quality monitoring: completeness checks (detect missing dates: expected_dates = trading_days (start, end), actual_dates = query prices, missing = expected - actual), outlier detection (daily return >20% flagged, volume =0 flagged, price =0 flagged), schema validation (every ingestion: assert columns match expected schema, types correct, constraints satisfied), quality score per ticker (95%+ = excellent, 80-95% = good, <80% = poor, exclude from analysis), alerts (Slack notification if: >10 tickers missing today's data, >5% data quality score drops), Access layer: REST API (FastAPI: GET /api/v1/prices/{ticker}?start=2024-01-01&end=2024-12-31, authentication: API key or JWT, rate limiting: 1000 requests/hour per user), Python library (import datalib; df = datalib.get_prices('AAPL', '2024-01-01', '2024-12-31'), caches locally in ~/.datalib/, version pinning ensures reproducibility), SQL views (materialized views for common queries: daily_returns, monthly_aggregates, fundamental_ratios, refresh nightly), Versioning system: version column in prices table (starts at 1, increments on correction), history table (prices_history: stores all versions, query historical state: SELECT * FROM prices_history WHERE ticker='AAPL' AND version=2), audit log (data_changes table: timestamp, ticker, field_changed, old_value, new_value, reason, changed_by), schema migrations (Alembic for database migrations, version control schema changes, apply migrations: alembic upgrade head), Deduplication: unique constraint on (ticker, date, source) prevents duplicates, on conflict do update (upsert pattern: if exists, update, else insert), hash comparison (calculate MD5 hash of row, skip if unchanged), Incremental updates: track last_updated timestamp per ticker (SELECT MAX(date) FROM prices WHERE ticker='AAPL'), fetch only data after last_updated, backfill missing dates if gaps detected, Compression: TimescaleDB compression (compress chunks >7 days old, saves 80-95% storage), Parquet compression (Snappy for S3, saves 70% vs CSV), vacuum database monthly (reclaim deleted space), Caching strategy: Redis for frequently accessed data (cache key: prices:AAPL:2024-01-01:2024-12-31, TTL: 1 hour), CDN for API responses (CloudFront caches GET requests, 85% hit rate = reduced backend load), local file cache (Python library caches CSVs locally, invalidate on version change), Environment consistency: dev/staging/prod all use same Docker images (postgres:14, timescaledb:latest-pg14), database migrations versioned (Alembic), apply same migrations to each environment, data subsets (dev uses 10 tickers, staging uses 100, prod uses all 5000), Restatements: detect restatement (company files 10-K/A amended, EDGAR indicates amendment), create new version (increment version number, insert corrected values), notify users (email subscribers that 'AAPL 2023 financials restated'), mark old version as superseded (restated=true flag), downstream impact (rerun backtests with corrected data, update research notebooks).`,
    keyPoints: [
      'Ingestion: Apache Airflow DAGs (Yahoo daily 5pm, Alpha Vantage hourly, FRED daily 9am, SEC EDGAR every 5min), validate on ingestion (no nulls, <50% price changes)',
      'Storage: Hot (30 days PostgreSQL TimescaleDB <100ms), warm (1-5yr partitioned <1s), cold (5+yr S3 Parquet <5s via Athena)',
      'Data quality: Completeness (detect missing dates), outliers (>20% flagged), schema validation (assert types/constraints), quality score 95%+ excellent',
      'Access: REST API (FastAPI, rate limit 1K/hr), Python library (caches locally), SQL views (materialized, refresh nightly)',
      'Versioning: Version column (increment on correction), history table (all versions), audit log (changes tracked), handle restatements (new version, notify users)',
    ],
  },
  {
    id: 'le-q-3',
    question:
      'Set up a paper trading environment for testing strategies before live trading: (1) Broker API integration (Alpaca for stocks, Binance testnet for crypto), (2) Order management (track pending/filled/canceled orders), (3) Position tracking (real-time P&L, margin calculations), (4) Performance analytics (Sharpe ratio, max drawdown, win rate), (5) Risk controls (position limits, daily loss limits, kill switch). Include simulation modes: replay historical data (as-of backtesting), live market data (paper trading with real prices). How do you ensure paper trading accurately reflects live trading (slippage, partial fills, latency)? Transition strategy from paper to live?',
    sampleAnswer: `Paper trading system design: Broker integration: Alpaca paper account (API: https://paper-api.alpaca.markets, unlimited paper capital $100K default, reset anytime), authentication (API key + secret, stored in .env file never committed to git), order submission (api.submit_order (symbol='AAPL', qty=100, side='buy', type='limit', limit_price=150.00, time_in_force='day')), Binance testnet (testnet.binance.vision, free testnet USDT for testing, same API as production but test environment), Order management system: orders table (id, strategy_id, ticker, side, order_type, quantity, limit_price, status, submitted_at, filled_at, fill_price, fill_quantity), status enum (pending, submitted, partially_filled, filled, canceled, rejected), polling: query broker API every 5 seconds for order updates (alpaca: api.get_order (order_id), binance: exchange.fetch_order (order_id)), webhooks: Alpaca trade_updates stream pushes order status changes (faster than polling), callback updates database, Position tracking: positions table (strategy_id, ticker, quantity, avg_entry_price, current_price, unrealized_pnl, realized_pnl, updated_at), real-time price updates (WebSocket stream from Alpaca: stream.subscribe_trades (on_trade_update, 'AAPL'), on each tick: update current_price, calculate unrealized_pnl = (current_price - avg_entry_price) × quantity), margin calculations (buying_power = account_equity × margin_multiplier (4x for day trading), used_margin = sum (position_values) / margin_multiplier, available_buying_power = total_buying_power - used_margin), Performance analytics: calculate daily at market close (4pm ET), Sharpe ratio = mean (daily_returns) / std (daily_returns) × sqrt(252), max drawdown = max (running_max - portfolio_value) / running_max, win rate = winning_trades / total_trades, avg win = mean (winning_trades), avg loss = mean (losing_trades), profit factor = gross_profits / gross_losses, store in performance table (date, strategy_id, pnl, sharpe, drawdown, win_rate), Risk controls: position limits (max_position_size = 0.10 × portfolio_value per stock, max_positions = 20 concurrent, check before each order), daily loss limit (if daily_pnl < -0.03 × portfolio_value (3% loss), close all positions, halt trading until next day), kill switch (manual button in dashboard, immediately cancels all orders, closes all positions at market, locks account), exposure limits (max_sector_exposure = 0.30 per sector, max_long = 1.5 × portfolio_value, max_short = 0.5 × portfolio_value), Simulation modes: Replay mode (historical backtesting): load historical data (OHLCV from database), simulate order execution (market order: fill at next bar's open, limit order: fill if price crosses limit during bar), advance time bar by bar (synchronous), useful for: strategy development, parameter optimization, multi-year backtests, Live paper mode (real-time testing): connect to live market data (actual prices from Alpaca/Binance), submit orders to paper account (broker simulates execution), asynchronous (real market timing), useful for: live strategy testing, monitoring dashboard, team collaboration, Realistic execution modeling: slippage simulation (market orders: fill_price = quoted_price + slippage (0.5× half_spread for stocks, 1-5 basis points for liquid stocks), limit orders: only fill if market crosses limit price), partial fills (large orders: break into chunks, fill over multiple bars/ticks, model: if order_size > 10× avg_volume_per_minute, split into 10 child orders over 10 minutes), latency simulation (add delay: order_submission_time + network_latency (50-200ms) + broker_processing (10-100ms) = total_latency ~100-300ms), reject orders (insufficient buying power, halted stocks, outside market hours), Transition to live trading: requirements checklist: (1) Sharpe >1.5 in paper trading (3+ months), (2) max drawdown <15%, (3) win rate >50% for mean-reversion or >40% for trend-following, (4) zero system errors (no crashes, order failures), gradual ramp: start with 1% of target capital (\$1K if planning $100K), trade 1 week, if successful (positive P&L, no issues), increase to 5% (\$5K), repeat weekly: 10%, 25%, 50%, 100%, monitoring: compare paper vs live (are fills matching? is P&L consistent?), if divergence, investigate: (slippage higher than expected?, commission impact?, liquidity issues?), rollback plan: if live trading loses >5% in first month, return to paper, analyze what changed, iterate and retry.`,
    keyPoints: [
      'Broker integration: Alpaca paper API (https://paper-api.alpaca.markets, $100K virtual capital), Binance testnet (testnet.binance.vision, free USDT)',
      'Order management: Track status (pending/filled/canceled), poll API every 5s or use webhooks (faster), store in orders table with timestamps',
      'Position tracking: Real-time P&L via WebSocket price updates, margin calculations (buying_power, used_margin, available), positions table',
      'Risk controls: Position limits (max 10% per stock), daily loss limit (halt if -3%), kill switch (close all + halt trading)',
      'Realistic execution: Slippage (0.5× half-spread), partial fills (split large orders), latency (100-300ms), transition to live gradually (1%→5%→10%→100% capital)',
    ],
  },
];
