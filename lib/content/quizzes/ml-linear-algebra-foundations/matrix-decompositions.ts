/**
 * Quiz questions for Matrix Decompositions section
 */

export const matrixdecompositionsQuiz = [
  {
    id: 'decomp-d1',
    question:
      'Explain why QR decomposition is preferred over normal equations (XᵀX)⁻¹Xᵀy for solving least squares problems. Discuss the numerical stability issues with normal equations and how QR avoids them.',
    sampleAnswer:
      "Normal equations and QR both solve least squares, but QR is much more numerically stable. The problem with normal equations: To solve Ax = b (overdetermined, A is m×n with m > n), normal equations give x = (AᵀA)⁻¹Aᵀb. Computing AᵀA squares the condition number: κ(AᵀA) = κ(A)². If A is already ill-conditioned (κ(A) = 10⁶), then κ(AᵀA) = 10¹². This means: (1) Small errors in data get amplified by factor 10¹², making solution unreliable. (2) AᵀA may become numerically singular even if A has full rank. (3) Loss of precision: forming AᵀA loses roughly half the significant digits. Example: If A has condition number 10⁸ and we work with 16 decimal digits, AᵀA has condition number 10¹⁶, leaving essentially no accurate digits! QR decomposition avoids this: A = QR where Q is orthogonal and R is upper triangular. To solve Ax = b: QRx = b → Rx = Qᵀb (multiply by Qᵀ). Since Q is orthogonal, Qᵀ doesn't amplify errors (κ(Q) = 1). We solve Rx = Qᵀb by back substitution. Condition number: κ(R) = κ(A), not κ(A)². Benefits: (1) Maintains original conditioning of A. (2) No squaring of errors. (3) Orthogonal Q preserves vector lengths and angles. (4) Direct computation without forming AᵀA. Practical impact: For moderately ill-conditioned problems (κ(A) = 10⁶), normal equations may fail completely while QR gives accurate solution. Example scenario: Linear regression with highly correlated features. If features have correlation 0.99999, normal equations become unstable, but QR handles it gracefully. Implementation note: Modern libraries (scikit-learn, np.linalg.lstsq) use QR or SVD internally, never normal equations in production code. Normal equations are only safe for well-conditioned problems (κ(A) < 10⁴) and are sometimes used for speed when stability isn't critical. In summary: QR costs slightly more (O(mn²) vs O(n³) for forming AᵀA + O(n³) for inversion), but this is negligible compared to the massive gain in numerical stability. Always prefer QR for least squares in practice.",
    keyPoints: [
      'Normal equations square condition number: κ(AᵀA) = κ(A)² (catastrophic instability)',
      "QR maintains κ(R) = κ(A): orthogonal Q doesn't amplify errors (κ(Q) = 1)",
      'Modern libraries (sklearn) always use QR/SVD for least squares, never normal equations',
    ],
  },
  {
    id: 'decomp-d2',
    question:
      "SVD provides the optimal low-rank approximation (Eckart-Young theorem). Explain how truncated SVD works, why it's optimal, and discuss its applications in dimensionality reduction, data compression, and recommender systems.",
    sampleAnswer:
      'Truncated SVD keeps only the k largest singular values, providing the best possible rank-k approximation of a matrix. Mechanics: Full SVD gives A = UΣVᵀ where Σ = diag(σ₁, σ₂, ..., σₙ) with σ₁ ≥ σ₂ ≥ ... ≥ σₙ ≥ 0. Truncated SVD keeps only first k components: A_k = Σᵢ₌₁ᵏ σᵢ uᵢvᵢᵀ = U_k Σ_k V_k^T, where U_k = first k columns of U, Σ_k = top-left k×k block of Σ, V_k^T = first k rows of Vᵀ. Optimality (Eckart-Young theorem): A_k minimizes ||A - B||_F over all rank-k matrices B. The error is ||A - A_k||_F = √(σₖ₊₁² + ... + σₙ²). This means no other rank-k matrix can approximate A better (in Frobenius or spectral norm). Intuition: Singular values measure "importance" of each direction. Large σᵢ = important structure, small σᵢ = noise. By keeping top k, we capture the k most important patterns while discarding noise. Applications: (1) Dimensionality Reduction (PCA): Data matrix X (n samples, d features). SVD: X = UΣVᵀ. Principal components = columns of V. Reduced data: X_reduced = XVₖ = UₖΣₖ (n×k). This projects data onto k principal axes capturing most variance. Example: MNIST digits (28×28 = 784 pixels). Top 50 singular values capture >90% of variance, reducing from 784 to 50 dimensions. (2) Data Compression: Images, video, text corpora. Grayscale image = m×n matrix. Store U_k (m×k) + Σ_k (k values) + V_k^T (k×n) = k(m+n+1) values instead of mn. Compression ratio = mn / k(m+n+1). Example: 1000×1000 image, rank-50 approximation: 10⁶ → 100,050 (10× compression) with minimal perceptible loss. (3) Recommender Systems: User-item matrix R (m users, n items), very sparse. SVD factors R ≈ UₖΣₖVₖᵀ. User i = row i of UₖΣₖ (low-dimensional user representation). Item j = column j of VₖᵀΣₖ (low-dimensional item representation). Predicted rating: rᵢⱼ = (UₖΣₖ)ᵢ · (VₖᵀΣₖ)ⱼ. This fills in missing entries (collaborative filtering). Netflix Prize used SVD extensively for recommendations. (4) Latent Semantic Analysis: Document-term matrix A (m documents, n terms). SVD reveals latent topics (k "concepts"). Documents/terms embedded in k-dimensional semantic space. Query matching in reduced space captures semantic similarity (synonyms, related concepts). Advantages: (1) Optimal approximation (provably best). (2) Automatic noise reduction (small singular values often = noise). (3) Reveals hidden structure (latent factors). (4) Dimensionality reduction with minimal information loss. Practical considerations: (1) Computation: Full SVD is O(mn²) for m×n matrix. For very large matrices, use randomized SVD or iterative methods (Lanczos) to compute only top k singular values/vectors. (2) Sparsity: SVD destroys sparsity (U, V are dense). For sparse data, consider sparse decompositions or matrix factorization methods (NMF, sparse coding). (3) Interpretability: PCA/SVD components are linear combinations, sometimes hard to interpret. Compare with sparse methods (e.g., sparse PCA) for more interpretable factors. In summary: Truncated SVD is the gold standard for low-rank approximation, combining optimality with computational efficiency and broad applicability across ML domains.',
    keyPoints: [
      'Eckart-Young: A_k = top k singular values is optimal rank-k approximation',
      'Error: ||A - A_k||_F = √(σₖ₊₁² + ... + σₙ²) (root sum of squares of discarded)',
      'Applications: PCA, image compression, recommender systems (Netflix), LSA',
    ],
  },
  {
    id: 'decomp-d3',
    question:
      'Compare and contrast the use cases, computational costs, and numerical stability of LU, QR, and Cholesky decompositions. When would you choose each one for solving linear systems or least squares problems in machine learning?',
    sampleAnswer:
      'LU, QR, and Cholesky are workhorses of numerical linear algebra, each with specific strengths. LU Decomposition (A = LU): Use case: Solving Ax = b for square, invertible A (with pivoting, works for all non-singular square matrices). Ideal when solving multiple systems with same A but different b vectors. Cost: O(n³) for decomposition, O(n²) per solve. For k systems: O(n³ + kn²) total. Stability: Moderate. LU without pivoting can be unstable. LU with partial pivoting is generally stable but can still fail for badly scaled matrices. Form AᵀA loses conditioning. ML usage: Less common in modern ML (QR preferred), but appears in specialized solvers and when repeated solves are needed. QR Decomposition (A = QR): Use case: Solving least squares (Ax = b, overdetermined). Works for any shape, most numerically stable option. Cost: O(mn²) for m×n matrix. For least squares, also O(mn²). Slightly more expensive than LU for square matrices, but stability makes it worthwhile. Stability: Excellent. Orthogonal transformations (Q) don\'t amplify errors. Condition number κ(R) = κ(A), not squared. "Gold standard" for numerical stability in least squares. ML usage: Linear regression, ridge regression, any least squares problem. Scikit-learn uses QR internally. Preferred over normal equations in production code. Cholesky Decomposition (A = LLᵀ): Use case: Symmetric positive definite matrices only (covariance matrices, kernel matrices, Hessians with positive curvature). Cost: O(n³/3), roughly half of LU. Fastest option for SPD matrices. Stability: Good for well-conditioned SPD matrices. Can fail if matrix is nearly singular or not quite positive definite due to numerical errors. ML usage: Covariance matrices in Gaussian processes, kernel ridge regression, optimization (Newton method with SPD Hessian), multivariate Gaussian sampling (L @ randn()). Decision Guide: (1) Square system, multiple b vectors, same A: Use LU (reuse factorization). (2) Least squares (overdetermined): Use QR (stability). If very large and sparse, consider iterative methods (CG, LSQR). (3) Symmetric positive definite (covariance, kernel): Use Cholesky (speed + natural for SPD). Check positive definiteness first. (4) Numerical stability critical: QR or SVD. Never use normal equations for ill-conditioned problems. (5) Rank-deficient or nearly singular: SVD with pseudoinverse. (6) Very large, sparse: Iterative methods (Conjugate Gradient for SPD, GMRES for general). Concrete ML Examples: Linear Regression: X is m×n, y is m×1, solve Xw = y. QR decomposition: X = QR, solve Rw = Qᵀy. Don\'t form XᵀX! Ridge Regression: Minimize ||Xw - y||² + λ||w||². Form (XᵀX + λI)w = Xᵀy (now SPD), solve with Cholesky. Or augment system and use QR on [X; √λI]. Gaussian Process: Predictive mean requires solving (K + σ²I)α = y where K is kernel matrix (SPD). Use Cholesky. Logistic Regression (Newton): Hessian H = XᵀWX (SPD for logistic). Solve Hδ = -∇L with Cholesky. Principal Component Analysis: Covariance C = XᵀX/n (SPD). Eigendecomposition or SVD of X. Numerical Stability Ranking: 1. SVD (most stable, most expensive) 2. QR 3. Cholesky (for SPD) 4. LU with partial pivoting 5. LU without pivoting 6. Normal equations (least stable) In modern ML: QR and Cholesky dominate for dense systems, iterative methods for large sparse systems, SVD for dimensionality reduction. LU is less common but still useful for specialized applications. Key lesson: Never sacrifice numerical stability for marginal speed gains. A fast but inaccurate solution is worthless. Choose decomposition based on matrix structure (SPD? square? rectangular?) and stability requirements.',
    keyPoints: [
      'LU: square systems, multiple solves; QR: least squares (most stable); Cholesky: SPD (fastest)',
      'Costs: Cholesky O(n³/3), LU O(n³), QR O(mn²); Stability: SVD > QR > Cholesky > LU',
      'ML: QR for linear regression, Cholesky for Gaussian processes/kernels, SVD for PCA',
    ],
  },
];
