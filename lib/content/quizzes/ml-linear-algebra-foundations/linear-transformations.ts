/**
 * Quiz questions for Linear Transformations section
 */

export const lineartransformationsQuiz = [
  {
    id: 'linear-trans-d1',
    question:
      'Explain why every linear transformation T: ℝⁿ → ℝᵐ can be represented as matrix multiplication T(x) = Ax. How do you construct the matrix A from the transformation T?',
    sampleAnswer:
      'Every linear transformation from ℝⁿ to ℝᵐ corresponds to a unique m×n matrix. This is a fundamental theorem in linear algebra. Proof and construction: Let T: ℝⁿ → ℝᵐ be a linear transformation. Let {e₁, e₂, ..., eₙ} be the standard basis of ℝⁿ (eᵢ has 1 in position i, 0 elsewhere). Any vector x ∈ ℝⁿ can be written as: x = x₁e₁ + x₂e₂ + ... + xₙeₙ. By linearity: T(x) = T(x₁e₁ + x₂e₂ + ... + xₙeₙ) = x₁T(e₁) + x₂T(e₂) + ... + xₙT(eₙ). Let aᵢ = T(eᵢ) be the image of the i-th basis vector (an m-dimensional vector). Then: T(x) = x₁a₁ + x₂a₂ + ... + xₙaₙ = [a₁ a₂ ... aₙ] [x₁, x₂, ..., xₙ]ᵀ = Ax. Construction: The matrix A has columns {a₁, a₂, ..., aₙ} where aᵢ = T(eᵢ). Column i of A tells us where the i-th basis vector goes under T. Example: T: ℝ² → ℝ² is rotation by 90°. Apply T to basis vectors: T(e₁) = T([1,0]) = [0,1] (horizontal vector rotates to vertical). T(e₂) = T([0,1]) = [-1,0] (vertical vector rotates to negative horizontal). Matrix: A = [[0, -1], [1, 0]]. Verify: A[x, y]ᵀ = [0, -1; 1, 0][x; y] = [-y; x], which is indeed (x,y) rotated 90° counterclockwise. Why this matters: (1) Linearity reduces infinite possibilities to finite representation (n² entries for n×n matrix). (2) Computing T(x) reduces to matrix-vector multiplication (efficient algorithms). (3) Composition of transformations = matrix multiplication. (4) Properties of T (invertibility, rank, null space) can be studied via linear algebra. Converse: Given any m×n matrix A, T(x) = Ax defines a linear transformation. The correspondence is bijective: every linear transformation ↔ unique matrix. This unification is powerful: abstract transformations become concrete matrices, enabling computation and analysis. In ML: Neural network layers (Wx + b), PCA projections, data preprocessing—all are linear transformations represented as matrices.',
    keyPoints: [
      'Matrix A construction: columns are T(eᵢ) where eᵢ are standard basis vectors',
      'Every linear T: ℝⁿ → ℝᵐ ↔ unique m×n matrix A (bijective correspondence)',
      'ML: neural layers (Wx+b), PCA projections are all matrix representations of linear T',
    ],
  },
  {
    id: 'linear-trans-d2',
    question:
      "Discuss the geometric meaning of the determinant of a transformation matrix. How does the sign and magnitude of det(A) relate to the transformation's effect on space?",
    sampleAnswer:
      "The determinant captures how a linear transformation scales volumes and whether it preserves orientation. Geometric interpretation: Magnitude: |det(A)| = volume scaling factor. If we transform a unit cube (or unit square in 2D), |det(A)| is the volume (or area) of the resulting parallelepiped (parallelogram). Example: A = [[2, 0], [0, 3]] (scaling by 2× horizontally, 3× vertically). det(A) = 6. A unit square (area 1) becomes a rectangle with area 6. Sign: det(A) > 0: Orientation preserved (right-handed basis stays right-handed). det(A) < 0: Orientation reversed (right-handed becomes left-handed, like mirror reflection). det(A) = 0: Space collapses to lower dimension (volume becomes 0). Examples: (1) Rotation matrix R(θ) = [[cos θ, -sin θ], [sin θ, cos θ]]. det(R) = cos²θ + sin²θ = 1. Rotations preserve volume and orientation (rigid motion). (2) Reflection across x-axis: F = [[1, 0], [0, -1]]. det(F) = -1. Preserves area (|det| = 1) but reverses orientation (det < 0). (3) Projection onto x-axis: P = [[1, 0], [0, 0]]. det(P) = 0. Collapses 2D to 1D line (area becomes 0). Information loss: determinant measures loss. Properties: (1) det(AB) = det(A)·det(B). Volume scales multiplicatively under composition. (2) det(A⁻¹) = 1/det(A). Inverse transformation scales by reciprocal. (3) det(Aᵀ) = det(A). Transpose doesn't change volume scaling. (4) det(cA) = cⁿ·det(A) for n×n matrix. Scaling all dimensions by c scales volume by cⁿ. Why determinant relates to invertibility: If det(A) = 0, transformation collapses space (loses dimension). Information is lost—cannot uniquely invert. Example: projection P maps infinite vectors to same output. If det(A) ≠ 0, transformation is bijective (one-to-one and onto). Can invert: A⁻¹ exists. In ML applications: (1) Checking invertibility: For autoencoders, encoder-decoder should be invertible (no information loss). Check if det ≈ 0. (2) Numerical stability: Near-zero determinant (det ≈ 10⁻¹⁰) indicates ill-conditioned matrix. Small input perturbations cause large output changes. Regularization or SVD helps. (3) Jacobian determinant: In normalizing flows (generative models), det(J) is the volume change for probability density transformation. (4) Data augmentation: Determinant tells if transformation preserves, expands, or contracts regions of feature space. In summary: determinant is not just an algebraic formula—it's the fundamental geometric quantity measuring how transformations warp space. Positive = preserve orientation, negative = flip, zero = collapse. Magnitude = volume scaling.",
    keyPoints: [
      'det(A) magnitude: volume scaling factor (|det|=1 preserves, >1 expands, <1 contracts)',
      'det(A) sign: positive preserves orientation, negative reverses (mirror), zero collapses',
      'ML: det≈0 → ill-conditioned (numerical instability); Jacobian det in normalizing flows',
    ],
  },
  {
    id: 'linear-trans-d3',
    question:
      'In neural networks, each layer computes h = σ(Wx + b) where σ is a non-linear activation. Explain why the non-linearity is essential: what would happen if we stacked multiple linear transformations without activation functions?',
    sampleAnswer:
      'Non-linear activation functions are crucial for neural networks\' expressive power. Without them, deep networks collapse to shallow linear models. The problem with stacking linear transformations: Consider a 2-layer network without activation: h₁ = W₁x + b₁ (first layer). h₂ = W₂h₁ + b₂ = W₂(W₁x + b₁) + b₂ = (W₂W₁)x + (W₂b₁ + b₂) = Wx + b. Where W = W₂W₁ and b = W₂b₁ + b₂. Result: Equivalent to a single linear transformation! Generalizing: Stack L layers, each computing hₗ = Wₗhₗ₋₁ + bₗ. Final output: h_L = W_combined·x + b_combined, where W_combined = W_L·W_{L-1}·...·W₁. Composition of linear transformations is linear. No matter how many layers, a purely linear network can only learn linear functions. Why this is limiting: Linear models can only separate data with linear boundaries (hyperplanes). Many real-world problems require non-linear decision boundaries: - XOR problem: Cannot be solved by any linear classifier. Points (0,0) and (1,1) in one class, (0,1) and (1,0) in another. Linear boundary cannot separate them. - Image classification: Distinguishing cats from dogs requires highly non-linear feature combinations. - Natural language: Semantic relationships are fundamentally non-linear. Example (XOR): Input: x ∈ {(0,0), (0,1), (1,0), (1,1)}. Target: y = x₁ XOR x₂ = {0, 1, 1, 0}. Any linear model: f(x) = w₁x₁ + w₂x₂ + b. Cannot fit this data—no choice of w₁, w₂, b works. With non-linearity (2-layer network): h = σ(W₁x + b₁). y = W₂h + b₂. With ReLU activation, this can solve XOR. Hidden units learn features that linearly separate in transformed space. Universal Approximation Theorem: A neural network with even a single hidden layer and non-linear activation can approximate any continuous function (given enough neurons). Key requirement: non-linearity. Role of different activations: (1) Sigmoid σ(z) = 1/(1+e⁻ᶻ): Smooth, bounded [0,1]. Historical favorite. Issues: vanishing gradients. (2) Tanh: Bounded [-1,1], zero-centered. Better than sigmoid. Still vanishing gradients. (3) ReLU σ(z) = max(0, z): Dominant in modern networks. Advantages: - No vanishing gradients for positive values. - Sparse activation (many neurons output 0). - Efficient computation. - Biologically inspired. (4) Leaky ReLU, ELU, GELU: Variants addressing "dying ReLU" problem. Why depth helps (with non-linearity): Each layer can learn increasingly abstract features. Early layers: edges, textures (simple non-linear combinations). Middle layers: parts, shapes (compositions of early features). Deep layers: objects, concepts (high-level semantic features). This hierarchical feature learning is only possible with non-linearity at each layer. Concrete example (vision): Layer 1: Learns edge detectors (Gabor-like filters). Layer 2: Combines edges into corners, curves. Layer 3: Combines corners into parts (wheel, face). Layer 4: Combines parts into objects (car, person). Without activation, all layers collapse to one linear transformation—no hierarchy! Practical implications: (1) Always use non-linear activations between layers (except final regression output). (2) ReLU is default choice; try others if it fails. (3) Batch normalization can reduce dependence on activation choice. (4) For purely linear relationships (rare), linear regression suffices—no need for deep network. In summary: Composition of linear functions is linear. Non-linear activations break this constraint, enabling neural networks to learn arbitrarily complex functions. Without activation, a 100-layer network has no more expressive power than logistic regression. This is why σ is as important as W!',
    keyPoints: [
      'Composition: T₂(T₁(x)) = A₂A₁x (matrix multiplication); order matters (non-commutative)',
      'Invertible ⟺ det(A)≠0 ⟺ full rank ⟺ bijection (lossless transformation)',
      'ML: backprop through layers reverses composition; autoencoders need invertibility',
    ],
  },
];
