/**
 * Quiz questions for Tensor Operations in Deep Learning section
 */

export const tensoroperationsQuiz = [
  {
    id: 'tensor-d1',
    question:
      'Explain broadcasting in NumPy/PyTorch. What are the rules for broadcasting, and why is it useful for deep learning? Provide examples showing both valid and invalid broadcasting scenarios.',
    sampleAnswer:
      "Broadcasting is a mechanism that allows tensor operations on arrays of different shapes without explicit replication, saving memory and computation. Rules: (1) Align shapes from the right (trailing dimensions). (2) Dimensions are compatible if equal or one is 1. (3) Missing dimensions are treated as 1. (4) Broadcast smaller tensor by repeating along dimensions of size 1. Examples: Valid broadcasting: (1) Scalar + Matrix: (1,) + (3, 4) → (3, 4). Scalar broadcasts to every element. (2) Vector + Matrix (row): (4,) + (3, 4) → (3, 4). Vector (4,) becomes (1, 4), broadcasts row-wise. (3) Column + Matrix: (3, 1) + (3, 4) → (3, 4). Column broadcasts across columns. (4) 3D + 2D: (2, 3, 4) + (3, 4) → (2, 3, 4). 2D (3, 4) broadcasts across first dimension. (5) Different missing dims: (5, 1, 7) + (1, 6, 1) → (5, 6, 7). Both broadcast along their size-1 dimensions. Invalid broadcasting: (1) (3,) + (4,): Shapes don't align. 3 ≠ 4 and neither is 1. (2) (3, 4) + (3, 5): Last dimensions 4 ≠ 5. (3) (2, 3) + (3, 2): Shapes differ in incompatible ways. Why useful in deep learning: (1) Adding bias: X @ W + b where X is (batch, features), b is (features,). Broadcasting adds bias to each sample without loops. (2) Batch normalization: Normalize features across batch, then scale/shift with learnable (1, features) parameters. (3) Attention masks: (batch, seq_len, seq_len) attention scores + (1, 1, seq_len) mask. (4) Memory efficiency: Don't need to explicitly replicate data. A (1000, 1) array broadcasted to (1000, 1000) uses 1000× less memory than materializing full array. (5) Performance: Vectorized operations faster than Python loops. Internals: Broadcasting doesn't copy data—it adjusts strides. For array A with shape (3, 1), stride (8, 8) becomes (3, 4) with stride (8, 0). Stride 0 means \"repeat same element.\" Common pitfalls: (1) Unexpected broadcasting: A (3,) + B (3, 1) → (3, 3), not (3,). Solution: be explicit about shapes (reshape). (2) Memory explosion: Broadcasting large tensors can consume lots of memory if result is materialized. (3) Debugging: Print shapes religiously! Most bugs are shape mismatches. Best practices: (1) Use keepdims=True in reductions to preserve dimensions: np.mean(X, axis=1, keepdims=True) gives (n, 1) not (n,). (2) Be explicit with unsqueeze/expand_dims when needed. (3) Understand your framework: PyTorch and NumPy follow same rules, but TensorFlow has subtle differences. In summary: Broadcasting is a powerful abstraction that makes tensor code concise and efficient. Master the rules and common patterns—they're fundamental to writing effective deep learning code.",
    keyPoints: [
      'Broadcasting rules: align shapes from right, dimensions compatible if equal or 1',
      'Enables (batch, features) + (features,) without loops; fails if incompatible',
      'ML: bias addition, batch norm (X-mean)/std, attention mechanisms use broadcasting',
    ],
  },
  {
    id: 'tensor-d2',
    question:
      'Compare memory layout (row-major vs column-major) and its impact on performance. Why does access pattern matter, and how should you structure loops and operations for optimal cache utilization?',
    sampleAnswer:
      "Memory layout determines how multi-dimensional arrays are stored in linear (1D) memory. This profoundly affects performance due to cache locality. Row-major (C-style): Store rows contiguously. For 2D array A[i,j], element A[i, j] is at offset i*n_cols + j. Consecutive elements in same row are adjacent in memory. Column-major (Fortran-style): Store columns contiguously. Element A[i, j] is at offset i + j*n_rows. Consecutive elements in same column are adjacent. NumPy default: Row-major. MATLAB/Fortran: Column-major. Why it matters: Modern CPUs have hierarchical cache (L1, L2, L3). Accessing contiguous memory is ~100× faster than random access (cache lines fetch ~64 bytes). Cache-friendly access: If data is contiguous, CPU prefetches nearby elements. Cache-hostile access: Jumping around memory causes cache misses, stalling computation. Example (row-major matrix): Good: Iterate rows in inner loop: for i: for j: A[i, j]. Elements A[i, 0], A[i, 1], ... are contiguous. Bad: Iterate columns in inner loop: for j: for i: A[i, j]. Elements A[0, j], A[1, j], ... are strided by n_cols. For 1000×1000 matrix, bad pattern is 10-100× slower! Matrix multiplication performance: Consider C = A @ B. Naive triple loop: for i: for j: for k: C[i,j] += A[i,k] * B[k,j]. Access patterns: A[i,k]: row-wise (good). B[k,j]: column-wise (bad if row-major). C[i,j]: scattered writes (also bad). Optimized: Transpose B first, or use blocked algorithms (BLAS libraries do this). Modern BLAS (like OpenBLAS, MKL) achieve >90% peak hardware performance via careful cache optimization. Deep learning implications: (1) Batch dimension first: Store data as (batch, features). When processing batch, consecutive samples are contiguous—good for cache. (2) Vectorization: Use built-in operations (numpy, PyTorch) that exploit SIMD and cache. Hand-written loops in Python are 100-1000× slower. (3) Memory-bound vs compute-bound: Small models are often memory-bound (waiting for data). Large models (GPUs) are compute-bound (waiting for FLOPS). Cache matters more for memory-bound. (4) Convolutional layers: Implement as im2col (image-to-column) + matrix multiply. Transforms 2D convolution into cache-friendly matmul. Practical tips: (1) Prefer np.sum (axis=1) over manual loops. NumPy/PyTorch iterate optimally. (2) Use contiguous() in PyTorch if tensor is strided: x = x.transpose(0, 1).contiguous(). (3) Profile code: Use tools to find cache misses (e.g., perf on Linux). (4) For large matrices: Use libraries (BLAS, cuBLAS). They've invested decades in optimization. (5) On GPUs: Memory coalescing similar concept. Threads in same warp should access contiguous memory. Example benchmark (1000×1000 matrix): Python loops (bad): 1000ms. NumPy (good): 10ms (100× faster). Optimized BLAS (best): 1ms (1000× faster than naive). Takeaway: Cache matters. Structure data access contiguously. Use vetorized libraries. Don't write raw loops in Python.",
    keyPoints: [
      'Batched ops: process multiple samples simultaneously (matrix ops, not loops)',
      'GPU efficiency: 1000s of cores parallelize; batch size 32-256 typical (powers of 2)',
      'Trade-off: larger batch → better GPU utilization but more memory, fewer updates',
    ],
  },
  {
    id: 'tensor-d3',
    question:
      'Explain the attention mechanism in Transformers using tensor operations. Describe the shapes at each step and the role of Q, K, V matrices. Why is attention computed as softmax(QKᵀ/√d)V?',
    sampleAnswer:
      'Attention is the core of Transformers, enabling models to focus on relevant parts of input. Mathematically, it\'s elegant tensor operations. Setup: Input sequence: X (seq_len, d_model). E.g., (10, 512) for 10 tokens, each 512-dim embedding. Learn three projections: Query Q = XWq, Key K = XWk, Value V = XWv. Where Wq, Wk, Wv are (d_model, d_k) matrices (d_k often = d_model). Shapes: Q: (seq_len, d_k). K: (seq_len, d_k). V: (seq_len, d_v), often d_v = d_k. Step 1: Compute attention scores. Scores = Q @ Kᵀ. Shape: (seq_len, d_k) @ (d_k, seq_len) = (seq_len, seq_len). Interpretation: Scores[i, j] = similarity between query i and key j (dot product). High score = query i attends strongly to position j. Step 2: Scale. Scores_scaled = Scores / √d_k. Why? Dot products grow with dimension d_k. For large d_k, scores can be huge, making softmax saturate (gradients vanish). Dividing by √d_k keeps variance ~1, stabilizing training. Step 3: Softmax. Attention_weights = softmax(Scores_scaled, dim=-1). Shape: (seq_len, seq_len). Each row is a probability distribution (sums to 1). Row i gives attention distribution: how much query i attends to each position. Step 4: Weighted sum of values. Output = Attention_weights @ V. Shape: (seq_len, seq_len) @ (seq_len, d_v) = (seq_len, d_v). Interpretation: Output[i] = weighted combination of all values, with weights from attention_weights[i]. Position i "looks at" all positions, weighted by relevance. Why this works: (1) Q @ Kᵀ measures compatibility (dot product = cosine similarity if normalized). (2) Softmax converts scores to probabilities (non-negative, sum to 1). (3) Weighted sum aggregates information from relevant positions. Multi-head attention: Instead of single attention, use h heads in parallel. Split d_model into h heads: each head has dimension d_k = d_model / h. Concatenate outputs from all heads, then project. Allows model to attend to different aspects (e.g., syntax, semantics) simultaneously. Masked attention (for autoregressive models): Add mask to prevent position i from attending to future positions j > i. Set Scores[i, j > i] = -∞ before softmax, so they get 0 weight. Efficiency considerations: (1) Scores matrix (seq_len, seq_len) is O(seq_len²) memory and compute. Problem for long sequences (e.g., 10k tokens → 100M matrix). (2) Solutions: Sparse attention (attend only to subset), linear attention (approximate with low-rank), FlashAttention (optimize memory access). Example (simplified):seq_len = 4, d_k = 8. Q = [[q1], [q2], [q3], [q4]], each qᵢ is 8-dim. K = [[k1], [k2], [k3], [k4]]. Scores = [[q1·k1, q1·k2, q1·k3, q1·k4], [q2·k1, q2·k2, q2·k3, q2·k4], ..]. Each row: how much token i attends to all tokens. After softmax, row 1 might be [0.1, 0.6, 0.2, 0.1]: token 1 mostly attends to token 2. Output[1] = 0.1*v1 + 0.6*v2 + 0.2*v3 + 0.1*v4. Why attention revolutionized NLP: (1) Captures long-range dependencies (RNNs struggled). (2) Parallelizable (unlike sequential RNNs). (3) Interpretable (attention weights show what model attends to). In code (PyTorch-style): scores = (Q @ K.transpose(-2, -1)) / math.sqrt (d_k). attn_weights = F.softmax (scores, dim=-1). output = attn_weights @ V. Three lines, yet transforms NLP! Understanding tensor shapes and operations is key to implementing and debugging Transformers.',
    keyPoints: [
      'Deep learning ops: matrix mult (layers), batch norm (normalization), attention (QK^T)',
      'einsum: flexible notation for tensor contractions (matrix mult, trace, etc.)',
      'Memory layout: row-major (C) vs column-major (Fortran); contiguous for efficiency',
    ],
  },
];
