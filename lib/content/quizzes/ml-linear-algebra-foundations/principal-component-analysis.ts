/**
 * Quiz questions for Principal Component Analysis (PCA) section
 */

export const principalcomponentanalysisQuiz = [
  {
    id: 'pca-d1',
    question:
      'Derive why the principal components of PCA are the eigenvectors of the covariance matrix. Start from the optimization problem: find unit vector v₁ that maximizes variance of projected data Xv₁.',
    sampleAnswer:
      'PCA seeks directions of maximum variance. Mathematical formulation: Given centered data X (n samples, d features), find unit vector v₁ that maximizes variance of projected data. Variance of projection: Var(Xv₁) = (1/n)·||Xv₁||² = (1/n)·(Xv₁)ᵀ(Xv₁) = (1/n)·v₁ᵀXᵀXv₁ = v₁ᵀCv₁, where C = (1/n)XᵀX is the covariance matrix. Optimization problem: maximize v₁ᵀCv₁ subject to ||v₁||² = v₁ᵀv₁ = 1 (unit vector constraint). Lagrangian: L(v₁, λ) = v₁ᵀCv₁ - λ(v₁ᵀv₁ - 1). Taking gradient with respect to v₁: ∇L = 2Cv₁ - 2λv₁ = 0. This gives: Cv₁ = λv₁. This is the eigenvector equation! v₁ is an eigenvector of C with eigenvalue λ. To maximize variance, substitute back: v₁ᵀCv₁ = v₁ᵀ(λv₁) = λ(v₁ᵀv₁) = λ. So variance equals the eigenvalue λ. To maximize variance, choose v₁ to be the eigenvector corresponding to the largest eigenvalue. For subsequent components: Second PC v₂ maximizes variance subject to: (a) ||v₂|| = 1, (b) v₂ ⊥ v₁ (orthogonal to first PC). This gives v₂ = eigenvector for second-largest eigenvalue. By induction, all PCs are eigenvectors of C ordered by eigenvalue. Geometric intuition: Covariance matrix C describes how features co-vary. Eigenvectors are the "natural axes" of the data distribution. Eigenvalues measure spread along each axis. PCA rotates coordinate system to align with these natural axes, ordering them by importance (variance). Alternative derivation via SVD: X = UΣVᵀ (SVD of data matrix). Then C = (1/n)XᵀX = (1/n)VΣᵀUᵀUΣVᵀ = (1/n)VΣ²Vᵀ (since UᵀU = I). This is eigendecomposition of C with eigenvalues λᵢ = σᵢ²/n and eigenvectors = columns of V. Why this is profound: PCA is not just a heuristic—it\'s the mathematically optimal linear dimensionality reduction that preserves maximum variance. The connection to eigenanalysis makes it tractable and gives deep geometric insight.',
    keyPoints: [
      'Optimization: max v₁ᵀCv₁ subject to ||v₁||=1 → Lagrangian → Cv₁ = λv₁ (eigenvector equation)',
      'Variance = eigenvalue λ; max variance → largest eigenvalue eigenvector',
      'PCA = mathematically optimal linear dimensionality reduction (preserves max variance)',
    ],
  },
  {
    id: 'pca-d2',
    question:
      'Compare PCA computed via eigendecomposition of the covariance matrix versus SVD of the data matrix. Discuss computational complexity, numerical stability, and when each approach is preferred.',
    sampleAnswer:
      "PCA can be computed in two ways: eigendecomposition of covariance matrix or SVD of data matrix. Both give identical principal components, but differ in efficiency and stability. Method 1 - Eigendecomposition of Covariance Matrix: Steps: (1) Center data X (n×d). (2) Compute covariance C = XᵀX/n (d×d). (3) Eigendecomposition: C = VΛVᵀ. (4) Principal components = columns of V, variances = diagonal of Λ. Complexity: O(nd²) to form C, O(d³) for eigendecomposition. Total: O(nd² + d³). For wide matrices (n < d), this is O(nd²). For tall matrices (n >> d), this is O(nd²). Memory: Store C (d×d), efficient for moderate d. Method 2 - SVD of Data Matrix: Steps: (1) Center data X (n×d). (2) SVD: X = UΣVᵀ. (3) Principal components = columns of V (or rows of Vᵀ). (4) Variances = σᵢ²/(n-1). Complexity: O(min (nd², n²d)) for full SVD. For n >> d: O(nd²). For d >> n: O(n²d). Thin SVD (only first min (n,d) components): O(ndk) where k = rank. Memory: Never form XᵀX, work directly with X. Comparison: Numerical Stability: SVD is significantly more stable. Reason: forming XᵀX squares condition number. If κ(X) = 10⁶, then κ(XᵀX) = 10¹². This causes loss of precision. SVD avoids this by working directly with X. For ill-conditioned data, eigendecomposition can produce inaccurate eigenvalues/eigenvectors, while SVD remains accurate. Recommendation: Always use SVD for PCA in practice. Computational Efficiency: Tall matrices (n >> d, e.g., 10,000 samples, 50 features): Eigen: O(nd²) = O(10,000·50²) = O(25M). SVD: O(nd²) = O(10,000·50²) = O(25M). Both are O(nd²), similar speed. Thin SVD can be faster. Wide matrices (d >> n, e.g., 100 samples, 10,000 features like gene expression): Eigen: O(nd²) = O(100·10,000²) = O(10B). SVD: O(n²d) = O(100²·10,000) = O(100M). SVD is much faster (100× speedup)! Use randomized SVD for even better performance. Memory: Eigen requires storing d×d covariance matrix. For d = 10,000, C has 100M entries (800MB for float64). SVD works directly with X, no covariance matrix needed. For large d, SVD is essential. When to use each: Use SVD (almost always): - Default choice for numerical stability. - Large d (wide matrices). - Limited memory. - Industry/production code. Use Eigendecomposition: - Educational purposes (more direct connection to theory). - When covariance matrix is already computed (e.g., from streaming data). - Very small d where stability isn't critical. Practical note: sklearn.decomposition.PCA uses randomized SVD by default for n_components < min (n, d), which is even faster for large matrices while maintaining accuracy. This allows PCA on millions of features. In summary: SVD is the gold standard for computing PCA—more stable, often faster, and scales to larger dimensions. Always prefer SVD in practice.",
    keyPoints: [
      'Eigen: forms XᵀX (squares κ, unstable); SVD: works on X directly (stable)',
      'Complexity: both O(nd²) for n>>d; SVD faster for d>>n (100× speedup possible)',
      'Always use SVD in practice: sklearn uses randomized SVD (stable, fast, scales)',
    ],
  },
  {
    id: 'pca-d3',
    question:
      'PCA assumes that directions of maximum variance correspond to the most important structure in data. Discuss scenarios where this assumption breaks down and alternative dimensionality reduction methods would be more appropriate.',
    sampleAnswer:
      'PCA\'s core assumption is that variance = importance. While often reasonable, this fails in several important scenarios: Scenario 1: Signal in low-variance directions. Example: Classifying digits. Suppose one pixel has high variance because it randomly flickers (noise), while another pixel with low variance contains the crucial edge of a digit (signal). PCA would emphasize the noisy pixel and discard the informative one! Alternative: Linear Discriminant Analysis (LDA) finds directions that maximize class separation rather than variance. For supervised tasks, LDA often outperforms PCA. Scenario 2: Nonlinear manifolds. Example: Swiss roll dataset. Data lives on a 2D manifold (intrinsic dimension = 2) embedded in 3D space. PCA requires all 3 dimensions because the manifold isn\'t aligned with any linear subspace. PCA "unfolds" by finding linear directions, which is inefficient. Alternative: Manifold learning methods (Isomap, Locally Linear Embedding, t-SNE, UMAP) preserve local neighborhood structure and can capture the 2D manifold with 2 components. These methods use geodesic distances or local neighborhoods rather than global variance. Scenario 3: Multimodal distributions. Example: Data with two clusters separated along axis 1, but high spread within each cluster along axis 2. PCA\'s first PC might point along axis 2 (high variance), missing the cluster separation (axis 1, lower variance but more meaningful). Alternative: Cluster-specific PCA, mixture models, or supervised methods like LDA. Scenario 4: Adversarial noise. Example: Image data with imperceptible adversarial perturbations. These perturbations might have low variance but are carefully crafted to fool classifiers. PCA focusing on high variance would keep the natural image variation and discard the adversarial signal. Alternative: Robust PCA separates low-rank structure from sparse outliers. Used in video surveillance to separate background (low-rank) from moving objects (sparse). Scenario 5: Temporal or spatial structure. Example: Time series where important patterns occur at specific frequencies or spatial correlations matter. PCA treats all timepoints/locations independently, ignoring temporal/spatial structure. Alternative: - Fourier/wavelet analysis for frequency-domain patterns. - Autoregressive models for temporal dependencies. - Convolutional autoencoders for spatial structure. Scenario 6: Sparse or interpretable structure. Example: Gene expression data where biologists want to identify specific genes (sparse loadings), not linear combinations of all genes (dense PCA loadings). Alternative: Sparse PCA constrains loadings to have many zeros, giving interpretable components. Non-negative Matrix Factorization (NMF) gives additive, parts-based representations. Scenario 7: Extreme high dimensions with limited samples (p >> n). Example: Genomics (20,000 genes, 100 patients). Covariance matrix is rank-deficient and many eigenvalues are effectively noise. PCA overfits to noise rather than signal. Alternative: - Regularized PCA (ridge PCA, sparse PCA). - Random projections (Johnson-Lindenstrauss lemma). - Domain-specific feature selection. Scenario 8: Non-Gaussian distributions. PCA is optimal for Gaussian data. For heavy-tailed or multi-modal distributions, variance may not capture important structure. Alternative: Independent Component Analysis (ICA) for non-Gaussian sources. Kernel PCA with appropriate kernel for specific distributions. When PCA works well: - Data is approximately Gaussian or unimodal. - Variance correlates with signal (not noise). - Linear structure dominates. - Unsupervised setting (no labels). - Need fast, deterministic method. - Want global, coarse-grained compression. When to try alternatives: - Supervised task: Try LDA first. - Nonlinear patterns: Kernel PCA, t-SNE, UMAP, autoencoders. - Sparse/interpretable: Sparse PCA, NMF. - Robust to outliers: Robust PCA. - Time/spatial structure: Domain-specific methods. - Extreme high-dim: Regularization, random projections. Practical workflow: (1) Start with PCA (fast baseline). (2) Check if downstream task benefits. (3) If poor performance, investigate: Is data nonlinear? Use manifold learning. Is variance uninformative? Try LDA or ICA. Need interpretability? Use sparse methods. (4) Visualize principal components and loadings for diagnostics. In summary: PCA is a powerful and widely applicable method, but not universal. Understanding its assumptions helps recognize when alternatives are needed. The "right" dimensionality reduction depends on the data structure, task, and goals (compression, visualization, classification, interpretability).',
    keyPoints: [
      'PCA fails when: signal in low-variance, nonlinear manifolds, multimodal data',
      'Alternatives: LDA (supervised), t-SNE/UMAP (nonlinear), Sparse PCA (interpretable)',
      'Choose based on: supervised vs unsupervised, linear vs nonlinear, interpretability needs',
    ],
  },
];
