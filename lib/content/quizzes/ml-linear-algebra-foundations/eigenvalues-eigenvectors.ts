/**
 * Quiz questions for Eigenvalues & Eigenvectors section
 */

export const eigenvalueseigenvectorsQuiz = [
  {
    id: 'eigen-d1',
    question:
      'Explain geometrically what eigenvalues and eigenvectors represent for a 2×2 matrix. Why do eigenvectors only get scaled and not rotated? Provide an example with a diagonal matrix and a general matrix.',
    sampleAnswer:
      'Eigenvalues and eigenvectors reveal the "invariant directions" of a linear transformation. When matrix A transforms space, most vectors change both direction and magnitude. But eigenvectors are special: they only get scaled (stretched or compressed) by factor λ (eigenvalue), maintaining their direction (or reversing if λ < 0). Geometric intuition: imagine A as stretching/rotating space. Eigenvectors point along axes where only stretching occurs, no rotation. Example 1 (Diagonal matrix): A = [[3, 0], [0, 2]] represents stretching by 3× horizontally and 2× vertically. Eigenvectors are [1,0] and [0,1] (coordinate axes) with eigenvalues 3 and 2. A already shows its action clearly—it scales each axis independently. Example 2 (General matrix): A = [[2, 1], [1, 2]] is not aligned with coordinate axes. Computing eigenvalues: det(A - λI) = (2-λ)² - 1 = λ² - 4λ + 3 = (λ-3)(λ-1) = 0, giving λ₁=3, λ₂=1. For λ=3: (A-3I)v = 0 → [[-1,1],[1,-1]]v = 0 → v₁ = [1,1] (direction along y=x). For λ=1: (A-I)v = 0 → [[1,1],[1,1]]v = 0 → v₂ = [1,-1] (direction along y=-x). This matrix stretches 3× along the [1,1] diagonal and 1× (no change) along the [1,-1] diagonal. Why no rotation for eigenvectors? Rotation would change direction, but Av = λv means A maps v to a scalar multiple of itself, which is parallel to v. Any rotation would violate this. Physically: eigenvectors represent "principal axes" of the transformation. In ML: for covariance matrix, eigenvectors are principal components—directions of maximum variance. Data naturally spreads along these directions, and we can understand data structure by examining these invariant directions.',
    keyPoints: [
      'Eigenvectors: invariant directions (only scaled, not rotated) Av = λv',
      'Eigenvalues: scaling factor along each eigenvector direction',
      'ML: covariance eigenvectors = principal components (max variance directions)',
    ],
  },
  {
    id: 'eigen-d2',
    question:
      'The Spectral Theorem states that symmetric matrices can be written as A = QΛQᵀ where Q is orthogonal. Explain why this decomposition is so powerful, particularly for computing matrix functions (like A^n, exp(A), sqrt(A)), and why symmetric matrices are ubiquitous in machine learning.',
    sampleAnswer:
      'The Spectral Theorem (A = QΛQᵀ for symmetric A) is extraordinarily powerful because: (1) Computational efficiency: To compute A^n = (QΛQᵀ)^n = QΛ^nQᵀ. Since Λ is diagonal, Λ^n just raises each diagonal entry to power n, taking O(n) time vs O(n³) for matrix multiplication. Similarly, exp(A) = Q·exp(Λ)·Qᵀ where exp(Λ) = diag(e^λ₁, e^λ₂, ...), and sqrt(A) = Q·sqrt(Λ)·Qᵀ = Q·diag(√λ₁, √λ₂, ...)·Qᵀ. This makes complex matrix functions tractable. (2) Interpretability: Q provides orthonormal basis of eigenvectors—the "natural coordinates" for the transformation. Λ shows pure scaling along each axis. This diagonal form reveals the transformation\'s essential behavior. (3) Stability: Orthogonal matrices preserve lengths and angles (QᵀQ=I), making computations numerically stable. No amplification of errors. (4) Real eigenvalues: Symmetric matrices have real λ (no complex numbers), simplifying analysis and ensuring positive definiteness when all λ>0. Why symmetric matrices dominate ML: (a) Covariance matrices: Cov(X) = E[(X-μ)(X-μ)ᵀ] is symmetric. Eigenvectors = principal components (PCA), eigenvalues = variance along each component. This is foundational for dimensionality reduction. (b) Kernel matrices: K(i,j) = k(xᵢ, xⱼ) is symmetric (kernel functions are symmetric). Used in SVMs, Gaussian processes, kernel PCA. (c) Graph Laplacians: L = D - A (degree - adjacency) is symmetric for undirected graphs. Spectral clustering uses eigenvectors of L. (d) Hessian matrices: Second derivatives H(i,j) = ∂²f/∂xᵢ∂xⱼ are symmetric (by Schwarz\'s theorem). Used in optimization (Newton method) to understand loss surface curvature. (e) Gram matrices: XᵀX is always symmetric. Appears in normal equations (linear regression), momentum in neural networks. Example: To find optimal learning rate in gradient descent, analyze eigenvalues of Hessian. Largest eigenvalue determines maximum stable learning rate. Without Spectral Theorem, this analysis would be intractable. The ubiquity of symmetric matrices in ML stems from: natural occurrence (covariance, similarity), mathematical properties (real eigenvalues, orthogonal eigenvectors), and computational advantages (efficient diagonalization). The Spectral Theorem turns abstract transformations into understandable, computable operations.',
    keyPoints: [
      'A = QΛQᵀ (symmetric): efficient matrix functions A^n = QΛ^nQᵀ (O(n) vs O(n³))',
      'Real eigenvalues, orthogonal eigenvectors: numerical stability, interpretability',
      'ML ubiquity: covariance, kernels, graph Laplacians, Hessians, Gram matrices',
    ],
  },
  {
    id: 'eigen-d3',
    question:
      'In PCA, why do we use eigenvectors of the covariance matrix as principal components? Explain the connection between eigenvalues and explained variance, and discuss how to choose the number of components to retain.',
    sampleAnswer:
      'PCA finds directions of maximum variance in data. These directions are precisely the eigenvectors of the covariance matrix, and their importance is measured by eigenvalues. Mathematical justification: Given centered data X (n samples, d features), we want to find unit vector v₁ that maximizes variance of projected data. Variance of X projected onto v₁ is Var(Xv₁) = v₁ᵀ·Cov(X)·v₁ = v₁ᵀCv₁ where C is the covariance matrix. Using Lagrange multipliers to maximize v₁ᵀCv₁ subject to ||v₁||=1, we get: Cv₁ = λv₁. This is exactly the eigenvector equation! The maximum variance equals the largest eigenvalue λ₁, achieved when v₁ is the corresponding eigenvector. For subsequent components: the second PC maximizes variance among directions orthogonal to v₁, which is the eigenvector for second-largest eigenvalue, etc. Why eigenvalues = variance: For principal component vᵢ, the variance of projected data is λᵢ. Total variance = Tr(C) = Σλᵢ. Thus, explained variance ratio = λᵢ / Σλⱼ. This quantifies how much information each PC captures. Example: If λ = [50, 30, 15, 5], total variance = 100. PC1 explains 50%, PC2 explains 30%, PC1+PC2 explain 80%. Choosing number of components: (1) Explained variance threshold: Keep components until cumulative explained variance ≥ target (e.g., 95%). If cumulative is [0.5, 0.8, 0.95, 1.0], keep 3 components. (2) Elbow method: Plot eigenvalues (scree plot). Look for "elbow" where curve flattens—subsequent components add little information. (3) Kaiser criterion: Keep components with λᵢ > 1 (for standardized data, this means PC captures more variance than a single feature). (4) Cross-validation: Choose number that optimizes downstream task performance. (5) Interpretability: Sometimes fewer components are preferred for visualization (2-3) even if explained variance is lower. Trade-offs: More components = more information retained but higher dimensionality. Fewer components = more compression but information loss. Practical considerations: (a) Curse of dimensionality: In high dimensions (d >> n), many eigenvalues are near-zero noise. Aggressive reduction often helps. (b) Computational cost: Full eigendecomposition is O(d³). For very high d, use truncated SVD to compute only top k components efficiently. (c) Non-linear structure: PCA finds linear directions. If data has non-linear structure (manifold), kernel PCA or autoencoders might be better. Why PCA works: Data often has redundancy (correlated features). PCA decorrelates features (PCs are orthogonal) and orders them by importance (eigenvalues). This separates signal (large λ) from noise (small λ), enabling effective dimensionality reduction. In summary: eigenvectors = directions of variance, eigenvalues = amount of variance. PCA exploits eigen-structure of covariance to find optimal low-rank approximation of data.',
    keyPoints: [
      'PCA: max variance direction solves Cv₁ = λv₁ (eigen problem)',
      'Eigenvalue λᵢ = variance explained by PCᵢ; explained variance ratio = λᵢ/Σλⱼ',
      'Choose components: 95% variance threshold, scree plot elbow, cross-validation',
    ],
  },
];
