/**
 * Quiz questions for Sparse Linear Algebra section
 */

export const sparselinearalgebraQuiz = [
  {
    id: 'sparse-d1',
    question:
      'Explain the three main sparse matrix formats (COO, CSR, CSC). When would you choose each one, and what are the trade-offs in terms of memory, construction time, and operation efficiency?',
    sampleAnswer:
      'Sparse matrix formats trade-off between construction simplicity, memory overhead, and operation efficiency. COO (Coordinate Format): Structure: Three arrays: row indices, column indices, data. Example: row=[0,1,2], col=[2,0,1], data=[5,3,7] represents matrix with (0,2)=5, (1,0)=3, (2,1)=7. Memory: 3×nnz (row, col, data). Pros: (1) Simple to construct—just append (row, col, val) triplets. (2) Good for building matrices incrementally. (3) Easy to understand. Cons: (1) No random access—must scan all entries to find element. (2) No efficient arithmetic (addition, multiplication). (3) Duplicate entries allowed (must sum on conversion). When to use: Initial construction, data loading, converting to other formats. Never use for arithmetic. CSR (Compressed Sparse Row): Structure: Three arrays: data (nnz values), indices (nnz column indices), indptr (n_rows+1 row pointers). Example: indptr=[0,2,3,5] means row 0 has elements at data[0:2], row 1 at data[2:3], row 2 at data[3:5]. Memory: 2×nnz + (n_rows+1). Slightly more efficient than COO. Pros: (1) Fast row slicing: row i is data[indptr[i]:indptr[i+1]]. O(nnz_row) not O(nnz). (2) Efficient matrix-vector product: y[i] = Σⱼ A[i,j]*x[j] iterates through row i. (3) Fast row operations (row sums, etc.). (4) Standard format for most sparse libraries. Cons: (1) Column slicing slow—must scan all rows. (2) Changing sparsity structure expensive (inserting/deleting non-zeros requires shifting arrays). (3) Construction slower than COO. When to use: Default format for arithmetic, matrix-vector products, iterative solvers. Especially when accessing rows. CSC (Compressed Sparse Column): Structure: Like CSR but column-oriented. data, indices (now row indices), indptr (now column pointers). Memory: Same as CSR. Pros: (1) Fast column slicing. (2) Efficient operations like Aᵀx (transpose-vector product) or column sums. Cons: (1) Row slicing slow. (2) Same structure change issues as CSR. When to use: When primarily accessing columns, or when computing Aᵀx frequently. Comparing CSR vs CSC: CSR: Prefer for A @ x (matrix-vector). CSC: Prefer for Aᵀ @ x (transpose-vector, common in optimization). Some libraries (like scipy) can convert between them, but conversion is O(nnz log nnz) (sorting required). Trade-off summary: COO → CSR/CSC: O(nnz log nnz) sorting. Worth it if doing multiple operations. CSR ↔ CSC: O(nnz log nnz). Expensive, avoid frequent conversion. Changing structure (insert/delete): Convert to LIL (List of Lists) or DOK (Dictionary of Keys), modify, convert back. Or rebuild from COO. Practical workflow: (1) Build with COO (simple). (2) Convert to CSR/CSC once. (3) Perform arithmetic in CSR/CSC. (4) Never modify structure of CSR/CSC directly. In deep learning: PyTorch sparse tensors primarily use COO. TensorFlow uses CSR/CSC. Choice depends on typical access patterns.',
    keyPoints: [
      'COO: (row, col, value) triplets, easy construction; CSR: fast row ops, mat-vec',
      'CSC: fast column ops; Trade-off: construction time vs operation speed',
      'Choose based on operations: CSR for sklearn models, CSC for matrix factorization',
    ],
  },
  {
    id: 'sparse-d2',
    question:
      'Discuss "fill-in" in sparse matrix factorization. Why does LU decomposition of a sparse matrix often produce dense factors? How do iterative solvers avoid this problem, and when should you use each approach?',
    sampleAnswer:
      "Fill-in is a major challenge in sparse linear algebra: factorizing sparse A often produces much denser factors. What is fill-in? Start with sparse A (most entries zero). Compute LU: A = LU. Often, L and U have many more non-zeros than A! Example: Tridiagonal matrix (3 diagonals, O(n) non-zeros). LU factors: L and U are lower/upper triangular with O(n²) non-zeros. We destroyed sparsity! Why fill-in occurs: LU elimination modifies matrix entries. Zero at (i,j) can become non-zero if elimination affects it. Specifically: A[i,j] → A[i,j] - A[i,k] * A[k,j] / A[k,k]. If A[i,k] ≠ 0 and A[k,j] ≠ 0, then A[i,j] becomes non-zero (even if originally zero). Geometric interpretation: Sparsity pattern reflects graph structure. Fill-in occurs when graph becomes more connected during elimination. Example: Sparse matrix for 2D grid (5-point stencil, nnz = O(n)). LU factors have nnz = O(n^{3/2}) for 2D grid. For 3D: A has O(n) but LU has O(n^{4/3}). Practical impact: Million-dimensional sparse system: A has ~10⁶ non-zeros (feasible). LU might have ~10¹² non-zeros (1 TB memory, infeasible). This makes direct solvers unusable for large sparse systems. Strategies to reduce fill-in: (1) Reordering: Permute rows/columns to minimize fill. Algorithms: Minimum degree, nested dissection, Cuthill-McKee. Can reduce fill significantly but doesn't eliminate it. (2) Incomplete factorizations: ILU (Incomplete LU) discards small entries during factorization, maintaining sparsity. Used as preconditioner for iterative methods. (3) Iterative solvers: Avoid factorization entirely! Iterative Methods (Krylov subspace): Instead of factoring A, iteratively improve solution. Conjugate Gradient (CG): For symmetric positive definite A. Iteration: xₖ₊₁ = xₖ + αₖpₖ (move along search direction pₖ). Each iteration requires matvec Apₖ, which is O(nnz). Never forms A⁻¹ or LU! Converges in ≤n iterations (theory), often much faster (practice). GMRES: For general (non-symmetric) A. Similar iteration with different search directions. Advantages of iterative: (1) No fill-in—always work with sparse A. (2) Memory: O(nnz) vs O(n²) for factors. (3) Can stop early (approximate solution often sufficient). (4) Embarrassingly parallelizable (matvec scales well). Disadvantages: (1) Convergence depends on conditioning. Ill-conditioned A requires many iterations. (2) No direct solution—only approximation. (3) Preconditioning often necessary (incomplete LU, multigrid). When to use direct (LU/Cholesky): (1) Small to medium systems (n < 10⁴). (2) Need exact solution. (3) Solve multiple systems with same A (factorize once, reuse). (4) Dense or low fill-in. When to use iterative: (1) Large sparse systems (n > 10⁵). (2) Approximate solution sufficient. (3) Good preconditioner available. (4) Memory constrained. Hybrid approach: Use iterative with direct preconditioner. ILU factors approximate A, accelerate CG/GMRES convergence. Deep learning context: Most DL systems are overdetermined (least squares), use normal equations (AᵀA)x = Aᵀb or QR (iterative often not needed for moderate size). For very large problems (billions of parameters), use stochastic gradient methods—never form Hessian! Takeaway: Fill-in makes direct sparse solvers impractical for large systems. Iterative methods are essential for scalability. Understanding trade-offs helps choose the right solver.",
    keyPoints: [
      'NLP: TF-IDF (10k-100k dims, <1% non-zero); Graphs: adjacency matrix (sparse)',
      'Recommenders: user-item matrix (millions users/items, sparse ratings)',
      'Sparse storage O(nnz) vs dense O(n²); operations O(nnz) vs O(n²)',
    ],
  },
  {
    id: 'sparse-d3',
    question:
      "In deep learning, neural network pruning creates sparse weight matrices. Discuss the benefits and challenges of sparse neural networks. Why haven't they completely replaced dense networks despite potential for huge speedups?",
    sampleAnswer:
      "Sparse neural networks promise massive compression and speedup by setting most weights to zero. Yet they haven't dominated. Why? Benefits of sparse networks: (1) Compression: 90-99% sparsity → 10-100× fewer parameters. A 100MB model becomes 1-10MB. Crucial for mobile/edge devices. (2) Theoretical speedup: O(nnz) vs O(n²) operations. Should be 10-100× faster! (3) Regularization: Sparsity can reduce overfitting (fewer parameters to overfit with). (4) Interpretability: Sparse = few connections, easier to understand. (5) Lottery Ticket Hypothesis: Sparse subnetworks exist that match dense performance. Finding them enables training smaller models from scratch. Challenges and why sparse isn't winning: (1) Hardware efficiency: Modern GPUs/TPUs optimized for dense matrix multiplication (cuBLAS, tensor cores). Dense matmul achieves >80% peak FLOPS. Sparse operations are memory-bound, achieve <20% peak. Even with 10× fewer operations, sparse can be slower than dense on GPU! Example: Dense 1000×1000 @ 1000 vector: 1ms (optimized). Sparse (10% nnz) same operation: 0.5ms (2× speedup, not 10× due to overhead). (2) Irregular memory access: Sparse = random access patterns, poor cache utilization. GPUs rely on coalesced memory access (threads access contiguous memory). Sparse breaks this. (3) Software support: PyTorch/TensorFlow sparse operations immature. Limited layer types (mostly linear), poor autograd support, bugs. Dense has decades of optimization (BLAS, cuDNN). (4) Structured vs unstructured sparsity: Unstructured: Any weight can be zero. Hard to exploit on hardware (irregular). Structured: Entire rows/cols/blocks zero. Easier to implement (prune neurons, not weights). But requires more sparsity for same speedup. (5) Training dynamics: Sparse networks harder to train. Dead neurons (zero gradient) never recover. Dynamic sparsity (change pattern during training) complex. Pruning: Train dense → prune → fine-tune. Requires training dense first! (6) Precision: Sparse often requires lower precision (int8) for real speedup. Combining sparsity + quantization complex. When sparse works well: (1) Extreme sparsity (>99%): At 99.9% sparsity, even inefficient sparse ops win. E.g., embeddings (vocab size 1M, most words rare). (2) CPU inference: CPUs have less memory bandwidth, benefit more from reduced mem access. (3) Specialized hardware: Dedicated sparse accelerators (Google Sparse Core, Cerebras, etc.). (4) Natural sparsity: Some domains inherently sparse (text sparse features, knowledge graphs). Lottery Ticket & Pruning Strategies: Lottery Ticket: Sparse subnetwork exists from random init that can train to same accuracy. Implication: Could skip dense training if we found the subnetwork. Challenge: Finding it requires training dense model (catch-22). Pruning methods: (1) Magnitude: Remove weights with smallest |w|. Simple, effective. (2) Gradient-based: Remove weights with smallest |w·∇w| (impact on loss). (3) Structured: Prune entire neurons/channels/layers. (4) Lottery Ticket Rewinding: Prune, reset to early weights (not init), retrain. Iterative Magnitude Pruning (IMP): Train → prune 20% → fine-tune → repeat. Can reach 90%+ sparsity with <1% accuracy loss. But still requires full dense training each cycle. Future of sparse: (1) Hardware: Specialized accelerators (Cerebras, Graphcore) improve sparse performance. (2) Algorithms: Sparse from scratch (RigL, SET—dynamically grow/prune during training). (3) Structured sparsity: Block-sparse, 2:4 sparsity (hardware-friendly). (4) Hybrid: Sparse backbone + dense heads. Current state: Sparse research active, but production models mostly dense. Exceptions: Embeddings (naturally sparse), MoE (Mixture of Experts—structured sparsity). Takeaway: Sparse neural nets theoretically compelling, practically challenging. Hardware and software need to catch up. Structured sparsity and specialized accelerators are promising directions. For now, dense wins in most scenarios due to mature infrastructure.",
    keyPoints: [
      'Iterative solvers: CG (symmetric positive definite), GMRES (general systems)',
      'Avoid dense ops: matrix multiply often destroys sparsity (fill-in problem)',
      'SciPy: scipy.sparse.linalg.cg, spsolve; scales to millions of variables',
    ],
  },
];
