export const bidAskSpreadDecompositionQuiz = [
  {
    id: 'basd-q-1',
    question:
      'Implement Glosten-Harris spread decomposition system that: (1) Regresses price changes on trade direction (ΔP = θ·Q + ψ·ΔQ), (2) Calculates adverse selection (θ, permanent) and order processing (ψ, transitory) components, (3) Tracks components over time (rolling window regression), (4) Detects regime changes (adverse selection spikes during news), (5) Provides spread predictions (future spread based on current adverse selection). Requirements: Handle 100K+ trades, statistical significance testing (t-stats, R²), regime detection (structural breaks), real-time updates. How do you handle: Multicollinearity (Q and ΔQ correlated)? Small sample bias (noisy estimates with few trades)? Non-constant components (θ and ψ change over time)?',
    sampleAnswer:
      'Glosten-Harris implementation: Model: ΔP_t = θ·Q_t + ψ·(Q_t - Q_{t-1}) + ε_t, θ = adverse selection (permanent price impact per trade direction), ψ = order processing (transitory, reverts), Interpretation: High θ = informed trading (price change persists), High ψ = liquidity cost (price bounces back). Implementation: class GlostenHarrisDecomposer: def decompose(trades, window=100): price_changes = diff(prices), Q_t = [1 if buy else -1 for each trade], delta_Q = diff(Q_t), X = stack([Q_t[1:], delta_Q]), y = price_changes, model = OLS(y, X).fit(), theta, psi = model.params, return theta (adverse_sel), psi (processing), model.rsquared. Rolling windows: For each window: Run regression on last 100 trades, Extract θ_t and ψ_t, Store with timestamp, Plot over time (detect regime changes). Statistical testing: T-statistics: t_theta = theta / se_theta, Significance: t > 2 (95% confidence), R-squared: Goodness of fit (>0.3 good, <0.1 poor), F-test: Overall model significance. Multicollinearity: Problem: Q_t and ΔQ_t correlated (VIF > 5), Effect: Unstable coefficient estimates (high standard errors), Detection: Calculate VIF = 1 / (1 - R²_j) for each variable. Solutions: Orthogonalization: Use Gram-Schmidt to decorrelate Q and ΔQ, Ridge regression: Add L2 penalty (reduces variance, slight bias), Separate regressions: Estimate θ from ΔP ~ Q, then ψ from residuals ~ ΔQ. Regime detection: Structural breaks: Chow test: Test if θ significantly different pre/post time T, Rolling Chow: Test at multiple breakpoints, find optimal split. CUSUM: Cumulative sum of residuals (detects shifts), Brown-Durbin-Evans: Test for parameter instability. News events: High adverse selection: θ spikes during earnings, economic releases, Regime 1 (calm): θ = 0.005 (0.5 bps per trade), Regime 2 (news): θ = 0.020 (2.0 bps, 4× higher). Time-varying parameters: Kalman filter: State-space model with θ_t and ψ_t as states, θ_t = θ_{t-1} + η_t (random walk), Measurement: ΔP_t = θ_t·Q_t + ψ_t·ΔQ_t + ε_t, Update: Recursive estimation (real-time). GARCH: Model volatility of θ (adverse selection varies over time), Conditional: θ_t | θ_{t-1} ~ N(μ, σ²_t), Forecast: Predict next period adverse selection. Application: Spread prediction: Given current θ and ψ, predict spread: Spread = 2(θ + ψ), Adjustment: If θ high (news period), widen spread preemptively, Market making: Quote bid-ask = mid ± (θ + ψ + inventory_adj). Performance: 100K trades: Process in batches (1K trades per batch), latency <1ms per batch, Memory: Rolling buffer (keep last 10K trades), Statistics: Update incrementally (online regression), Real-time: New trade → update regression → emit new θ, ψ.',
    keyPoints: [
      'Model: ΔP = θ·Q + ψ·ΔQ, θ=adverse selection (permanent), ψ=order processing (transitory), OLS regression, extract coefficients',
      'Rolling windows: 100-trade windows, track θ_t and ψ_t over time, detect regime changes (Chow test, CUSUM)',
      'Multicollinearity: VIF detection, solutions (orthogonalization, ridge regression, separate regressions)',
      'Time-varying: Kalman filter (state-space model), GARCH for volatility, recursive estimation',
      'Applications: Spread = 2(θ+ψ), widen during high θ (news), market making quotes',
    ],
  },
  {
    id: 'basd-q-2',
    question:
      'Build Kyle lambda estimator that measures adverse selection via price impact per unit volume (λ = Cov(ΔP, Q) / Var(Q)). Provide rolling lambda (time-series), compare across stocks (liquidity vs adverse selection), detect informed trading periods (lambda spikes), generate trading signals (avoid high-lambda periods). Requirements: Statistical robustness (handle outliers), comparison to other measures (VPIN, Glosten-Harris θ), visualization (lambda heatmap by hour/stock). How do you handle: Zero variance (all trades same direction)? Outliers (fat-finger trades)? Comparison metrics (lambda vs VPIN correlation)?',
    sampleAnswer:
      "Kyle lambda estimation: Definition: λ = price impact per unit volume, Formula: λ = Cov(ΔP, Q) / Var(Q), where ΔP=price change, Q=signed volume (+buy, -sell), Interpretation: High λ = informed trading (price moves with order flow), Low λ = uninformed trading (price independent of flow). Implementation: def kyle_lambda(prices, signed_volumes, window=50): price_changes = diff(log(prices)), for i in range(window, len(prices)): dp = price_changes[i-window:i], q = signed_volumes[i-window:i], lambda_i = cov(dp, q) / var(q) if var(q) > 0 else 0, yield lambda_i. Rolling estimation: Window: 50 trades (balance: responsive vs stable), Slide: Compute lambda after each new trade, Output: Time-series of λ_t. Zero variance handling: Problem: All trades buy (or all sell) → Var(Q) = 0 → division by zero, Detection: Check if var(q) < threshold (e.g., 1e-6), Fallback: Return NaN or previous lambda, Alert: Flag period as one-sided flow (potential informed trading). Outlier handling: Detection: |ΔP| > 3σ (price change outlier), or |Q| > Q_99th_percentile (volume outlier), Winsorization: Cap outliers at 99th percentile (reduce extreme influence), Robust estimation: Use median absolute deviation (MAD) instead of variance. Comparison to VPIN: VPIN = |buy_vol - sell_vol| / total_vol (imbalance), Lambda = price impact per volume (different concept), Correlation: Typically 0.4-0.6 (moderately correlated), Interpretation: High VPIN + high lambda = strong informed trading. Comparison to θ (Glosten-Harris): θ = adverse selection from regression (ΔP ~ Q), Lambda = covariance-based measure (similar concept, different method), Comparison: Correlation typically 0.6-0.8 (highly correlated), Advantage λ: Simpler (no regression), univariate. Cross-stock comparison: Normalization: λ varies by stock (liquid vs illiquid), Normalize: λ_norm = λ / (σ × √V), where σ=volatility, V=avg volume, Comparison: High λ_norm = high adverse selection relative to liquidity. Informed trading detection: Threshold: Define λ_high = mean(λ) + 2×std(λ), Signal: If λ_t > λ_high → informed trading period, Duration: Count consecutive high-lambda periods, Action: Market makers widen spread, traders avoid taking positions. Trading signals: Avoid high-lambda: Don't trade when λ > threshold (likely adverse selection), Wait for low-lambda: Trade when λ < mean (uninformed flow dominates), Timing: Execute during low-lambda hours (mid-day typically lower). Visualization: Heatmap: Rows=stocks, Columns=hours, Color=lambda value, Pattern: High lambda during open/close (informed trading), low mid-day (uninformed). Time-series: Plot λ_t over day, overlay VPIN for comparison, Annotate: News events (earnings → lambda spike). Performance: Computation: O(n) per window (covariance and variance), Efficient: Incremental calculation (online algorithm), don't recompute from scratch. Real-time: Update after each trade (rolling window), latency <100 μs per update.",
    keyPoints: [
      'Formula: λ = Cov(ΔP, Q) / Var(Q), high λ = informed trading (price impact), low λ = uninformed',
      'Rolling: 50-trade window, update after each trade, time-series of λ_t',
      'Zero variance: All trades one direction, return NaN or previous, alert one-sided flow',
      'Outliers: Winsorize (cap at 99th percentile), robust (use MAD instead of variance)',
      'Comparison: VPIN (correlation 0.4-0.6), θ (correlation 0.6-0.8), normalize across stocks: λ/(σ×√V)',
    ],
  },
  {
    id: 'basd-q-3',
    question:
      'Create Roll spread estimator (Spread = 2√(-Cov(ΔP_t, ΔP_{t-1}))) that works when: (1) Serial covariance is positive (informed trading, Roll fails), (2) Spread varies over time (not constant), (3) Multiple stocks simultaneously (compare spreads). Requirements: Handle edge cases (positive cov → return NaN), provide confidence intervals (bootstrap), compare to quoted spread (Roll underestimates typically). How do you validate: Accuracy (vs actual quoted spreads)? Conditions (when does Roll work well)?',
    sampleAnswer:
      'Roll spread estimator: Formula: S = 2√(-Cov(ΔP_t, ΔP_{t-1})), Assumption: Trades alternate bid/ask (bid-ask bounce), no information, Serial covariance: Negative due to bounce (up then down), Spread: Magnitude of bounce. Implementation: def roll_estimator(prices): price_changes = diff(prices), cov = np.cov(price_changes[:-1], price_changes[1:])[0,1], if cov < 0: spread = 2 × sqrt(-cov), else: spread = NaN (Roll assumption violated), return spread, spread_bps. Positive covariance handling: Problem: Cov > 0 implies momentum (informed trading, not bounce), Roll fails: Cannot take sqrt of negative number, Return: NaN or 0 (indicate failure), Alternative: Use other estimators (Glosten-Harris, Kyle lambda). Time-varying spread: Rolling window: Compute Roll spread over last 100 price changes, Update: After each new price change, Plot: Time-series of estimated spread. Bootstrap confidence intervals: Resample: Draw price changes with replacement (1000 iterations), Compute: Roll spread for each bootstrap sample, CI: 2.5th to 97.5th percentile (95% confidence interval), Width: Narrow CI = reliable estimate, wide CI = noisy. Multi-stock comparison: Normalize: Spread as % of price (bps), Rank: Order stocks by Roll spread (widest to tightest), Compare: Quoted spread vs Roll spread (validation). Validation: Quoted spread: Bid-ask spread from Level 1 data (ground truth), Comparison: Roll spread typically 0.5-0.8× quoted spread (underestimates), Reason: Roll ignores adverse selection component (only captures processing). Accuracy metrics: MAE = mean(|Roll - Quoted|), RMSE = sqrt(mean((Roll - Quoted)²)), Correlation: Corr(Roll, Quoted) (typically 0.6-0.8). Conditions for Roll: Works well: High-frequency trading, uninformed flow, tight spreads, Fails: Informed trading (positive autocorrelation), wide spreads, illiquid stocks. Enhanced Roll: Two-lag: S = 2√(-0.5×(Cov_1 + Cov_2)) (use two lags, more robust), Weighted: Give more weight to recent price changes (adaptive).',
    keyPoints: [
      'Formula: S = 2√(-Cov(ΔP_t, ΔP_{t-1})), assumes bid-ask bounce, no information',
      'Positive cov: Return NaN (Roll fails, informed trading present), use alternative estimator',
      'Time-varying: Rolling window (100 price changes), update after each, time-series plot',
      'Bootstrap CI: Resample 1000 times, 2.5th-97.5th percentile = 95% CI',
      'Validation: Roll typically 0.5-0.8× quoted spread (underestimates, ignores adverse selection)',
    ],
  },
];
