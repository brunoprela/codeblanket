export const priceDiscoveryProcessQuiz = [
    {
        id: 'pdp-q-1',
        question:
            'Design a real-time price discovery monitoring system that: (1) Detects informed trading via VPIN calculation (volume buckets, imbalance measurement), (2) Measures price impact decomposition (permanent vs temporary), (3) Calculates information share across venues (which exchange leads?), (4) Generates alerts for price discovery breakdown (flash crashes, excessive reversals), (5) Provides execution cost estimates (market impact based on square-root law). Requirements: Process 10K+ trades per second, real-time VPIN updates, lead-lag analysis across 5 venues, anomaly detection (volatility spikes, quote instability). How do you handle: VPIN bucket synchronization (volume-based time)? Impact attribution (separate info from liquidity effects)? Multi-venue data (timestamp alignment, latency differences)? False positives (normal volatility vs crash)?',
        sampleAnswer:
            'Price discovery monitoring system: Architecture: Data ingestion: Real-time trade feeds from all venues (NASDAQ, NYSE, BATS, IEX, dark pools), Quote feeds: Level 1 data (BBO updates), Timestamp synchronization: NTP sync to microsecond precision, align timestamps across venues. Processing pipeline: (1) Trade classification (buy/sell using Lee-Ready algorithm), (2) VPIN calculation (volume-bucketed imbalance), (3) Price impact decomposition (immediate vs long-term), (4) Lead-lag analysis (cross-correlation across venues), (5) Anomaly detection (volatility, reversals, volume), (6) Alert generation (thresholds breached). VPIN calculation: Volume buckets: Fixed size (e.g., 50K shares per bucket), accumulate trades until bucket full, Trade classification: Buy if price > mid, sell if price < mid, use tick test for mid-trades, Imbalance per bucket: |buy_vol - sell_vol| / total_vol, Rolling VPIN: Average imbalance over last N buckets (e.g., N=50), Update: After each bucket completion (every 50K shares). Implementation: class VPINCalculator: def __init__(self, bucket_size=50000, window=50): self.bucket_size = bucket_size, self.window = window, self.current_bucket = {buy: 0, sell: 0}, self.buckets = deque(maxlen=window), def add_trade(self, quantity, side): if side == BUY: self.current_bucket[buy] += quantity, else: self.current_bucket[sell] += quantity, total = sum(self.current_bucket.values()), if total >= self.bucket_size: imbalance = abs(self.current_bucket[buy] - self.current_bucket[sell]) / total, self.buckets.append(imbalance), self.current_bucket = {buy: 0, sell: 0}, def get_vpin(self): return np.mean(self.buckets) if self.buckets else 0. Price impact decomposition: Immediate impact: Price change at execution (T+0), Short-term impact: Price at T+1 second (includes reversion), Long-term impact: Price at T+60 seconds (permanent component), Temporary: Immediate - Long-term (reverts), Permanent: Long-term impact (information). Calculation: For each trade: p_before = price[-1] (before trade), p_immediate = trade_price, p_short = price at T+1s, p_long = price at T+60s, immediate_impact = (p_immediate - p_before) / p_before, permanent_impact = (p_long - p_before) / p_before, temporary_impact = immediate_impact - permanent_impact. Storage: Time-series of impacts (track over time), Aggregate: Average permanent/temporary by trade size, hour, etc. Information share (venue leadership): Lead-lag analysis: For each pair of venues (A, B): Calculate cross-correlation: corr(returns_A[t], returns_B[t+lag]) for lag in [-10, +10], Optimal lag: Lag with maximum correlation, Leader: Venue with positive optimal lag (changes first). Hasbrouck information share: Vector autoregression (VAR) model: price_A[t] = f(price_A[t-1], price_B[t-1], ...), Decompose variance: Which venue contributes most to price discovery?, Information share: % of price discovery attributed to each venue. Implementation: from statsmodels.tsa.api import VAR, prices_matrix = np.column_stack([prices_A, prices_B, prices_C]), model = VAR(prices_matrix).fit(maxlags=5), information_shares = compute_information_share(model), (custom function based on Hasbrouck 1995 methodology). Multi-venue timestamp alignment: Challenge: Each venue has own timestamp (exchange time), network latency varies, Synchronization: Convert all to common clock (e.g., UTC), align trades within microsecond window, Matching: If trades at multiple venues within 1ms, consider simultaneous, Order: Sort by exchange timestamp (adjusted for known latencies). Latency calibration: Measure: Round-trip time (RTT) to each exchange (send message, measure echo), Adjust: Subtract 0.5 × RTT from timestamps (one-way latency estimate), Update: Re-calibrate hourly (latencies drift). Anomaly detection: Volatility spike: Rolling volatility (1-minute windows), threshold: σ > 3 × normal (99.7% confidence), Alert: If sustained for >5 seconds (not just noise). Excessive reversals: Price sign changes: count(sign(price[t] - price[t-1]) ≠ sign(price[t-1] - price[t-2])), Threshold: >60% reversals in 1-minute window, Indication: Price instability, possible fat-finger or algo error. Quote instability: Spread widening: If spread > 2 × normal spread, alert, BBO flicker: Rapid quote updates (>100/second) with no trades, Volume surge: 5× normal volume in 1 minute. Flash crash detection: Combination: Volatility spike + excessive reversals + volume surge + spread widening, Definition: If 3 out of 4 conditions met, declare flash crash, Action: Pause trading alerts, widen market making quotes, notify risk managers. Execution cost estimation: Square-root law: Impact = γ × σ × sqrt(Q / V), where γ ≈ 0.7, σ = daily volatility, Q = order size, V = daily volume, Real-time: Update σ and V from recent data (last 20 days), Adjust: For current market conditions (high VPIN → higher γ, e.g., γ = 1.0). Example: Order 10K shares, Daily volume 1M, Volatility 2%, Participation = 10K / 1M = 1%, Impact = 0.7 × 0.02 × sqrt(0.01) = 0.7 × 0.02 × 0.1 = 0.0014 = 14 bps. Cost: 10K × $100 × 0.0014 = $1,400 estimated impact cost. Dashboard: Metrics displayed: VPIN (current + historical chart), Price impacts (permanent/temporary breakdown), Venue leadership (information shares), Anomaly flags (current alerts), Execution cost estimates (for various order sizes). Alerts: VPIN > 0.7: High informed trading detected, Flash crash: Price discovery breakdown, Venue outage: One exchange not reporting data, Latency spike: Data delayed by >10ms.',
        keyPoints: [
            'VPIN: Volume buckets (e.g., 50K shares), trade classification (buy if price > mid), imbalance = |buy - sell| / total, rolling average over 50 buckets, update after each bucket',
            'Price impact: Immediate (T+0), permanent (T+60s, information), temporary (immediate - permanent, reverts), decompose per trade, aggregate by size/time',
            'Lead-lag: Cross-correlation (corr(A[t], B[t+lag])), optimal lag = max correlation, leader = positive lag (changes first), Hasbrouck information share (VAR model)',
            'Anomaly detection: Volatility > 3σ, reversals > 60%, spread > 2× normal, volume > 5× normal, flash crash = 3/4 conditions met, alert and widen quotes',
            'Execution cost: Impact = 0.7 × σ × sqrt(Q/V), update σ and V from 20-day history, adjust γ for VPIN (high VPIN → γ = 1.0, low VPIN → γ = 0.5)',
        ],
    },
    {
        id: 'pdp-q-2',
        question:
            'Implement a variance ratio test framework to detect market efficiency violations (mean reversion, momentum). Test multiple time horizons (1-day vs 5-day vs 20-day returns), generate statistical significance (t-tests, bootstrap confidence intervals), visualize results (VR by horizon, rejection regions), backtest trading strategies based on inefficiencies (e.g., if VR < 1, trade mean reversion). Requirements: Handle missing data (weekends, holidays), adjust for microstructure noise (bid-ask bounce), provide regime detection (efficiency changes over time). How do you handle: Overlapping returns (artificially high correlation)? Small sample bias (VR estimates noisy)? Structural breaks (efficiency changes)? Transaction costs (inefficiency might not be tradable)?',
        sampleAnswer:
            'Variance ratio test framework: Variance ratio (VR) definition: VR(q) = Var(q-period returns) / (q × Var(1-period returns)), Interpretation: VR ≈ 1 (random walk, efficient), VR < 1 (mean reversion, negative autocorrelation), VR > 1 (momentum, positive autocorrelation). Lo-MacKinlay test: Test statistic: z = (VR - 1) × sqrt(n × q / (2 × (q - 1))), Under H0 (random walk): z ~ N(0, 1), Rejection: |z| > 1.96 (95% confidence, two-tailed), p-value: 2 × (1 - Φ(|z|)) where Φ = standard normal CDF. Implementation: def variance_ratio_test(returns, q, robust=True): n = len(returns), var_1 = np.var(returns, ddof=1), non_overlapping_returns = returns[::q][:n//q].reshape(-1, q).sum(axis=1), var_q = np.var(non_overlapping_returns, ddof=1), VR = var_q / (q × var_1), if robust: adjust for heteroskedasticity (White correction), z_stat = (VR - 1) × sqrt(n × q / (2 × (q-1))), p_value = 2 × (1 - stats.norm.cdf(abs(z_stat))), return VR, z_stat, p_value. Multiple horizons: Test q = 2, 5, 10, 20, 60 days, Plot: VR vs horizon (should be flat at 1.0 if efficient), Interpretation: VR < 1 at short horizons (microstructure noise), VR > 1 at long horizons (momentum). Bootstrap confidence intervals: Resample returns: Draw with replacement (1000 iterations), Calculate VR: For each bootstrap sample, Confidence interval: 2.5th to 97.5th percentile (95% CI), Robust: Doesn\'t assume normality (useful for fat tails). Overlapping returns: Problem: q-period returns overlap (e.g., [1-5], [2-6], [3-7]), Induces: Artificial correlation (returns share common days), Bias: Overestimates autocorrelation (VR biased). Solution: Use non-overlapping returns only: q=5: Use [1-5], [6-10], [11-15], ... (no overlap), Trade-off: Smaller sample size (n/q observations vs n-q+1), Better: Unbiased estimates (worth the sample size reduction). Microstructure noise: Bid-ask bounce: Prices bounce between bid and ask (not true price changes), Effect: Artificially negative autocorrelation (VR < 1 at 1-day horizon), Adjustment: Use mid-prices (not last trade prices), or filter: σ_true² = σ_observed² - 2 × σ_noise² (Roll 1984). Structural breaks: Problem: Market efficiency changes over time (crisis vs calm), Example: VR = 0.8 (pre-crisis), VR = 1.2 (crisis), aggregate = 1.0 (misleading), Detection: Rolling VR: Calculate VR over moving windows (e.g., 252-day windows), Chow test: Test if VR different between periods, Regime switching: Markov-switching model (two regimes: efficient vs inefficient). Regime detection: def rolling_vr(returns, q, window=252): results = [], for i in range(window, len(returns)): window_returns = returns[i-window:i], VR, _, p = variance_ratio_test(window_returns, q), results.append((i, VR, p)), return results, Plot: VR over time (detect regime changes), Alert: If VR crosses 1.0 (efficiency change), Strategy adjustment: Trade mean reversion in VR < 1 regimes, trade momentum in VR > 1 regimes. Transaction costs: Inefficiency vs tradability: VR = 0.9 (mean reversion, 10% deviation from random walk), Profit per trade: ~1% (if correctly timed), Transaction costs: Spread 0.1% + commission 0.05% = 0.15% roundtrip, Net profit: 1% - 0.15% = 0.85% (still profitable, but reduced). Profitability check: Estimate: Expected profit from inefficiency, Costs: Spread + commission + market impact, Net: Profit - costs (must be positive to trade), If negative: Inefficiency not tradable (market is "efficiently inefficient"). Backtesting strategy: Mean reversion (VR < 1): Entry: If price deviates >2σ from moving average, trade reversion, Exit: When price returns to mean, Position size: Scale by VR deviation (lower VR → larger position). Momentum (VR > 1): Entry: Buy if price breaks above moving average, trend following, Exit: When momentum fades (price crosses below MA), Position size: Scale by VR (higher VR → larger position). Performance metrics: Sharpe ratio: Risk-adjusted returns, Win rate: % of trades profitable, Max drawdown: Worst peak-to-trough, Transaction costs: Total costs as % of PnL (should be <30%). Handling missing data: Weekends/holidays: Skip missing days (don\'t fill), adjust return calculations, Calendar days vs trading days: Use trading days only (252 per year, not 365), Gaps: If gap > 5 days, exclude from sample (e.g., trading halt), Interpolation: Generally avoid (introduces bias), exception: mid-quote from bid/ask. Results visualization: VR by horizon: Line plot (VR on y-axis, q on x-axis), 95% confidence band (bootstrap), Reference line at VR = 1.0 (random walk), Rejection regions shaded (VR significantly ≠ 1). Time series: Rolling VR over time, Regime annotations (crisis periods, efficiency changes), Strategy PnL overlaid (show if strategy profitable). Statistical table: Horizon | VR | z-stat | p-value | Significant?, q=2 | 0.85 | -2.5 | 0.01 | Yes**, q=5 | 0.90 | -1.8 | 0.07 | No, q=20 | 1.10 | 1.5 | 0.13 | No. Real-world application: Liquid stocks: Typically efficient (VR ≈ 1) at daily horizon, Illiquid stocks: Mean reversion due to microstructure (VR < 1), Intraday: High-frequency noise (VR < 1 at short intervals), Long-term: Potential momentum (VR > 1 at multi-month horizons).',
        keyPoints: [
            'Variance ratio: VR(q) = Var(q-period) / (q × Var(1-period)), VR = 1 (random walk), VR < 1 (mean reversion), VR > 1 (momentum), test z = (VR-1) × sqrt(n×q/(2(q-1)))',
            'Non-overlapping returns: Use [1-q], [q+1-2q], ... (avoid artificial correlation), trade-off: smaller sample (n/q observations) but unbiased estimates',
            'Regime detection: Rolling VR (252-day windows), detect structural breaks (Chow test), adjust strategy (mean reversion if VR < 1, momentum if VR > 1)',
            'Transaction costs: Net profit = inefficiency profit - (spread + commission + impact), check profitability before trading, inefficiency may not be exploitable',
            'Bootstrap CI: Resample 1000 times, calculate VR each iteration, 2.5th to 97.5th percentile = 95% CI, robust to non-normality',
        ],
    },
    {
        id: 'pdp-q-3',
        question:
            'Build a trade classification system (Lee-Ready algorithm) that: (1) Classifies each trade as buy-initiated or sell-initiated using quotes, (2) Handles edge cases (trades at midpoint, stale quotes, crossed markets), (3) Validates accuracy using exchange-provided aggressor flags (when available), (4) Provides confidence scores (certain vs uncertain classifications), (5) Aggregates to order flow metrics (buy/sell imbalance, buy/sell pressure). Requirements: Match ~85%+ accuracy vs exchange flags, handle 100K+ trades per second, deal with quote lags (trade timestamp before quote update). How do you handle: Midpoint trades (exactly at bid-ask mid)? Crossed quotes (bid > ask, invalid)? Latency (trade arrives before quote update)? Validation (measure classification accuracy)?',
        sampleAnswer:
            'Trade classification system: Lee-Ready algorithm: Step 1 - Quote rule: If trade_price > mid_price: classify as buy, If trade_price < mid_price: classify as sell, If trade_price == mid_price: go to Step 2 (tick test). Step 2 - Tick test: If trade_price > previous_trade_price: classify as buy (uptick), If trade_price < previous_trade_price: classify as sell (downtick), If trade_price == previous_trade_price: use previous classification (repeat tick). Implementation: def lee_ready_classify(trade_price, mid_price, prev_trade_price, prev_classification): if trade_price > mid_price: return BUY, confidence=HIGH, elif trade_price < mid_price: return SELL, confidence=HIGH, else: if trade_price > prev_trade_price: return BUY, confidence=MEDIUM, elif trade_price < prev_trade_price: return SELL, confidence=MEDIUM, else: return prev_classification, confidence=LOW. Confidence scoring: HIGH: Quote rule (trade not at mid), confidence=0.9 (90% accurate typically), MEDIUM: Tick test (uptick/downtick), confidence=0.7 (70% accurate), LOW: Repeated tick (multiple trades at same price), confidence=0.5 (50%, essentially random guess). Edge cases: Midpoint trades: Common: ~20-30% of trades (especially in liquid stocks, dark pools), Quote rule: Fails (trade_price == mid), must use tick test, Tick test: Works if price trending, fails if price oscillating, Solution: Use trade size (large trades more likely buy in uptrend, sell in downtrend), or signed volume (from order book if available). Crossed quotes: Definition: bid > ask (impossible in equilibrium, but occurs briefly due to latency), Handling: If quotes crossed, discard (don\'t use for classification), Fallback: Use previous valid quote (within last 1 second), or use tick test only (ignore quote rule). Stale quotes: Problem: Trade timestamp = T, but quote update delayed until T+100ms, Quote at T: Stale (doesn\'t reflect true mid at trade time), Solution: Look-ahead: Use quote at T+100ms (next update after trade), Caution: Introduces look-ahead bias (not usable in real-time), Alternative: Real-time system must use quote at T (accept lower accuracy). Latency handling: Trade-quote asynchrony: Trades from exchange A, quotes from exchange B (different latencies), Alignment: Match trade to quote with closest timestamp (within ±1 second window), Priority: If multiple quotes, use most recent before trade (no look-ahead). Clock synchronization: NTP sync: All timestamps to common clock (UTC), Precision: Microsecond accuracy (exchange clocks well-synchronized), Validation: Check for timestamp inversions (trade before quote, implies latency issue). Quote snapshot: Per trade: Find closest quote snapshot (bid, ask) within 1-second window, Cache: Maintain recent quotes in memory (rolling buffer, last 1000 quotes), Lookup: Binary search by timestamp (O(log n) complexity, fast). Validation with exchange flags: Exchange data: Some exchanges provide aggressor indicator (buy=1, sell=-1), Sample: 10-20% of trades (not all exchanges provide), Accuracy measurement: Compare: Lee-Ready classification vs exchange flag, Metrics: Accuracy = % matches, Confusion matrix: True positive (buy classified as buy), false positive (sell classified as buy), Precision/recall. Benchmark: Target accuracy: 85%+ (Lee-Ready typically achieves 80-90%), Breakdown: Quote rule: 90-95% accurate (when not at mid), Tick test: 60-70% accurate (noisy), Overall: Weighted average based on usage frequency. Continuous validation: Rolling window: Last 10K trades with exchange flags, Real-time: Update accuracy metric after each trade, Alert: If accuracy drops below 80% (model degradation, data quality issue), Recalibration: Adjust thresholds or add features (trade size, order book imbalance). Order flow metrics: Buy/sell volume: Aggregate: sum(trade_qty × indicator), where indicator = +1 (buy) or -1 (sell), Rolling: Last N seconds or M trades, Imbalance: (buy_vol - sell_vol) / (buy_vol + sell_vol), range [-1, +1]. Buy/sell pressure: Pressure = imbalance × sqrt(volume), Intuition: Imbalance weighted by activity level (high volume = more pressure), Use: Predictive signal (high buy pressure → price likely to rise). Dollar flow: Dollar_buy = sum(trade_price × qty) for buys, Dollar_sell = sum(trade_price × qty) for sells, Dollar_imbalance = (Dollar_buy - Dollar_sell) / (Dollar_buy + Dollar_sell), More accurate: Captures value, not just quantity (large trades count more). Performance optimization: High throughput: 100K+ trades per second, Data structures: Ring buffer for quotes (fast lookups, O(1) append), Hash map for trade state (O(1) lookups), Batch processing: Process trades in batches (1000 at a time), reduce overhead, Parallelization: Multi-threading (separate threads per symbol). Memory management: Quote buffer: Keep last 1000 quotes (enough for 1-second window at 1000 quotes/sec), Eviction: Drop quotes older than 5 seconds (no longer needed), Trade history: Store last 100 trades (for tick test look-back), Efficient: Circular buffer (no allocations). Alternative methods: Bulk volume classification (BVC): Aggregate trades: Group by price level, net buying/selling, Faster: No per-trade classification, less accurate, Use: When individual trade classification not needed. VWAP-based: Compare: Trade price vs VWAP over recent window, If trade_price > VWAP: likely buy, If trade_price < VWAP: likely sell, Advantage: Robust to quote noise. Machine learning: Features: Trade price, mid price, spread, trade size, time of day, order book imbalance, Label: Exchange aggressor flag (when available), Model: Random forest or XGBoost (85-90% accuracy), Update: Retrain daily on recent data (model drift). Monitoring: Metrics dashboard: Classification rate (trades/second), Accuracy (vs exchange flags, rolling 10K trades), Quote availability (% trades with valid quotes within 1s), Latency (time from trade receipt to classification). Alerts: Accuracy drop: Below 80% (investigate data quality), Quote gap: >10% trades without quotes (connectivity issue), Crossed quotes: >1% (unusual, exchange issue). Real-world performance: Liquid stocks: 85-90% accuracy (tight spreads, clear price movements), Illiquid stocks: 70-80% accuracy (wide spreads, more midpoint trades), Dark pools: 60-70% accuracy (all midpoint trades, tick test only), High-frequency: 80-85% accuracy (quote rule dominates, less ambiguity).',
        keyPoints: [
            'Lee-Ready: Quote rule (trade > mid = buy, trade < mid = sell), tick test if trade == mid (uptick = buy, downtick = sell), confidence: quote rule (90%), tick test (70%), repeat tick (50%)',
            'Edge cases: Midpoint trades (20-30%, use tick test), crossed quotes (discard, use prev valid quote), stale quotes (use next update, look-ahead bias in backtest)',
            'Latency: Trade-quote asynchrony (different exchanges), match to closest quote within ±1s, NTP sync (microsecond precision), cache recent quotes (rolling buffer)',
            'Validation: Compare vs exchange aggressor flags (10-20% sample), target 85%+ accuracy, confusion matrix (TP/FP/TN/FN), rolling window (10K trades), alert if < 80%',
            'Order flow: Buy/sell imbalance = (buy_vol - sell_vol) / total, pressure = imbalance × sqrt(volume), dollar imbalance (weighted by price), aggregate over rolling window',
        ],
    },
];

