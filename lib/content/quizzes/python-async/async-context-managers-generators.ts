export const asyncContextManagersGeneratorsQuiz = [
  {
    id: 'acmg-q-1',
    question:
      'Design an async connection pool manager that: (1) Maintains 10-50 database connections (scales based on demand), (2) Provides transactions with automatic commit/rollback, (3) Implements connection health checks (ping every 30s), (4) Handles connection failures gracefully (recreate failed connections), (5) Properly cleans up all resources on shutdown. Use async context managers. Explain __aenter__/__aexit__ lifecycle, exception handling in context managers, and why this pattern is superior to manual resource management.',
    sampleAnswer:
      'Async connection pool with context managers: Code structure: import asyncio, asyncpg; from contextlib import asynccontextmanager; from typing import Optional. class AsyncConnectionPool: def __init__(self, dsn, min_size=10, max_size=50): self.dsn = dsn; self.min_size = min_size; self.max_size = max_size; self.pool: Optional[asyncpg.Pool] = None; self.health_check_task = None; self.closed = False. async def __aenter__(self): """Setup: Create pool and start health checks"""; print(f"Creating connection pool ({self.min_size}-{self.max_size})"); self.pool = await asyncpg.create_pool(self.dsn, min_size=self.min_size, max_size=self.max_size, command_timeout=60); # Start health check task; self.health_check_task = asyncio.create_task(self._health_check_loop()); return self. async def __aexit__(self, exc_type, exc_val, exc_tb): """Cleanup: Stop health checks, close pool"""; print(f"Shutting down pool (exception: {exc_type})"); self.closed = True; # Stop health checks; if self.health_check_task: self.health_check_task.cancel(); try: await self.health_check_task; except asyncio.CancelledError: pass; # Close all connections; if self.pool: await self.pool.close(); print("Pool closed"); return False; # Propagate exceptions. async def _health_check_loop(self): """Ping connections every 30s"""; while not self.closed: try: await asyncio.sleep(30); if self.pool: # Ping by executing simple query; async with self.pool.acquire() as conn: await conn.execute("SELECT 1"); print("Health check: OK"); except asyncio.CancelledError: break; except Exception as e: print(f"Health check failed: {e}"); # Pool will recreate connections automatically. @asynccontextmanager; async def acquire(self): """Acquire connection from pool"""; if self.closed: raise RuntimeError("Pool is closed"); async with self.pool.acquire() as conn: try: yield conn; except Exception as e: print(f"Connection error: {e}"); raise. @asynccontextmanager; async def transaction(self): """Transaction with auto commit/rollback"""; async with self.acquire() as conn: tx = conn.transaction(); await tx.start(); try: yield conn; await tx.commit(); print("Transaction committed"); except Exception as e: await tx.rollback(); print(f"Transaction rolled back: {e}"); raise. (1) Scaling connections (10-50): asyncpg.create_pool handles scaling automatically. min_size=10: Always maintain 10 connections (ready for burst). max_size=50: Create up to 50 connections on demand (high load). Idle connections above min_size closed after timeout. Benefits: Low latency (connections pre-warmed). Handles traffic spikes (scales to 50). Resource-efficient (scales down when idle). (2) Transactions with auto commit/rollback: @asynccontextmanager for transaction context. await tx.start() begins transaction. If block completes: await tx.commit(). If exception: await tx.rollback() in except. Pattern: async with pool.transaction() as conn: await conn.execute("INSERT ..."); await conn.execute("UPDATE ..."); # Auto-commits on success, rolls back on exception. Why important: Prevents partial updates (data consistency). No forgotten commits/rollbacks. Exception-safe (always rolls back on error). (3) Connection health checks: Background task: self.health_check_task = asyncio.create_task(self._health_check_loop()). Pings every 30 seconds: await asyncio.sleep(30); await conn.execute("SELECT 1"). Detects dead connections (network issues, database restarts). asyncpg pool recreates failed connections automatically. Why needed: Database may close idle connections. Network issues can make connections unusable. Ensures all pool connections are healthy. (4) Graceful failure handling: Health check catches exceptions: except Exception as e: print(f"Health check failed: {e}"). Pool recreates connections: asyncpg pool automatically replaces failed connections. Transaction context catches exceptions: except Exception in transaction() rolls back. Connection acquire catches exceptions: except Exception in acquire() propagates with logging. Pattern: try: async with pool.acquire() as conn: await conn.execute(...); except asyncpg.PostgresError as e: # Handle database errors; except Exception as e: # Handle other errors. (5) Cleanup on shutdown: __aexit__ guarantees cleanup. Cancel health check: self.health_check_task.cancel(); await self.health_check_task. Close pool: await self.pool.close() closes all connections. Set closed flag: self.closed = True prevents new operations. Order matters: Stop background tasks first → Close connections → Cleanup complete. Context manager lifecycle: Entry (__aenter__): Called when entering async with block. Setup: Create resources (pool, background tasks). Returns: self or resource to use in block. Async: Can perform async operations (await). Exit (__aexit__): Called when exiting async with block (normal or exception). Cleanup: Close resources, stop tasks. Arguments: exc_type, exc_val, exc_tb (exception info if occurred). Return: False = propagate exception, True = suppress exception. Async: Can perform async cleanup (await). Guaranteed: Always called (even if exception in block). Exception handling in context managers: Normal exit (no exception): __aexit__ called with (None, None, None). Exception in block: __aexit__ called with exception info. Can inspect: if exc_type is asyncpg.PostgresError: handle_db_error(). Can suppress: return True suppresses exception, return False propagates. Cleanup before decision: Always clean up first, then decide to suppress/propagate. Example: async def __aexit__(self, exc_type, exc_val, exc_tb): await self.cleanup(); # Always clean up; if exc_type is ValueError: print("Suppressing ValueError"); return True; # Suppress; return False; # Propagate others. Why context managers superior to manual management: Problem with manual: resource = await create_resource(); try: await use_resource(resource); finally: await cleanup_resource(resource); # Easy to forget finally; # Hard to handle exceptions correctly; # Verbose and error-prone. Solution with context manager: async with ResourceManager() as resource: await use_resource(resource); # Cleanup guaranteed; # Exception handling automatic; # Concise and clear. Benefits: Guaranteed cleanup (even if exception, return, break in block). Exception safety (proper rollback on error). Composable (nest context managers easily). Readable (clear resource lifecycle). Testable (mock __aenter__/__aexit__). Production usage: async def process_batch(items): async with AsyncConnectionPool(dsn) as pool: async with pool.transaction() as conn: for item in items: await conn.execute("INSERT INTO data VALUES ($1)", item); # Transaction commits if loop completes; # Transaction rolls back if exception; # Pool cleans up connections when done. Testing connection pool: Test 1: Verify pool creation: async with AsyncConnectionPool(dsn) as pool: assert pool.pool is not None; assert pool.health_check_task is not None. Test 2: Verify cleanup on exception: with pytest.raises(ValueError): async with AsyncConnectionPool(dsn) as pool: raise ValueError(); # Verify pool closed despite exception; assert pool.closed. Test 3: Verify transaction rollback: async with pool.transaction() as conn: await conn.execute("INSERT ..."); raise ValueError(); # Verify nothing committed. Test 4: Verify health checks: async with AsyncConnectionPool(dsn) as pool: await asyncio.sleep(31); # Wait for health check; # Verify health check executed (check logs). Test 5: Verify resource cleanup: async with AsyncConnectionPool(dsn) as pool: pass; # Verify all connections closed; assert pool.pool is None. Critical lesson: Context managers guarantee resource cleanup. Async context managers allow async setup/cleanup. __aenter__/__aexit__ always called (exception-safe). Superior to manual resource management (less error-prone).',
    keyPoints: [
      'Lifecycle: __aenter__ creates pool and starts health checks, __aexit__ cancels tasks and closes pool (guaranteed)',
      'Transaction context manager: start transaction, yield connection, commit on success, rollback on exception',
      'Health checks: background task pings every 30s, pool recreates failed connections automatically',
      'Exception handling: __aexit__ receives exception info, can log and suppress/propagate, always cleans up first',
      'Superior to manual: guaranteed cleanup, exception-safe, composable, readable, no forgotten cleanup',
    ],
  },
  {
    id: 'acmg-q-2',
    question:
      "Implement a streaming ETL pipeline using async generators that: (1) Extracts 1 million records from paginated API (1000 per page), (2) Transforms each record (CPU-intensive: parse JSON, validate, enrich), (3) Loads in batches of 100 to database, (4) Processes memory-efficiently (don't load all 1M records), (5) Allows cancellation mid-pipeline with graceful cleanup. Explain async generator mechanics, memory benefits vs loading all data, and how to combine generators for pipeline composition.",
    sampleAnswer:
      'Streaming ETL with async generators: Complete implementation: import asyncio, aiohttp, asyncpg; from typing import AsyncIterator. async def extract_records(api_url: str, total_pages: int) -> AsyncIterator[dict]: """Extract: Fetch records from paginated API"""; async with aiohttp.ClientSession() as session: for page in range(1, total_pages + 1): try: async with session.get(f"{api_url}?page={page}&limit=1000") as response: data = await response.json(); for record in data["records"]: yield record; print(f"Extracted page {page}/{total_pages}"); except Exception as e: print(f"Error fetching page {page}: {e}"); # Continue with next page. async def transform_record(record: dict) -> dict: """Transform: Parse, validate, enrich record"""; # CPU-intensive transformation; import json; parsed = json.loads(record.get("raw_data", "{}")); # Validation; if not parsed.get("email"): return None; # Invalid; # Enrichment; enriched = {"id": record["id"], "email": parsed["email"].lower(), "name": parsed.get("name", "").upper(), "created_at": record.get("timestamp"), "enriched_data": {"source": "api", "processed": True}}; # Simulate CPU work; await asyncio.sleep(0.001); return enriched. async def transform_records(records: AsyncIterator[dict]) -> AsyncIterator[dict]: """Transform: Process each record"""; async for record in records: transformed = await transform_record(record); if transformed: # Skip invalid; yield transformed. async def load_records(records: AsyncIterator[dict], db_pool, batch_size=100): """Load: Insert records in batches"""; batch = []; total_loaded = 0; try: async for record in records: batch.append(record); if len(batch) >= batch_size: await insert_batch(db_pool, batch); total_loaded += len(batch); print(f"Loaded {total_loaded} records"); batch = []; # Insert remaining; if batch: await insert_batch(db_pool, batch); total_loaded += len(batch); print(f"Loaded {total_loaded} records (final)"); except asyncio.CancelledError: # Cancellation: Load current batch, then cleanup; print(f"Cancellation: Loading final batch of {len(batch)}"); if batch: await insert_batch(db_pool, batch); total_loaded += len(batch); print(f"Cancelled after loading {total_loaded} records"); raise; finally: print(f"Total loaded: {total_loaded} records"). async def insert_batch(db_pool, batch): """Insert batch into database"""; async with db_pool.acquire() as conn: await conn.executemany("INSERT INTO processed_data (id, email, name, created_at, data) VALUES ($1, $2, $3, $4, $5)", [(r["id"], r["email"], r["name"], r["created_at"], r["enriched_data"]) for r in batch]). async def etl_pipeline(api_url: str, db_pool, total_records=1_000_000): """Complete ETL pipeline"""; total_pages = total_records // 1000; # 1000 per page; # Compose pipeline; records = extract_records(api_url, total_pages); transformed = transform_records(records); await load_records(transformed, db_pool, batch_size=100). async def main(): api_url = "https://api.example.com/data"; db_pool = await asyncpg.create_pool("postgresql://..."); try: await etl_pipeline(api_url, db_pool); finally: await db_pool.close(). (1) Extract 1M records from paginated API: Async generator: async def extract_records(...) -> AsyncIterator[dict]: yield record. Paginated fetching: for page in range(1, total_pages + 1): fetch page, yield each record. Memory: Only current page (1000 records) in memory, not all 1M. Streaming: Yields records as they arrive, doesn\'t wait for all pages. Benefits: Start processing immediately (don\'t wait for all). Constant memory usage. Responsive to cancellation. (2) Transform each record (CPU-intensive): Async generator: async def transform_records(records: AsyncIterator[dict]): async for record in records: yield transformed. Consumes upstream generator: async for record in records: processes each as yielded. Yields to downstream: yield transformed for next stage. CPU-intensive work: If truly CPU-bound, offload to process pool: transformed = await loop.run_in_executor(process_pool, sync_transform, record). For moderate CPU (JSON parsing), asyncio fine with small sleeps. (3) Load in batches of 100: Batch accumulation: batch.append(record); if len(batch) >= 100: insert_batch(batch). Final batch: if batch: insert_batch(batch) handles remainder. Transaction per batch: async with conn.transaction() ensures atomicity. Why batching: Single insert expensive (network roundtrip per record). Batch insert: 100 records in one roundtrip (100× faster). Balance: Small batches (responsive), large batches (throughput). (4) Memory-efficient processing: Problem: records = await fetch_all(); # Loads 1M × 1KB = 1GB memory!. Solution: Stream with generators. Memory at each stage: Extract: 1 page (1000 records × 1KB = 1MB). Transform: 1 record being processed (~1KB). Load: 1 batch (100 records × 1KB = 100KB). Total: ~1MB vs 1GB (1000× reduction!). Key: Process and discard immediately, don\'t accumulate. (5) Graceful cancellation: Handle CancelledError: except asyncio.CancelledError: load final batch; raise. Cleanup in finally: finally: print total loaded. Partial success: Records loaded before cancellation are committed. Pattern: task = asyncio.create_task(etl_pipeline(...)); await asyncio.sleep(10); task.cancel(); # Gracefully stops; try: await task; except CancelledError: print("Pipeline cancelled"). Async generator mechanics: Definition: async def + yield creates async generator. Iteration: async for item in generator: consumes. Execution: Generator suspended at yield, resumed by async for. Control flow: Yields values to consumer, receives control back from consumer. Lazy: Only executes when consumed (not when created). Example: async def numbers(): for i in range(3): print(f"Generating {i}"); await asyncio.sleep(0.1); yield i; async for n in numbers(): print(f"Consumed {n}"); # Output: Generating 0, Consumed 0, Generating 1, Consumed 1, Generating 2, Consumed 2. Interleaved execution, not parallel. Memory benefits: Traditional (load all): data = []; for page in pages: data.extend(fetch(page)); # Memory: all pages; for record in data: process(record); # Total: 1M records in memory. Streaming (generators): async for record in fetch_streaming(): process(record); # Memory: 1 record in memory. Benefits: Constant memory (1 record vs 1M records). Start processing immediately (don\'t wait for all). Can handle infinite streams. Better cache locality. Drawback: Can\'t random access (must consume sequentially). Pipeline composition: Extract → Transform → Load composed naturally. records = extract(...); # AsyncIterator[RawRecord]; transformed = transform(records); # AsyncIterator[TransformedRecord]; await load(transformed); # Consumes AsyncIterator. Each stage: Consumes from upstream async generator. Processes each item. Yields to downstream async generator. Composable: Add stages easily: records = extract(...); validated = validate(records); # New stage; transformed = transform(validated); enriched = enrich(transformed); # Another stage; await load(enriched). Lazy evaluation: Entire pipeline executes only when load() consumes. Benefits: Declarative (define pipeline, execute later). Flexible (add/remove stages easily). Efficient (only process what\'s needed). Testing: Extract: Mock API, verify pagination. Transform: Test transform_record() with sample data. Load: Mock database, verify batching. Integration: Run mini pipeline (10 records), verify end-to-end. Cancellation: Cancel mid-pipeline, verify graceful cleanup. Production considerations: Error handling: except per stage with logging, don\'t stop entire pipeline. Backpressure: If load slower than extract, queue grows. Solution: Semaphore to limit pending items. Monitoring: Track records per stage, detect bottlenecks. Retry: Retry failed pages/batches with exponential backoff. Checkpointing: Save progress (last processed page), resume on failure. Parallelism: Run N pipelines for different page ranges. Performance: Extract: 1M records from API = 1000 pages × 0.5s = 500s = 8 minutes. Transform: 1M × 1ms CPU = 1000s = 17 minutes (parallelize with process pool). Load: 1M / 100 batch × 50ms insert = 500s = 8 minutes. Total: Limited by slowest stage (transform). Optimize: Parallelize transform with ProcessPoolExecutor. Critical lesson: Async generators enable memory-efficient streaming. Compose generators for multi-stage pipelines. Handle cancellation gracefully (load partial results). Profile to find bottleneck, optimize that stage.',
    keyPoints: [
      'Async generator: async def + yield, consumed with async for, lazy evaluation, memory-efficient streaming',
      'Memory benefits: 1MB (current page + batch) vs 1GB (all 1M records), 1000× reduction',
      'Pipeline composition: extract → transform → load naturally with generators, add stages easily',
      'Graceful cancellation: catch CancelledError, load final batch, raise, track progress in finally',
      'Performance: Profile stages, optimize bottleneck (CPU-bound transform uses ProcessPoolExecutor)',
    ],
  },
  {
    id: 'acmg-q-3',
    question:
      'Compare using async context managers vs manual resource management for: (1) database transactions, (2) file operations, (3) HTTP sessions, (4) locks/semaphores. For each: show both approaches, explain exception safety differences, demonstrate what happens when exceptions occur at different points, and show testing strategies. Why are context managers considered best practice?',
    sampleAnswer:
      'Context managers vs manual resource management: (1) Database transactions: Manual approach: conn = await get_connection(); tx = conn.transaction(); await tx.start(); try: await conn.execute("INSERT ..."); await conn.execute("UPDATE ..."); await tx.commit(); except Exception: await tx.rollback(); raise; finally: await conn.close(). Problems: Forgot finally → connection leak. Forgot rollback in except → partial updates. Verbose and error-prone. Context manager approach: async with get_connection() as conn: async with conn.transaction(): await conn.execute("INSERT ..."); await conn.execute("UPDATE ..."); # Auto-commit on success, rollback on exception. Benefits: Automatic commit/rollback. Connection always closed. Concise and clear. Exception scenarios: Scenario A: Exception in query: Manual: If forgot await tx.rollback() in except, partial update persists (data corruption). Context manager: Automatic rollback in __aexit__, no partial updates. Scenario B: Exception in commit: Manual: Connection not closed if exception during commit (leak). Context manager: __aexit__ always called, connection closed. Scenario C: Exception in cleanup: Manual: If conn.close() raises, exception propagates, tx not rolled back. Context manager: __aexit__ handles cleanup exceptions properly. (2) File operations: Manual approach: file = await aiofiles.open("data.txt", "w"); try: await file.write("data"); await file.flush(); finally: await file.close(). Problems: Forgot finally → file handle leak. Multiple return paths → need finally in each. Exception during flush → file not closed. Context manager approach: async with aiofiles.open("data.txt", "w") as file: await file.write("data"); await file.flush(); # Auto-closed. Benefits: File always closed. No leaked handles. Exception scenarios: Scenario A: Exception during write: Manual: File not closed if finally forgotten. Context manager: __aexit__ closes file. Scenario B: Early return: Manual: Must ensure finally on all return paths. Context manager: __aexit__ called on return. Scenario C: Multiple files: Manual: Complex nesting with try/finally for each. Context manager: Simple nesting: async with aiofiles.open("a") as fa: async with aiofiles.open("b") as fb: use both; # Both auto-closed. (3) HTTP sessions: Manual approach: session = aiohttp.ClientSession(); try: response = await session.get(url); data = await response.json(); await response.close(); finally: await session.close(). Problems: Response not closed if exception in json(). Session not closed if exception in response close. Nested try/finally needed for both. Context manager approach: async with aiohttp.ClientSession() as session: async with session.get(url) as response: data = await response.json(); # Both auto-closed. Benefits: Session closed even if exception. Response closed even if exception. Clear resource lifecycle. Exception scenarios: Scenario A: Exception in json(): Manual: Response not closed (leak). Context manager: Response __aexit__ closes it. Scenario B: Exception in response close: Manual: Session close not reached (leak). Context manager: Session __aexit__ still called (independent). Scenario C: Timeout: Manual: Must cancel manually, complex cleanup. Context manager: __aexit__ handles cancellation cleanup. (4) Locks/Semaphores: Manual approach: lock = asyncio.Lock(); await lock.acquire(); try: # Critical section; await critical_operation(); finally: lock.release(). Problems: Forgot finally → deadlock (lock never released). Exception during release → lock state corrupted. Nested locks → complex nested try/finally. Context manager approach: async with lock: # Critical section; await critical_operation(); # Auto-released. Benefits: Lock always released. No deadlocks from forgotten release. Exception scenarios: Scenario A: Exception in critical section: Manual: Lock released if finally present. Context manager: Lock released in __aexit__. Scenario B: Forgot finally: Manual: Lock never released → deadlock (other tasks wait forever). Context manager: Impossible to forget (automatic). Scenario C: Multiple locks: Manual: Nested try/finally, risk of deadlock if wrong order. Context manager: async with lock1: async with lock2: ...; # Clear order, auto-release. Exception safety comparison: Manual: try: setup(); operation(); cleanup(); except: error_handling(); finally: forced_cleanup(). Issues: Easy to forget finally. Cleanup may be skipped. Error handling verbose. Hard to compose. Context manager: async with manager: operation(); # Setup in __aenter__, cleanup in __aexit__, guaranteed. Benefits: Setup/cleanup guaranteed. Exception-safe by design. Composable (nest easily). Clear intent. Testing strategies: Test context manager: Test 1: Normal flow: async with MyResource() as r: assert r.setup_called; assert r.cleanup_called. Test 2: Exception in block: with pytest.raises(ValueError): async with MyResource() as r: raise ValueError(); assert r.cleanup_called; # Verify cleanup despite exception. Test 3: Exception in __aenter__: with pytest.raises(ConnectionError): async with FailingResource(): pass; # Verify __aexit__ NOT called (setup failed). Test 4: Exception in __aexit__: Ensure exception propagated, no silent failures. Test manual management: Test 1: Normal flow: resource = create(); try: use(resource); finally: cleanup(resource); assert resource.cleaned. Test 2: Forgot finally: resource = create(); use(resource); # No cleanup!; # Hard to test (requires code coverage). Test 3: Exception in cleanup: try: finally: cleanup(); # raises; # Hard to ensure proper handling. Why context managers best practice: (1) Correctness: Guaranteed cleanup (manual: easy to forget). Exception-safe (manual: complex error handling). No resource leaks (manual: leaks common). (2) Readability: Clear intent (with resource: use it). Concise (less boilerplate). Self-documenting (resource lifecycle obvious). (3) Composability: Easy nesting (multiple resources). Standardized pattern (consistent across codebase). Testable (mock __aenter__/__aexit__). (4) Maintainability: Hard to misuse (automatic cleanup). Less error-prone (fewer failure modes). Easier to review (less code to verify). Real-world consequences: Manual management failures: Connection leaks → out of memory, crashed services. File handle leaks → "too many open files" error. Lock not released → deadlock, system hang. Partial transactions → data corruption. Context manager prevents all these: Automatic cleanup prevents leaks. Exception safety prevents corruption. Standard pattern reduces bugs. Production statistics: Manual resource management: 30% of resource-related bugs (forgotten cleanup). 20% from exception handling errors. 10% from complex nested cleanup. Context managers: <5% resource bugs (mostly logic errors, not cleanup). Clear winner for reliability. Example production issue: Manual management bug: async def process_batch(items): conn = await db.connect(); for item in items: if item.invalid: return; # ❌ Connection not closed!; await conn.execute(...); await conn.close(). After 1000 invalid items → 1000 leaked connections → database refuses new connections. Context manager fix: async def process_batch(items): async with db.connect() as conn: for item in items: if item.invalid: return; # ✅ Connection auto-closed!; await conn.execute(...). No leaks, system stable. Critical lesson: Context managers guarantee resource cleanup. Manual management error-prone (forgotten finally, exception handling). Always use context managers for resources in production. Testing must verify cleanup on exceptions.',
    keyPoints: [
      'Database: context manager auto-commits/rollbacks, manual risks partial updates if forgot rollback',
      'Files/HTTP: context manager always closes, manual leaks if exception before finally',
      'Locks: context manager prevents deadlocks, manual risks never releasing if forgot finally',
      'Exception safety: __aexit__ always called (even on exception/return), manual requires correct finally placement',
      'Best practice: guaranteed cleanup, readable, composable, testable, prevents 90% of resource bugs',
    ],
  },
];
