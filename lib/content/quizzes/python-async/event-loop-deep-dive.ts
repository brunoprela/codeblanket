export const eventLoopDeepDiveQuiz = [
  {
    id: 'eld-q-1',
    question:
      'Design an async web framework from scratch that handles HTTP requests using a custom event loop. Address: (1) how your event loop monitors incoming socket connections (I/O multiplexing), (2) parsing HTTP requests without blocking other connections, (3) handling 10,000 concurrent keep-alive connections efficiently, (4) implementing request timeouts, (5) graceful shutdown when receiving SIGTERM. Explain your architecture and provide Python pseudocode for the core event loop logic.',
    sampleAnswer:
      'Custom async web framework with event loop: Architecture: Event loop monitors listening socket (port 80) for incoming connections. When connection arrives, accept it and register with selector (epoll/kqueue). Read HTTP request data when socket readable. Parse request in chunks (streaming). Dispatch to request handler (async). Write response when socket writable. (1) I/O multiplexing: Use selectors module (abstraction over epoll/select/kqueue). import selectors; selector = selectors.DefaultSelector(). Register server socket: selector.register (server_sock, selectors.EVENT_READ, accept_callback). Event loop: while running: events = selector.select (timeout=1.0); for key, mask in events: callback = key.data; callback (key.fileobj, mask). When server socket readable → accept new connection. When client socket readable → read request data. When client socket writable → send response data. Pseudocode: def accept_connection (server_sock, mask): client_sock, addr = server_sock.accept(); client_sock.setblocking(False); selector.register (client_sock, EVENT_READ, read_request). (2) Parsing without blocking: Read in chunks: data = sock.recv(4096); request_buffer += data. Parse incrementally: if "\\r\\n\\r\\n" in request_buffer: headers = parse_headers (request_buffer); body_length = int (headers.get("Content-Length", 0)). Don\'t wait for entire request: If body incomplete, return and wait for more data. Event loop continues processing other sockets. Stream large requests: For chunked encoding, process chunks as they arrive. (3) 10,000 keep-alive connections: Challenge: Most connections idle (waiting for next request). Solution: Selector only wakes for active sockets (readable/writable). Memory per connection: ~4KB (socket buffer + connection state). Total: 40MB for 10K connections. Keep-alive timer: Track last activity per connection: connections[sock] = {"last_activity": time.time()}. Periodically (every second): now = time.time(); expired = [s for s, c in connections.items() if now - c["last_activity"] > 75]; for sock in expired: sock.close(); selector.unregister (sock). Register timeout using loop.call_later(). (4) Request timeouts: Set timeout per connection: when request starts, schedule timeout callback. timeout_handle = loop.call_later(30.0, lambda: timeout_connection (sock)). If request completes before timeout: timeout_handle.cancel(). If timeout fires: Close connection, send 408 Request Timeout. Pseudocode: def start_request (sock): timeout = loop.call_later(30.0, lambda: timeout (sock)); connections[sock]["timeout"] = timeout. def finish_request (sock): timeout = connections[sock].get("timeout"); if timeout: timeout.cancel(). (5) Graceful shutdown: Handle SIGTERM: import signal; signal.signal (signal.SIGTERM, shutdown_handler). Shutdown handler: def shutdown_handler (sig, frame): global running; running = False; print("Shutting down..."). In event loop: while running: events = selector.select (timeout=1.0); process (events). After loop exits: Close all connections: for sock in list (connections.keys()): sock.close(). Close server socket: server_sock.close(). Wait for pending requests: asyncio.gather(*pending_tasks, return_exceptions=True). Complete event loop structure: import selectors, socket, signal, time; selector = selectors.DefaultSelector(); connections = {}; running = True. def start_server(): server = socket.socket(); server.bind(("0.0.0.0", 8000)); server.listen(128); server.setblocking(False); selector.register (server, selectors.EVENT_READ, accept_conn); return server. def accept_conn (server, mask): client, addr = server.accept(); client.setblocking(False); connections[client] = {"buffer": b"", "response": None}; selector.register (client, selectors.EVENT_READ, read_request). def read_request (sock, mask): try: data = sock.recv(4096); if not data: close_connection (sock); return; connections[sock]["buffer"] += data; if b"\\r\\n\\r\\n" in connections[sock]["buffer"]: process_request (sock); selector.modify (sock, selectors.EVENT_WRITE, send_response); except ConnectionResetError: close_connection (sock). def send_response (sock, mask): response = connections[sock]["response"]; sent = sock.send (response); if sent == len (response): close_connection (sock). def close_connection (sock): selector.unregister (sock); sock.close(); del connections[sock]. def run(): signal.signal (signal.SIGTERM, lambda s, f: globals().update (running=False)); server = start_server(); print("Server running on port 8000"); while running: events = selector.select (timeout=1.0); for key, mask in events: callback = key.data; callback (key.fileobj, mask); cleanup_idle(); server.close(). This is simplified framework; production would use asyncio\'s event loop (already optimized). Learning: Event loop = I/O multiplexing (selector) + callbacks + state management.',
    keyPoints: [
      'I/O multiplexing: selectors module (epoll/kqueue) monitors sockets, event loop processes ready sockets',
      'Non-blocking parsing: read in chunks (4KB), parse incrementally, continue on other sockets if incomplete',
      'Keep-alive: selector only wakes for active sockets, ~4KB per connection = 40MB for 10K connections',
      'Timeouts: loop.call_later() schedules timeout callback per request, cancel if request completes',
      'Graceful shutdown: SIGTERM sets running=False, close all connections, wait for pending tasks',
    ],
  },
  {
    id: 'eld-q-2',
    question:
      'Explain how the event loop handles different types of I/O operations: (1) network sockets (TCP connections), (2) file I/O (reading large files), (3) subprocesses (running external commands), (4) inter-process communication (pipes, queues). For each, discuss: Does the event loop use true async I/O or thread pools? What are the performance implications? When would you use asyncio vs threading vs multiprocessing? Provide specific examples of blocking operations that can freeze an event loop.',
    sampleAnswer:
      'Event loop I/O handling deep dive: (1) Network sockets (TCP): Event loop support: TRUE ASYNC I/O (epoll/kqueue/IOCP). How: Register socket with selector: loop.add_reader (sock.fileno(), callback). OS notifies when socket readable/writable. No threads involved—single event loop thread. Performance: Excellent. Can handle 10,000+ concurrent connections. Latency: <1ms overhead. Memory: ~8KB per connection. Example: async with aiohttp.ClientSession() as session: responses = await asyncio.gather(*[session.get (url) for url in urls]). Can have 1000 concurrent requests with minimal overhead. Blocking operations that freeze event loop: requests.get (url): Synchronous HTTP, blocks entire event loop. time.sleep(1): Blocks event loop for 1 second. socket.recv(): If socket blocking, freezes event loop. (2) File I/O: Event loop support: THREAD POOL (not true async). Why: Most operating systems don\'t support true async file I/O. Linux: io_uring (new), but not standard in Python asyncio. Windows: IOCP supports file async, but Python doesn\'t use it fully. How: loop.run_in_executor(None, blocking_file_op) runs in thread pool. Default thread pool size: ThreadPoolExecutor (max_workers=min(32, cpu_count + 4)). Performance: Good for I/O-bound file operations. Overhead: Thread creation/switching (~10-100μs). Limitation: Still limited by thread pool size (default ~32). Example: async with aiofiles.open("large_file.txt") as f: data = await f.read(). This uses thread pool internally! Blocking operations: with open("file.txt") as f: data = f.read(): Synchronous, blocks event loop. For small files (<1MB), blocking acceptable. For large files (>100MB) or many files, use aiofiles. (3) Subprocesses: Event loop support: TRUE ASYNC I/O for subprocess stdout/stderr/stdin (pipes). How: asyncio.create_subprocess_exec() creates process with async pipes. Event loop monitors pipe file descriptors (like sockets). Can read/write to process without blocking. Performance: Excellent for I/O with subprocess. Process creation still has overhead (~1-10ms). Can run many subprocesses concurrently (limited by OS, typically 1000+). Example: process = await asyncio.create_subprocess_exec("python", "script.py", stdout=asyncio.subprocess.PIPE); stdout, stderr = await process.communicate(). Event loop waits for process output asynchronously. Blocking operations: subprocess.run(): Synchronous, blocks event loop until process completes. os.system(): Blocks event loop. Use asyncio.create_subprocess_exec() instead. (4) Inter-process communication (IPC): Pipes: TRUE ASYNC I/O. asyncio supports async pipes (like subprocess pipes). reader, writer = await asyncio.open_unix_connection("/tmp/pipe"). Queues: multiprocessing.Queue BLOCKS event loop (uses locks/semaphores). Solution: Use asyncio-compatible queue: async_queue = asyncio.Queue() for async tasks. For IPC between processes: Use aioredis, aio_pika (async message queue). Shared memory: multiprocessing.shared_memory BLOCKS (synchronous access). Solution: Use async coordination: redis, memcached with async clients. Example: from aio_pika import connect; connection = await connect("amqp://localhost/"); channel = await connection.channel(); queue = await channel.declare_queue("messages"); async for message in queue: await process (message). True async IPC without blocking. When to use asyncio vs threading vs multiprocessing: (1) Network I/O (many connections): Use asyncio. Why: True async I/O, can handle 10K+ connections, single thread, low memory. Example: Web server, web scraper, API client. (2) File I/O (few large files): Use threading (or asyncio with thread pool). Why: File I/O not truly async (OS limitation), threads work well. Example: Reading 10 large log files concurrently. (3) CPU-intensive subprocess: Use multiprocessing. Why: Subprocess does heavy computation, need multiple CPU cores. Example: Video encoding, data processing pipeline. (4) Mixed workload (network + CPU): Use asyncio for network, multiprocessing for CPU. Example: Web server that offloads image processing: async def handle_request (image_data): processed = await loop.run_in_executor (process_pool, resize_image, image_data); return response. Process pool for CPU work, async for I/O. Blocking operations that freeze event loop (recap): time.sleep(): Use await asyncio.sleep() instead. requests.get(): Use aiohttp instead. open().read(): Use aiofiles for large files. subprocess.run(): Use asyncio.create_subprocess_exec(). Any synchronous I/O: Database queries (use async drivers like asyncpg). long_computation(): CPU-bound, use loop.run_in_executor (process_pool, ...). Testing for blocking: Enable debug mode: asyncio.run (main(), debug=True). Python warns if operation blocks >100ms: "Executing <Task> took 0.150 seconds". Profile with: import asyncio; asyncio.run (main(), debug=True). Summary table: | I/O Type | Async Support | Mechanism | Performance | Best For | |----------|---------------|-----------|-------------|----------| | Network | True async | epoll/kqueue | Excellent (10K+ conn) | APIs, scrapers, servers | | File | Thread pool | Threads | Good (limited by pool) | Large files, many files | | Subprocess | True async (pipes) | Pipe monitoring | Excellent | External commands | | IPC | Depends | Varies | Use async libs | Message queues, RPC | Critical: Event loop is single-threaded. Any blocking operation freezes entire loop. Always use async alternatives (aiohttp, aiofiles, asyncpg) or offload to thread/process pool.',
    keyPoints: [
      'Network: true async I/O (epoll/kqueue), excellent for 10K+ connections, <1ms latency',
      'File: thread pool (OS limitation), good but limited by pool size, use aiofiles for large files',
      'Subprocesses: true async I/O for pipes, excellent performance, use asyncio.create_subprocess_exec()',
      'IPC: pipes are async, multiprocessing.Queue blocks, use async message queues (aio_pika, aioredis)',
      'Blocking operations freeze event loop: requests.get(), time.sleep(), open().read(), subprocess.run()',
    ],
  },
  {
    id: 'eld-q-3',
    question:
      'Compare event loop implementations: (1) CPython asyncio (default), (2) uvloop (high-performance), (3) Trio (structured concurrency). For each, discuss: architecture differences, performance characteristics, when to use, migration considerations. Then design a benchmark to measure event loop overhead for: network I/O (1000 concurrent HTTP requests), task creation (creating 10,000 tasks), and context switching (10,000 tasks yielding control 100 times each). What metrics matter most for production applications?',
    sampleAnswer:
      'Event loop implementation comparison: (1) CPython asyncio (default): Architecture: Pure Python event loop built on selectors (epoll/kqueue). Single-threaded, callback-based. Policy system for extensibility. Performance: Good. Network I/O: ~10,000 req/sec per core. Task creation: ~15μs per task. Context switching: ~2μs per switch. Memory: ~8KB per concurrent connection. Advantages: Standard library (no dependencies). Cross-platform (Windows, Linux, Mac). Well-documented, mature. Disadvantages: Slower than uvloop (2-4×). Pure Python overhead. When to use: Default choice for most applications. When dependencies not allowed. Windows support required (uvloop Unix-only). Migration: N/A (default). (2) uvloop (high-performance): Architecture: Written in Cython, wraps libuv (C library used by Node.js). Drop-in replacement for asyncio event loop. Same API, different implementation. Performance: Excellent. 2-4× faster than asyncio. Network I/O: ~40,000 req/sec per core. Task creation: ~5μs per task. Context switching: <1μs per switch. Memory: Similar to asyncio (~8KB per conn). Advantages: Massive performance improvement. Drop-in replacement (easy migration). Production-proven (used by many high-traffic services). Disadvantages: Unix/Linux only (no Windows). Extra dependency. C extension (harder to debug). When to use: Production applications needing maximum performance. Linux/Unix deployment. High-traffic APIs (>10K req/sec). Migration: import uvloop; asyncio.set_event_loop_policy (uvloop.EventLoopPolicy()). One line change! Existing asyncio code works unchanged. (3) Trio (structured concurrency): Architecture: Different design philosophy (structured concurrency). Task lifetimes tied to explicit scopes. Cancel operations cancel entire scope. No detached tasks (prevents leaks). Performance: Similar to asyncio. Focus on correctness over speed. Network I/O: ~8,000 req/sec (slightly slower). Task creation: ~20μs per task. Context switching: ~3μs per switch. Advantages: Easier to reason about (structured). Better cancellation (no orphaned tasks). Prevents common async bugs (forgot to await). Disadvantages: Different API (not asyncio-compatible). Smaller ecosystem. Not standard library. When to use: New projects valuing correctness. Complex control flow with cancellation. Teaching async programming (clearer model). Migration: Significant rewrite (different API). Must convert asyncio coroutines to Trio. Example: async with trio.open_nursery() as nursery: nursery.start_soon (task1); nursery.start_soon (task2). All tasks cancelled when nursery exits. Benchmark design: import asyncio, time, aiohttp, statistics. Benchmark 1: Network I/O (1000 concurrent HTTP requests). async def network_benchmark(): urls = ["http://httpbin.org/delay/0" for _ in range(1000)]; start = time.time(); async with aiohttp.ClientSession() as session: tasks = [session.get (url) for url in urls]; responses = await asyncio.gather(*tasks); elapsed = time.time() - start; print(f"Network: {elapsed:.2f}s, {1000/elapsed:.0f} req/sec"). Metrics: Total time, requests per second, memory usage. Expected: asyncio: ~1.0s (1000 req/sec); uvloop: ~0.5s (2000 req/sec). Benchmark 2: Task creation (10,000 tasks). async def task_creation_benchmark(): async def empty_task(): pass. start = time.time(); tasks = [asyncio.create_task (empty_task()) for _ in range(10000)]; await asyncio.gather(*tasks); elapsed = time.time() - start; per_task = elapsed / 10000 * 1000000; # μs; print(f"Task creation: {per_task:.2f}μs per task"). Metrics: Time per task creation, overhead. Expected: asyncio: 15μs; uvloop: 5μs; Trio: 20μs. Benchmark 3: Context switching (10,000 tasks × 100 switches). async def context_switch_benchmark(): async def switch_task(): for _ in range(100): await asyncio.sleep(0); start = time.time(); await asyncio.gather(*[switch_task() for _ in range(1000)]); elapsed = time.time() - start; switches = 1000 * 100; per_switch = elapsed / switches * 1000000; # μs; print(f"Context switch: {per_switch:.2f}μs"). Metrics: Time per context switch. Expected: asyncio: 2μs; uvloop: 0.5μs; Trio: 3μs. Production metrics that matter most: (1) Latency (p50, p95, p99): Time from request to response. Target: p99 <100ms for APIs. Measure: Track per-request timing, calculate percentiles. Why: User experience depends on tail latency, not average. (2) Throughput: Requests per second. Target: >1000 req/sec per core for APIs. Measure: Total requests / time. Why: Determines scaling needs (how many servers?). (3) Memory per connection: RAM usage for 10K concurrent connections. Target: <100MB for 10K connections. Measure: memory_profiler or /proc/self/status. Why: Limits max concurrent connections. (4) Event loop lag: Time event loop spends blocked. Target: <10ms at p99. Measure: asyncio debug mode, loop.time() deltas. Why: High lag means blocking operations (bad!). (5) Task queue depth: Number of pending tasks. Target: <1000 pending tasks. Measure: len (asyncio.all_tasks()). Why: High queue depth indicates backpressure. Benchmark results (typical): | Metric | asyncio | uvloop | Trio | |--------|---------|--------|------| | Network I/O (req/sec) | 1000 | 2500 | 800 | | Task creation (μs) | 15 | 5 | 20 | | Context switch (μs) | 2 | 0.5 | 3 | | Memory per conn (KB) | 8 | 8 | 10 | Production recommendation: Default: asyncio (standard library, good enough). High-performance: uvloop (2-4× speedup, one line change). Correctness-critical: Trio (structured concurrency prevents bugs). Always profile your specific workload. Microbenchmarks don\'t always predict real-world performance!',
    keyPoints: [
      'CPython asyncio: standard library, cross-platform, ~10K req/sec, 15μs task creation, good default',
      'uvloop: 2-4× faster, drop-in replacement, ~40K req/sec, 5μs task creation, production choice for Linux',
      'Trio: structured concurrency, different API, similar performance, prevents async bugs, good for new projects',
      'Benchmark: network I/O (throughput), task creation (overhead), context switching (scheduler efficiency)',
      'Production metrics: latency (p99 <100ms), throughput (>1K req/sec), memory (<100MB for 10K), event loop lag (<10ms)',
    ],
  },
];
