export const asyncHttpAiohttpQuiz = [
  {
    id: 'aha-q-1',
    question:
      'Design a production-ready async web scraper that fetches 10,000 product pages with: (1) Rate limiting (max 100 concurrent, 50 requests/second), (2) Retry logic with exponential backoff for transient failures, (3) Circuit breaker pattern (stop requests if failure rate >20%), (4) Connection pooling optimization, (5) Progress monitoring and ETA calculation. Explain aiohttp session configuration, ClientConnectorError vs ClientResponseError handling, and why connection pooling is critical at scale.',
    sampleAnswer:
      'Production web scraper with aiohttp: Implementation: import aiohttp, asyncio, time; from aiohttp import TCPConnector, ClientTimeout; from collections import deque. class WebScraper: def __init__(self): self.connector = TCPConnector(limit=100, limit_per_host=10, ttl_dns_cache=300); self.timeout = ClientTimeout(total=30, connect=10); self.session = None; self.stats = {"success": 0, "failed": 0, "retried": 0}; self.start_time = None; self.semaphore = asyncio.Semaphore(100); self.rate_limiter = RateLimiter(50, 1.0); self.circuit_breaker = CircuitBreaker(failure_threshold=0.2). (1) Rate limiting: Semaphore for concurrent limit: async with self.semaphore: limits to 100. Rate limiter for requests/second: class RateLimiter: def __init__(self, rate, per): self.rate = rate; self.tokens = rate; self.last_update = time.time(). async def acquire(self): now = time.time(); elapsed = now - self.last_update; self.tokens = min(self.rate, self.tokens + elapsed * (self.rate / per)); if self.tokens < 1: await asyncio.sleep((1 - self.tokens) / rate); self.last_update = now. (2) Retry with exponential backoff: async def fetch_with_retry(self, url, retries=3): for attempt in range(retries): try: return await self.fetch(url); except aiohttp.ClientError: if attempt == retries - 1: raise; wait = 2 ** attempt; await asyncio.sleep(wait); self.stats["retried"] += 1. (3) Circuit breaker: class CircuitBreaker: def __init__(self, threshold): self.failure_rate_threshold = threshold; self.recent_results = deque(maxlen=100); self.is_open = False. async def call(self, func, *args): if self.is_open: raise CircuitOpenError(); try: result = await func(*args); self.recent_results.append(True); return result; except Exception: self.recent_results.append(False); failure_rate = 1 - sum(self.recent_results) / len(self.recent_results); if failure_rate > self.failure_rate_threshold: self.is_open = True; raise. (4) Connection pooling: connector = TCPConnector(limit=100, limit_per_host=10): Reuses TCP connections (avoid handshake overhead). limit=100: Max total connections. limit_per_host=10: Max per domain (respect server). ttl_dns_cache=300: Cache DNS for 5 minutes. Why critical: New connection: TCP handshake (50-100ms) + TLS (100-200ms) = 150-300ms overhead. Reused connection: ~0ms overhead. At 10K requests: New: 1500-3000s overhead. Reused: ~0s. (5) Progress monitoring: self.start_time = time.time(); completed = self.stats["success"] + self.stats["failed"]; elapsed = time.time() - self.start_time; rate = completed / elapsed; eta = (total - completed) / rate if rate > 0 else float("inf"); print(f"Progress: {completed}/{total}, Rate: {rate:.1f}/s, ETA: {eta:.0f}s"). Error handling: ClientConnectorError: Connection-level errors (DNS, network, timeout during connect). Causes: Domain doesn\'t exist, network down, firewall blocking. Handling: Retry immediately unlikely to help. Log and skip URL, or retry after long delay. ClientResponseError: HTTP-level errors (4xx, 5xx status codes). Causes: 404 not found, 500 server error, 503 service unavailable. Handling: 4xx: Don\'t retry (client error). 5xx: Retry with backoff (server error, might recover). Circuit breaker critical: Prevents overwhelming failing service. If >20% requests fail → stop all requests for cooldown period. Allows service to recover. Alternative: Reduce rate instead of complete stop. Complete implementation ensures reliability at scale.',
    keyPoints: [
      'Rate limiting: Semaphore(100) for concurrent, token bucket for 50 req/s, prevents overwhelming servers',
      "Retry: exponential backoff (1s, 2s, 4s) for transient failures, don't retry 4xx client errors",
      'Circuit breaker: tracks failure rate, opens circuit if >20%, prevents cascade failures',
      'Connection pooling: TCPConnector(limit=100, per_host=10), reuses connections, saves 150-300ms per request',
      'ClientConnectorError (network): rarely retry, ClientResponseError (HTTP): retry 5xx not 4xx',
    ],
  },
  {
    id: 'aha-q-2',
    question:
      "Compare aiohttp vs requests for: (1) Concurrent request handling, (2) Connection pooling, (3) Memory usage with 1000 concurrent requests, (4) Streaming large responses. Provide benchmarks showing time and memory for fetching 100 URLs (1 second each). Explain when requests is actually acceptable despite being blocking. Why can't you just use requests with ThreadPoolExecutor?",
    sampleAnswer:
      'aiohttp vs requests comparison: (1) Concurrent handling: requests (blocking): For sequential execution only. For concurrent: Must use ThreadPoolExecutor. Each request blocks thread. Limited by thread pool size (~32 threads). aiohttp (async): True async concurrency. Single thread, event loop manages all. Can handle 10,000+ concurrent requests. Example: requests sequential: 100 URLs × 1s = 100 seconds. requests with ThreadPoolExecutor(32): 100 / 32 = ~4 seconds (limited by threads). aiohttp async: ~1 second (all concurrent). (2) Connection pooling: requests.Session: Has connection pooling. But per-thread (not shared across threads). Each thread maintains own pool. aiohttp.ClientSession: Shared connection pool across all concurrent requests. More efficient reuse. Configurable limits (per-host, total). Benchmark: requests: 32 threads × 5 connections each = 160 connections. aiohttp: Single session, 100 connections shared = 100 connections (less overhead). (3) Memory usage (1000 concurrent): requests + ThreadPoolExecutor: Thread overhead: 1000 threads × 8MB stack = 8GB (impossible!). Realistic limit: 32-64 threads. Can only handle ~60 concurrent with reasonable memory. aiohttp: Coroutine overhead: 1000 coroutines × 8KB = 8MB. Can handle 1000+ concurrent easily. 1000× less memory! (4) Streaming large responses: requests: response.iter_content(chunk_size=8192). Streams in current thread (blocks other operations). Must use separate thread per stream. Limited to ~32 concurrent streams. aiohttp: async for chunk in response.content.iter_chunked(8192). Streams without blocking event loop. Can stream 1000+ files concurrently. Memory efficient for all. Benchmark (100 URLs, 1s each): Test setup: 100 URLs, each returns after 1 second. Sequential: for url in urls: requests.get(url). Time: 100 seconds. Memory: ~50MB (one request at a time). ThreadPoolExecutor: with ThreadPoolExecutor(32): executor.map(requests.get, urls). Time: 100 / 32 = 3.2 seconds. Memory: 32 threads × 8MB = 256MB. CPU: 32 threads = context switching overhead. aiohttp: async with ClientSession(): gather(*[session.get(url) for url in urls]). Time: ~1.0 second (all concurrent). Memory: ~60MB (session + 100 coroutines). CPU: Single thread, event loop efficient. Results: | Metric | Sequential | ThreadPool (32) | aiohttp | |--------|-----------|----------------|--------| | Time | 100s | 3.2s | 1.0s | | Memory | 50MB | 256MB | 60MB | | Speedup | 1× | 31× | 100× | | Scalability | N/A | Max 32-64 | 10,000+ | When requests acceptable: Small-scale: <10 concurrent requests (complexity not worth it). Quick scripts: One-off tasks, not production. Synchronous codebase: Can\'t introduce async (major refactor). External constraint: Library only supports requests. No async available: Some APIs only have requests wrapper. But: aiohttp almost always better for production. Why not requests + ThreadPoolExecutor: GIL limitations: Python GIL allows only one thread executing bytecode. For I/O-bound (network), threads release GIL during I/O waits (some benefit). But: Thread overhead still exists (memory, context switching). Thread pool size limited: Max ~32-64 threads practically. More threads: High memory (8MB each), context switching overhead, diminishing returns. Example: 1000 concurrent requests: requests + threads: Need 1000 threads = 8GB RAM (not feasible). aiohttp async: 1000 coroutines = 8MB RAM (easy). Not true concurrency: GIL means no parallel execution. Only helps when waiting (I/O). aiohttp more efficient: Single-threaded, no GIL contention, lower overhead. Blocking nature: Even in thread, requests.get() blocks that thread. If any CPU work in thread, blocks other requests in that thread. aiohttp: Cooperative multitasking, explicitly yields (await). Benchmark proof: import concurrent.futures, requests, aiohttp, asyncio, time. # Requests + ThreadPoolExecutor; start = time.time(); with concurrent.futures.ThreadPoolExecutor(32) as executor: results = list(executor.map(requests.get, urls)); print(f"ThreadPool: {time.time() - start:.2f}s"). # aiohttp async; start = time.time(); async def fetch_all(): async with aiohttp.ClientSession() as session: return await asyncio.gather(*[session.get(url) for url in urls]); results = asyncio.run(fetch_all()); print(f"aiohttp: {time.time() - start:.2f}s"). # aiohttp 3-10× faster even with threads helping requests! Critical lesson: requests + ThreadPoolExecutor limited by threads (max 32-64 concurrent). aiohttp truly async (10,000+ concurrent, less memory). Always prefer aiohttp for production concurrent HTTP.',
    keyPoints: [
      'Concurrent: requests limited to ~32-64 threads, aiohttp handles 10,000+ (single-threaded)',
      'Memory: requests 8MB per thread, aiohttp 8KB per coroutine (1000× less)',
      'Benchmark: 100 URLs (1s each): sequential 100s, ThreadPool(32) 3.2s, aiohttp 1.0s',
      'ThreadPoolExecutor not solution: GIL, limited threads, 8MB per thread, context switching overhead',
      "Use requests only for: quick scripts, <10 concurrent, can't introduce async; otherwise aiohttp always better",
    ],
  },
  {
    id: 'aha-q-3',
    question:
      'Implement a production aiohttp client wrapper that provides: (1) Automatic retry with exponential backoff, (2) Request/response logging for debugging, (3) Metrics collection (latency, success rate), (4) Context manager for session lifecycle, (5) Type hints for API methods. Explain why ClientSession must be reused (not created per-request), how to properly close sessions, and testing strategies for async HTTP clients.',
    sampleAnswer:
      'Production aiohttp client wrapper: from typing import Optional, Dict, Any; import aiohttp, asyncio, time, logging; from dataclasses import dataclass. @dataclass; class RequestMetrics: latency: float; status: int; success: bool; retries: int. class HTTPClient: def __init__(self, base_url: str, timeout: float = 30.0, max_retries: int = 3): self.base_url = base_url; self.max_retries = max_retries; self.metrics: list[RequestMetrics] = []; self.logger = logging.getLogger(__name__); connector = aiohttp.TCPConnector(limit=100, limit_per_host=30); self._timeout = aiohttp.ClientTimeout(total=timeout); self.session: Optional[aiohttp.ClientSession] = None. async def __aenter__(self): await self.start(); return self. async def __aexit__(self, *args): await self.close(). async def start(self): if not self.session: self.session = aiohttp.ClientSession(connector=aiohttp.TCPConnector(limit=100), timeout=self._timeout); self.logger.info("HTTP client session started"). async def close(self): if self.session: await self.session.close(); self.logger.info("HTTP client session closed"); self.session = None. async def get(self, path: str, params: Optional[Dict] = None) -> Dict[str, Any]: return await self._request("GET", path, params=params). async def post(self, path: str, json: Optional[Dict] = None) -> Dict[str, Any]: return await self._request("POST", path, json=json). async def _request(self, method: str, path: str, retries: int = 0, **kwargs) -> Dict[str, Any]: if not self.session: raise RuntimeError("Session not started"); url = f"{self.base_url}{path}"; start_time = time.time(); self.logger.debug(f"{method} {url}"); try: async with self.session.request(method, url, **kwargs) as response: latency = time.time() - start_time; self.logger.info(f"{method} {url} - {response.status} ({latency:.3f}s)"); self.metrics.append(RequestMetrics(latency=latency, status=response.status, success=response.ok, retries=retries)); response.raise_for_status(); return await response.json(); except (aiohttp.ClientError, asyncio.TimeoutError) as e: latency = time.time() - start_time; self.logger.warning(f"{method} {url} failed: {e}"); if retries < self.max_retries: wait = 2 ** retries; self.logger.info(f"Retrying in {wait}s... (attempt {retries + 1}/{self.max_retries})"); await asyncio.sleep(wait); return await self._request(method, path, retries=retries + 1, **kwargs); self.metrics.append(RequestMetrics(latency=latency, status=0, success=False, retries=retries)); raise. def get_metrics(self) -> Dict: if not self.metrics: return {}; latencies = [m.latency for m in self.metrics]; successes = sum(1 for m in self.metrics if m.success); return {"total_requests": len(self.metrics), "success_rate": successes / len(self.metrics), "avg_latency": sum(latencies) / len(latencies), "p95_latency": sorted(latencies)[int(len(latencies) * 0.95)], "p99_latency": sorted(latencies)[int(len(latencies) * 0.99)]}. (1) Automatic retry: Implemented in _request method. Catches aiohttp.ClientError and TimeoutError. Exponential backoff: wait = 2 ** retries (1s, 2s, 4s). Tracks retry count in metrics. (2) Request/response logging: Debug: Request details (method, URL). Info: Response (status, latency). Warning: Failures with exception. (3) Metrics collection: RequestMetrics dataclass stores per-request data. get_metrics() computes aggregates (success rate, latencies). P95/P99 latency for tail analysis. (4) Context manager: __aenter__ starts session. __aexit__ ensures cleanup. Usage: async with HTTPClient("https://api.com") as client: data = await client.get("/users"). (5) Type hints: Parameters: path: str, params: Optional[Dict]. Return: -> Dict[str, Any]. Helps IDEs and type checkers. Why ClientSession must be reused: Performance: Session maintains connection pool. Creating per-request: New connections (TCP handshake + TLS) every time = 150-300ms overhead. Reusing session: Connections reused = ~0ms overhead. Resource exhaustion: Each session: File descriptors for sockets, memory for buffers. Many sessions: "Too many open files" error, memory leak. One session: Controlled resource usage. Example: Bad (1000 requests): for i in range(1000): async with aiohttp.ClientSession() as session: await session.get(url); # Creates 1000 sessions, 1000 connection handshakes = slow + resource leak. Good (1000 requests): async with aiohttp.ClientSession() as session: await asyncio.gather(*[session.get(url) for _ in range(1000)]); # One session, reuses connections = fast + efficient. Proper session closure: Async context manager (best): async with ClientSession() as session: ... automatically calls await session.close(). Manual: session = ClientSession(); try: ...; finally: await session.close(). Why important: Close connections gracefully. Release file descriptors. Flush pending data. Avoid resource leaks. Testing strategies: Test 1: Mock aiohttp responses: from aiotest import ClientSession; async def test_get(): client = HTTPClient("https://api.com"); with aiotest.mock.patch("aiohttp.ClientSession.get") as mock_get: mock_get.return_value.__aenter__.return_value.status = 200; mock_get.return_value.__aenter__.return_value.json.return_value = {"data": "test"}; result = await client.get("/test"); assert result == {"data": "test"}. Test 2: Test retry logic: with aiotest.mock.patch as mock_get: mock_get.side_effect = [aiohttp.ClientError, aiohttp.ClientError, {"data": "success"}]; result = await client.get("/test"); assert client.metrics[-1].retries == 2; assert result == {"data": "success"}. Test 3: Test timeout: with aiotest.mock.patch: mock_get.side_effect = asyncio.TimeoutError; with pytest.raises(asyncio.TimeoutError): await client.get("/test"). Test 4: Integration test with real server: async def test_integration(): async with HTTPClient("https://httpbin.org") as client: data = await client.get("/json"); assert "slideshow" in data. Test 5: Verify session reuse: client = HTTPClient("https://api.com"); await client.start(); session1 = client.session; await client.get("/test"); session2 = client.session; assert session1 is session2; # Same session reused. Critical production patterns: Rate limiting: Add semaphore to limit concurrent requests. Circuit breaker: Stop requests if failure rate high. Connection pooling: Configure limits appropriately. Monitoring: Export metrics to Prometheus/Datadog. Timeout tuning: Set based on API SLA. This wrapper provides production-ready HTTP client with observability and reliability.',
    keyPoints: [
      'Retry: exponential backoff in _request, track retries in metrics, catches ClientError and TimeoutError',
      'Logging: debug (request), info (response + latency), warning (failures), helps debugging production issues',
      'Metrics: collect per-request (latency, status, success, retries), aggregate (success rate, p95/p99)',
      'ClientSession must be reused: connection pooling (saves 150-300ms per request), one session prevents resource leaks',
      'Testing: mock aiohttp responses, test retry logic, verify session reuse, integration tests with real server',
    ],
  },
];
