export const asyncioBuiltinFunctionsQuiz = [
  {
    id: 'abf-q-1',
    question:
      'You need to fetch data from 50 APIs with rate limit of 10 requests/second. Design a solution using asyncio.Semaphore and asyncio.gather(). Address: (1) How semaphore enforces rate limit, (2) Handling API failures without stopping others, (3) Retrying failed requests with exponential backoff, (4) Collecting partial results if some APIs timeout, (5) Monitoring current rate and queue depth. Provide code with proper error handling and explain semaphore acquisition mechanics.',
    sampleAnswer:
      'Rate-limited API fetching with Semaphore: Implementation: import asyncio, aiohttp, time; from typing import Optional. class RateLimitedFetcher: def __init__(self, rate_limit=10): self.semaphore = asyncio.Semaphore(rate_limit); self.start_time = time.time(); self.request_times = []. async def fetch_with_retry(self, session, url, retries=3) -> Optional[dict]: """Fetch with exponential backoff retry"""; for attempt in range(retries): try: async with self.semaphore: # Rate limiting; self.request_times.append(time.time()); print(f"Request {len(self.request_times)}: {url}"); async with session.get(url, timeout=aiohttp.ClientTimeout(total=5)) as response: return await response.json(); except asyncio.TimeoutError: if attempt == retries - 1: print(f"Timeout after {retries} attempts: {url}"); return None; wait = 2 ** attempt; await asyncio.sleep(wait); except aiohttp.ClientError as e: if attempt == retries - 1: print(f"Failed after {retries} attempts: {url}: {e}"); return None; await asyncio.sleep(2 ** attempt); return None. async def fetch_all(self, urls): """Fetch all URLs with rate limiting"""; async with aiohttp.ClientSession() as session: tasks = [self.fetch_with_retry(session, url) for url in urls]; results = await asyncio.gather(*tasks, return_exceptions=True); # Collect all; return results. def get_stats(self): """Get rate limiting statistics"""; elapsed = time.time() - self.start_time; rate = len(self.request_times) / elapsed if elapsed > 0 else 0; return {"total_requests": len(self.request_times), "elapsed": elapsed, "rate": rate}. (1) Semaphore enforces rate limit: Semaphore(10) allows 10 concurrent operations. Each fetch acquires semaphore: async with self.semaphore: await session.get(url). If 10 requests running, 11th blocks at semaphore acquire. Releases when request completes (automatic with context manager). Result: Maximum 10 concurrent requests. Rate limiting: Semaphore limits concurrent, not rate. For true rate limit (10/sec), need token bucket. Simplified: Concurrent limit approximates rate limit. With 0.5s avg request time: 10 concurrent Ã— (1 / 0.5s) = 20 req/sec. Adjust semaphore value for desired rate. (2) Handle failures without stopping others: Use return_exceptions=True in gather(): results = await asyncio.gather(*tasks, return_exceptions=True). Each task exception captured as result, not raised. Check results: for i, result in enumerate(results): if isinstance(result, Exception): print(f"Task {i} failed: {result}"); elif result is None: print(f"Task {i} timed out"); else: process(result). Benefits: All tasks complete. Partial success useful. Know which specific tasks failed. (3) Retry with exponential backoff: Pattern: for attempt in range(retries): try: return await fetch(); except: if attempt == retries - 1: raise; wait = 2 ** attempt; await asyncio.sleep(wait). Backoff: attempt 0: wait 1s, attempt 1: wait 2s, attempt 2: wait 4s. Prevents overwhelming failing service. (4) Partial results with timeout: Per-request timeout: timeout=aiohttp.ClientTimeout(total=5). Return None on timeout: except asyncio.TimeoutError: return None. Collect all: results include None for timeouts. Process: successful = [r for r in results if r and not isinstance(r, Exception)]. (5) Monitor rate and queue depth: Track requests: self.request_times.append(time.time()). Calculate rate: rate = len(requests) / elapsed_time. Queue depth: tasks waiting = len(pending_tasks) - semaphore._value. Current rate: recent_rate = len([t for t in requests if t > time.time() - 1]). Dashboard: print(f"Rate: {rate:.1f} req/s, Queue: {queue_depth}"). Semaphore acquisition mechanics: Create: sem = asyncio.Semaphore(10) sets _value = 10. Acquire: async with sem: decrements _value. If _value > 0: proceed immediately. If _value == 0: wait in queue (await blocks). Release: exit context manager increments _value, wakes one waiter. Pattern: async with semaphore: # _value decremented; # Only N can be here; # _value incremented on exit. Testing rate limiting: Test 1: Verify max concurrent: sem = Semaphore(5); active = 0; max_active = 0; async def task(): nonlocal active, max_active; active += 1; max_active = max(max_active, active); async with sem: await asyncio.sleep(0.1); active -= 1; await gather(*[task() for _ in range(20)]); assert max_active <= 5. Test 2: Verify rate: start = time.time(); fetcher = RateLimitedFetcher(10); await fetcher.fetch_all(urls); stats = fetcher.get_stats(); assert stats["rate"] <= 12; # Allow some variance. Production considerations: Adaptive rate limiting: If errors spike, reduce semaphore value dynamically. Burst handling: Allow temporary exceeding rate for burst traffic. Monitoring: Alert if rate drops below threshold (indicates issues). Token bucket: For true rate limiting (not just concurrent limit): class TokenBucket: def __init__(self, rate, per): self.rate = rate; self.per = per; self.tokens = rate; self.last_update = time.time(). async def acquire(self): # Add tokens for time passed; now = time.time(); elapsed = now - self.last_update; self.tokens = min(self.rate, self.tokens + elapsed * (self.rate / self.per)); # Wait if no tokens; if self.tokens < 1: wait = (1 - self.tokens) * (self.per / self.rate); await asyncio.sleep(wait); self.tokens = 0; else: self.tokens -= 1; self.last_update = now. Critical lesson: Semaphore limits concurrency, approximates rate limiting. return_exceptions=True allows partial success. Exponential backoff prevents overwhelming failing services.',
    keyPoints: [
      'Semaphore(10): max 10 concurrent, blocks 11th until one completes, approximates rate limit',
      "return_exceptions=True: collect all results/exceptions, partial success, don't stop on first error",
      'Retry: exponential backoff (1s, 2s, 4s), per-request timeout (5s), return None on final failure',
      'Monitoring: track request_times, calculate rate = requests / elapsed, queue depth = pending - semaphore._value',
      'Token bucket: for true rate limiting (tokens refill at rate), semaphore only limits concurrent',
    ],
  },
  {
    id: 'abf-q-2',
    question:
      'Implement a producer-consumer system using asyncio.Queue where: (1) 3 producers fetch data from APIs at different rates, (2) 5 consumers process items with varying CPU intensity, (3) Queue is bounded (max 100 items) to prevent memory issues, (4) Graceful shutdown when producers done, (5) Monitor queue depth and throughput. Explain queue operations (put, get, task_done, join), backpressure handling, and why this pattern is essential for data pipelines.',
    sampleAnswer:
      'Producer-consumer with asyncio.Queue: Complete implementation: import asyncio, random, time. class DataPipeline: def __init__(self, queue_size=100): self.queue = asyncio.Queue(maxsize=queue_size); self.stats = {"produced": 0, "consumed": 0, "queue_depths": []}. async def producer(self, producer_id, rate): """Produce items at specified rate"""; try: while True: item = {"id": self.stats["produced"], "data": f"Item from producer {producer_id}", "timestamp": time.time()}; await self.queue.put(item); # Blocks if queue full; self.stats["produced"] += 1; print(f"Producer {producer_id}: Produced item {item["id"]}"); await asyncio.sleep(1.0 / rate); # Rate limiting; except asyncio.CancelledError: print(f"Producer {producer_id}: Cancelled"); raise. async def consumer(self, consumer_id, process_time_range): """Consume and process items"""; try: while True: item = await self.queue.get(); # Blocks if queue empty; print(f"  Consumer {consumer_id}: Processing item {item["id"]}"); # Simulate processing (varying CPU intensity); process_time = random.uniform(*process_time_range); await asyncio.sleep(process_time); self.stats["consumed"] += 1; self.queue.task_done(); # Mark item as done; except asyncio.CancelledError: print(f"  Consumer {consumer_id}: Cancelled"); raise. async def monitor(self): """Monitor queue depth and throughput"""; start_time = time.time(); last_consumed = 0; while True: await asyncio.sleep(1); depth = self.queue.qsize(); self.stats["queue_depths"].append(depth); elapsed = time.time() - start_time; consumed_delta = self.stats["consumed"] - last_consumed; last_consumed = self.stats["consumed"]; throughput = consumed_delta; # Items per second; print(f"[Monitor] Queue: {depth:3d}, Throughput: {throughput:2d} items/s, Total: {self.stats["consumed"]}/{self.stats["produced"]}"). async def run(self, duration=10): """Run pipeline for specified duration"""; # Create producers (different rates); producers = [asyncio.create_task(self.producer(i, rate)) for i, rate in enumerate([5, 10, 15], 1)]; # 5, 10, 15 items/sec. # Create consumers (varying processing times); consumers = [asyncio.create_task(self.consumer(i, time_range)) for i, time_range in enumerate([(0.1, 0.3), (0.2, 0.4), (0.15, 0.35), (0.1, 0.5), (0.2, 0.3)], 1)]. # Start monitor; monitor_task = asyncio.create_task(self.monitor()). # Run for duration; await asyncio.sleep(duration). # Graceful shutdown; print("\n[Pipeline] Stopping producers..."); for p in producers: p.cancel(); await asyncio.gather(*producers, return_exceptions=True). print("[Pipeline] Waiting for queue to empty..."); await self.queue.join(); # Wait for all items processed. print("[Pipeline] Stopping consumers..."); for c in consumers: c.cancel(); await asyncio.gather(*consumers, return_exceptions=True). monitor_task.cancel(); print(f"\n[Pipeline] Completed: Produced {self.stats["produced"]}, Consumed {self.stats["consumed"]}"). (1) Producers at different rates: Three producers: 5, 10, 15 items/sec. Total production rate: 30 items/sec. Each producer: await asyncio.sleep(1.0 / rate) paces production. Independent: Each producer runs concurrently. (2) Consumers with varying CPU: Five consumers with different process times: (0.1-0.3s), (0.2-0.4s), etc. Varying intensity simulates real workload (some items fast, some slow). Concurrent processing: All consumers work simultaneously. (3) Bounded queue (max 100): Queue(maxsize=100) limits memory usage. Backpressure: If queue full, await queue.put(item) blocks producer. Producer slows to consumer rate automatically. Why bounded: Unbounded queue grows infinitely if producers > consumers. Memory exhaustion. Bounded provides backpressure mechanism. (4) Graceful shutdown: Step 1: Cancel producers: for p in producers: p.cancel(); await gather(*producers, return_exceptions=True). Step 2: Wait for queue empty: await self.queue.join() waits until all items processed. Step 3: Cancel consumers: for c in consumers: c.cancel(). Result: All produced items processed. No data loss. Clean shutdown. (5) Monitor queue depth and throughput: Queue depth: self.queue.qsize() shows pending items. Throughput: consumed_delta / elapsed_time (items per second). Trends: If queue growing â†’ producers too fast, need more consumers. If queue empty â†’ consumers too fast, producers bottleneck. Queue operations explained: put(item): Add item to queue. If queue full (size == maxsize): blocks until space available. Non-blocking variant: put_nowait(item) raises QueueFull if full. get(): Remove and return item from queue. If queue empty: blocks until item available. Non-blocking variant: get_nowait() raises QueueEmpty if empty. task_done(): Mark item as processed. Called by consumer after processing. Decrements internal counter. join(): Wait until all items processed. Blocks until queue.empty() and all task_done() called. Use for graceful shutdown. Example: queue.put(item1); queue.put(item2); item = queue.get(); queue.task_done(); item = queue.get(); queue.task_done(); await queue.join(); # Now unblocks (all items processed). Backpressure handling: Problem: Producers faster than consumers â†’ queue grows â†’ memory issues. Solution: Bounded queue provides automatic backpressure. When queue full: await queue.put() blocks producer. Producer pauses until consumer frees space. Result: Production rate matches consumption rate automatically. Example: Producers: 30 items/sec. Consumers: 20 items/sec. Queue fills to 100 items. Producers block. Effective production drops to 20 items/sec. Queue stable at ~100 items. Alternative: Dynamic consumer scaling (add consumers if queue depth > threshold). Why this pattern essential: Decoupling: Producers and consumers independent. Rate matching: Automatic through backpressure. Memory safety: Bounded queue prevents OOM. Scalability: Add more producers/consumers easily. Reliability: Graceful shutdown prevents data loss. Testing: Test 1: Verify backpressure: queue = Queue(maxsize=5); slow_consumer: await asyncio.sleep(10) before get(); fast_producer: put 100 items; await asyncio.sleep(1); assert queue.qsize() == 5; # Queue full, producer blocked. Test 2: Verify graceful shutdown: Run pipeline 5s; Cancel producers; assert queue.qsize() > 0; # Items remaining; await queue.join(); assert queue.qsize() == 0; # All processed. Test 3: Verify throughput: Producers: 10 items/sec; Consumers: 20 items/sec (fast); await asyncio.sleep(10); assert consumed >= 95; # Allow some variance. Production considerations: Monitoring: Alert if queue consistently full (need more consumers). Alert if queue consistently empty (need more producers). Alert if throughput drops below threshold. Priority queue: Use asyncio.PriorityQueue for priority items. Multiple queues: Different queues for different item types. Persistent queue: Use Redis/RabbitMQ for durability across restarts. Critical lesson: asyncio.Queue provides backpressure automatically. Bounded queue essential for memory safety. join() ensures all items processed before shutdown.',
    keyPoints: [
      'Queue operations: put (blocks if full), get (blocks if empty), task_done (mark processed), join (wait for all)',
      'Backpressure: bounded queue (maxsize=100), put blocks when full, production rate auto-matches consumption',
      'Graceful shutdown: cancel producers, await queue.join() (wait for empty), cancel consumers, no data loss',
      'Monitoring: qsize() for depth, consumed/elapsed for throughput, alert if full (need consumers) or empty (need producers)',
      'Essential for: decoupling producers/consumers, rate matching, memory safety, scalable pipelines',
    ],
  },
  {
    id: 'abf-q-3',
    question:
      "Compare asyncio.gather() vs asyncio.wait() vs asyncio.as_completed() for fetching 100 URLs. For each: (1) API and return format, (2) Handling timeouts (per-URL vs overall), (3) Canceling remaining on first failure, (4) Processing results progressively, (5) Memory usage patterns. Provide code examples demonstrating when each is optimal. Why can't you use gather() for some scenarios that require wait()?",
    sampleAnswer:
      'gather() vs wait() vs as_completed() deep comparison: (1) API and return format: gather(*coros): results = await asyncio.gather(fetch(url1), fetch(url2), ...). Returns: List of results in order. Order preserved: results[0] is url1 result even if url2 finished first. Exceptions: By default raises first exception. return_exceptions=True collects exceptions as results. wait(tasks, timeout=None, return_when=ALL_COMPLETED): done, pending = await asyncio.wait({task1, task2, ...}). Returns: Tuple of (done: set[Task], pending: set[Task]). Sets are unordered. Must get results: [t.result() for t in done]. Exceptions: Never raises, exceptions stored in task.exception(). as_completed(coros, timeout=None): for coro in asyncio.as_completed([fetch(url1), fetch(url2), ...]): result = await coro. Returns: Iterator yielding coroutines in completion order (fastest first). Must await each coroutine to get result. Exceptions: Each await can raise, handle per-item. (2) Handling timeouts: gather() - No built-in per-URL timeout: Must wrap each coroutine: await gather(*[asyncio.wait_for(fetch(url), timeout=5) for url in urls]). Overall timeout: await asyncio.wait_for(gather(...), timeout=30). wait() - Per-URL and overall timeout supported: # Overall timeout; done, pending = await wait(tasks, timeout=30). # Per-URL: wrap tasks; tasks = [asyncio.wait_for(fetch(url), timeout=5) for url in urls]. as_completed() - Overall timeout only: for coro in asyncio.as_completed(coros, timeout=30): result = await coro. # Per-URL: wrap each: coros = [asyncio.wait_for(fetch(url), 5) for url in urls]. (3) Cancel remaining on first failure: gather() - Cannot cancel on first failure: Exception stops gather, but other tasks continue running! Must explicitly cancel: tasks = [create_task(fetch(url)) for url in urls]; try: await gather(*tasks); except Exception: for t in tasks: t.cancel(). wait() - Easy to cancel remaining: done, pending = await wait(tasks, return_when=FIRST_EXCEPTION); for t in pending: t.cancel(). Built-in support with return_when. as_completed() - Manual cancellation: remaining_tasks = [...]; for coro in as_completed(remaining_tasks): try: result = await coro; except Exception: # Cancel all remaining; for t in remaining_tasks: if not t.done(): t.cancel(); break. (4) Progressive result processing: gather() - All-or-nothing: await gather(...); # Blocks until ALL complete; for result in results: process(result). Cannot process as results arrive. Must wait for slowest. as_completed() - Best for progressive: for coro in asyncio.as_completed(coros): result = await coro; # Process immediately!; process(result); # Show to user right away. Results arrive in completion order. Ideal for progressive UI updates. wait() - Can process progressively with loop: pending = set(tasks); while pending: done, pending = await wait(pending, return_when=FIRST_COMPLETED); for task in done: result = task.result(); process(result). More verbose than as_completed(). (5) Memory usage patterns: gather() - All results in memory: results = await gather(...); # List of 100 results; Memory: 100 Ã— result_size simultaneously. Cannot discard early results. as_completed() - Process and discard: for coro in as_completed(coros): result = await coro; process(result); # Discard after processing; # Memory: O(1) for results (only current one). Can process huge number of results. wait() - All task objects in memory: done, pending = await wait(tasks); # All task objects exist; results = [t.result() for t in done]. Memory: All task objects + results. Code examples: Example 1: gather() - simple case: urls = ["url1", "url2", "url3"]; results = await asyncio.gather(*[fetch(url) for url in urls]); # Wait for all; for url, result in zip(urls, results): print(f"{url}: {len(result)} bytes"). Best for: Simple concurrent execution. Need all results in order. Small number of tasks (<1000). Example 2: wait() - cancel on first failure: tasks = {asyncio.create_task(fetch(url)) for url in urls}; done, pending = await asyncio.wait(tasks, return_when=asyncio.FIRST_EXCEPTION). # Check for exceptions; for task in done: try: result = task.result(); except Exception as e: print(f"Task failed: {e}"); # Cancel remaining; for task in pending: task.cancel(); break. Best for: Need to cancel on first failure. Want control over done/pending. Timeouts with partial results. Example 3: as_completed() - progressive UI: total = len(urls); for i, coro in enumerate(asyncio.as_completed([fetch(url) for url in urls]), 1): result = await coro; print(f"Progress: {i}/{total}"); render_result(result); # Show immediately. Best for: Progressive rendering (show results as they arrive). Streaming large number of results. Order doesn\'t matter (want completion order). Why can\'t use gather() for some scenarios: Scenario 1: Cancel on first failure: gather() doesn\'t cancel other tasks on exception. They continue running (wasted resources). wait() with FIRST_EXCEPTION cancels remaining. Scenario 2: Timeout with partial results: gather() with timeout raises TimeoutError, loses all partial results. wait() with timeout returns (done, pending), can get partial results. Scenario 3: Race condition (first to complete wins): gather() waits for all, can\'t return after first. wait() with FIRST_COMPLETED returns after first. Scenario 4: Progressive processing: gather() blocks until all complete. as_completed() yields results as they arrive. Performance comparison (100 URLs, 1s each): gather(): Wait 1s (all concurrent), get all 100 results at once. Memory: 100 results in list. as_completed(): Get first result at 1s, continue getting results. Memory: 1 result at a time. wait(): Similar to gather, but can process progressively with loop. Decision tree: Simple concurrent execution â†’ gather(). Need to cancel on first error â†’ wait(return_when=FIRST_EXCEPTION). Progressive rendering â†’ as_completed(). Timeout with partial results â†’ wait(timeout=...). Race condition (first wins) â†’ wait(return_when=FIRST_COMPLETED). Critical lesson: gather() is simplest but least flexible. wait() provides control over done/pending. as_completed() best for progressive processing. Choose based on requirements, not just convenience.',
    keyPoints: [
      'gather(): returns list in order, raises on exception, all-or-nothing, simplest but inflexible',
      'wait(): returns (done, pending) sets, never raises, can cancel remaining, flexible with return_when',
      'as_completed(): iterator in completion order, progressive processing, memory-efficient (O(1) results)',
      'Timeouts: gather needs wrapping, wait supports timeout natively, as_completed has timeout parameter',
      'Use gather for simple cases, wait for control (cancel/partial results), as_completed for progressive/streaming',
    ],
  },
];
