export const tasksAndFuturesQuiz = [
  {
    id: 'taf-q-1',
    question:
      "You have a system that fetches data from 50 microservices to build a dashboard. Some services are fast (100ms), others slow (2s). Requirements: (1) Show data as it arrives (don't wait for all), (2) Timeout slow services after 1s (use cached data instead), (3) If >10 services fail, abort the entire dashboard request, (4) Track which services are complete for progress bar. Design this using tasks, asyncio.as_completed(), and proper error handling. Provide code structure and explain task lifecycle management.",
    sampleAnswer:
      'Dashboard with progressive loading and timeouts: Architecture: Create task per service. Process results as they complete (asyncio.as_completed). Apply per-service timeout. Track failures, abort if >10 fail. Return partial results for progressive rendering. Code structure: import asyncio, aiohttp; from typing import Optional. async def fetch_service (session, service_name: str) -> dict: async with session.get (f"http://{service_name}/data") as response: return await response.json(). async def fetch_with_timeout (session, service_name: str, cache, timeout=1.0) -> tuple[str, Optional[dict], bool]: try: data = await asyncio.wait_for (fetch_service (session, service_name), timeout=timeout); return (service_name, data, False); except asyncio.TimeoutError: print(f"{service_name} timed out, using cache"); return (service_name, cache.get (service_name), False); except Exception as e: print(f"{service_name} failed: {e}"); return (service_name, None, True). async def build_dashboard (services: list[str], cache: dict): results = {}; failures = 0; completed = 0; MAX_FAILURES = 10. async with aiohttp.ClientSession() as session: tasks = [fetch_with_timeout (session, svc, cache) for svc in services]. for coro in asyncio.as_completed (tasks): service_name, data, is_failure = await coro; completed += 1. if is_failure: failures += 1; if failures > MAX_FAILURES: print(f"Aborting: {failures} failures"); # Cancel remaining tasks; remaining = [t for t in asyncio.all_tasks() if not t.done()]; for task in remaining: task.cancel(); break. if data: results[service_name] = data; yield {"service": service_name, "data": data, "progress": completed / len (services)}. # If exited early due to failures; if failures > MAX_FAILURES: raise Exception (f"Dashboard failed: {failures} services unavailable"). return results. # Usage: async for update in build_dashboard (services, cache): print(f"Progress: {update["progress"]:.0%}"); render_partial_dashboard (update). Key design decisions: (1) Progressive loading: Use asyncio.as_completed() to process results in completion order, not submission order. Fast services (100ms) rendered first, slow services (2s) rendered later. Improves perceived performance (users see data immediately). Implementation: for coro in asyncio.as_completed (tasks): # Yields tasks as they complete, not in original order; update = await coro; yield update. (2) Per-service timeout: Use asyncio.wait_for (fetch_service(), timeout=1.0) per service. Services >1s timeout individually without affecting others. Fallback to cached data on timeout: except asyncio.TimeoutError: return (service, cache.get (service), False). Why not overall timeout: Overall timeout would abort fast services that could succeed. Per-service timeout allows fast services to complete while slow ones timeout. (3) Failure tracking and abort: Count failures: if is_failure: failures += 1. Abort if >10 fail: if failures > MAX_FAILURES: cancel remaining tasks. Cancel remaining tasks: remaining = [t for t in asyncio.all_tasks() if not t.done()]; for task in remaining: task.cancel(). Why cancel: No point fetching more data if dashboard unusable. Saves resources (network, API quotas). (4) Progress tracking: Track completed: completed += 1; progress = completed / len (services). Yield progress with each update: yield {"progress": progress}. Frontend uses for progress bar: <ProgressBar value={progress} />. Task lifecycle management: Creation: tasks = [fetch_with_timeout(...) for svc in services]; all 50 tasks created immediately. Execution: Tasks run concurrently on event loop. Fast tasks complete in 100ms, slow in 2s (or timeout at 1s). Completion tracking: asyncio.as_completed (tasks) yields tasks as they finish. Cancellation: On abort (>10 failures), cancel remaining: task.cancel(). Awaiting cancellation: await asyncio.gather(*remaining, return_exceptions=True) waits for cancellation to complete. Resource cleanup: async with ClientSession ensures connection pool closed. Alternative approaches: Approach A: gather() with timeout: results = await asyncio.gather(*tasks, return_exceptions=True). Problem: Can\'t show progressive results. All-or-nothing at timeout. Approach B: wait() with timeout: done, pending = await asyncio.wait (tasks, timeout=5.0). Problem: Single overall timeout, not per-service. Fast services penalized by slow ones. Approach C: as_completed() with individual timeouts (chosen): Progressive results. Per-service timeouts. Flexible failure handling. Best for user experience. Performance comparison: Sequential (no async): 50 services × 2s = 100 seconds. Gather (all-or-nothing): Wait 2s for slowest service before showing anything. as_completed + timeouts: Show first results at 100ms (fast services). Continue showing results up to 1s (timeout). Total: 1 second to show all available data. 100× faster than sequential, better UX than gather(). Error handling patterns: Pattern 1: Collect errors, don\'t stop: results = await asyncio.gather(*tasks, return_exceptions=True); failures = [r for r in results if isinstance (r, Exception)]. Pattern 2: Stop on first error: results = await asyncio.gather(*tasks); # Raises on first exception. Pattern 3: Stop after N errors (used here): Track error count, cancel remaining if >threshold. Allows some failures (expected with microservices), aborts if too many. Production considerations: Retry transient failures: for attempt in range(3): try: return await fetch(...); except aiohttp.ClientError: if attempt == 2: raise; await asyncio.sleep(0.5). Circuit breaker: Track service health, skip known-failing services. Monitoring: Log which services slow/failing for alerting. Caching strategy: TTL-based cache, refresh in background. Load balancing: Spread load across multiple instances per service. Testing: Mock slow services: await asyncio.sleep(2) to test timeout logic. Mock failures: raise Exception() to test error handling. Verify cancellation: Ensure tasks actually stop (not just ignored). Real-world example: Netflix dashboard: 50+ microservices (recommendations, continue watching, new releases, etc.). Can\'t wait for slowest service. Shows sections as they load. Degrades gracefully if services down. This pattern essential for high-performance UX with distributed systems.',
    keyPoints: [
      'Progressive loading: asyncio.as_completed() renders fast services (100ms) first, slow later',
      'Per-service timeout: wait_for (fetch(), 1.0) per service with cache fallback, not overall timeout',
      'Failure tracking: Count failures, abort if >10, cancel remaining tasks to save resources',
      'Task lifecycle: Create all immediately, process as completed, cancel on abort, cleanup with context manager',
      'Performance: 100s sequential → 1s async with timeouts, better UX than gather() (progressive vs all-or-nothing)',
    ],
  },
  {
    id: 'taf-q-2',
    question:
      "Compare asyncio.gather(), asyncio.wait(), and asyncio.as_completed() for collecting results from 100 tasks. For each: (1) API and return format, (2) error handling behavior (stop on first error vs collect all), (3) result ordering, (4) memory usage during execution, (5) use case where it's optimal. Provide code examples showing the differences and explain when to use each.",
    sampleAnswer:
      'Comparison of async collection methods: (1) asyncio.gather(*coros, return_exceptions=False): API: results = await asyncio.gather (coro1(), coro2(), ...). Takes variable args (*coros) or unpacked list (*tasks). Returns: List of results in same order as input coroutines. Error handling: Default (return_exceptions=False): Stops on first exception, raises it. return_exceptions=True: Collects all exceptions as results. Result ordering: Preserves submission order, not completion order. Even if task 3 completes before task 1, results[0] is still task 1\'s result. Memory: All tasks run concurrently. Memory = sum (all task memory) = high for many tasks. Use case: Need all results in specific order. Want to stop on first error (default). Simple syntax for small number of tasks. Example: async def task (n): await asyncio.sleep (n * 0.1); return n **2. # Stop on first error; try: results = await asyncio.gather (task(1), task(2), task(3)); print(results); # [1, 4, 9] in order; except Exception as e: print(f"Failed: {e}"). # Collect all errors; results = await asyncio.gather (task(1), task(2), task(3), return_exceptions=True); for i, r in enumerate (results): if isinstance (r, Exception): print(f"Task {i} failed: {r}"); else: print(f"Task {i}: {r}"). (2) asyncio.wait (tasks, timeout=None, return_when=ALL_COMPLETED): API: done, pending = await asyncio.wait (task_set). Takes set/list of Task objects (not coroutines directly). Returns: Tuple of (done: set[Task], pending: set[Task]). Error handling: Never raises. Exceptions stored in task objects. Check with task.exception(). Errors don\'t stop other tasks. Result ordering: No ordering. Returns sets (unordered). Must manually get results: [t.result() for t in done]. Memory: Same as gather (all concurrent). Control: return_when parameter: ALL_COMPLETED (default): Wait for all. FIRST_COMPLETED: Return when first completes. FIRST_EXCEPTION: Return when first fails. Use case: Need fine control over completion strategy. Want to handle first completion (race conditions). Need to distinguish done vs pending (for cancellation). Example: tasks = {asyncio.create_task (task (i)) for i in range(3)}. # Wait for all; done, pending = await asyncio.wait (tasks); results = [t.result() for t in done]; print(f"Results: {sorted (results)}"); # [1, 4, 9] unordered. # Wait for first; done, pending = await asyncio.wait (tasks, return_when=asyncio.FIRST_COMPLETED); first_result = list (done)[0].result(); print(f"First: {first_result}"); # Cancel remaining; for t in pending: t.cancel(). # Timeout; done, pending = await asyncio.wait (tasks, timeout=1.0); print(f"Completed: {len (done)}, Pending: {len (pending)}"); for t in pending: t.cancel(). (3) asyncio.as_completed (tasks, timeout=None): API: for coro in asyncio.as_completed (tasks): result = await coro. Takes iterable of coroutines or tasks. Returns: Iterator of coroutines (must await each). Error handling: Each coroutine can raise independently. Use try/except around await coro. Errors don\'t stop iteration. Result ordering: Completion order (fastest first). Results arrive as tasks finish, not submission order. Memory: All tasks start immediately (high initial memory). But can process results incrementally (low result memory). Use case: Process results as soon as available (progressive rendering). Order doesn\'t matter or prefer completion order. Large number of tasks with varying completion times. Example: tasks = [task (i) for i in range(3, 0, -1)]; # task(3), task(2), task(1). # Process in completion order; for coro in asyncio.as_completed (tasks): result = await coro; print(f"Got: {result}"); # Output: 1, 4, 9 (task(1) completes first). # With error handling; for coro in asyncio.as_completed (tasks): try: result = await coro; print(f"Success: {result}"); except Exception as e: print(f"Failed: {e}"). # With timeout; for coro in asyncio.as_completed (tasks, timeout=1.0): try: result = await coro; print(f"Got: {result}"); except asyncio.TimeoutError: print("Timeout!"); break. Comparison table: | Feature | gather() | wait() | as_completed() | |---------|----------|--------|----------------| | Return format | List | (done, pending) sets | Iterator | | Result order | Submission | Unordered | Completion | | Error stops iteration? | Yes (default) | No | No | | Syntax complexity | ★★★★★ Simple | ★★★ Moderate | ★★★ Moderate | | Order preservation | Yes | No | No (completion) | | Progressive results | No | No | Yes | | Cancel pending | Manual | Easy (have pending set) | Manual | | Use for race | No | Yes | Yes | Code examples comparing all three: import asyncio, time. async def slow_task (n): await asyncio.sleep (n); return n. async def compare_methods(): # gather: Wait for all, preserve order; start = time.time(); results = await asyncio.gather (slow_task(2), slow_task(1), slow_task(3)); print(f"gather: {results} in {time.time()-start:.1f}s"); # [2, 1, 3] in 3s. # wait: Wait for all, no order; start = time.time(); tasks = {asyncio.create_task (slow_task (i)) for i in [2, 1, 3]}; done, pending = await asyncio.wait (tasks); results = [t.result() for t in done]; print(f"wait: {sorted (results)} in {time.time()-start:.1f}s"); # [1, 2, 3] in 3s. # as_completed: Process as they finish; start = time.time(); for coro in asyncio.as_completed([slow_task(2), slow_task(1), slow_task(3)]): result = await coro; print(f"as_completed: {result} at {time.time()-start:.1f}s"); # 1 at 1s, 2 at 2s, 3 at 3s. When to use each: gather(): Default for most cases. Need results in specific order. Want simple syntax. Want to stop on first error (default). Example: profile, orders, reviews = await asyncio.gather (get_profile(), get_orders(), get_reviews()). wait(): Need to distinguish done vs pending. Implementing timeouts with partial results. Race conditions (FIRST_COMPLETED). Cancel pending tasks easily. Example: done, pending = await asyncio.wait (tasks, timeout=5.0); results = [t.result() for t in done]; for t in pending: t.cancel(). as_completed(): Progressive result rendering. Process results as soon as available. Order doesn\'t matter. Prefer completion order (fastest first). Example: for coro in asyncio.as_completed (fetch_pages (urls)): page = await coro; render (page); # Show page immediately. Performance characteristics: Setup time: All three: O(n) to create tasks. Completion time: All three: max (task_times) (all run concurrently). Memory during execution: gather: n tasks in memory. wait: n tasks in memory. as_completed: n tasks in memory (same). Memory for results: gather: All results in list (high if large results). wait: All results in task objects (high). as_completed: Can process and discard incrementally (low if processed incrementally). Example with memory optimization: # Bad: Loads all 1000 pages in memory; results = await asyncio.gather(*[fetch_page (url) for url in urls]); # 1000 × 1MB = 1GB memory. # Good: Process incrementally; for coro in asyncio.as_completed([fetch_page (url) for url in urls]): page = await coro; process (page); # Process and discard immediately; # Memory: max (concurrent_pages) × 1MB = ~100MB. Production recommendations: Default: Use gather() for simplicity. Timeouts: Use wait() with timeout parameter for partial results. Progressive UI: Use as_completed() to render results as they arrive. Error handling: Use gather(..., return_exceptions=True) to collect all errors. Race conditions: Use wait(..., return_when=FIRST_COMPLETED) to get winner. All three have place in production code—choose based on requirements, not performance (all similar speed).',
    keyPoints: [
      'gather(): Returns list in submission order, stops on first error (default), simple syntax, use for ordered results',
      'wait(): Returns (done, pending) sets unordered, never raises, best for timeouts/partial results/race conditions',
      'as_completed(): Iterator in completion order, progressive processing, best for rendering results as available',
      'Performance: All have same speed (concurrent), differ in result handling and memory for results',
      'Decision: gather() for simplicity, wait() for control, as_completed() for progressive/streaming use cases',
    ],
  },
  {
    id: 'taf-q-3',
    question:
      'Design a task cancellation system for a long-running data processing pipeline: (1) User can cancel at any time, (2) In-progress work must finish gracefully (commit current batch to database), (3) Pending work should be skipped, (4) Cleanup (close connections, temp files) must always happen. Explain: task cancellation mechanics, CancelledError handling, cleanup patterns, preventing resource leaks, testing cancellation. Provide code showing proper cancellation architecture.',
    sampleAnswer:
      'Graceful cancellation system for data pipeline: Architecture: Main task orchestrates worker tasks. User triggers cancellation. Workers check cancellation frequently. In-progress work completes (commit batch). Pending work cancelled (skip). Cleanup guaranteed (finally/context managers). Code structure: import asyncio, contextlib. class DataPipeline: def __init__(self, db): self.db = db; self.current_batch = []; self.cancelled = False; self.cleanup_tasks = []. async def process_item (self, item): # Process individual item; await asyncio.sleep(0.1); # Simulate work; return item * 2. async def commit_batch (self): if self.current_batch: await self.db.execute("INSERT ...", self.current_batch); print(f"Committed batch of {len (self.current_batch)} items"); self.current_batch = []. async def worker (self, items): try: for item in items: # Check cancellation before each item; if self.cancelled: print("Worker: Cancellation requested, finishing current batch"); break. result = await self.process_item (item); self.current_batch.append (result). # Commit batch when it reaches size limit; if len (self.current_batch) >= 100: await self.commit_batch(). # Commit final batch (even if incomplete); await self.commit_batch(); except asyncio.CancelledError: # Cancellation: Commit current work, then cleanup; print("Worker: CancelledError caught, committing current batch"); await self.commit_batch(); raise; # Must re-raise!; except Exception as e: print(f"Worker error: {e}"); await self.commit_batch(); # Commit what we have; raise; finally: print("Worker: Cleanup in finally"). async def run (self, all_items): try: # Split work into chunks for multiple workers; chunk_size = len (all_items) // 4; chunks = [all_items[i:i+chunk_size] for i in range(0, len (all_items), chunk_size)]. # Create worker tasks; tasks = [asyncio.create_task (self.worker (chunk), name=f"worker-{i}") for i, chunk in enumerate (chunks)]. # Wait for completion or cancellation; await asyncio.gather(*tasks); except asyncio.CancelledError: print("Pipeline: CancelledError in main task"); # Signal workers to stop; self.cancelled = True; # Wait for workers to finish gracefully; await asyncio.gather(*tasks, return_exceptions=True); raise; finally: # Guaranteed cleanup; print("Pipeline: Cleanup in finally"); await self.cleanup(). async def cleanup (self): print("Cleaning up resources..."); await self.db.close(); # Close any temp files; # Release any locks; print("Cleanup complete"). # Usage: async def main(): db = DatabaseConnection(); pipeline = DataPipeline (db); items = list (range(1000)). # Create pipeline task; pipeline_task = asyncio.create_task (pipeline.run (items)). # Simulate user cancellation after 2 seconds; await asyncio.sleep(2); print("USER: Cancelling pipeline..."); pipeline_task.cancel(). try: await pipeline_task; except asyncio.CancelledError: print("Pipeline cancelled successfully"). Key design principles: (1) User can cancel anytime: Expose cancel() method or use task.cancel() externally. Cancellation is cooperative—task must check for it. Check methods: Poll self.cancelled flag: if self.cancelled: break. Catch CancelledError exception. Use asyncio.current_task().cancelled() (Python 3.11+). Best: Catch CancelledError (most robust). (2) Finish in-progress work gracefully: Don\'t immediately stop mid-operation. Complete current unit of work (batch). Commit to database before exiting. Pattern: except asyncio.CancelledError: await commit_batch(); # Finish current work; raise; # Then propagate cancellation. Why: Prevent data loss (partial batch lost). Maintain consistency (transaction completed). Allow recovery (know what was processed). (3) Skip pending work: Check cancellation before starting new work: if self.cancelled: break. Don\'t start new batches after cancellation requested. Cancel pending tasks: for task in pending_tasks: task.cancel(). (4) Guaranteed cleanup: Use finally blocks for cleanup that MUST run. Use async context managers for resources. Always close database connections, files, locks. Pattern: try: await do_work(); except asyncio.CancelledError: await finish_current_work(); raise; finally: await cleanup(); # ALWAYS runs. Cancellation mechanics: Task cancellation flow: 1. User calls task.cancel(). 2. Next await point in task raises CancelledError. 3. Task catches CancelledError, does cleanup. 4. Task re-raises CancelledError. 5. Caller sees CancelledError or checks task.cancelled(). Example: task = asyncio.create_task (long_operation()); await asyncio.sleep(1); task.cancel(); # Triggers cancellation; try: await task; except asyncio.CancelledError: print("Task was cancelled"). CancelledError propagation: Must re-raise CancelledError after catching! except asyncio.CancelledError: cleanup(); raise; # Critical!. Why: Allows higher-level cancellation to work. Marks task as cancelled (task.cancelled() returns True). If not re-raised, task appears to complete normally (wrong!). Cleanup patterns: Pattern 1: try/finally: try: await work(); finally: await cleanup(); # Always runs. Pattern 2: Context managers: async with resource as r: await work (r); # Cleanup automatic in __aexit__. Pattern 3: Explicit cleanup tracking: self.cleanup_tasks.append (cleanup_func); finally: for cleanup in self.cleanup_tasks: await cleanup(). Best: Use async context managers (most reliable). Preventing resource leaks: Problem: Cancelled tasks may not close resources. Solution: async with AsyncResource() as res: await work (res); # __aexit__ called even if cancelled. Example: async with aiohttp.ClientSession() as session: await fetch (session); # Session closed even if cancelled. Database connections: async with database.transaction() as tx: await tx.execute(...); # Auto-rollback if cancelled. Temp files: async with aiofiles.tempfile.NamedTemporaryFile() as f: await f.write (data); # Auto-deleted if cancelled. Testing cancellation: Test 1: Verify cleanup happens: mock_cleanup = Mock(); try: task = asyncio.create_task (pipeline()); await asyncio.sleep(0.1); task.cancel(); await task; except CancelledError: pass; mock_cleanup.assert_called_once(); # Verify called. Test 2: Verify in-progress work commits: pipeline = Pipeline(); task = asyncio.create_task (pipeline.run (items)); await asyncio.sleep(0.5); # Let some work happen; task.cancel(); try: await task; except CancelledError: pass; assert pipeline.committed_count > 0; # Some work committed. Test 3: Verify pending work skipped: assert pipeline.processed_count < len (items); # Not all processed. Test 4: Verify no resource leaks: Check open connections: assert db.connection_count == 0. Check open files: assert len (open_files) == 0. Use tracemalloc to detect leaks. Advanced patterns: Pattern: Cancellation with timeout: try: await asyncio.wait_for (pipeline.run(), timeout=60.0); except asyncio.TimeoutError: print("Timeout, cancelling..."); # Task already cancelled by wait_for. Pattern: Graceful shutdown on signal: import signal; def shutdown_handler (sig, frame): for task in asyncio.all_tasks(): task.cancel(); signal.signal (signal.SIGTERM, shutdown_handler); signal.signal (signal.SIGINT, shutdown_handler). Pattern: Partial cancellation (cancel some workers, not all): for worker in slow_workers: worker.cancel(); for worker in remaining_workers: await worker; # Keep running. Production considerations: Idempotency: Operations should be safe to retry (in case cancellation interrupts). Transactions: Use database transactions to ensure atomicity. Monitoring: Log cancellations: when, why, what was in progress. Graceful period: Allow N seconds for cleanup before force-kill. Timeouts: Set maximum graceful shutdown time. Backpressure: If cancellation slow (large batch), reduce batch size. Critical lesson: Cancellation is cooperative. Tasks must handle CancelledError properly. Cleanup must be guaranteed (finally/context managers). In-progress work should complete gracefully. Re-raise CancelledError always.',
    keyPoints: [
      'Cancellation mechanics: task.cancel() → CancelledError at next await → catch, cleanup, re-raise',
      'Finish in-progress: catch CancelledError, commit current batch, then raise (prevents data loss)',
      'Skip pending: check self.cancelled before new work, cancel pending tasks in gather()',
      'Guaranteed cleanup: use finally blocks and async context managers, always close resources',
      'Testing: verify cleanup called, in-progress commits, pending skipped, no resource leaks with tracemalloc',
    ],
  },
];
