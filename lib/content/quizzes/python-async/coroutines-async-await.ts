export const coroutinesAsyncAwaitQuiz = [
  {
    id: 'caa-q-1',
    question:
      'A junior developer writes this async code that isn\'t achieving the expected speedup. Identify the problems and fix them: ```python\nasync def fetch_data (url):\n    return requests.get (url).json()\n\nasync def process_batch (urls):\n    results = [await fetch_data (url) for url in urls]\n    return results\n``` The code takes 10 seconds for 10 URLs (1 second each). Explain: (1) Why it\'s slow despite being "async", (2) Multiple issues preventing true concurrency, (3) Corrected version with proper async I/O, (4) Additional optimizations (connection pooling, error handling), (5) Performance comparison before/after.',
    sampleAnswer:
      'Problems in async code: (1) Why slow: Despite using async/await, code runs sequentially (10 URLs × 1s = 10s). Two critical mistakes prevent concurrency: Using blocking library (requests), Sequential await in comprehension. (2) Multiple issues: Issue A: requests.get() is synchronous/blocking. Blocks entire event loop while waiting for HTTP response. Event loop can\'t switch to other tasks. Fix: Use async HTTP library like aiohttp. Issue B: [await fetch_data (url) for url in urls] is sequential comprehension. Awaits first URL completion before starting second. Fix: Create all tasks first, then await them together with asyncio.gather(). Issue C: No connection reuse. Creating new connection for each request (TCP handshake overhead). Fix: Use aiohttp.ClientSession() for connection pooling. Issue D: No error handling. One failed request breaks entire batch. Fix: Use try/except or return_exceptions=True. (3) Corrected version: import aiohttp, asyncio; async def fetch_data (session, url): try: async with session.get (url, timeout=10) as response: return await response.json(); except Exception as e: return {"error": str (e), "url": url}. async def process_batch (urls): async with aiohttp.ClientSession() as session: tasks = [fetch_data (session, url) for url in urls]; results = await asyncio.gather(*tasks, return_exceptions=True); return results. Key changes: aiohttp instead of requests (true async I/O). ClientSession for connection pooling (reuse connections). Create all tasks before awaiting (concurrent execution). gather() for parallel waiting. Error handling per request (failures don\'t stop batch). (4) Additional optimizations: Connection pooling configuration: connector = aiohttp.TCPConnector (limit=100, limit_per_host=10); session = aiohttp.ClientSession (connector=connector). Limits: 100 total connections, max 10 per host (respect server limits). Timeout configuration: timeout = aiohttp.ClientTimeout (total=30, connect=10, sock_read=10); session = aiohttp.ClientSession (timeout=timeout). Prevents hanging on slow servers. Retry logic for transient failures: async def fetch_with_retry (session, url, retries=3): for attempt in range (retries): try: return await fetch_data (session, url); except aiohttp.ClientError as e: if attempt == retries - 1: return {"error": str (e)}; await asyncio.sleep(2 ** attempt). Exponential backoff: wait 1s, 2s, 4s. Batch size control for very large lists: async def process_large_batch (urls, batch_size=100): results = []; for i in range(0, len (urls), batch_size): batch = urls[i:i+batch_size]; batch_results = await process_batch (batch); results.extend (batch_results). Prevents overwhelming the system with thousands of concurrent connections. (5) Performance comparison: Original (broken async): Time: 10 seconds (sequential). Memory: Low (one request at a time). CPU: Idle most of time (blocked on I/O). Throughput: 1 request/second. Fixed async: Time: ~1.5 seconds (concurrent, limited by slowest request + overhead). Memory: Higher (~1MB per concurrent connection × 10 = ~10MB). CPU: Still mostly idle (I/O-bound), but event loop active. Throughput: ~6.7 requests/second. Speedup: 10s → 1.5s = 6.7× faster. With connection pooling: Time: ~1.2 seconds (reuse connections, skip TCP handshake). Reduced latency per request (~200ms saved). With batching (1000 URLs, batch_size=100): Time: ~15 seconds (10 batches × 1.5s). vs 1000 seconds sequentially = 66× faster. vs crashing with 1000 concurrent connections. Critical lessons: async/await syntax alone doesn\'t make code fast. Must use async-compatible libraries (aiohttp, asyncpg, aiofiles). Sequential await in loops defeats concurrency. Use asyncio.gather() for parallel execution. Connection pooling reduces latency significantly. Error handling prevents cascading failures. Batch processing prevents resource exhaustion.',
    keyPoints: [
      'Problem: requests.get() is blocking (not async), [await ...] comprehension is sequential',
      'Fix: Use aiohttp for async I/O, create all tasks before awaiting, use asyncio.gather()',
      'Optimization: ClientSession for connection pooling, timeouts, retry logic, batch processing',
      'Performance: 10s → 1.5s with fixes (6.7× faster), ~1.2s with connection pooling',
      'Key lesson: async/await syntax alone insufficient—need async libraries and concurrent task creation',
    ],
  },
  {
    id: 'caa-q-2',
    question:
      'Design a real-time WebSocket server using coroutines that: (1) Accepts 10,000 concurrent WebSocket connections, (2) Receives messages from clients asynchronously, (3) Broadcasts messages to all connected clients (10,000 simultaneous sends), (4) Handles client disconnections gracefully without blocking other connections, (5) Implements connection limits and back-pressure. Provide architecture, key coroutines, error handling, and memory considerations. Why is async essential for this use case?',
    sampleAnswer:
      'WebSocket server with coroutines: (1) Architecture: Use FastAPI with WebSocket support (built on Starlette/asyncio). ConnectionManager maintains list of active WebSocket connections. Each connection handled by dedicated coroutine (lightweight). Broadcasting uses asyncio.gather() for concurrent sends. (2) Core implementation: from fastapi import FastAPI, WebSocket, WebSocketDisconnect; import asyncio; app = FastAPI(); class ConnectionManager: def __init__(self): self.active_connections: list[WebSocket] = []; self.lock = asyncio.Lock(). async def connect (self, websocket: WebSocket): await websocket.accept(); async with self.lock: self.active_connections.append (websocket); print(f"Connected: {len (self.active_connections)} clients"). async def disconnect (self, websocket: WebSocket): async with self.lock: self.active_connections.remove (websocket); print(f"Disconnected: {len (self.active_connections)} clients"). async def broadcast (self, message: dict): # Send to all clients concurrently; disconnected = []; async def send_to_client (connection): try: await connection.send_json (message); except Exception: disconnected.append (connection). # Send to all clients in parallel; await asyncio.gather(*[send_to_client (c) for c in self.active_connections], return_exceptions=True); # Clean up disconnected; for conn in disconnected: await self.disconnect (conn). manager = ConnectionManager(); @app.websocket("/ws/{client_id}"); async def websocket_endpoint (websocket: WebSocket, client_id: str): await manager.connect (websocket); try: while True: data = await websocket.receive_json(); await manager.broadcast({"client": client_id, "message": data}); except WebSocketDisconnect: await manager.disconnect (websocket). (3) Receiving messages asynchronously: Each connection has independent coroutine: while True: data = await websocket.receive_json(); await process_message (data). await websocket.receive_json() is non-blocking. While this connection waits for message, event loop handles other connections. Multiplexing: One event loop thread handles all 10,000 connections efficiently. (4) Broadcasting to 10,000 clients: Challenge: Serial sending would take: 10,000 × 1ms = 10 seconds. Solution: Concurrent sends with asyncio.gather(). async def broadcast (message): tasks = [conn.send_json (message) for conn in self.active_connections]; results = await asyncio.gather(*tasks, return_exceptions=True); # Handle failures; for i, result in enumerate (results): if isinstance (result, Exception): await self.disconnect (self.active_connections[i]). Result: All 10,000 sends start immediately. Total time: ~max (individual_send_times) ≈ 10-50ms (network-limited). Failure handling: return_exceptions=True prevents one failure from breaking broadcast. Disconnected clients removed asynchronously. (5) Connection limits and back-pressure: Connection limit (prevent DoS): MAX_CONNECTIONS = 10000; active_count = len (manager.active_connections); if active_count >= MAX_CONNECTIONS: await websocket.close (code=1008, reason="Server at capacity"). Back-pressure (slow clients): If client can\'t keep up with broadcast rate, buffer fills up. Solution: Set send timeout and disconnect slow clients. async def send_with_timeout (conn, message, timeout=5.0): try: await asyncio.wait_for (conn.send_json (message), timeout=timeout); except asyncio.TimeoutError: # Client too slow, disconnect; await manager.disconnect (conn). Memory management: Per-connection memory: ~8KB (WebSocket overhead) + ~4KB (Python object) = ~12KB. 10,000 connections: 10,000 × 12KB = ~120MB. Reasonable for modern servers. Message buffer limits: If broadcast rate > client consumption rate, buffer grows unboundedly. Solution: Drop messages for slow clients or implement queue size limits. class ConnectionManager: def __init__(self, max_queue_size=100): self.client_queues = {}; self.max_queue_size = max_queue_size. async def broadcast (self, message): for client_id, queue in self.client_queues.items(): if queue.qsize() < self.max_queue_size: await queue.put (message); else: # Queue full, drop message or disconnect; print(f"Client {client_id} queue full, dropping message"). Error handling: Connection errors: try: await websocket.send_json (message); except WebSocketDisconnect: await manager.disconnect (websocket); except Exception as e: print(f"Error sending to client: {e}"); await manager.disconnect (websocket). Broadcast errors: Use return_exceptions=True to continue broadcasting even if some fail. Graceful shutdown: async def shutdown(): # Close all connections gracefully; close_tasks = [conn.close() for conn in manager.active_connections]; await asyncio.gather(*close_tasks, return_exceptions=True). Why async essential: Scale: 10,000 concurrent connections on single thread. Threading would need 10,000 threads: 10,000 × 8MB/thread = 80GB memory! Async: 10,000 coroutines: 10,000 × 12KB = 120MB. 666× less memory. Performance: Broadcasting to 10,000 clients. Sequential: 10,000 × 1ms = 10s per broadcast. Async concurrent: max (send_times) = 10-50ms. 100-1000× faster broadcasts. Simplicity: Single-threaded = no race conditions, locks, or thread-safety concerns. Code is simpler and more maintainable. Real-world example: Chat server with 10,000 users: Without async: Need 10,000 threads, ~80GB memory, slow broadcasts. With async: Single thread, ~120MB memory, instant broadcasts. Production deployment: Use uvicorn with async workers: uvicorn main:app --workers 4. 4 processes × 10,000 connections = 40,000 capacity. Monitoring: Track connection count, broadcast latency, message queue sizes. Load balancing: Use Redis pub/sub to broadcast across multiple server instances. Testing: Use websocket-bench: websocket-bench -c 10000 -o ws://localhost:8000/ws. Critical: Async is THE solution for high-concurrency I/O. 10,000 connections impossible with threading.',
    keyPoints: [
      'Architecture: ConnectionManager tracks connections, each connection handled by coroutine, broadcast uses gather()',
      'Broadcasting: asyncio.gather() sends to all 10,000 clients concurrently (10-50ms vs 10s sequential)',
      'Connection limits: MAX_CONNECTIONS cap, slow client timeouts, message queue size limits',
      'Memory: ~12KB per connection = 120MB for 10K (vs 80GB with threading)',
      'Why async essential: 10K connections on single thread, 100-1000× faster broadcasts, 666× less memory',
    ],
  },
  {
    id: 'caa-q-3',
    question:
      'Compare these three approaches for making 100 API calls: (1) Sequential await in loop, (2) asyncio.gather() with all 100 tasks, (3) Semaphore to limit to 10 concurrent. For each: estimate time (each call takes 1s), memory usage, error handling behavior, server impact. When would you use each? Provide code examples and explain the trade-offs.',
    sampleAnswer:
      'Three approaches for 100 API calls: Setup: Each API call takes 1 second. Total sequential time would be 100 seconds. (1) Sequential await in loop: Code: async def sequential (urls): results = []; for url in urls: result = await fetch (url); results.append (result); return results. Execution: Wait for call 1 (1s) → call 2 (1s) → ... → call 100 (1s). Time: 100 × 1s = 100 seconds. Memory: Very low. Only 1 request active at a time (~1MB). CPU: Event loop mostly idle (waiting for I/O). Error handling: First error stops processing. Remaining URLs never attempted. Use try/except per iteration to continue on errors. Server impact: Minimal. Only 1 concurrent request. Polite to server. When to use: Server has rate limits (must stay under 1 req/sec). Debugging (easier to trace sequential execution). Very limited client resources. Need to process results in order before next request. Pros: Simple, predictable order, minimal memory. Cons: Extremely slow (100× slower than concurrent). (2) asyncio.gather() - All 100 concurrent: Code: async def all_concurrent (urls): tasks = [fetch (url) for url in urls]; results = await asyncio.gather(*tasks); return results. Execution: Start all 100 calls immediately. Wait for all to complete. Time: max (call_times) = ~1 second! (All happen together). Memory: High. 100 concurrent requests (~100MB). CPU: Event loop active managing 100 operations. Error handling: By default, first exception stops all and raises. Use return_exceptions=True to collect all results/errors. results = await asyncio.gather(*tasks, return_exceptions=True); for i, result in enumerate (results): if isinstance (result, Exception): print(f"Task {i} failed: {result}"). Server impact: HIGH. 100 simultaneous connections. Could overwhelm server or trigger rate limiting. May hit connection limits (some servers limit to 10-50 concurrent from same IP). When to use: Server can handle high concurrency. You control the server (no rate limits). Small number of total requests (<100). Fast completion critical (real-time dashboard). Have confidence in server stability. Pros: Fastest possible (1s vs 100s). Simple code. Cons: High memory usage. May overwhelm server. Can trigger rate limits. Risk of connection limit errors. (3) Semaphore - Limit to 10 concurrent: Code: async def controlled_concurrent (urls, concurrency=10): semaphore = asyncio.Semaphore (concurrency); async def fetch_with_limit (url): async with semaphore: return await fetch (url); tasks = [fetch_with_limit (url) for url in urls]; results = await asyncio.gather(*tasks, return_exceptions=True); return results. Execution: Start 10 requests. When one completes, start next. Always ~10 active at any moment. Time: 100 URLs / 10 concurrent / 1s per request = ~10 seconds. Memory: Moderate. Max 10 concurrent requests (~10MB). CPU: Event loop active but not overwhelmed. Error handling: Use return_exceptions=True to handle individual failures. Graceful degradation (9/10 succeed if 1 fails). Server impact: Moderate. 10 concurrent requests is reasonable for most servers. Respects server capacity. When to use: Production default for external APIs. Unknown server capacity (play it safe). Large number of requests (1000+, batch in groups of 10). Need balance between speed and server load. API rate limits (adjust concurrency to stay under limit). Pros: Good speed (10× faster than sequential). Controlled resource usage. Plays nice with servers. Scalable to thousands of URLs (just batches internally). Cons: More complex than gather(). Slightly slower than unlimited concurrency. Comparison table: | Aspect | Sequential | All Concurrent (100) | Semaphore (10) | |--------|------------|---------------------|----------------| | Time | 100s | ~1s | ~10s | | Memory | ~1MB | ~100MB | ~10MB | | Server Load | 1 req/s | 100 req/s | 10 req/s | | Risk | None | High (overwhelm) | Low | | Simplicity | ★★★★★ | ★★★★ | ★★★ | | Speed | ★ | ★★★★★ | ★★★★ | | Production Ready | No | Risky | Yes ★ | Code with all three: import asyncio, aiohttp, time. async def fetch (session, url): async with session.get (url) as response: return await response.text(). async def benchmark_all_approaches (urls): async with aiohttp.ClientSession() as session: # Sequential; start = time.time(); sequential_results = []; for url in urls: sequential_results.append (await fetch (session, url)); print(f"Sequential: {time.time() - start:.2f}s"). # All concurrent; start = time.time(); concurrent_results = await asyncio.gather(*[fetch (session, url) for url in urls]); print(f"All concurrent: {time.time() - start:.2f}s"). # Semaphore (10); start = time.time(); semaphore = asyncio.Semaphore(10); async def fetch_limited (url): async with semaphore: return await fetch (session, url); limited_results = await asyncio.gather(*[fetch_limited (url) for url in urls]); print(f"Semaphore (10): {time.time() - start:.2f}s"). Production recommendation: Default: Use Semaphore with concurrency=10-50 (depending on server). For your own servers: Can use higher concurrency or all-concurrent. For rate-limited APIs: Calculate: concurrency = rate_limit / requests_per_second. Example: API allows 100 req/min → concurrency = 100/60 ≈ 2. For debugging: Use sequential to isolate issues. Dynamic concurrency: Adjust based on error rates. If seeing many 429 (rate limit) errors, decrease concurrency. class AdaptiveSemaphore: def __init__(self, initial=10): self.semaphore = asyncio.Semaphore (initial); self.error_count = 0. async def fetch (self, url): async with self.semaphore: try: return await fetch (url); except RateLimitError: self.error_count += 1; if self.error_count > 5: # Reduce concurrency; self.semaphore._value = max(1, self.semaphore._value - 1); raise. Final answer: For 100 API calls to external server, use Semaphore with concurrency=10-20. Balance of speed (5-10× speedup) and server-friendliness.',
    keyPoints: [
      'Sequential: 100s, ~1MB, safest but slowest, use for debugging or strict rate limits',
      'All concurrent: ~1s, ~100MB, fastest but risky (can overwhelm server), use for own servers',
      'Semaphore (10): ~10s, ~10MB, balanced approach, use for production external APIs',
      'Trade-offs: Speed vs memory vs server load—semaphore offers best balance',
      'Production default: Semaphore with concurrency=10-50, adjust based on rate limits and error rates',
    ],
  },
];
