export const concurrencyFundamentalsQuiz = [
  {
    id: 'cf-q-1',
    question:
      "Design a high-performance web scraper that needs to fetch 10,000 product pages from an e-commerce site. Address: (1) concurrency strategy (async vs threading vs multiprocessing), (2) rate limiting to avoid being blocked (max 100 requests/second), (3) handling failures and retries (network errors, timeouts), (4) memory management (can't load all 10,000 pages in memory), (5) parsing strategy (HTML parsing is CPU-intensive). Provide specific Python code structure and explain your architectural decisions.",
    sampleAnswer:
      "High-performance web scraper architecture: (1) Concurrency strategy: Use asyncio + aiohttp for I/O-bound fetching. Web requests are I/O-bound (waiting for network response). Async allows concurrent requests without threading overhead. Example: async with aiohttp.ClientSession() as session: tasks = [fetch_page(session, url) for url in batch]; pages = await asyncio.gather(*tasks, return_exceptions=True). Can handle 100+ concurrent requests efficiently. NOT multiprocessing: Overkill for I/O, process creation overhead expensive. NOT threading: GIL contention, more memory per thread. (2) Rate limiting: Use asyncio.Semaphore to limit concurrent requests. semaphore = asyncio.Semaphore(100) ensures max 100 concurrent requests. Add delay between batches: await asyncio.sleep(1.0) after each batch of 100 requests. Respect robots.txt: Parse allowed paths and crawl delay. User-Agent header: Identify your scraper politely. (3) Failure handling: Wrap requests in try/except: catch aiohttp.ClientError, asyncio.TimeoutError. Exponential backoff retry: for attempt in range(3): try: return await fetch(); except: wait = 2 ** attempt; await asyncio.sleep(wait). Circuit breaker: After 10 consecutive failures for a domain, pause scraping that domain for 5 minutes. Log all failures: url, error type, timestamp for debugging. (4) Memory management: Process in batches of 100 pages: Fetch 100 → Parse → Save to database → Release memory. Use generators: for batch in batches(urls, 100): pages = await fetch_batch(batch); results = parse_batch(pages); save_results(results); del pages # explicit cleanup. Stream to database/disk: Don't accumulate results in memory. Write to PostgreSQL or files after each batch. (5) Parsing strategy: HTML parsing (BeautifulSoup) is CPU-bound. Option A: Parse in event loop (simple, works if parsing is fast <50ms). Option B: Offload to thread pool: executor = ThreadPoolExecutor(4); loop.run_in_executor(executor, parse_html, html). Option C: Offload to process pool for very CPU-intensive parsing. Best: Option A initially, profile with cProfile. If parsing is bottleneck, switch to Option B. Code structure: async def fetch_page(session, url, semaphore): async with semaphore: try: async with session.get(url, timeout=10) as response: return await response.text(); except Exception as e: log_error(url, e); return None. async def scrape_all(urls): semaphore = asyncio.Semaphore(100); async with aiohttp.ClientSession() as session: for batch in batches(urls, 100): tasks = [fetch_page(session, url, semaphore) for url in batch]; pages = await asyncio.gather(*tasks); results = [parse_page(p) for p in pages if p]; save_to_db(results); await asyncio.sleep(1.0). Key decisions: Async for I/O (10,000 pages in minutes not hours). Semaphore for rate limiting (avoid bans). Batching for memory efficiency (constant memory usage). Retry logic for reliability (handle transient failures). Metrics: Time: ~100 seconds for 10K pages (100 pages/second). Memory: Constant ~100MB (batch processing). Success rate: >95% with retries. Alternative: Scrapy framework (built-in rate limiting, retries, pipelines). But understanding asyncio fundamentals is valuable.",
    keyPoints: [
      'Concurrency: asyncio + aiohttp for I/O-bound fetching (100+ concurrent), not multiprocessing (overkill)',
      'Rate limiting: asyncio.Semaphore(100) limits concurrent requests, sleep(1.0) between batches',
      'Failure handling: try/except with exponential backoff retries, circuit breaker after 10 failures',
      'Memory: batch processing (100 pages at a time), stream to database, explicit cleanup',
      'Parsing: CPU-bound, start with sync parsing in event loop, profile, offload to ThreadPoolExecutor if needed',
    ],
  },
  {
    id: 'cf-q-2',
    question:
      'Compare the performance characteristics of threading, multiprocessing, and async for: (1) web server handling 10,000 concurrent connections, (2) image processing pipeline (resize 1,000 images), (3) database ETL job (extract from API, transform data, load to database). For each scenario, explain which approach is optimal, provide specific Python code examples, discuss memory usage, CPU utilization, and latency. Why does the Global Interpreter Lock (GIL) matter for these choices?',
    sampleAnswer:
      'Performance comparison across concurrency models: Scenario 1: Web server (10,000 concurrent connections). Optimal: Asyncio (async/await). Why: Connections spend 95% time waiting for I/O (client requests, database queries, external APIs). CPU mostly idle. Asyncio: Single event loop handles all connections with minimal memory. Memory: ~8KB per connection = 80MB for 10K connections. CPU: <10% on single core. Latency: <10ms overhead. Code: async def handler(request): data = await db.query(); return response. Threading: GIL prevents true parallelism. Context switching overhead. Memory: ~8MB per thread = 80GB for 10K threads (impossible!). Max ~1,000 threads realistically. Multiprocessing: Massive overkill. Process creation expensive. Memory: ~20MB per process. CPU: Wasted on process management. Code example (async): from fastapi import FastAPI; app = FastAPI(); @app.get("/users/{id}") async def get_user(id): user = await db.fetch_user(id); return user. Result: 10,000 concurrent connections on single core with asyncio. Threading can only handle ~1,000. Multiprocessing not viable. Scenario 2: Image processing (resize 1,000 images). Optimal: Multiprocessing. Why: Image resizing is CPU-bound (pure computation, no I/O). GIL prevents threading from using multiple cores. Need true parallelism. Multiprocessing: Bypass GIL with separate processes. Each process on different CPU core. Memory: Each process has own memory space, but images processed sequentially per process. CPU: 100% on all cores (8 cores = 8× speedup). Code: from concurrent.futures import ProcessPoolExecutor; def resize_image(path): img = Image.open(path); img.thumbnail((800, 600)); img.save(path); return path. with ProcessPoolExecutor(max_workers=8) as executor: results = list(executor.map(resize_image, image_paths)). Time: Sequential = 100s. Multiprocessing (8 cores) = 12.5s (8× speedup). Threading: GIL limits to 1 core. Time: Still ~100s (no speedup). Asyncio: No benefit (not I/O bound). Adding async overhead makes it slower. Result: Multiprocessing achieves near-linear speedup with cores. Threading and asyncio provide no benefit. Scenario 3: Database ETL (Extract from API, Transform, Load to DB). Optimal: Asyncio for extract and load (I/O-bound). Multiprocessing for transform if CPU-intensive. Hybrid approach. Why: Extract from API: I/O-bound (network requests). Async perfect. Transform: Depends on complexity. Simple transforms (filter, map): Async works. Complex (aggregations, ML): CPU-bound, use multiprocessing. Load to database: I/O-bound (network). Async perfect. Code (hybrid): async def extract(session, page): async with session.get(f"api.com/data?page={page}") as r: return await r.json(). def transform_heavy(data): # CPU-intensive transform; result = complex_calculation(data); return result. async def load(conn, data): await conn.execute("INSERT INTO table VALUES (...)", data). async def etl_pipeline(): async with aiohttp.ClientSession() as session: # Extract (async I/O): pages = await asyncio.gather(*[extract(session, i) for i in range(100)]). # Transform (multiprocessing if heavy): with ProcessPoolExecutor() as executor: loop = asyncio.get_event_loop(); transformed = await loop.run_in_executor(executor, transform_heavy, pages). # Load (async I/O): async with asyncpg.connect() as conn: await load(conn, transformed). Result: Best of both worlds. Async for I/O, multiprocessing for CPU. GIL Impact: Threading: GIL allows only one thread to execute Python bytecode at a time. I/O-bound: Thread releases GIL during I/O wait (read, network), other threads run. Still some benefit. CPU-bound: Thread holds GIL during computation. Other threads blocked. No speedup. Example: import threading, time; def cpu_work(): sum(i**2 for i in range(10000000)). Sequential: 2.5s. Threading (4 threads): 2.5s (no speedup due to GIL). Multiprocessing (4 cores): 0.65s (4× speedup, no GIL). Asyncio: GIL not issue (single-threaded). No parallelism, but efficient I/O concurrency. Summary table: | Scenario | Best Approach | Memory | CPU | Latency | Why | |----------|---------------|--------|-----|---------|-----| | Web server 10K conn | Asyncio | 80MB | <10% | <10ms | I/O-bound, minimal overhead | | Image processing 1K | Multiprocessing | High | 100% all cores | Low | CPU-bound, true parallelism | | Database ETL | Async + Multiproc | Medium | Variable | Low | Hybrid I/O + CPU | Threading: Legacy approach. Use for I/O if async not available. Asyncio: Modern I/O concurrency. Single-threaded, scales to 10K+ connections. Multiprocessing: True parallelism for CPU-bound. Bypass GIL. Critical decision: Profile your specific workload. Is it I/O or CPU bound? Choice wrong and performance suffers massively.',
    keyPoints: [
      'Web server (10K connections): Asyncio optimal (80MB, single core), threading limited to ~1K connections',
      'Image processing: Multiprocessing only option (8× speedup on 8 cores), threading/async no benefit due to GIL',
      'Database ETL: Hybrid approach (async for I/O extract/load, multiprocessing for CPU-heavy transform)',
      "GIL impact: Blocks threading for CPU tasks, doesn't affect I/O threading, not relevant for async/multiprocessing",
      'Decision: Profile workload (I/O-bound → async, CPU-bound → multiprocessing, hybrid → both)',
    ],
  },
  {
    id: 'cf-q-3',
    question:
      "Design a real-time stock market data processing system that receives 100,000 market updates per second from a WebSocket feed, processes them (calculate VWAP, detect price anomalies), and serves the results via a REST API to 1,000 concurrent users. Address: (1) concurrency model for receiving updates (can't drop any ticks), (2) processing strategy (some calculations are CPU-intensive), (3) serving API requests without blocking data ingestion, (4) memory management (store last 1 hour of data = 360M ticks), (5) latency requirements (API responses <50ms). Provide architecture diagram and justify your concurrency choices.",
    sampleAnswer:
      'Real-time market data system architecture: Requirements analysis: Ingestion: 100,000 ticks/sec = very high throughput. Can\'t drop data. Processing: VWAP, anomaly detection (CPU-intensive). Serving: 1,000 concurrent API users, <50ms latency. Storage: 1 hour × 100K ticks/sec = 360M ticks in memory. Architecture: (1) Data ingestion (asyncio WebSocket): async def ingest_market_data(): async with websockets.connect("wss://market-feed.com") as ws: async for message in ws: tick = parse_tick(message); await queue.put(tick). Why asyncio: WebSocket is I/O-bound (waiting for network data). Asyncio handles high-throughput streams efficiently. Non-blocking: Can receive ticks without processing blocking it. Queue: asyncio.Queue(maxsize=10000) buffers ticks. If processing falls behind, queue grows (backpressure). (2) Data processing (multiprocessing pool): def process_batch(ticks): vwaps = calculate_vwap(ticks); anomalies = detect_anomalies(ticks); return vwaps, anomalies. async def processor(): batch = []; while True: tick = await queue.get(); batch.append(tick); if len(batch) >= 1000: # Process batch in separate process; loop = asyncio.get_event_loop(); with ProcessPoolExecutor() as executor: results = await loop.run_in_executor(executor, process_batch, batch); update_state(results); batch = []. Why multiprocessing: VWAP and anomaly detection are CPU-intensive. Need multiple cores to keep up with 100K ticks/sec. Batching: Process 1000 ticks at once (amortize process communication overhead). Alternative: Use numpy/pandas for vectorized operations (10-100× faster than Python loops). (3) Serving API (FastAPI async): from fastapi import FastAPI; app = FastAPI(). current_state = {} # Latest VWAP and anomalies. @app.get("/symbol/{symbol}/vwap") async def get_vwap(symbol: str): return current_state.get(symbol, {}).get("vwap"). Why asyncio API: 1,000 concurrent users = I/O-bound (users waiting for response). FastAPI async handles many connections on single thread. Non-blocking: API reads from current_state (in-memory dict). No processing in request handler. Latency: <1ms to read from dict + JSON serialization = <10ms total. (4) Memory management: Challenge: 360M ticks × 50 bytes/tick = 18GB. Solution: Time-series database in memory (Redis TimeSeries or custom ring buffer). Ring buffer: class TickBuffer: def __init__(self, max_size): self.buffer = deque(maxsize=max_size); self.index = {}. def add(self, tick): if len(self.buffer) == self.buffer.maxsize: oldest = self.buffer[0]; del self.index[oldest.symbol]. self.buffer.append(tick); self.index[tick.symbol] = tick. Eviction: Oldest ticks automatically evicted when buffer full. Only keep 1 hour (automatically handled by ring buffer). Aggregation: Store aggregates (VWAP, min, max) instead of every tick. Reduces memory 100×. (5) Latency optimization: API latency: Serve from in-memory state (no database query). Pre-calculate: Update VWAP incrementally on each tick, don\'t recalculate on API request. Caching: Use Redis for frequently accessed data. Response: {"symbol": "AAPL", "vwap": 150.23, "last_update": "2024-10-01T12:34:56Z"}. Processing latency: Batch processing: 1000 ticks/batch every 10ms. Vectorized operations: numpy for VWAP (100× faster than loops). Parallel processing: 8 cores process 8 batches concurrently. Architecture diagram: WebSocket Feed (100K/sec) → [Asyncio Ingestion] → Queue → [Multiprocessing Pool (8 workers)] → Update State (in-memory dict) → [FastAPI Async (1000 concurrent)] → Users. Concurrency justification: Asyncio ingestion: I/O-bound, handles high-throughput WebSocket stream. Multiprocessing processing: CPU-bound, requires multiple cores for real-time processing. Asyncio API: I/O-bound, handles 1000 concurrent users efficiently. Separation: Ingestion and serving don\'t block each other (separate async tasks). Testing and monitoring: Load testing: Simulate 100K ticks/sec, measure queue depth, processing latency, API latency. Metrics: Ticks received/sec, ticks processed/sec, queue depth (alert if >5000), API p99 latency. Alerting: If processing can\'t keep up, scale horizontally (more processing workers). Failure modes: WebSocket disconnect: Auto-reconnect with exponential backoff. Processing crash: Restart worker, replay from queue. Memory overflow: Alert if memory >80%, implement aggressive eviction. Production considerations: Horizontal scaling: Multiple ingestion instances (shard by symbol). Database: Persist to TimescaleDB for historical queries (async writes). Monitoring: Prometheus metrics + Grafana dashboards. Result: System handles 100K ticks/sec with <50ms API latency using hybrid async (I/O) + multiprocessing (CPU) architecture.',
    keyPoints: [
      'Ingestion: asyncio WebSocket handles 100K ticks/sec (I/O-bound), asyncio.Queue for backpressure',
      'Processing: multiprocessing pool (8 workers) for CPU-intensive VWAP/anomaly detection, batch 1000 ticks',
      'API: FastAPI async serves 1000 concurrent users from in-memory state (<10ms latency)',
      'Memory: ring buffer with 360M ticks (18GB), aggregate storage, automatic eviction of old data',
      'Hybrid architecture: async for I/O (ingest + serve), multiprocessing for CPU (processing), separate concerns',
    ],
  },
];
