/**
 * Quiz questions for Advanced Feature Engineering section
 */

export const advancedfeatureengineeringQuiz = [
  {
    id: 'q1',
    question:
      'Compare and contrast manual feature engineering with automated feature generation. What are the strengths and weaknesses of each approach, and how would you combine them effectively?',
    sampleAnswer:
      'MANUAL FEATURE ENGINEERING: Human-designed features based on domain knowledge and intuition. STRENGTHS: (1) Interpretable - "customer_lifetime_value" immediately understandable. (2) Efficient - creates few but powerful features. (3) Domain expertise - incorporates years of business knowledge. (4) Causal - based on understanding of what drives outcomes. (5) Production-friendly - simple features easy to compute. WEAKNESSES: (1) Limited by human imagination - may miss non-obvious patterns. (2) Time-consuming - requires expert involvement. (3) Domain-specific - doesn\'t transfer across problems. (4) Subjective - different experts create different features. AUTOMATED FEATURE GENERATION: Algorithms systematically create features (Featuretools, polynomial features, genetic algorithms). STRENGTHS: (1) Scalable - creates hundreds/thousands of features quickly. (2) Discovers unexpected patterns - finds relationships humans miss. (3) No domain expertise required - works across domains. (4) Systematic - explores full feature space. (5) Rapid experimentation - test many feature combinations. WEAKNESSES: (1) Creates many irrelevant features - need aggressive feature selection. (2) Uninterpretable - "feature_847 = (x1^2 * log(x3)) / x5" hard to explain. (3) Computationally expensive - training slows with thousands of features. (4) Overfitting risk - too many features for limited data. (5) Production complexity - some features hard to compute at scale. EFFECTIVE COMBINATION STRATEGY: (1) START WITH DOMAIN: Create 10-20 core features based on business knowledge. (2) AUGMENT WITH AUTOMATION: Use automated tools to create candidate features. (3) SELECT BEST: Use feature selection (Random Forest importance, LASSO) to identify top automated features. (4) VALIDATE: Check that selected automated features make business sense. (5) HYBRID FEATURES: Use domain knowledge to guide automated feature generation. REAL EXAMPLE: E-commerce churn prediction. MANUAL: customer_lifetime_value, days_since_last_purchase, engagement_score. AUTOMATED: Featuretools creates 500 features from transaction history. SELECTION: Keep top 30 by Random Forest importance. RESULT: 15 manual + 30 automated = 45 features. Performance improvement: Manual alone (75% AUC), Automated alone (78% AUC), Combined (85% AUC)! BEST PRACTICE: Start manual (establish baseline), add automated (boost performance), select rigorously (avoid overfitting), validate interpretability (ensure business alignment).',
    keyPoints: [
      'Manual: interpretable, efficient, domain-driven, limited by imagination',
      'Automated: scalable, discovers patterns, creates many features, less interpretable',
      'Manual best for interpretability and production simplicity',
      'Automated best for discovery and rapid experimentation',
      'Combine: domain foundation + automated augmentation + rigorous selection',
      'Always validate that automated features make business sense',
    ],
  },
  {
    id: 'q2',
    question:
      "Explain feature selection and why it's critical. Describe at least three different feature selection methods and when you would use each.",
    sampleAnswer:
      'FEATURE SELECTION: Process of identifying most relevant features and removing irrelevant/redundant ones. WHY CRITICAL: (1) CURSE OF DIMENSIONALITY: With 1000 features and 500 samples, model has more parameters than data → guaranteed overfitting. (2) TRAINING SPEED: Fewer features = faster training. 10 features vs 1000 = 100x speedup. (3) GENERALIZATION: Removing noise features improves test performance. (4) INTERPRETABILITY: 10 key features easier to explain than 1000. (5) PRODUCTION: Fewer features = simpler deployment, faster inference. (6) COST: Each feature costs money to collect/compute in production. FEATURE SELECTION METHODS: (1) FILTER METHODS (Univariate Selection): Test each feature independently against target. METHODS: ANOVA F-test (continuous target), Chi-square (categorical target), Mutual Information (any type), Correlation coefficient. WHEN TO USE: Fast screening before modeling, very high dimensionality (10K+ features), preliminary exploration. PROS: Fast (no model training), independent of algorithm. CONS: Ignores feature interactions, each feature evaluated in isolation. EXAMPLE: SelectKBest with f_classif - rank features by ANOVA F-statistic, keep top K. (2) WRAPPER METHODS (Model-Based): Use model performance to evaluate feature subsets. METHODS: Recursive Feature Elimination (RFE) - iteratively remove worst features, Forward Selection - iteratively add best features, Backward Elimination - remove features one by one. WHEN TO USE: Small-medium feature sets (<100), specific target model in mind, need optimal subset for that model. PROS: Considers feature interactions, optimized for specific model. CONS: VERY computationally expensive (trains many models), risks overfitting, model-specific (best features for RF might differ from linear). EXAMPLE: RFE with Logistic Regression - train model, remove least important feature, repeat until K features remain. (3) EMBEDDED METHODS (Regularization): Feature selection during model training. METHODS: LASSO (L1 regularization) - drives coefficients to zero, Ridge (L2) - shrinks coefficients, Random Forest importance - use feature importance scores, XGBoost importance - built-in feature ranking. WHEN TO USE: Large feature sets, want to train model simultaneously with selection, using tree-based or regularized linear models. PROS: Efficient (single training run), considers interactions, regularization prevents overfitting. CONS: Model-specific, hyperparameter tuning required (regularization strength). EXAMPLE: SelectFromModel with Lasso - train Lasso, keep features with non-zero coefficients. (4) HYBRID APPROACHES: Combine multiple methods for robustness. CONSENSUS SELECTION: Run multiple methods, keep features selected by majority. Reduces method-specific bias. PRACTICAL WORKFLOW: (1) Filter first: Remove obvious non-informative features (variance threshold, zero correlation). (2) Try embedded: Train Random Forest or Lasso, check feature importance. (3) If needed, RFE: Use RFE for final refinement on top features. (4) Cross-validate: Ensure selected features generalize. REAL EXAMPLE: Kaggle competition with 300 features, 10K samples. Filter (mutual info): 300→100 features. Random Forest importance: 100→30 features. RFE with XGBoost: 30→15 features. Final model with 15 features: same performance as 300, 10x faster training!',
    keyPoints: [
      'Feature selection prevents overfitting and improves generalization',
      'Filter methods: fast univariate tests, ignore interactions',
      'Wrapper methods: model-based, expensive, optimal for specific model',
      'Embedded methods: regularization during training, efficient',
      'Use filter for initial screening, embedded for final selection',
      'Always cross-validate to ensure features generalize',
    ],
  },
  {
    id: 'q3',
    question:
      'In quantitative finance, technical indicators like MACD, RSI, and Bollinger Bands are commonly used features. Explain what these capture and why they might be predictive. Are there any risks in using them?',
    sampleAnswer:
      'Technical indicators transform price/volume data into features that capture market momentum, volatility, and trends. MOVING AVERAGE CONVERGENCE DIVERGENCE (MACD): CALCULATION: MACD = EMA(12) - EMA(26), Signal = EMA(9) of MACD, Histogram = MACD - Signal. CAPTURES: Momentum and trend changes. Crossovers signal potential buy/sell points. Histogram shows momentum strength. WHY PREDICTIVE: Identifies shifts between bullish and bearish momentum. Widely watched by traders → self-fulfilling prophecy (many traders act on MACD signals). INTERPRETATION: MACD > Signal = bullish, MACD < Signal = bearish. RELATIVE STRENGTH INDEX (RSI): CALCULATION: RSI = 100 - (100 / (1 + RS)), where RS = Average Gain / Average Loss over 14 periods. CAPTURES: Overbought/oversold conditions. Range: 0-100. INTERPRETATION: RSI > 70 = overbought (potential reversal down), RSI < 30 = oversold (potential reversal up). WHY PREDICTIVE: Mean reversion - extreme price movements tend to reverse. Market psychology - fear and greed cycles. LIMITATION: In strong trends, can stay overbought/oversold for long periods. BOLLINGER BANDS: CALCULATION: Middle = 20-day SMA, Upper = Middle + (2 × std), Lower = Middle - (2 × std). CAPTURES: Volatility and relative price levels. Bands expand during high volatility, contract during low volatility. WHY PREDICTIVE: Price tends to stay within bands (2 std = 95% of normal distribution). Touches of bands suggest potential reversals. Band squeezes (narrow bands) often precede big moves. "Bollinger Bounce" - price bounces off bands. WHY THESE MIGHT WORK: (1) BEHAVIORAL FINANCE: Traders act on these signals → self-fulfilling. If many traders sell when RSI > 70, it creates selling pressure. (2) MEAN REVERSION: Markets exhibit temporary overshoots that correct. (3) MOMENTUM: Trends persist due to herding behavior. (4) VOLATILITY CLUSTERING: Periods of high/low volatility persist. RISKS AND LIMITATIONS: (1) LAGGING INDICATORS: All use past data, react slowly to sudden changes. MACD crossover may occur after trend already changed. (2) FALSE SIGNALS: In ranging markets, generate many false signals. Whipsaws during choppy conditions. (3) PARAMETER SENSITIVITY: RSI(14) vs RSI(21) give different signals. Optimal parameters change over time and markets. (4) DATA MINING BIAS: These indicators "work" in backtests because they were developed by testing many possibilities. May not work out-of-sample. (5) MARKET EFFICIENCY: If indicators truly worked, everyone would use them → arbitraged away. Efficient market hypothesis suggests technical analysis shouldn\'t work. (6) OVERFITTING: Combining many indicators creates overfitting risk. Model learns noise in training data. (7) REGIME CHANGES: Indicators developed in one market regime may fail in another. 2008 crisis vs 2020 covid vs normal markets behave differently. BEST PRACTICES: (1) Use as features in ML model, not rigid rules. Let model learn when indicators are predictive. (2) Combine with fundamental features (not just technical). (3) Add regime detection - indicators work differently in trending vs mean-reverting markets. (4) Out-of-sample testing crucial - heavily penalize overfitting. (5) Monitor performance in production - indicators may stop working. (6) Consider transaction costs - high-frequency trading on indicators often unprofitable after costs. REALISTIC EXPECTATION: Technical indicators alone rarely sufficient for profitable trading. Useful as features in ensemble with fundamentals, sentiment, and alternative data. Slight edge (51-55% accuracy) may be profitable with proper risk management.',
    keyPoints: [
      'Technical indicators capture momentum, volatility, and trends',
      'MACD: trend changes, RSI: overbought/oversold, Bollinger: volatility',
      'May work due to trader behavior (self-fulfilling) and mean reversion',
      'Risks: lagging, false signals, parameter sensitivity, overfitting',
      "Market efficiency suggests they shouldn't work long-term",
      'Best as features in ML models, not rigid trading rules',
    ],
  },
];
