export const marketDataFundamentalsQuiz = [
  {
    id: 'market-data-fundamentals-q-1',
    question:
      'Design a production-grade market data aggregation system that consumes real-time data from multiple sources (e.g., IEX, Polygon, Alpaca) and provides a unified WebSocket API to downstream clients. The system must: (1) Handle 1000+ symbols with 100-500 updates/second per symbol, (2) Detect and handle data quality issues (stale quotes, bad ticks, gaps), (3) Implement smart failover (switch to backup feed if primary fails), (4) Maintain sequence integrity across all feeds, (5) Support conflation (merge rapid updates to prevent client overload), (6) Provide latency monitoring (track end-to-end delay). Provide the complete architecture, explain the data flow, describe failover logic, and show core Python implementation for the aggregation engine and conflation logic.',
    sampleAnswer:
      'Production market data aggregation system design:\n\n**Architecture Overview**:\n\nComponents:\n1. Feed Connectors (per source): Websocket clients to IEX, Polygon, Alpaca\n2. Normalization Layer: Convert each source format to internal Quote model\n3. Aggregation Engine: Merge quotes from multiple sources, select best quote\n4. Quality Control: Validate data, detect anomalies, handle gaps\n5. Conflation Engine: Rate-limit updates to prevent client overload\n6. Distribution Layer: WebSocket server broadcasting to clients\n7. Monitoring System: Track latency, throughput, data quality\n\n**Data Flow**:\nSource Feed → Feed Connector → Normalization → Quality Check → Aggregation → Conflation → Distribution → Clients\n(ws recv)     (parse)          (to Quote)      (validate)     (best quote)   (rate limit)  (ws send)      (users)\n\n**Core Data Structures**:\n```python\nfrom dataclasses import dataclass\nfrom datetime import datetime\nfrom decimal import Decimal\nfrom enum import Enum\nfrom typing import Optional\n\nclass DataSource(Enum):\n    IEX = "iex"\n    POLYGON = "polygon"\n    ALPACA = "alpaca"\n\n@dataclass\nclass Quote:\n    symbol: str\n    source: DataSource\n    exchange_timestamp: datetime\n    receive_timestamp: datetime\n    bid_price: Decimal\n    bid_size: int\n    ask_price: Decimal\n    ask_size: int\n    sequence: int\n    \n    @property\n    def latency_ms(self) -> float:\n        return (self.receive_timestamp - self.exchange_timestamp).total_seconds() * 1000\n    \n    @property\n    def spread(self) -> Decimal:\n        return self.ask_price - self.bid_price\n\n@dataclass\nclass AggregatedQuote:\n    symbol: str\n    timestamp: datetime\n    best_bid: Decimal\n    best_bid_size: int\n    best_ask: Decimal\n    best_ask_size: int\n    sources: list[DataSource]  # Which sources contributed\n    avg_latency_ms: float\n```\n\n**Aggregation Engine Implementation**:\n```python\nimport asyncio\nfrom collections import defaultdict, deque\nfrom typing import Dict, List\n\nclass MarketDataAggregator:\n    def __init__(self):\n        # Store latest quote from each source\n        self.latest_quotes: Dict[str, Dict[DataSource, Quote]] = defaultdict(dict)\n        \n        # Sequence tracking per source\n        self.sequences: Dict[DataSource, Dict[str, int]] = {\n            source: {} for source in DataSource\n        }\n        \n        # Stale quote threshold (100ms)\n        self.stale_threshold_ms = 100\n        \n        # Quality metrics\n        self.gap_counts: Dict[DataSource, int] = defaultdict(int)\n        self.stale_counts: Dict[DataSource, int] = defaultdict(int)\n    \n    def check_sequence(self, quote: Quote) -> bool:\n        \"\"\"Check sequence number and detect gaps\"\"\"\n        symbol_seqs = self.sequences[quote.source]\n        \n        if quote.symbol not in symbol_seqs:\n            symbol_seqs[quote.symbol] = quote.sequence\n            return True\n        \n        expected = symbol_seqs[quote.symbol] + 1\n        \n        if quote.sequence == expected:\n            symbol_seqs[quote.symbol] = quote.sequence\n            return True\n        elif quote.sequence < expected:\n            # Duplicate or out-of-order\n            return False\n        else:\n            # Gap detected\n            gap_size = quote.sequence - expected\n            self.gap_counts[quote.source] += gap_size\n            logger.warning(\n                f"Gap detected: {quote.symbol} from {quote.source}, "\n                f"missing {gap_size} messages"\n            )\n            symbol_seqs[quote.symbol] = quote.sequence\n            return True\n    \n    def is_stale(self, quote: Quote) -> bool:\n        \"\"\"Check if quote is stale (latency > threshold)\"\"\"\n        if quote.latency_ms > self.stale_threshold_ms:\n            self.stale_counts[quote.source] += 1\n            return True\n        return False\n    \n    def validate_quote(self, quote: Quote) -> bool:\n        \"\"\"Validate quote data quality\"\"\"\n        # Check for bad prices\n        if quote.bid_price <= 0 or quote.ask_price <= 0:\n            logger.error(f"Invalid price: {quote.symbol} bid={quote.bid_price} ask={quote.ask_price}")\n            return False\n        \n        # Check for inverted market (bid > ask)\n        if quote.bid_price >= quote.ask_price:\n            logger.error(f"Inverted market: {quote.symbol} bid={quote.bid_price} > ask={quote.ask_price}")\n            return False\n        \n        # Check for unreasonable spread (>10%)\n        if quote.spread / quote.ask_price > 0.1:\n            logger.warning(f"Wide spread: {quote.symbol} spread={quote.spread} ({quote.spread/quote.ask_price*100:.1f}%)")\n            # Don\'t reject, but log\n        \n        # Check sizes\n        if quote.bid_size <= 0 or quote.ask_size <= 0:\n            logger.error(f"Invalid size: {quote.symbol} bid_size={quote.bid_size} ask_size={quote.ask_size}")\n            return False\n        \n        return True\n    \n    async def process_quote(self, quote: Quote) -> Optional[AggregatedQuote]:\n        \"\"\"Process incoming quote and return aggregated result\"\"\"\n        # Check sequence\n        if not self.check_sequence(quote):\n            return None  # Duplicate, ignore\n        \n        # Validate\n        if not self.validate_quote(quote):\n            return None  # Bad data, ignore\n        \n        # Check staleness\n        if self.is_stale(quote):\n            logger.warning(f"Stale quote: {quote.symbol} from {quote.source}, latency={quote.latency_ms:.1f}ms")\n            # Still use it, but log\n        \n        # Store quote\n        self.latest_quotes[quote.symbol][quote.source] = quote\n        \n        # Aggregate across sources\n        aggregated = self.aggregate_symbol(quote.symbol)\n        return aggregated\n    \n    def aggregate_symbol(self, symbol: str) -> AggregatedQuote:\n        \"\"\"Aggregate quotes from all sources for a symbol\"\"\"\n        quotes = self.latest_quotes[symbol]\n        \n        if not quotes:\n            return None\n        \n        # Find best bid (highest) and best ask (lowest)\n        best_bid_price = Decimal(\'0\')\n        best_bid_size = 0\n        best_ask_price = Decimal(\'999999\')\n        best_ask_size = 0\n        sources_used = []\n        total_latency = 0\n        \n        for source, quote in quotes.items():\n            # Best bid\n            if quote.bid_price > best_bid_price:\n                best_bid_price = quote.bid_price\n                best_bid_size = quote.bid_size\n            \n            # Best ask\n            if quote.ask_price < best_ask_price:\n                best_ask_price = quote.ask_price\n                best_ask_size = quote.ask_size\n            \n            sources_used.append(source)\n            total_latency += quote.latency_ms\n        \n        return AggregatedQuote(\n            symbol=symbol,\n            timestamp=datetime.utcnow(),\n            best_bid=best_bid_price,\n            best_bid_size=best_bid_size,\n            best_ask=best_ask_price,\n            best_ask_size=best_ask_size,\n            sources=sources_used,\n            avg_latency_ms=total_latency / len(sources_used)\n        )\n```\n\n**Failover Logic**:\n```python\nclass FeedManager:\n    def __init__(self):\n        self.sources = [DataSource.IEX, DataSource.POLYGON, DataSource.ALPACA]\n        self.source_health: Dict[DataSource, bool] = {s: True for s in self.sources}\n        self.source_last_message: Dict[DataSource, datetime] = {}\n        self.failover_threshold_seconds = 5\n    \n    async def monitor_health(self):\n        \"\"\"Monitor feed health and trigger failover\"\"\"\n        while True:\n            now = datetime.utcnow()\n            \n            for source in self.sources:\n                if source not in self.source_last_message:\n                    continue\n                \n                seconds_since_last = (now - self.source_last_message[source]).total_seconds()\n                \n                if seconds_since_last > self.failover_threshold_seconds:\n                    if self.source_health[source]:\n                        logger.error(f"Feed {source} is down (no messages for {seconds_since_last:.1f}s)")\n                        self.source_health[source] = False\n                        await self.trigger_failover(source)\n                else:\n                    if not self.source_health[source]:\n                        logger.info(f"Feed {source} recovered")\n                        self.source_health[source] = True\n            \n            await asyncio.sleep(1)\n    \n    async def trigger_failover(self, failed_source: DataSource):\n        \"\"\"Trigger failover to backup feed\"\"\"\n        # Find healthy sources\n        healthy = [s for s in self.sources if self.source_health[s]]\n        \n        if not healthy:\n            logger.critical("All feeds are down! System degraded.")\n            # Send alert\n            return\n        \n        logger.info(f"Failing over from {failed_source} to {healthy}")\n        # Aggregator automatically uses healthy sources\n```\n\n**Conflation Engine** (rate limiting for clients):\n```python\nclass ConflationEngine:\n    def __init__(self, max_updates_per_second: int = 10):\n        self.max_updates_per_second = max_updates_per_second\n        self.min_interval_ms = 1000 / max_updates_per_second\n        \n        # Track last send time per symbol\n        self.last_send: Dict[str, datetime] = {}\n        \n        # Buffer latest quote per symbol\n        self.buffered: Dict[str, AggregatedQuote] = {}\n    \n    def should_send(self, symbol: str) -> bool:\n        \"\"\"Check if enough time has passed to send update\"\"\"\n        if symbol not in self.last_send:\n            return True\n        \n        elapsed_ms = (datetime.utcnow() - self.last_send[symbol]).total_seconds() * 1000\n        return elapsed_ms >= self.min_interval_ms\n    \n    async def add_quote(self, quote: AggregatedQuote) -> Optional[AggregatedQuote]:\n        \"\"\"Add quote to conflation engine\"\"\"\n        # Always buffer latest\n        self.buffered[quote.symbol] = quote\n        \n        # Check if should send\n        if self.should_send(quote.symbol):\n            self.last_send[quote.symbol] = datetime.utcnow()\n            return self.buffered.pop(quote.symbol)\n        \n        return None  # Conflated (rate limited)\n    \n    async def flush_buffered(self):\n        \"\"\"Periodically flush buffered quotes (ensure no quote waits forever)\"\"\"\n        while True:\n            await asyncio.sleep(0.1)  # Check every 100ms\n            \n            now = datetime.utcnow()\n            to_send = []\n            \n            for symbol, quote in list(self.buffered.items()):\n                if symbol in self.last_send:\n                    elapsed_ms = (now - self.last_send[symbol]).total_seconds() * 1000\n                    if elapsed_ms >= self.min_interval_ms:\n                        to_send.append(self.buffered.pop(symbol))\n                        self.last_send[symbol] = now\n            \n            for quote in to_send:\n                # Send to clients\n                await self.broadcast(quote)\n```\n\n**Latency Monitoring**:\n```python\nclass LatencyMonitor:\n    def __init__(self):\n        self.latencies: Dict[DataSource, deque] = {\n            source: deque(maxlen=1000) for source in DataSource\n        }\n    \n    def record(self, quote: Quote):\n        \"\"\"Record latency for quote\"\"\"\n        self.latencies[quote.source].append(quote.latency_ms)\n    \n    def get_stats(self, source: DataSource) -> dict:\n        \"\"\"Get latency statistics\"\"\"\n        latencies = list(self.latencies[source])\n        if not latencies:\n            return {}\n        \n        latencies.sort()\n        return {\n            \'mean\': sum(latencies) / len(latencies),\n            \'p50\': latencies[len(latencies) // 2],\n            \'p95\': latencies[int(len(latencies) * 0.95)],\n            \'p99\': latencies[int(len(latencies) * 0.99)],\n            \'max\': max(latencies)\n        }\n```\n\n**System Capacity**:\n- 1000 symbols × 500 updates/sec = 500K updates/sec input\n- Single aggregator process: Can handle 100K-500K messages/sec (Python asyncio)\n- Conflation reduces output: 1000 symbols × 10 updates/sec = 10K updates/sec to clients\n- Horizontal scaling: Partition symbols across multiple aggregator instances\n- Database writes: Batch writes, 10K inserts/sec (TimescaleDB)\n\n**Production Considerations**:\n- Deploy aggregator in same region as data sources (reduce latency)\n- Use Redis for shared state if running multiple aggregators\n- Monitor gaps and stale quotes (alert if > 1% of messages)\n- Log all quality issues for post-analysis\n- Implement circuit breakers (stop consuming if downstream is down)\n- Use WebSocket compression (reduces bandwidth by 70-90%)\n- Test failover regularly (simulate feed failures)\n\nThis architecture provides robust, production-grade market data aggregation with smart failover, quality control, and rate limiting for scalable client distribution.',
    keyPoints: [
      'Multi-source aggregation: Consume from IEX, Polygon, Alpaca; merge quotes to find best bid/ask',
      'Quality control: Validate prices, sizes, detect inverted markets, track sequence gaps',
      'Smart failover: Monitor feed health (5s timeout), automatically switch to backup sources',
      'Conflation: Rate-limit updates to 10/sec per symbol to prevent client overload (1000 symbols = 10K msg/sec output)',
      'Latency monitoring: Track exchange → receive → process → send latency; alert if p99 > 100ms',
    ],
  },
  {
    id: 'market-data-fundamentals-q-2',
    question:
      'You are building a market data replay system for backtesting that must: (1) Read historical tick data from storage (billions of ticks), (2) Replay data at configurable speeds (1×, 10×, 100× real-time), (3) Support time-travel (jump to specific timestamp, rewind), (4) Handle multiple concurrent backtest sessions, (5) Maintain exact tick ordering and timing, (6) Support both full replay (all ticks) and sampled replay (e.g., every 100th tick for fast backtests). Design the complete system architecture, explain the storage format for efficient random access, describe the replay engine algorithm, and provide Python implementation for the core replay loop with speed control and time-travel.',
    sampleAnswer:
      'Market data replay system for backtesting:\n\n**Architecture Overview**:\n\nComponents:\n1. Storage Layer: Parquet files partitioned by date and symbol\n2. Index Service: Maps timestamps to file offsets for fast seeking\n3. Replay Engine: Reads ticks, maintains timing, controls speed\n4. Session Manager: Handles multiple concurrent backtest sessions\n5. Cache Layer: LRU cache for frequently accessed data\n6. API Layer: REST/WebSocket API for backtest control\n\n**Storage Format** (optimized for random access):\n```python\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nfrom datetime import datetime, date\nimport pandas as pd\n\n# Parquet schema for tick data\ntick_schema = pa.schema([\n    (\'symbol\', pa.string()),\n    (\'timestamp\', pa.timestamp(\'ns\')),  # Nanosecond precision\n    (\'type\', pa.string()),  # \'quote\' or \'trade\'\n    (\'bid_price\', pa.decimal128(18, 4)),\n    (\'bid_size\', pa.int32()),\n    (\'ask_price\', pa.decimal128(18, 4)),\n    (\'ask_size\', pa.int32()),\n    (\'trade_price\', pa.decimal128(18, 4)),\n    (\'trade_size\', pa.int32()),\n    (\'exchange\', pa.string()),\n    (\'sequence\', pa.int64())\n])\n\n# Partition structure:\n# /data/ticks/date=2024-01-15/symbol=AAPL/data.parquet\n# /data/ticks/date=2024-01-15/symbol=MSFT/data.parquet\n# Allows efficient filtering by date and symbol\n\ndef write_ticks_to_parquet(ticks: pd.DataFrame, base_path: str, date: date, symbol: str):\n    \"\"\"Write ticks to partitioned parquet\"\"\"\n    path = f"{base_path}/date={date}/symbol={symbol}/data.parquet"\n    \n    # Write with row group size = 100K ticks (~10MB per row group)\n    pq.write_table(\n        pa.Table.from_pandas(ticks),\n        path,\n        compression=\'snappy\',  # Fast compression\n        row_group_size=100000,\n        use_dictionary=True,  # Compress repeated strings\n        write_statistics=True  # Enable predicate pushdown\n    )\n```\n\n**Index Service** (for fast time-travel):\n```python\nimport bisect\nfrom typing import Dict, List, Tuple\n\nclass TickIndex:\n    \"\"\"Index for fast timestamp lookup\"\"\"\n    \n    def __init__(self):\n        # symbol -> [(timestamp, file_path, row_offset), ...]\n        self.indices: Dict[str, List[Tuple[datetime, str, int]]] = {}\n    \n    def build_index(self, base_path: str, date: date, symbols: List[str]):\n        \"\"\"Build index by reading parquet metadata\"\"\"\n        for symbol in symbols:\n            path = f"{base_path}/date={date}/symbol={symbol}/data.parquet"\n            \n            # Read parquet metadata (fast, doesn\'t load data)\n            parquet_file = pq.ParquetFile(path)\n            \n            symbol_index = []\n            row_offset = 0\n            \n            # Iterate row groups\n            for i in range(parquet_file.num_row_groups):\n                rg_metadata = parquet_file.metadata.row_group(i)\n                \n                # Get min/max timestamp from statistics\n                timestamp_col = rg_metadata.column(1)  # timestamp column\n                min_ts = timestamp_col.statistics.min\n                max_ts = timestamp_col.statistics.max\n                \n                symbol_index.append((\n                    min_ts,\n                    path,\n                    row_offset\n                ))\n                \n                row_offset += rg_metadata.num_rows\n            \n            self.indices[symbol] = symbol_index\n    \n    def find_position(self, symbol: str, target_timestamp: datetime) -> Tuple[str, int]:\n        \"\"\"Find file and row offset for timestamp\"\"\"\n        if symbol not in self.indices:\n            raise ValueError(f"No index for {symbol}")\n        \n        index = self.indices[symbol]\n        timestamps = [ts for ts, _, _ in index]\n        \n        # Binary search\n        pos = bisect.bisect_left(timestamps, target_timestamp)\n        \n        if pos == 0:\n            return index[0][1], index[0][2]  # Start of file\n        \n        # Return previous row group (target is in this or next group)\n        return index[pos - 1][1], index[pos - 1][2]\n```\n\n**Replay Engine Implementation**:\n```python\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import AsyncIterator, Optional\nimport time\n\n@dataclass\nclass ReplayConfig:\n    symbols: List[str]\n    start_time: datetime\n    end_time: datetime\n    speed_multiplier: float = 1.0  # 1× = real-time, 10× = 10× faster\n    sample_rate: int = 1  # 1 = all ticks, 100 = every 100th tick\n    mode: str = \'sequential\'  # \'sequential\' or \'parallel\'\n\nclass ReplayEngine:\n    def __init__(self, base_path: str, index: TickIndex):\n        self.base_path = base_path\n        self.index = index\n        self.current_time: Optional[datetime] = None\n        self.paused = False\n        self.speed = 1.0\n    \n    async def replay(\n        self,\n        config: ReplayConfig,\n        callback\n    ) -> AsyncIterator[dict]:\n        \"\"\"Main replay loop\"\"\"\n        # Load tick data for all symbols\n        tick_iterators = []\n        \n        for symbol in config.symbols:\n            # Find starting position\n            file_path, offset = self.index.find_position(symbol, config.start_time)\n            \n            # Read parquet file\n            df = pd.read_parquet(file_path)\n            \n            # Filter to time range\n            df = df[\n                (df[\'timestamp\'] >= config.start_time) &\n                (df[\'timestamp\'] <= config.end_time)\n            ]\n            \n            # Apply sampling\n            if config.sample_rate > 1:\n                df = df.iloc[::config.sample_rate]\n            \n            tick_iterators.append((\n                symbol,\n                df.iterrows()\n            ))\n        \n        # Merge iterators in timestamp order\n        async for tick in self._merge_iterators(tick_iterators, config):\n            # Handle pause\n            while self.paused:\n                await asyncio.sleep(0.1)\n            \n            # Timing control\n            await self._timing_delay(tick[\'timestamp\'], config.speed_multiplier)\n            \n            # Update current time\n            self.current_time = tick[\'timestamp\']\n            \n            # Yield tick to callback\n            await callback(tick)\n    \n    async def _merge_iterators(\n        self,\n        iterators: List[Tuple[str, any]],\n        config: ReplayConfig\n    ) -> AsyncIterator[dict]:\n        \"\"\"Merge multiple tick iterators in timestamp order\"\"\"\n        import heapq\n        \n        # Initialize heap with first tick from each iterator\n        heap = []\n        \n        for symbol, iterator in iterators:\n            try:\n                idx, row = next(iterator)\n                heapq.heappush(heap, (\n                    row[\'timestamp\'],\n                    symbol,\n                    row.to_dict(),\n                    iterator\n                ))\n            except StopIteration:\n                pass\n        \n        # Pop ticks in order\n        while heap:\n            timestamp, symbol, tick, iterator = heapq.heappop(heap)\n            \n            yield tick\n            \n            # Add next tick from this iterator\n            try:\n                idx, row = next(iterator)\n                heapq.heappush(heap, (\n                    row[\'timestamp\'],\n                    symbol,\n                    row.to_dict(),\n                    iterator\n                ))\n            except StopIteration:\n                pass\n    \n    async def _timing_delay(self, tick_timestamp: datetime, speed: float):\n        \"\"\"Calculate and apply delay to maintain replay speed\"\"\"\n        if not hasattr(self, \'replay_start_real\'):\n            # First tick\n            self.replay_start_real = time.time()\n            self.replay_start_sim = tick_timestamp\n            return\n        \n        # Calculate elapsed time in simulation\n        sim_elapsed = (tick_timestamp - self.replay_start_sim).total_seconds()\n        \n        # Calculate elapsed time in real world\n        real_elapsed = time.time() - self.replay_start_real\n        \n        # Calculate how long this tick should have taken at replay speed\n        expected_elapsed = sim_elapsed / speed\n        \n        # Sleep if we\'re ahead\n        if real_elapsed < expected_elapsed:\n            delay = expected_elapsed - real_elapsed\n            await asyncio.sleep(delay)\n    \n    async def seek(self, target_timestamp: datetime):\n        \"\"\"Time travel to specific timestamp\"\"\"\n        # Reset replay state\n        self.current_time = target_timestamp\n        delattr(self, \'replay_start_real\')  # Force recalibration\n        \n        # Replay will restart from this timestamp\n    \n    def set_speed(self, speed: float):\n        \"\"\"Change replay speed (1× = real-time)\"\"\"\n        self.speed = speed\n        # Recalibrate timing\n        self.replay_start_real = time.time()\n        self.replay_start_sim = self.current_time\n    \n    def pause(self):\n        self.paused = True\n    \n    def resume(self):\n        self.paused = False\n        # Recalibrate timing\n        self.replay_start_real = time.time()\n        self.replay_start_sim = self.current_time\n```\n\n**Session Manager** (for concurrent backtests):\n```python\nclass SessionManager:\n    def __init__(self, base_path: str, index: TickIndex, max_sessions: int = 10):\n        self.base_path = base_path\n        self.index = index\n        self.max_sessions = max_sessions\n        self.sessions: Dict[str, ReplayEngine] = {}\n    \n    def create_session(self, session_id: str, config: ReplayConfig) -> ReplayEngine:\n        \"\"\"Create new replay session\"\"\"\n        if len(self.sessions) >= self.max_sessions:\n            raise Exception("Max sessions reached")\n        \n        engine = ReplayEngine(self.base_path, self.index)\n        self.sessions[session_id] = engine\n        \n        return engine\n    \n    def get_session(self, session_id: str) -> ReplayEngine:\n        return self.sessions.get(session_id)\n    \n    def close_session(self, session_id: str):\n        if session_id in self.sessions:\n            del self.sessions[session_id]\n```\n\n**Usage Example**:\n```python\nasync def run_backtest():\n    # Build index\n    index = TickIndex()\n    index.build_index(\n        base_path="/data/ticks",\n        date=date(2024, 1, 15),\n        symbols=[\"AAPL\", "MSFT\", \"GOOGL\"]\n    )\n    \n    # Create session\n    manager = SessionManager("/data/ticks", index)\n    engine = manager.create_session("backtest-001", ReplayConfig(\n        symbols=[\"AAPL\", \"MSFT\"],\n        start_time=datetime(2024, 1, 15, 9, 30),\n        end_time=datetime(2024, 1, 15, 16, 0),\n        speed_multiplier=10.0,  # 10× faster than real-time\n        sample_rate=1  # All ticks\n    ))\n    \n    # Define callback\n    tick_count = 0\n    async def handle_tick(tick: dict):\n        nonlocal tick_count\n        tick_count += 1\n        print(f"[{tick[\'timestamp\']}] {tick[\'symbol\']}: {tick[\'bid_price\']} x {tick[\'ask_price\']}")\n    \n    # Run replay\n    await engine.replay(config, handle_tick)\n    \n    print(f"Replayed {tick_count} ticks")\n\nasyncio.run(run_backtest())\n```\n\n**Performance Characteristics**:\n- Index build time: ~1 second per GB of parquet files\n- Random seek: < 10ms (with index)\n- Replay throughput: 100K-1M ticks/sec (single core)\n- Memory usage: ~100-500 MB per session\n- Storage: Parquet achieves 5-10× compression vs CSV\n\n**Production Optimizations**:\n- Use memory-mapped I/O for large files\n- Cache hot partitions (e.g., last 30 days) in RAM\n- Implement query result caching (repeated backtests)\n- Partition by hour for intraday strategies (faster seeks)\n- Use Apache Arrow for zero-copy data sharing across sessions\n- Implement snapshot/checkpoint system (resume backtests)\n\nThis design provides efficient, flexible market data replay with precise timing control and support for large-scale backtesting operations.',
    keyPoints: [
      'Parquet storage: Partitioned by date/symbol, row groups = 100K ticks, enables fast random access and predicate pushdown',
      'Index service: Maps timestamps to file offsets using parquet metadata, binary search for O(log n) seeks',
      'Replay timing: Track real_elapsed vs sim_elapsed, sleep if ahead to maintain speed (1× = real-time, 10× = 10× faster)',
      'Merge iterators: Use heap to merge multiple symbol streams in timestamp order, maintains exact tick sequence',
      'Session isolation: Multiple concurrent backtests with independent replay engines, LRU cache for shared data access',
    ],
  },
  {
    id: 'market-data-fundamentals-q-3',
    question:
      'Design a market data cost optimization system for a trading firm that consumes data from multiple vendors (Bloomberg, Refinitiv, IEX, Polygon). The system must: (1) Determine which symbols to subscribe to from which vendors (minimize cost while meeting latency requirements), (2) Implement smart routing (use cheap source if sufficient, upgrade to expensive source if needed), (3) Track usage and costs per strategy, (4) Automatically unsubscribe from unused symbols after inactivity threshold, (5) Support "shared subscriptions" (multiple strategies sharing one symbol subscription), (6) Generate cost reports and optimization recommendations. Provide the architecture, cost model, smart routing algorithm, and Python implementation for subscription management and cost tracking.',
    sampleAnswer:
      'Market data cost optimization system design:\n\n**Architecture Overview**:\n\nComponents:\n1. Subscription Manager: Tracks all active subscriptions and strategies\n2. Cost Model: Pricing for each vendor (per-symbol, volume tiers)\n3. Router: Determines optimal source for each symbol/strategy\n4. Usage Tracker: Records data consumption per strategy\n5. Cleanup Service: Auto-unsubscribe inactive symbols\n6. Cost Analytics: Reports, trends, optimization suggestions\n\n**Cost Model**:\n```python\nfrom dataclasses import dataclass\nfrom enum import Enum\nfrom typing import Dict\n\nclass DataVendor(Enum):\n    BLOOMBERG = "bloomberg"\n    REFINITIV = "refinitiv"\n    IEX = "iex"\n    POLYGON = "polygon"\n\nclass LatencyTier(Enum):\n    ULTRA_LOW = "ultra_low"  # < 1ms\n    LOW = "low"  # < 10ms\n    MEDIUM = "medium"  # < 100ms\n    HIGH = "high"  # < 1s\n\n@dataclass\nclass VendorPricing:\n    vendor: DataVendor\n    base_fee_monthly: float  # Fixed fee\n    per_symbol_monthly: float  # Per symbol cost\n    volume_tiers: Dict[int, float]  # {symbols: discount_multiplier}\n    latency_tier: LatencyTier\n    real_time: bool\n    \n# Example pricing (simplified)\nPRICING_MODEL = {\n    DataVendor.BLOOMBERG: VendorPricing(\n        vendor=DataVendor.BLOOMBERG,\n        base_fee_monthly=2000,  # Terminal fee\n        per_symbol_monthly=50,  # Expensive per symbol\n        volume_tiers={100: 0.9, 500: 0.8, 1000: 0.7},\n        latency_tier=LatencyTier.LOW,\n        real_time=True\n    ),\n    DataVendor.REFINITIV: VendorPricing(\n        vendor=DataVendor.REFINITIV,\n        base_fee_monthly=1500,\n        per_symbol_monthly=30,\n        volume_tiers={100: 0.9, 500: 0.85, 1000: 0.75},\n        latency_tier=LatencyTier.LOW,\n        real_time=True\n    ),\n    DataVendor.IEX: VendorPricing(\n        vendor=DataVendor.IEX,\n        base_fee_monthly=0,  # Free!\n        per_symbol_monthly=0,\n        volume_tiers={},\n        latency_tier=LatencyTier.MEDIUM,\n        real_time=True\n    ),\n    DataVendor.POLYGON: VendorPricing(\n        vendor=DataVendor.POLYGON,\n        base_fee_monthly=200,\n        per_symbol_monthly=2,  # Cheap\n        volume_tiers={1000: 0.8, 5000: 0.6},\n        latency_tier=LatencyTier.MEDIUM,\n        real_time=True\n    )\n}\n\ndef calculate_cost(vendor: DataVendor, num_symbols: int) -> float:\n    \"\"\"Calculate monthly cost for vendor\"\"\"\n    pricing = PRICING_MODEL[vendor]\n    \n    # Base fee\n    cost = pricing.base_fee_monthly\n    \n    # Per-symbol cost with volume discount\n    discount = 1.0\n    for tier_size, tier_discount in sorted(pricing.volume_tiers.items()):\n        if num_symbols >= tier_size:\n            discount = tier_discount\n    \n    cost += num_symbols * pricing.per_symbol_monthly * discount\n    \n    return cost\n```\n\n**Smart Routing Algorithm**:\n```python\nfrom typing import List, Optional\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n@dataclass\nclass StrategyRequirements:\n    strategy_id: str\n    symbols: List[str]\n    latency_requirement: LatencyTier\n    real_time_required: bool\n    budget_per_symbol: float  # Max monthly cost per symbol\n\nclass SmartRouter:\n    def __init__(self):\n        self.vendor_priorities = self._calculate_priorities()\n    \n    def _calculate_priorities(self) -> Dict[LatencyTier, List[DataVendor]]:\n        \"\"\"Calculate vendor priority by latency tier (cheapest first)\"\"\"\n        return {\n            LatencyTier.ULTRA_LOW: [\n                DataVendor.BLOOMBERG,\n                DataVendor.REFINITIV\n            ],\n            LatencyTier.LOW: [\n                DataVendor.BLOOMBERG,\n                DataVendor.REFINITIV\n            ],\n            LatencyTier.MEDIUM: [\n                DataVendor.IEX,  # Free!\n                DataVendor.POLYGON,\n                DataVendor.REFINITIV,\n                DataVendor.BLOOMBERG\n            ],\n            LatencyTier.HIGH: [\n                DataVendor.IEX,\n                DataVendor.POLYGON,\n                DataVendor.REFINITIV,\n                DataVendor.BLOOMBERG\n            ]\n        }\n    \n    def route(self, requirements: StrategyRequirements) -> Dict[str, DataVendor]:\n        \"\"\"Determine optimal vendor for each symbol\"\"\"\n        routing = {}\n        \n        # Get candidate vendors that meet requirements\n        candidates = self._get_candidates(\n            requirements.latency_requirement,\n            requirements.real_time_required\n        )\n        \n        for symbol in requirements.symbols:\n            # Find cheapest vendor that meets requirements\n            selected_vendor = None\n            \n            for vendor in candidates:\n                pricing = PRICING_MODEL[vendor]\n                cost = pricing.per_symbol_monthly\n                \n                if cost <= requirements.budget_per_symbol:\n                    selected_vendor = vendor\n                    break\n            \n            if not selected_vendor:\n                # No vendor within budget, use cheapest\n                selected_vendor = candidates[-1]\n                logger.warning(\n                    f"Symbol {symbol} exceeds budget: "\n                    f"using {selected_vendor} at \${PRICING_MODEL[selected_vendor].per_symbol_monthly}/mo"\n                )\n            \n            routing[symbol] = selected_vendor\n        \n        return routing\n    \n    def _get_candidates(\n        self,\n        latency_tier: LatencyTier,\n        real_time: bool\n    ) -> List[DataVendor]:\n        \"\"\"Get candidate vendors that meet requirements\"\"\"\n        candidates = self.vendor_priorities.get(latency_tier, [])\n        \n        if real_time:\n            candidates = [\n                v for v in candidates\n                if PRICING_MODEL[v].real_time\n            ]\n        \n        return candidates\n```\n\n**Subscription Manager**:\n```python\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nclass SubscriptionManager:\n    def __init__(self, router: SmartRouter):\n        self.router = router\n        \n        # Track subscriptions: vendor -> {symbols}\n        self.active_subs: Dict[DataVendor, set] = defaultdict(set)\n        \n        # Track which strategies use which symbols\n        self.symbol_strategies: Dict[str, set] = defaultdict(set)\n        \n        # Track last activity per symbol\n        self.last_activity: Dict[str, datetime] = {}\n        \n        # Inactive threshold\n        self.inactive_threshold = timedelta(hours=24)\n    \n    def subscribe_strategy(self, requirements: StrategyRequirements):\n        \"\"\"Subscribe symbols for strategy\"\"\"\n        # Route symbols to vendors\n        routing = self.router.route(requirements)\n        \n        for symbol, vendor in routing.items():\n            # Add subscription if not already active\n            if symbol not in self.active_subs[vendor]:\n                self._add_subscription(vendor, symbol)\n            \n            # Track strategy usage\n            self.symbol_strategies[symbol].add(requirements.strategy_id)\n            \n            # Update activity\n            self.last_activity[symbol] = datetime.utcnow()\n        \n        logger.info(\n            f"Strategy {requirements.strategy_id} subscribed to "\n            f"{len(requirements.symbols)} symbols"\n        )\n    \n    def _add_subscription(self, vendor: DataVendor, symbol: str):\n        \"\"\"Add subscription to vendor\"\"\"\n        self.active_subs[vendor].add(symbol)\n        logger.info(f"Added subscription: {symbol} from {vendor}")\n        # In production: call vendor API to actually subscribe\n    \n    def unsubscribe_strategy(self, strategy_id: str):\n        \"\"\"Unsubscribe all symbols for strategy\"\"\"\n        # Find symbols used by this strategy\n        symbols_to_check = [\n            symbol for symbol, strategies in self.symbol_strategies.items()\n            if strategy_id in strategies\n        ]\n        \n        for symbol in symbols_to_check:\n            # Remove strategy from symbol\n            self.symbol_strategies[symbol].discard(strategy_id)\n            \n            # If no strategies use this symbol, unsubscribe\n            if not self.symbol_strategies[symbol]:\n                self._remove_subscription(symbol)\n        \n        logger.info(f"Strategy {strategy_id} unsubscribed")\n    \n    def _remove_subscription(self, symbol: str):\n        \"\"\"Remove subscription for symbol from all vendors\"\"\"\n        for vendor, symbols in self.active_subs.items():\n            if symbol in symbols:\n                symbols.discard(symbol)\n                logger.info(f"Removed subscription: {symbol} from {vendor}")\n                # In production: call vendor API to unsubscribe\n        \n        # Cleanup\n        if symbol in self.last_activity:\n            del self.last_activity[symbol]\n        if symbol in self.symbol_strategies:\n            del self.symbol_strategies[symbol]\n    \n    def record_activity(self, symbol: str):\n        \"\"\"Record activity for symbol (data received)\"\"\"\n        self.last_activity[symbol] = datetime.utcnow()\n    \n    async def cleanup_inactive(self):\n        \"\"\"Auto-unsubscribe inactive symbols\"\"\"\n        while True:\n            await asyncio.sleep(3600)  # Check hourly\n            \n            now = datetime.utcnow()\n            inactive_symbols = []\n            \n            for symbol, last_time in self.last_activity.items():\n                if now - last_time > self.inactive_threshold:\n                    inactive_symbols.append(symbol)\n            \n            for symbol in inactive_symbols:\n                logger.info(f"Auto-unsubscribing inactive symbol: {symbol}")\n                self._remove_subscription(symbol)\n    \n    def get_total_cost(self) -> Dict[DataVendor, float]:\n        \"\"\"Calculate current monthly costs by vendor\"\"\"\n        costs = {}\n        \n        for vendor, symbols in self.active_subs.items():\n            if symbols:\n                costs[vendor] = calculate_cost(vendor, len(symbols))\n        \n        return costs\n```\n\n**Usage Tracker**:\n```python\nfrom collections import Counter\n\nclass UsageTracker:\n    def __init__(self):\n        # Track messages received per strategy/symbol\n        self.message_counts: Dict[str, Counter] = defaultdict(Counter)  # strategy_id -> {symbol: count}\n        self.costs: Dict[str, float] = {}  # strategy_id -> allocated_cost\n    \n    def record_message(self, strategy_id: str, symbol: str):\n        \"\"\"Record that strategy received message for symbol\"\"\"\n        self.message_counts[strategy_id][symbol] += 1\n    \n    def allocate_costs(\n        self,\n        sub_manager: SubscriptionManager\n    ) -> Dict[str, float]:\n        \"\"\"Allocate vendor costs to strategies based on usage\"\"\"\n        # Get total costs by vendor\n        vendor_costs = sub_manager.get_total_cost()\n        \n        # Calculate cost per symbol (shared across strategies)\n        symbol_costs = {}\n        \n        for vendor, symbols in sub_manager.active_subs.items():\n            if not symbols:\n                continue\n            \n            cost = vendor_costs[vendor]\n            cost_per_symbol = cost / len(symbols)\n            \n            for symbol in symbols:\n                symbol_costs[symbol] = cost_per_symbol\n        \n        # Allocate to strategies based on usage\n        strategy_costs = defaultdict(float)\n        \n        for strategy_id, symbol_counts in self.message_counts.items():\n            for symbol, count in symbol_counts.items():\n                if symbol in symbol_costs:\n                    # Split cost proportionally\n                    num_strategies_using = len(sub_manager.symbol_strategies.get(symbol, set()))\n                    if num_strategies_using > 0:\n                        allocated_cost = symbol_costs[symbol] / num_strategies_using\n                        strategy_costs[strategy_id] += allocated_cost\n        \n        self.costs = dict(strategy_costs)\n        return self.costs\n```\n\n**Cost Analytics**:\n```python\nclass CostAnalytics:\n    def generate_report(self, sub_manager: SubscriptionManager, tracker: UsageTracker) -> str:\n        \"\"\"Generate cost optimization report\"\"\"\n        report = []\n        report.append("=== Market Data Cost Report ===")\n        report.append("")\n        \n        # Current costs\n        vendor_costs = sub_manager.get_total_cost()\n        total_cost = sum(vendor_costs.values())\n        \n        report.append(f"Total Monthly Cost: \${total_cost:,.2f}")\n        report.append("")\n        report.append("By Vendor:")\n        for vendor, cost in vendor_costs.items():\n            num_symbols = len(sub_manager.active_subs[vendor])\n            report.append(f"  {vendor.value}: \${cost:,.2f} ({num_symbols} symbols)")\n        \n        report.append("")\n        \n        # By strategy\n        strategy_costs = tracker.allocate_costs(sub_manager)\n        report.append("By Strategy:")\n        for strategy_id, cost in sorted(strategy_costs.items(), key=lambda x: x[1], reverse=True):\n            report.append(f"  {strategy_id}: \${cost:,.2f}")\n        \n        report.append("")\n        \n        # Optimization suggestions\n        suggestions = self._generate_suggestions(sub_manager)\n        if suggestions:\n            report.append("Optimization Suggestions:")\n            for suggestion in suggestions:\n                report.append(f"  • {suggestion}")\n        \n        return "\\n".join(report)\n    \n    def _generate_suggestions(self, sub_manager: SubscriptionManager) -> List[str]:\n        \"\"\"Generate cost optimization suggestions\"\"\"\n        suggestions = []\n        \n        # Check for symbols on expensive vendors that could move to cheap ones\n        for vendor, symbols in sub_manager.active_subs.items():\n            if vendor in [DataVendor.BLOOMBERG, DataVendor.REFINITIV]:\n                # Check if strategies using these symbols need high latency\n                for symbol in symbols:\n                    strategies = sub_manager.symbol_strategies.get(symbol, set())\n                    # In production: check if all strategies can tolerate higher latency\n                    suggestions.append(\n                        f"Consider moving {symbol} from {vendor.value} to IEX/Polygon (potential savings: \${PRICING_MODEL[vendor].per_symbol_monthly - PRICING_MODEL[DataVendor.POLYGON].per_symbol_monthly:.2f}/mo)"\n                    )\n        \n        return suggestions\n```\n\n**Complete Usage Example**:\n```python\nasync def main():\n    # Initialize components\n    router = SmartRouter()\n    sub_manager = SubscriptionManager(router)\n    tracker = UsageTracker()\n    analytics = CostAnalytics()\n    \n    # Subscribe strategies\n    sub_manager.subscribe_strategy(StrategyRequirements(\n        strategy_id="momentum-strategy",\n        symbols=["AAPL", "MSFT", "GOOGL"],\n        latency_requirement=LatencyTier.LOW,\n        real_time_required=True,\n        budget_per_symbol=100\n    ))\n    \n    sub_manager.subscribe_strategy(StrategyRequirements(\n        strategy_id="mean-reversion",\n        symbols=["AAPL", "TSLA", "NVDA"],  # AAPL shared!\n        latency_requirement=LatencyTier.MEDIUM,\n        real_time_required=True,\n        budget_per_symbol=10\n    ))\n    \n    # Start cleanup task\n    asyncio.create_task(sub_manager.cleanup_inactive())\n    \n    # Simulate data consumption\n    tracker.record_message("momentum-strategy", "AAPL")\n    tracker.record_message("mean-reversion", "AAPL")\n    \n    # Generate report\n    report = analytics.generate_report(sub_manager, tracker)\n    print(report)\n\nasyncio.run(main())\n```\n\n**Expected Savings**:\n- Shared subscriptions: Save 50% on overlapping symbols\n- Smart routing: Use IEX (free) for medium-latency needs instead of Bloomberg ($50/symbol/mo)\n- Auto-cleanup: Eliminate $2K-5K/mo in unused subscriptions\n- Total potential savings: 30-60% of market data budget\n\nThis system enables data-driven cost optimization while maintaining performance requirements for all trading strategies.',
    keyPoints: [
      'Smart routing: Match strategy latency/budget requirements to cheapest vendor (IEX free, Polygon $2/mo, Bloomberg $50/mo)',
      'Shared subscriptions: Multiple strategies share symbol subscriptions, split costs proportionally based on usage',
      'Auto-cleanup: Unsubscribe symbols with no activity for 24 hours, eliminates waste',
      'Cost allocation: Track messages per strategy/symbol, allocate vendor costs proportionally to actual usage',
      'Optimization suggestions: Identify expensive subscriptions that could move to cheaper vendors (e.g., Bloomberg → IEX for medium latency)',
    ],
  },
];
