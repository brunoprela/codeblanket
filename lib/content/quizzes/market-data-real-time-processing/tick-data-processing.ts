export const tickDataProcessingQuiz = [
  {
    id: 'tick-data-processing-q-1',
    question:
      'Design a high-performance tick data storage and retrieval system that: (1) Ingests 1M ticks/second from multiple exchanges, (2) Stores ticks with microsecond timestamp precision for 5 years, (3) Supports fast queries (< 100ms) for arbitrary date ranges and symbols, (4) Achieves 10:1 compression ratio, (5) Handles concurrent reads while writing, (6) Costs < $10K/month for storage. Provide architecture, storage format, compression strategy, query optimization techniques, and estimated costs.',
    sampleAnswer:
      'High-performance tick storage system:\n\n**Architecture**: Hybrid hot/warm/cold storage with TimescaleDB for recent data (30 days), compressed Parquet in S3 for historical (5 years), Redis cache for hot queries.\n\n**Storage Format**: Columnar Parquet with schema: [timestamp BIGINT, symbol STRING, bid_price DECIMAL, bid_size INT, ask_price DECIMAL, ask_size INT, exchange STRING]. Partitioned by date and symbol (e.g., /2024-01-15/AAPL.parquet). Each file = 1 day × 1 symbol ≈ 50K ticks = 5MB uncompressed.\n\n**Compression Strategy**: (1) Delta encoding for prices (store differences), (2) Dictionary encoding for symbols/exchanges, (3) Run-length encoding for repeated sizes, (4) Snappy compression (3× faster than gzip, 90% of compression). Achieves 10:1 ratio: 100 bytes/tick → 10 bytes compressed.\n\n**Data Flow**: Ticks → Kafka (buffer) → TimescaleDB (hot) → Daily batch job → Parquet in S3 (cold) → Delete from TimescaleDB after 30 days.\n\n**Query Optimization**: (1) Partition pruning (only scan relevant date/symbol files), (2) Predicate pushdown (filter at read time), (3) Columnar format (read only needed columns), (4) Bloom filters (skip files without symbol), (5) Query result cache in Redis (1 hour TTL).\n\n**Concurrent Access**: TimescaleDB handles concurrent reads/writes natively. S3 is read-only (immutable files), unlimited concurrent reads. Write isolation: New ticks → new partitions, never modify existing.\n\n**Cost Estimation**: Storage: 1M ticks/sec × 86400 sec/day × 365 days × 5 years × 10 bytes (compressed) = 1.4 PB. S3 Standard-IA: $0.0125/GB/mo = $18K/mo (too high!). Solution: Use S3 Glacier Deep Archive for data > 1 year old: $0.00099/GB/mo = $1.4K/mo. Hot storage (30 days): 1M × 86400 × 30 × 10 = 26 TB. TimescaleDB on RDS: 26TB = $3K/mo. Total: $1.4K (cold) + $3K (hot) + $1K (compute) = $5.4K/mo ✅ under $10K. Further optimization: Sample ticks (store every 10th tick for data > 1 year) reduces to $540/mo.\n\n**Query Performance**: Hot queries (last 30 days): TimescaleDB < 50ms. Cold queries (1+ years): S3 Select + Parquet = 1-5 seconds for 1 day of data. Parallel queries across dates: 10 days in 5 seconds.\n\n**Python Implementation**: Use pyarrow for Parquet I/O, dask for parallel processing, boto3 for S3, psycopg2 for TimescaleDB. Batch writes every 10K ticks or 1 second.',
    keyPoints: [
      'Hybrid storage: TimescaleDB (30 days hot), Parquet in S3 (5 years cold), Redis cache for queries',
      'Compression: Delta encoding + dictionary + RLE + Snappy achieves 10:1 ratio (100 bytes → 10 bytes/tick)',
      'Partitioning: By date and symbol for efficient pruning, columnar Parquet for fast column-only reads',
      'Cost: S3 Glacier Deep Archive ($0.00099/GB/mo) for old data = $1.4K/mo for 1.4 PB, total $5.4K/mo',
      'Query optimization: Partition pruning, predicate pushdown, bloom filters, Redis cache (< 100ms for hot data)',
    ],
  },
  {
    id: 'tick-data-processing-q-2',
    question:
      'You are processing tick data from 3 exchanges (NASDAQ, NYSE, BATS) for the same stock (AAPL). Ticks arrive out-of-order due to network delays. Design a tick consolidation system that: (1) Merges ticks from all exchanges into single unified stream, (2) Reorders ticks by exchange timestamp, (3) Detects conflicting quotes (e.g., NASDAQ bid > NYSE ask), (4) Calculates National Best Bid/Offer (NBBO), (5) Handles 10K ticks/sec with < 1ms consolidation latency. Provide algorithm, data structures, and implementation.',
    sampleAnswer:
      "Multi-exchange tick consolidation system:\n\n**Architecture**: Per-exchange ingestion threads → Timestamp-based merge → Conflict detection → NBBO calculation → Unified output stream.\n\n**Algorithm (Time-Ordered Merge)**:\n1. Maintain min-heap with next tick from each exchange (keyed by timestamp)\n2. Pop earliest tick from heap\n3. Add to NBBO calculator\n4. Push next tick from that exchange onto heap\n5. Repeat\n\nThis ensures global timestamp order across exchanges.\n\n**Data Structures**:\n```python\nimport heapq\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom decimal import Decimal\nfrom typing import Dict, List, Optional\n\n@dataclass\nclass ExchangeTick:\n    exchange: str\n    timestamp: datetime\n    bid_price: Decimal\n    bid_size: int\n    ask_price: Decimal\n    ask_size: int\n\nclass NBBOCalculator:\n    def __init__(self):\n        # Best quotes per exchange\n        self.exchange_quotes: Dict[str, ExchangeTick] = {}\n        self.current_nbbo: Optional[tuple] = None\n    \n    def update(self, tick: ExchangeTick) -> Optional[tuple]:\n        # Store this exchange's quote\n        self.exchange_quotes[tick.exchange] = tick\n        \n        # Find best bid across all exchanges (highest)\n        best_bid = Decimal('0')\n        best_bid_exchange = None\n        for exchange, quote in self.exchange_quotes.items():\n            if quote.bid_price > best_bid:\n                best_bid = quote.bid_price\n                best_bid_exchange = exchange\n        \n        # Find best ask across all exchanges (lowest)\n        best_ask = Decimal('999999')\n        best_ask_exchange = None\n        for exchange, quote in self.exchange_quotes.items():\n            if quote.ask_price < best_ask:\n                best_ask = quote.ask_price\n                best_ask_exchange = exchange\n        \n        # Check for locked/crossed market\n        if best_bid >= best_ask:\n            print(f\"ALERT: Crossed market! Bid {best_bid} >= Ask {best_ask}\")\n        \n        # Create NBBO\n        nbbo = (best_bid, best_ask, best_bid_exchange, best_ask_exchange)\n        \n        # Only emit if changed\n        if nbbo != self.current_nbbo:\n            self.current_nbbo = nbbo\n            return nbbo\n        \n        return None\n\nclass TickConsolidator:\n    def __init__(self, exchanges: List[str], max_delay_ms: int = 100):\n        self.exchanges = exchanges\n        self.max_delay_ms = max_delay_ms\n        \n        # Min-heap for time-ordered merge\n        self.heap = []\n        \n        # NBBO calculator\n        self.nbbo_calc = NBBOCalculator()\n        \n        # Stats\n        self.ticks_processed = 0\n        self.nbbo_updates = 0\n        self.conflicts_detected = 0\n    \n    def add_tick(self, tick: ExchangeTick) -> List[dict]:\n        # Add to heap (sorted by timestamp)\n        heapq.heappush(self.heap, (tick.timestamp, tick))\n        \n        # Determine watermark (latest tick - max_delay)\n        if len(self.heap) > 0:\n            latest = max(t[0] for t in self.heap)\n            watermark = latest - timedelta(milliseconds=self.max_delay_ms)\n        else:\n            watermark = tick.timestamp\n        \n        # Emit ticks before watermark\n        results = []\n        while self.heap and self.heap[0][0] <= watermark:\n            timestamp, tick = heapq.heappop(self.heap)\n            \n            # Update NBBO\n            nbbo = self.nbbo_calc.update(tick)\n            \n            if nbbo:\n                self.nbbo_updates += 1\n                results.append({\n                    'timestamp': timestamp,\n                    'nbbo_bid': nbbo[0],\n                    'nbbo_ask': nbbo[1],\n                    'bid_exchange': nbbo[2],\n                    'ask_exchange': nbbo[3],\n                    'tick': tick\n                })\n            \n            self.ticks_processed += 1\n        \n        return results\n    \n    def flush(self) -> List[dict]:\n        \"\"\"Flush remaining ticks\"\"\"\n        results = []\n        while self.heap:\n            timestamp, tick = heapq.heappop(self.heap)\n            nbbo = self.nbbo_calc.update(tick)\n            if nbbo:\n                results.append({...})  # Same as above\n        return results\n```\n\n**Conflict Detection**:\n```python\ndef detect_conflicts(self, quotes: Dict[str, ExchangeTick]) -> List[str]:\n    conflicts = []\n    \n    # Check for crossed markets (bid >= ask on single exchange)\n    for exchange, quote in quotes.items():\n        if quote.bid_price >= quote.ask_price:\n            conflicts.append(\n                f\"Crossed market on {exchange}: \"\n                f\"bid {quote.bid_price} >= ask {quote.ask_price}\"\n            )\n    \n    # Check for locked markets (one exchange bid = another's ask)\n    exchanges = list(quotes.keys())\n    for i in range(len(exchanges)):\n        for j in range(i+1, len(exchanges)):\n            ex1, ex2 = exchanges[i], exchanges[j]\n            if quotes[ex1].bid_price == quotes[ex2].ask_price:\n                conflicts.append(\n                    f\"Locked market: {ex1} bid = {ex2} ask at {quotes[ex1].bid_price}\"\n                )\n            if quotes[ex1].ask_price == quotes[ex2].bid_price:\n                conflicts.append(\n                    f\"Locked market: {ex1} ask = {ex2} bid at {quotes[ex1].ask_price}\"\n                )\n    \n    # Check for inverted markets (one exchange bid > another's ask)\n    for i in range(len(exchanges)):\n        for j in range(i+1, len(exchanges)):\n            ex1, ex2 = exchanges[i], exchanges[j]\n            if quotes[ex1].bid_price > quotes[ex2].ask_price:\n                conflicts.append(\n                    f\"Inverted market: {ex1} bid {quotes[ex1].bid_price} > \"\n                    f\"{ex2} ask {quotes[ex2].ask_price}\"\n                )\n    \n    return conflicts\n```\n\n**Performance Optimization**:\n- Heap operations: O(log N) where N = exchanges × buffer size\n- With 3 exchanges and 100ms buffer @ 10K ticks/sec: N = 3 × 1000 = 3000 ticks buffered\n- Heap push/pop: ~15 microseconds per operation\n- NBBO calculation: O(E) where E = exchanges = 3, ~5 microseconds\n- Total latency: 20 microseconds per tick ✅ << 1ms requirement\n\n**Usage Example**:\n```python\nconsolidator = TickConsolidator(\n    exchanges=['NASDAQ', 'NYSE', 'BATS'],\n    max_delay_ms=100\n)\n\n# Simulate ticks from different exchanges\nticks = [\n    ExchangeTick('NASDAQ', datetime(2024,1,15,9,30,0,100000), \n                 Decimal('150.25'), 500, Decimal('150.26'), 300),\n    ExchangeTick('NYSE', datetime(2024,1,15,9,30,0,80000),  # Earlier!\n                 Decimal('150.24'), 600, Decimal('150.25'), 400),\n    ExchangeTick('BATS', datetime(2024,1,15,9,30,0,120000),\n                 Decimal('150.26'), 200, Decimal('150.27'), 250),\n]\n\nfor tick in ticks:\n    results = consolidator.add_tick(tick)\n    for result in results:\n        print(f\"NBBO: {result['nbbo_bid']} × {result['nbbo_ask']} \"\n              f\"({result['bid_exchange']} × {result['ask_exchange']})\")\n\n# Output (after watermark expires):\n# NBBO: 150.24 × 150.25 (NYSE × NASDAQ)\n# NBBO: 150.25 × 150.25 (NASDAQ × NASDAQ)  # Locked market!\n# NBBO: 150.25 × 150.26 (NASDAQ × NASDAQ)\n```\n\n**Handling 10K ticks/sec**:\n- Single-threaded: 20 μs/tick × 10K = 200ms CPU time/sec = 20% CPU ✅\n- Multi-threaded: 1 thread per exchange for ingestion, 1 for consolidation\n- Heap size limit: Prevent memory overflow if one exchange lags (max 10K buffered)\n\nThis design achieves < 1ms latency with robust conflict detection and NBBO calculation.",
    keyPoints: [
      'Time-ordered merge: Min-heap with ticks from all exchanges, pop earliest by timestamp for global ordering',
      'NBBO calculation: Track best quote per exchange, select highest bid and lowest ask across all exchanges',
      'Conflict detection: Check for crossed (bid >= ask), locked (bid = ask), inverted (exchange1 bid > exchange2 ask) markets',
      'Watermark: Only emit ticks older than (latest - 100ms) to allow reordering, prevents early emission',
      'Performance: Heap O(log N) + NBBO O(E) = 20 μs/tick, handles 10K/sec at 20% CPU utilization',
    ],
  },
  {
    id: 'tick-data-processing-q-3',
    question:
      'Design a tick data cleaning and validation pipeline that identifies and corrects: (1) Bad ticks (negative prices, zero sizes, inverted spreads), (2) Duplicate ticks (same timestamp and values), (3) Stale ticks (delayed > 1 second), (4) Spike ticks (price moves > 5% from moving average), (5) Flash crash ticks (temporary price dislocations). The system must process 100K ticks/sec, maintain < 10ms validation latency, and provide detailed quality reports. Implement validation rules, correction strategies, and quality metrics.',
    sampleAnswer:
      "Comprehensive tick data cleaning and validation pipeline:\n\n**Architecture**: Stream processing with multi-stage validation: Raw ticks → Basic validation → Statistical validation → Anomaly detection → Corrected ticks + Quality report.\n\n**Validation Stages**:\n\n**Stage 1: Basic Validation** (< 1 μs/tick)\n```python\nfrom decimal import Decimal\nfrom datetime import datetime, timedelta\n\nclass BasicValidator:\n    def validate(self, tick: QuoteTick) -> tuple[bool, str]:\n        # Check for negative prices\n        if tick.bid_price < 0 or tick.ask_price < 0:\n            return False, \"Negative price\"\n        \n        # Check for zero prices (invalid for most stocks)\n        if tick.bid_price == 0 and tick.ask_price == 0:\n            return False, \"Zero price\"\n        \n        # Check for zero sizes (invalid)\n        if tick.bid_size <= 0 or tick.ask_size <= 0:\n            return False, \"Invalid size\"\n        \n        # Check for inverted market (bid >= ask)\n        if tick.bid_price >= tick.ask_price:\n            return False, f\"Inverted: bid {tick.bid_price} >= ask {tick.ask_price}\"\n        \n        # Check for unrealistic prices (> $100K for most stocks)\n        if tick.ask_price > Decimal('100000'):\n            return False, \"Unrealistic price\"\n        \n        # Check for unrealistic spread (> 10%)\n        spread_pct = (tick.ask_price - tick.bid_price) / tick.ask_price\n        if spread_pct > Decimal('0.1'):\n            return False, f\"Wide spread: {spread_pct*100:.1f}%\"\n        \n        return True, \"OK\"\n```\n\n**Stage 2: Duplicate Detection** (< 2 μs/tick)\n```python\nfrom collections import deque\n\nclass DuplicateDetector:\n    def __init__(self, window_size: int = 1000):\n        # Rolling window of recent ticks (hash of key fields)\n        self.recent_hashes = deque(maxlen=window_size)\n    \n    def is_duplicate(self, tick: QuoteTick) -> bool:\n        # Create hash from key fields\n        tick_hash = hash((\n            tick.symbol,\n            tick.exchange_timestamp,\n            tick.bid_price,\n            tick.bid_size,\n            tick.ask_price,\n            tick.ask_size\n        ))\n        \n        if tick_hash in self.recent_hashes:\n            return True\n        \n        self.recent_hashes.append(tick_hash)\n        return False\n```\n\n**Stage 3: Staleness Check** (< 1 μs/tick)\n```python\nclass StalenessDetector:\n    def __init__(self, max_delay_seconds: float = 1.0):\n        self.max_delay = timedelta(seconds=max_delay_seconds)\n    \n    def is_stale(self, tick: QuoteTick) -> tuple[bool, float]:\n        delay = tick.receive_timestamp - tick.exchange_timestamp\n        delay_seconds = delay.total_seconds()\n        \n        if delay > self.max_delay:\n            return True, delay_seconds\n        \n        return False, delay_seconds\n```\n\n**Stage 4: Spike Detection** (< 5 μs/tick)\n```python\nimport numpy as np\n\nclass SpikeDetector:\n    def __init__(self, window_size: int = 100, threshold_pct: float = 0.05):\n        self.window_size = window_size\n        self.threshold = threshold_pct\n        \n        # Rolling window of recent prices\n        self.prices = deque(maxlen=window_size)\n    \n    def is_spike(self, tick: QuoteTick) -> tuple[bool, float]:\n        mid_price = float(tick.mid_price)\n        \n        if len(self.prices) < 10:\n            # Not enough history\n            self.prices.append(mid_price)\n            return False, 0.0\n        \n        # Calculate moving average\n        prices_array = np.array(self.prices)\n        mean_price = prices_array.mean()\n        \n        # Calculate deviation\n        deviation = abs(mid_price - mean_price) / mean_price\n        \n        is_spike = deviation > self.threshold\n        \n        # Add to window even if spike (for adaptation)\n        self.prices.append(mid_price)\n        \n        return is_spike, deviation\n```\n\n**Stage 5: Flash Crash Detection** (< 5 μs/tick)\n```python\nclass FlashCrashDetector:\n    def __init__(self, crash_threshold: float = 0.10, recovery_window: int = 100):\n        self.crash_threshold = crash_threshold\n        self.recovery_window = recovery_window\n        \n        self.baseline_price = None\n        self.crash_detected = False\n        self.recovery_counter = 0\n    \n    def detect(self, tick: QuoteTick) -> tuple[bool, str]:\n        mid_price = float(tick.mid_price)\n        \n        if self.baseline_price is None:\n            self.baseline_price = mid_price\n            return False, \"OK\"\n        \n        # Calculate drop from baseline\n        drop = (self.baseline_price - mid_price) / self.baseline_price\n        \n        if not self.crash_detected:\n            # Check for sudden drop\n            if drop > self.crash_threshold:\n                self.crash_detected = True\n                return True, f\"Flash crash: {drop*100:.1f}% drop\"\n            \n            # Update baseline gradually\n            self.baseline_price = 0.99 * self.baseline_price + 0.01 * mid_price\n        else:\n            # In crash mode - check for recovery\n            recovery = (mid_price - self.baseline_price) / self.baseline_price\n            \n            if abs(recovery) < 0.02:  # Within 2% of baseline\n                self.recovery_counter += 1\n                if self.recovery_counter >= self.recovery_window:\n                    # Recovered\n                    self.crash_detected = False\n                    self.recovery_counter = 0\n                    self.baseline_price = mid_price\n                    return False, \"Recovered from flash crash\"\n            else:\n                self.recovery_counter = 0\n            \n            return True, \"Flash crash ongoing\"\n        \n        return False, \"OK\"\n```\n\n**Complete Pipeline**:\n```python\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n@dataclass\nclass ValidationResult:\n    valid: bool\n    tick: QuoteTick\n    errors: list[str]\n    warnings: list[str]\n    corrected: bool = False\n    correction_applied: Optional[str] = None\n\nclass TickValidationPipeline:\n    def __init__(self):\n        self.basic_validator = BasicValidator()\n        self.dup_detector = DuplicateDetector()\n        self.stale_detector = StalenessDetector()\n        self.spike_detector = SpikeDetector()\n        self.crash_detector = FlashCrashDetector()\n        \n        # Quality metrics\n        self.metrics = {\n            'total_ticks': 0,\n            'valid_ticks': 0,\n            'basic_errors': 0,\n            'duplicates': 0,\n            'stale_ticks': 0,\n            'spikes': 0,\n            'flash_crashes': 0,\n            'corrected': 0\n        }\n    \n    def validate(self, tick: QuoteTick) -> ValidationResult:\n        self.metrics['total_ticks'] += 1\n        \n        errors = []\n        warnings = []\n        corrected = False\n        correction = None\n        \n        # Stage 1: Basic validation\n        valid, reason = self.basic_validator.validate(tick)\n        if not valid:\n            errors.append(f\"Basic: {reason}\")\n            self.metrics['basic_errors'] += 1\n            # Try to correct\n            if \"Inverted\" in reason:\n                # Swap bid and ask\n                tick.bid_price, tick.ask_price = tick.ask_price, tick.bid_price\n                tick.bid_size, tick.ask_size = tick.ask_size, tick.bid_size\n                corrected = True\n                correction = \"Swapped bid/ask\"\n                self.metrics['corrected'] += 1\n        \n        # Stage 2: Duplicate detection\n        if self.dup_detector.is_duplicate(tick):\n            warnings.append(\"Duplicate tick\")\n            self.metrics['duplicates'] += 1\n        \n        # Stage 3: Staleness\n        is_stale, delay = self.stale_detector.is_stale(tick)\n        if is_stale:\n            warnings.append(f\"Stale: {delay:.3f}s delay\")\n            self.metrics['stale_ticks'] += 1\n        \n        # Stage 4: Spike detection\n        is_spike, deviation = self.spike_detector.is_spike(tick)\n        if is_spike:\n            warnings.append(f\"Price spike: {deviation*100:.1f}% from average\")\n            self.metrics['spikes'] += 1\n        \n        # Stage 5: Flash crash\n        is_crash, crash_msg = self.crash_detector.detect(tick)\n        if is_crash:\n            warnings.append(f\"Flash crash: {crash_msg}\")\n            self.metrics['flash_crashes'] += 1\n        \n        # Determine overall validity\n        valid = len(errors) == 0\n        if valid:\n            self.metrics['valid_ticks'] += 1\n        \n        return ValidationResult(\n            valid=valid,\n            tick=tick,\n            errors=errors,\n            warnings=warnings,\n            corrected=corrected,\n            correction_applied=correction\n        )\n    \n    def get_quality_report(self) -> dict:\n        total = self.metrics['total_ticks']\n        if total == 0:\n            return {}\n        \n        return {\n            'total_ticks': total,\n            'valid_rate': self.metrics['valid_ticks'] / total,\n            'error_rate': self.metrics['basic_errors'] / total,\n            'duplicate_rate': self.metrics['duplicates'] / total,\n            'stale_rate': self.metrics['stale_ticks'] / total,\n            'spike_rate': self.metrics['spikes'] / total,\n            'flash_crash_rate': self.metrics['flash_crashes'] / total,\n            'correction_rate': self.metrics['corrected'] / total,\n            'metrics': self.metrics\n        }\n```\n\n**Performance Analysis**:\n- Stage 1 (Basic): 1 μs\n- Stage 2 (Duplicates): 2 μs\n- Stage 3 (Staleness): 1 μs\n- Stage 4 (Spikes): 5 μs\n- Stage 5 (Flash crash): 5 μs\n- Total: 14 μs per tick ✅ << 10ms\n\nAt 100K ticks/sec: 14 μs × 100K = 1.4 seconds of CPU time per second = 140% CPU (need 2 cores) ✅\n\n**Usage Example**:\n```python\npipeline = TickValidationPipeline()\n\nfor tick in tick_stream:\n    result = pipeline.validate(tick)\n    \n    if result.valid:\n        # Process clean tick\n        process_tick(result.tick)\n    else:\n        # Log error\n        logger.error(f\"Invalid tick: {result.errors}\")\n    \n    if result.warnings:\n        logger.warning(f\"Warnings: {result.warnings}\")\n\n# Print quality report every 10K ticks\nif pipeline.metrics['total_ticks'] % 10000 == 0:\n    report = pipeline.get_quality_report()\n    print(f\"Quality: {report['valid_rate']*100:.1f}% valid, \"\n          f\"{report['error_rate']*100:.2f}% errors\")\n```\n\n**Expected Quality Metrics** (real-world):\n- Valid rate: 98-99.5%\n- Error rate: 0.1-0.5%\n- Duplicate rate: 0.5-2%\n- Stale rate: 1-5% (depends on network)\n- Spike rate: 0.01-0.1%\n- Flash crash rate: < 0.001%\n\nThis pipeline provides production-grade tick cleaning with comprehensive validation, automatic correction, and detailed quality reporting.",
    keyPoints: [
      'Multi-stage validation: Basic (1μs) → Duplicates (2μs) → Staleness (1μs) → Spikes (5μs) → Flash crash (5μs) = 14μs total',
      'Basic validation: Check negative prices, zero sizes, inverted spreads, unrealistic values; auto-correct by swapping bid/ask',
      'Spike detection: Compare to rolling 100-tick moving average, flag if >5% deviation, adapt baseline gradually',
      'Flash crash detection: Track baseline price, detect >10% sudden drop, monitor for recovery (100 ticks within 2%)',
      'Quality metrics: Track valid rate (98-99%), error rate (0.1-0.5%), duplicate rate (0.5-2%), provide detailed reports',
    ],
  },
];
