export const statisticsInferenceQuiz = [
  {
    id: 'si-q-1',
    question:
      'Jane Street: "You backtest a trading strategy over 1,000 days and observe Sharpe ratio of 1.2. Construct: (1) 95% confidence interval for true Sharpe ratio, (2) hypothesis test for H₀: Sharpe ≤ 0.5 vs H₁: Sharpe > 0.5, (3) required sample size for 90% power to detect Sharpe of 0.8 vs null of 0. Then discuss: if you tested 50 strategies and only reported this one, how does that change your conclusion?" Provide complete statistical analysis with corrections for multiple testing.',
    sampleAnswer:
      "Complete Sharpe ratio inference: (1) 95% CI for Sharpe ratio S=1.2, n=1000: SE(S) ≈ √((1 + S²/2)/n) = √((1 + 1.44/2)/1000) = √(1.72/1000) = √0.00172 ≈ 0.0415. 95% CI: 1.2 ± 1.96(0.0415) = 1.2 ± 0.081 = [1.12, 1.28]. Interpretation: We're 95% confident true Sharpe is between 1.12 and 1.28 (excellent range). (2) Hypothesis test H₀: S ≤ 0.5 vs H₁: S > 0.5: Under H₀, approximate t-statistic: t = (S_obs - S_0) / SE(S) = (1.2 - 0.5) / 0.0415 ≈ 16.87. This is far beyond critical value (z_0.95 = 1.645 for one-tailed test). P-value ≈ 0 (< 0.0001). Strong evidence to reject H₀. Strategy Sharpe significantly exceeds 0.5. (3) Sample size for 90% power to detect S=0.8 vs S=0: Using power formula for Sharpe ratio: n ≈ ((z_{1-α} + z_{1-β})/δ)² × (1 + S²/2), where δ = S_alt = 0.8. For α=0.05 (one-tailed), β=0.10: z_0.95 = 1.645, z_0.90 = 1.28. n ≈ ((1.645 + 1.28)/0.8)² × (1 + 0.64/2) = (3.66)² × 1.32 = 13.4 × 1.32 ≈ 17.7. Wait, this seems too small. Let me recalculate using correct Sharpe ratio power formula: n ≈ 2(z_{α/2} + z_β)²(1 + S²/4)/S². For S=0.8, one-tailed α=0.05, β=0.10: n ≈ 2(1.645 + 1.28)²(1 + 0.16)/0.64 = 2(8.55)(1.16)/0.64 ≈ 30.8. Need about 31 days for 90% power (seems low - Sharpe of 0.8 is quite detectable). (4) Multiple testing correction: Testing 50 strategies and only reporting best one is severe data snooping! Expected max Sharpe under null: E[max S] ≈ √(2 ln k) for k strategies ≈ √(2 ln 50) ≈ √7.8 ≈ 2.8 for large samples. Bonferroni correction: α_adj = 0.05/50 = 0.001. Re-test with α=0.001: Critical value z_0.999 = 3.09. Our t = 16.87 still exceeds this, so even with harsh correction, result holds. However, with 50 tests, probability of at least one false positive at α=0.05 is 1-(1-0.05)^50 ≈ 92%! Proper approach: (a) Pre-register strategy before testing, (b) Use separate validation set, (c) Apply Bonferroni or FDR correction, (d) Report all 50 strategies (not just winner). With 50 strategies tested, I'd increase skepticism and require out-of-sample validation before trusting the result.",
    keyPoints: [
      '95% CI for Sharpe: 1.2 ± 0.081 = [1.12, 1.28] using SE ≈ √((1+S²/2)/n)',
      'Hypothesis test: t = (1.2-0.5)/0.0415 ≈ 16.87, strong rejection of H₀',
      'Sample size for power: depends on effect size, approximately 30-50 days for large Sharpe',
      'Multiple testing: 50 strategies → Bonferroni α_adj = 0.001, high false positive risk',
      'Data snooping severe issue: require validation set and pre-registration',
    ],
  },
  {
    id: 'si-q-2',
    question:
      'Citadel: "Two trading algorithms A and B are tested over same 500-day period. Algorithm A: mean return 0.08%/day, std 1.2%. Algorithm B: mean return 0.10%/day, std 1.5%. Daily returns have correlation 0.6. Test if B significantly outperforms A using paired t-test. Then explain why paired test is more powerful than two-sample test here. Finally, what minimum difference would be detectable with 80% power?" Show complete paired analysis.',
    sampleAnswer:
      'Complete paired comparison: (1) Paired t-test setup: Since both algorithms tested over same 500 days, use paired test on differences D_i = Return_B,i - Return_A,i. Null hypothesis: μ_D = 0 (no difference). Alternative: μ_D > 0 (B outperforms A). Given data: μ_B - μ_A = 0.10 - 0.08 = 0.02% (mean difference). Need variance of differences: Var(D) = Var(B) + Var(A) - 2·Cov(A,B) = σ_B² + σ_A² - 2ρσ_Aσ_B = 1.5² + 1.2² - 2(0.6)(1.5)(1.2) = 2.25 + 1.44 - 2.16 = 1.53. σ_D = √1.53 ≈ 1.237%. (2) T-statistic: t = (D̄ - 0) / (σ_D/√n) = 0.02 / (1.237/√500) = 0.02 / 0.0553 ≈ 0.362. Critical value for one-tailed test at α=0.05 with df=499: t_crit ≈ 1.648. Decision: t = 0.362 < 1.648 → Fail to reject H₀. Insufficient evidence that B outperforms A despite higher mean return. P-value ≈ 0.36 (36% chance of seeing this difference by chance). (3) Why paired test more powerful: Two-sample test treats A and B as independent. Variance for two-sample difference: Var(X̄_B - X̄_A) = σ_B²/n + σ_A²/n = (1.5² + 1.2²)/500 = 3.69/500 = 0.00738. SE = 0.0859. Paired test variance: Var(D̄) = (σ_B² + σ_A² - 2ρσ_Aσ_B)/n = 1.53/500 = 0.00306. SE = 0.0553. Paired SE is smaller! Ratio: 0.0553/0.0859 ≈ 0.64. Paired test is ~36% more powerful because it accounts for positive correlation (when A has good day, B tends to have good day too - removes common variance). (4) Minimum detectable difference with 80% power: Use power formula: δ_min = (z_{1-α} + z_{1-β}) × σ_D / √n. For α=0.05 (one-tailed), β=0.20: z_0.95=1.645, z_0.80=0.84. δ_min = (1.645 + 0.84) × 1.237 / √500 = 2.485 × 1.237 / 22.36 = 3.076 / 22.36 ≈ 0.138%. To detect difference with 80% power, algorithms must differ by at least 0.14%/day. Current difference (0.02%) is far below this threshold, explaining why test fails to show significance.',
    keyPoints: [
      'Paired test: D = B - A has σ_D = √(σ_B² + σ_A² - 2ρσ_Aσ_B) ≈ 1.237%',
      'T-statistic: t = 0.02/(1.237/√500) ≈ 0.362, fail to reject H₀',
      'Paired test more powerful: removes common variance from positive correlation',
      'Power gain: paired SE (0.0553) < two-sample SE (0.0859) by factor of 0.64',
      'Minimum detectable: 0.14%/day for 80% power, current 0.02% too small',
    ],
  },
  {
    id: 'si-q-3',
    question:
      'Two Sigma: "A high-frequency trading strategy makes 10,000 trades per day with 50.5% win rate (vs 50% random). Is this statistically significant? Calculate p-value. Then, if transaction costs are 0.1% per trade, what minimum win rate is needed to be profitable? Finally, discuss survivor bias: if 1,000 HFT firms exist and you observe one with 51% win rate over 1 year, should you invest?" Provide complete analysis of statistical vs economic significance and selection bias.',
    sampleAnswer:
      "Complete HFT analysis: (1) Statistical significance of 50.5% win rate: n = 10,000 trades, p_obs = 0.505, p_0 = 0.50 (null: random). This is binomial proportion test. Under H₀: p̂ ~ N(0.50, √(0.5·0.5/10000)) = N(0.50, 0.005). Z-statistic: z = (0.505 - 0.50) / 0.005 = 0.005 / 0.005 = 1.0. P-value (one-tailed): P(Z > 1.0) ≈ 0.159 (15.9%). Decision: Not statistically significant at α=0.05. High probability (16%) of observing 50.5% win rate by pure chance with 10k trades. Need more data for conclusive evidence. (2) Minimum win rate for profitability with 0.1% costs: Break-even condition: win_rate × (avg_win - cost) + (1 - win_rate) × (-avg_loss - cost) > 0. Assume symmetric: avg_win = avg_loss = amount. Simplify: 2·win_rate·amount - amount - 2·cost > 0 → win_rate > 0.5 + cost/amount. If typical trade is ~1% move, cost/amount = 0.1%/1% = 0.1. Minimum win rate ≈ 0.5 + 0.1 = 60%! Alternative: with cost c per trade, expected profit per trade = win_rate·(+1-c) + (1-win_rate)·(-1-c) = 2·win_rate - 1 - 2c. For profit > 0: win_rate > (1 + 2c)/2. With c=0.001: win_rate > (1 + 0.002)/2 = 0.501 = 50.1%. So need just over 50% to break even with 0.1% costs (assuming ±1 unit payoffs). But with typical HFT, if trading smaller moves, break-even rate is higher. (3) Survivor bias with 1,000 HFT firms: Expected maximum win rate under null (all random): For k=1000 firms each with n=10,000 trades (total 250 trading days × 40 trades/day), expected max p-value ≈ 1/k = 0.001. To see what win rate corresponds to p=0.001: z_0.001 = 3.09, so p̂_max ≈ 0.50 + 3.09(0.005) = 0.515. Expected that at least one firm shows 51.5% win rate by pure chance! Observing 51% is less extreme than expected maximum. P(observing at least one firm with 51%+ by chance) is very high. Decision: Do NOT invest based solely on this track record. It's well within range of random variation when considering 1,000 firms. This is textbook survivorship bias - we only see survivors, not the 999 other firms. Proper approach: require out-of-sample validation, theoretical justification for edge, independent verification, and awareness that extreme observations are expected in large populations.",
    keyPoints: [
      'Statistical test: z = (0.505-0.50)/0.005 = 1.0, p-value ≈ 0.16 (not significant)',
      'Break-even with costs: need win_rate > 50.1% for 0.1% costs (symmetric payoffs)',
      'Survivor bias: with 1,000 firms, expect max ~51.5% win rate by chance',
      'Observing 51% among 1,000 firms is not impressive (expected maximum)',
      'Investment decision: require out-of-sample validation, theoretical edge, independent verification',
    ],
  },
];
