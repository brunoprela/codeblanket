/**
 * Quiz questions for Hinted Handoff section
 */

export const hintedhandoffQuiz = [
  {
    id: 'q1',
    question:
      'Explain how hinted handoff improves write availability during temporary node failures and why hints are not considered true replicas. Provide a concrete example with replication factor 3.',
    sampleAnswer:
      'Hinted handoff maintains write availability during temporary failures by storing writes for unavailable replicas on other nodes, but hints don\'t provide read consistency guarantees.Scenario without hinted handoff: RF = 3(replication factor), W=3(write quorum requires all 3 replicas).Normal: key="user:123" should replicate to { Node A, B, C }.Node C crashes.Write attempt: Coordinator tries to write to A, B, C.A: Success.B: Success.C: Timeout (down).Only 2 / 3 writes succeeded, W = 3 not met.Write fails ❌.User receives error, must retry.With hinted handoff: Write attempt during C\'s failure: A: Success. B: Success. C: Timeout. Coordinator stores "hint" on Node D (temporary substitute): "When C recovers, write user:123=data." Write succeeds (3 acks: A, B, D\'s hint) ✅.Later when C recovers: Node D detects C is back.D sends hint to C: "Write user:123=data." C applies write.D deletes hint.Final state: Data on A, B, C(correct replicas).Why hints aren\'t true replicas: Read scenario: RF=3, R=2 (read quorum). Node C still down, data on A, B, hint on D. Read request: Coordinator reads from A, B, D. A: Returns user:123=data ✅. B: Returns user:123=data ✅. D: Has hint, not actual data—can\'t return value! ❌.R = 2 satisfied only by A and B(actual replicas), not D.Critical distinction: Hints improve write availability (allows W to be satisfied with temporary substitute). But hints don\'t improve read consistency (R must come from actual replicas). Read quorum calculations exclude hints. Concrete example timing: T=0: Write arrives, C is down. T=1: Coordinator writes to A, B, stores hint on D. Returns success to client. T=2: C still down. Read arrives. T=3: Coordinator reads from A, B (satisfies R=2). C is down, so doesn\'t read from it.D has hint but doesn\'t return data for reads. Returns data from A, B. T=10: C recovers. T=11: D replays hint to C. T=12: C now has data. Future reads can use C. Why this design: Hints are for catching up unavailable nodes, not for serving reads. Hints may be stale (if TTL approaching or data updated since hint created). Reads require actual replicated data for consistency. Production implications: With hinted handoff: Write availability maintained even during single node failure. Without: Write availability reduced (need all RF nodes up for W=RF). Read consistency unaffected: Always requires R actual replicas. Cassandra default: hints enabled, TTL=3 hours (tradeoff availability vs resource usage).',
    keyPoints: [
      'Hinted handoff: Store hints for down replicas on other nodes, write succeeds immediately',
      'Hints improve write availability: W can be satisfied with hints as temporary substitutes',
      "Hints are NOT replicas: Don't count toward read quorum, don't return data for reads",
      'Example: RF=3, C down, write to A/B + hint on D succeeds; read from A/B only (not D)',
      'Design reason: Hints for catch-up, not serving; may be stale or deleted before recovery',
    ],
  },
  {
    id: 'q2',
    question:
      'Describe the challenges of hint accumulation when a node is down for an extended period and explain the trade-offs in choosing hint TTL (time-to-live).',
    sampleAnswer:
      'Hint accumulation creates resource exhaustion; TTL trades off recovery speed vs resource consumption. Challenge: Unlimited hint accumulation. Scenario: Node C fails. Write rate: 10,000 writes/sec to data owned by C. Without TTL: Hour 1: 36 million hints stored (10K × 3600 seconds). Hour 2: 72 million hints. Day 1: 864 million hints ❌. Storage exhausted (hints stored on disk). Memory pressure (hints indexed in memory). Network: When C recovers, replay 864M hints = hours of network transfer. Node storing hints (say, D) overwhelmed. Production impact: Node D runs out of disk space. D stops accepting new hints (write failures). D itself crashes (resource exhaustion) → cascading failure! TTL solution - Trade-offs: Short TTL (1 hour): Pros: Minimal resource consumption (only 36M hints max). Fast replay when node recovers within 1 hour. Disk usage bounded (predictable capacity planning). Cons: If node down > 1 hour, hints discarded. Must rely on anti-entropy (slow) to catch up. Node misses data written during downtime until full repair. Example use case: Stable cluster, fast hardware recovery (<1 hour expected). Medium TTL (3 hours, Cassandra default): Pros: Balances resource usage (108M hints max) vs recovery window. Covers typical hardware replacement time. Most temporary failures caught (network issues, brief crashes). Cons: If node down >3 hours (e.g., datacenter failure), hints lost. Still risk of resource exhaustion if multiple nodes fail. Example use case: Production systems with good hardware reliability, occasional failures. Long TTL (24 hours): Pros: Catches extended outages (datacenter maintenance, prolonged failures). Higher recovery speed for returning nodes. Cons: Very high resource consumption (864M hints for single node!). If multiple nodes fail, storage exhausted quickly. Replay time very long (hours to transfer). Example use case: Systems where anti-entropy is very expensive (huge datasets) or slow. Additional mitigation strategies: Size-based limits (complement TTL): max_hints_per_node = 10 GB. If exceeded, stop storing new hints, rely on anti-entropy. Prevents single node failure from exhausting all storage. Hint compression: Compress hints before storing (2-5x space reduction). Trade CPU for disk space. Throttled replay: Limit hint replay rate (e.g., 1 MB/sec per node). Prevents overwhelming recovering node. Graduated approach: Node down <3 hours: Use hints (fast catch-up). Node down 3-24 hours: Use hints + stream recent writes. Node down >24 hours: Full anti-entropy repair (Merkle tree comparison). Example decision process for production: Data volume: 10K writes/sec = 36M hints/hour. Storage budget: 100 GB for hints (can accommodate ~3 hours at this rate with compression). Recovery SLA: Node should catch up within 1 hour of recovery. Network capacity: 100 MB/sec available for hint replay. Decision: TTL = 3 hours (balances storage vs recovery window). Size limit = 50 GB per node (safety valve). Compression enabled (2x benefit). Replay throttled to 50 MB/sec (leaves headroom). Monitor: Hint count, disk usage, replay lag. Alert if hints approaching limit (indicates sustained failure, trigger manual intervention). Cassandra production experience: Default 3-hour TTL works well for most failures (brief crashes, network blips). Extended outages (>3 hours) rare enough that anti-entropy acceptable. Hint-related outages usually from multiple concurrent node failures (mitigation: faster hardware replacement, better monitoring).',
    keyPoints: [
      'Challenge: 10K writes/sec × 3600 sec/hour = 36M hints/hour, unbounded growth',
      'Short TTL (1h): Low resource usage but misses extended outages (>1h)',
      'Medium TTL (3h): Cassandra default, balances resources vs typical recovery time',
      'Long TTL (24h): Catches extended failures but high resource consumption (864M hints)',
      'Mitigation: Size limits, compression, throttled replay, graduated approach (hints + anti-entropy)',
    ],
  },
  {
    id: 'q3',
    question:
      'Explain how stale hints can occur and what mechanisms prevent hints from overwriting newer data on a recovering node. Design a solution to handle this race condition.',
    sampleAnswer:
      "Stale hint race condition occurs when hint reflects old data but node has received newer write directly; timestamps and version vectors prevent corruption. Race condition scenario: T=0: Node C down. Write X=1 to replicas A, B. Hint stored on D for C. T=1: C comes back online. T=2: Write X=2 directly to A, B, C (all healthy now). C has X=2. T=3: D notices C recovered, starts replaying hints. T=4: D sends hint X=1 to C. T=5: If C blindly accepts hint: C overwrites X=2 with X=1 (stale!) ❌. Result: Newest data lost, regression to old value. Why this happens: Hints are asynchronous, replay happens in background. Hint represents write at T=0, but current time is T=5. Newer writes may have reached node directly after recovery. No coordination between hint replay and normal writes. Solution 1: Timestamp-based rejection (Last-Write-Wins). Implementation: Every write includes timestamp. Node stores (value, timestamp). When hint arrives: Compare hint timestamp vs local timestamp. If hint_timestamp > local_timestamp: Accept hint (newer). If hint_timestamp ≤ local_timestamp: Reject hint (stale). Example: C has (X=2, timestamp=T2). Hint arrives (X=1, timestamp=T0). Compare: T0 < T2 → Reject hint. C keeps X=2 ✅. Pros: Simple, works with synchronized clocks. Cons: Requires accurate clock sync (NTP). Clock skew can cause incorrect accept/reject. Solution 2: Version vectors. Implementation: Every write includes vector clock (captures causality). Node stores (value, version_vector). When hint arrives: Compare hint vector vs local vector. If hint vector dominates (causally newer): Accept. If local vector dominates: Reject (local is newer). If concurrent (neither dominates): Conflict! Keep both, merge. Example: C has (X=2, vector={A:2, B:2, C:1}). Hint arrives (X=1, vector={A:1, B:1, C:0}). Local vector dominates hint vector (A:2>1, B:2>1, C:1>0). Reject hint, keep X=2 ✅. Pros: Doesn't require clock sync, captures causality accurately.Cons: More complex, larger metadata overhead.Solution 3: Hint sequence numbers with validation.Implementation: Hints tagged with sequence number (increasing per node).Node tracks last_applied_hint_sequence per source.When hint arrives: If hint_seq ≤ last_applied_hint_sequence: Reject (already applied or obsolete).If hint_seq > last_applied_hint_sequence: Validate hint is still relevant (timestamp check).Accept and update last_applied_hint_sequence.Example: C recovered, applied hints up to seq = 100 from D.Hint seq = 95 arrives (old, replayed twice ?).Reject(95 ≤ 100).Hint seq = 105 arrives.Check timestamp, accept if still valid.Pros: Prevents duplicate hint application, adds ordering.Cons: Requires maintaining sequence state.Comprehensive solution (production - grade): Layer 1 - Hint metadata: Include timestamp, version vector, hint sequence.Layer 2 - Node - side validation: Check timestamp: Reject if hint_timestamp < local_timestamp - grace_period (e.g., 5 minutes grace for clock skew).Check version vector: Reject if local vector causally dominates hint.Check sequence: Skip if already applied.Layer 3 - Idempotency: Make hint application idempotent (applying twice has same effect as once).Use upsert operations (insert if not exists, update if newer).Layer 4 - Monitoring: Track rejected hints (stale_hints_rejected metric).High reject rate indicates recovery + concurrent writes (normal).Alert if accept rate unexpectedly drops (clock skew ?).Layer 5 - Testing: Chaos engineering: Kill node, write data, recover node, write more data, replay hints.Verify newest data retained.Production example - Cassandra: Uses timestamps (writetime).Hints include timestamp of original write.On replay, C compares: If hint writetime < local writetime: Discard hint.Else: Apply hint (but local timestamp may still be newer from concurrent write, LWW applies).Additional optimization - Hint validation before replay: Before sending hint, check if C already has this data (cheap check).If C's current data is newer (timestamp check), don't send hint.Saves network transfer.Trade - off: Extra round - trip (check before send) vs wasted transfer (send stale hint).Best practice: Check for batch of hints, send only relevant ones.",
    keyPoints: [
      'Race condition: Hint (T=0) replayed at T=5 after newer write (T=2) already on node',
      'Timestamp solution: Reject hint if hint_timestamp < local_timestamp (requires clock sync)',
      'Version vector: Reject if local vector causally dominates hint (no clock needed)',
      'Hint sequence: Track last applied sequence per source, reject duplicates/old hints',
      'Production: Multi-layer (timestamp + vector + sequence + idempotent + monitoring)',
    ],
  },
];
