/**
 * Quiz questions for Split-Brain Resolution section
 */

export const splitbrainresolutionQuiz = [
  {
    id: 'q1',
    question:
      'Explain why split-brain is dangerous in distributed systems and walk through a scenario where split-brain causes data inconsistency. How would quorum-based decisions prevent this?',
    sampleAnswer: `Split-brain occurs when network partition creates multiple independent groups, each operating independently, causing catastrophic inconsistency. Dangerous scenario - Banking system: Initial state: Account balance = $1000, Cluster = {Node A (leader), Node B, Node C}. T=0: Network partition creates {A} and {B, C}. Without split-brain prevention: T=1: Partition 1 (Node A) thinks it's still leader.User withdraws $500 via Node A.A updates: balance = $500.T = 2: Partition 2(B, C) elects Node B as leader.Same user withdraws $600 via Node B.B updates: balance = $400.T = 3: Network partition heals.Reconciliation attempt: Node A says balance = $500.Node B says balance = $400.Which is correct? BOTH operations were valid in their partition.T = 4: Using last- write - wins (timestamps): If A's write happened at T=1.0, B's at T = 2.0.Keep B's value = $400. But user withdrew $1100 total (should be impossible from $1000!). Data corruption: Account should be -$100 or one withdrawal should have failed. Result: Money lost, audit trail broken, compliance violation. Quorum-based prevention: Cluster size: 5 nodes {A,B,C,D,E}. Quorum = 3 (majority). Partition: {A,B} (2 nodes) vs {C,D,E} (3 nodes). With quorum requirement: Partition {A,B}: Has 2 nodes < quorum (3). Cannot elect leader or accept writes. Becomes read-only or unavailable. Partition {C,D,E}: Has 3 nodes = quorum! Can elect leader (say, Node C). Accepts writes with consistency. Scenario replay with quorum: T=1: User tries to withdraw via Node A (in minority partition). Node A: "I don't have quorum, cannot process write." Request fails immediately. T=2: User tries via Node C (in majority partition). Node C has quorum, processes withdraw $500. Balance = $500 (recorded on C, D, E). T=3: Partition heals. Partition {A,B}: Sync from majority partition. Learn balance = $500. Result: Only one partition could operate (consistency), minor partition unavailable (availability sacrifice). Data integrity maintained: Only valid withdrawal recorded. This is CAP theorem in action: Chose consistency over availability during partition. Trade-off: Users in minority partition experienced unavailability (couldn't withdraw). Better than data corruption! Production implications: Financial systems always use quorum (consistency critical). Social media might choose availability (allow both partitions to operate, merge later with conflict resolution).`,
    keyPoints: [
      'Split-brain danger: Multiple leaders accept conflicting operations, data corruption',
      'Example: Two withdrawals from same account in different partitions, both succeed',
      'Quorum prevention: Require majority (3 of 5) to operate, only one partition can have majority',
      'Result: Minority partition unavailable, majority operates, consistency maintained',
      'CAP trade-off: Consistency over availability during partition (critical for financial systems)',
    ],
  },
  {
    id: 'q2',
    question:
      'Design a split-brain prevention system using fencing tokens. How do tokens work, and how do they prevent stale leaders from causing damage even if quorum-based election fails?',
    sampleAnswer: `Fencing tokens provide defense-in-depth: even if split-brain occurs (quorum mechanisms fail), tokens prevent stale leaders from corrupting data. Fencing token mechanism: Leader gets monotonically increasing token (generation number) when elected. All operations include token. Resources (databases, file systems) track highest token seen, reject lower tokens. Basic design: Token Authority (can be distributed Raft/Paxos cluster): Maintains current_token counter. On leader election: Increment counter, give new token to elected leader. Leader includes token in all operations. Resources implement token checking. Scenario without fencing tokens: T=0: Node A elected leader, token=5. T=10: Network partition. Both partitions think they should elect leader. T=11: Partition 1: A stays leader (no quorum, but doesn\`t know). Partition 2: Elects B as leader (has quorum).T = 12: Both A and B try to write to shared storage.Storage accepts both writes → Data corruption! Scenario with fencing tokens: T = 0: Node A elected, receives token = 5. T = 10: Network partition.T = 11: Partition 1: A still has token = 5, but can't renew (no quorum). Partition 2: Elects B, token authority increments: B receives token=6. T=12: Write operations: A tries: write (key="account:123", value="$500", token=5). B tries: write (key="account:123", value="$400", token=6). Storage token check: current_highest_token = 0 initially. A's write arrives first: token = 5 > 0 → Accept, set current_highest_token = 5. B's write arrives: token=6 > 5 → Accept, set current_highest_token=6. A sends another write: token=5 < 6 → REJECT (stale token). Result: Even though A thought it was leader, its writes eventually rejected. B's writes accepted (newer token).Defense in depth: Layer 1: Quorum prevents split - brain election (only partition with majority can elect). Layer 2: If quorum fails (bug, misconfiguration), fencing tokens prevent stale leader damage.Layer 3: Monitoring detects multiple leaders (alert operators).Implementation details: Token storage: Token authority must be highly available (use Raft / Paxos cluster).Tokens must be persistent (survive restarts).Token atomicity: Getting token and becoming leader must be atomic.If leader elected but fails to get token, re - election needed.Token in every operation: Leader must include token in EVERY write.Even read operations should include token (ensure reading from current leader).Resource implementation: Database: UPDATE accounts SET balance = $400 WHERE id = 123 AND fencing_token >= 6. Reject if fencing_token < current_highest_token.File system(GFS, HDFS): Chunk servers check lease token before accepting writes.Old lease tokens rejected.Edge case - Clock skew: Problem: If using timestamps as tokens, clock skew allows lower timestamp to be "newer." Solution: Use logical sequence numbers(1, 2, 3, ...), not timestamps.Monotonically increasing, no clock dependency.Production examples: Google Chubby: Uses sequence numbers for lock leases, prevents stale lock holders.HDFS: Chunk servers use generation stamps (fencing tokens) to reject stale writes.Patroni(PostgreSQL HA): Uses etcd - based generation numbers to prevent split - brain.`,
    keyPoints: [
      'Fencing tokens: Monotonically increasing generation numbers assigned to each elected leader',
      'Resources reject old tokens: Track highest token seen, reject operations with lower tokens',
      'Defense in depth: Quorum prevents election, tokens prevent damage if election fails',
      'Example: Old leader (token=5) rejected after new leader (token=6) operates',
      'Production: Chubby (sequence numbers), HDFS (generation stamps), Patroni (etcd generations)',
    ],
  },
  {
    id: 'q3',
    question:
      'Compare three split-brain resolution strategies: manual intervention, last-write-wins with timestamps, and vector clocks with application merge. When would you use each?',
    sampleAnswer: `Three strategies represent increasing automation with different trade-offs in correctness and complexity. Strategy 1: Manual Intervention. Process: Detect split-brain → Halt all operations → Alert operators → Operator inspects state in both partitions → Operator chooses which partition's data to keep (or manually merges) → Restart system with chosen state.Example: E - commerce order system after partition: Partition A: Processed 100 orders.Partition B: Processed 150 orders.Operator: Manually reviews both sets, identifies overlaps, merges carefully, ensures no duplicate charges.Pros: Safest—human judgment prevents data corruption.Can handle complex conflicts requiring business logic.Guarantees correctness (if operator skilled).Cons: Downtime during resolution (minutes to hours).Requires skilled operators (expensive, may not be available 24 / 7).Doesn't scale (can't manually resolve thousands of conflicts). When to use: Financial transactions (banking, payments)—data integrity critical, brief downtime acceptable.Rare partitions where complexity of conflict justifies manual intervention.Systems where automated merge might cause compliance / legal issues.Strategy 2: Last - Write - Wins(LWW) with Timestamps.Process: Each write tagged with timestamp.After partition heals: For each conflicting key, keep write with latest timestamp.Discard earlier writes.Example: User profile updates: Partition A at T = 100: user.city = "New York".Partition B at T = 150: user.city = "San Francisco".After merge: Keep "San Francisco"(later timestamp).Pros: Simple, fully automatic, no downtime.Works well when "latest is correct" assumption holds.Easy to implement.Cons: Requires synchronized clocks(NTP)—clock skew causes incorrect resolution.Lost writes—earlier writes discarded even if valid.Doesn't detect concurrent writes (simultaneous in both partitions). Example failure: A at T=100, B at T=150, but B's clock is 1 hour fast.Keeps B's stale data incorrectly. When to use: Collaborative applications where latest edit wins (documents, wikis). Non-critical data where occasional lost update acceptable. Environment with reliable clock sync (NTP within 100ms). Strategy 3: Vector Clocks with Application Merge. Process: Each write tagged with vector clock (captures causality). After partition heals: Compare vector clocks: If A's clock dominates B's → Keep A (B is outdated). If B's clock dominates A's → Keep B (A is outdated). If concurrent (neither dominates) → Keep both versions, application resolves. Example: Shopping cart (Amazon Dynamo approach): Partition A: Add item X, vector={A:1, B:0}. Partition B: Add item Y, vector={A:0, B:1}. After merge: Vectors concurrent (neither dominates). Keep both: cart = {X, Y} (union). Application logic: Merge is union (reasonable for shopping cart). Pros: Preserves all data—no lost writes. Detects true concurrency (independent of clocks). Application can implement correct business logic for merging. Cons: Complexity—vector clocks harder to understand and implement. Application must handle conflicts (CRDTs can help). Occasional conflicts exposed to users (Amazon shows "these versions differ"). Storage overhead (vector clocks for every write). When to use: Eventually consistent systems where preserving all data matters (Riak, Dynamo). Applications with clear merge semantics (shopping cart union, collaborative editing). Systems that can't rely on clock sync (global distribution).Comparison summary: Manual: Maximum safety, minimum automation.LWW: Maximum automation, requires assumptions (clock sync, latest wins).Vector clocks: Balance—automatic but preserves data, requires app logic.Real - world systems often combine: Use vector clocks to detect conflicts + LWW as fallback + manual for critical conflicts.Example: Riak uses vector clocks for conflict detection, but applications can choose LWW or custom merge.`,
    keyPoints: [
      'Manual: Operator decides, safest but downtime/cost (financial systems)',
      'Last-Write-Wins: Keep latest timestamp, simple but requires clock sync, loses data',
      'Vector clocks: Detect causality, preserve all data, app merges conflicts (Dynamo, Riak)',
      'Trade-offs: Safety vs automation vs complexity vs data preservation',
      'Hybrid: Vector clocks + LWW fallback + manual escalation (production best practice)',
    ],
  },
];
