/**
 * Quiz questions for Read Repair section
 */

export const readrepairQuiz = [
    {
        id: 'q1',
        question:
            'Explain how read repair differs from anti-entropy in maintaining consistency, and why production systems use both rather than just one mechanism.',
        sampleAnswer: `Read repair and anti-entropy are complementary—read repair fixes hot data quickly, anti-entropy ensures complete coverage including cold data. Read repair characteristics: Trigger: Read operations (piggyback on client reads). Scope: Only keys that are read. Coverage: Hot data (frequently accessed) automatically repaired. Speed: Immediate for read keys (fixed on first read after inconsistency). Overhead: Proportional to read traffic (more reads = more repair opportunities). Blind spot: Cold data (never/rarely read) may remain inconsistent indefinitely. Anti-entropy characteristics: Trigger: Background process (scheduled, e.g., weekly). Scope: Entire dataset (scans all keys). Coverage: All data (hot and cold). Speed: Slow (hours to days for complete scan of large dataset). Overhead: Constant (independent of read traffic). Guarantee: Eventually all data consistent. Why both needed - Example scenario: Dataset: 1 billion keys. Read distribution: Power law (1% of keys = 90% of reads = "hot data"). Inconsistency introduced: Network partition at T=0 caused 100K keys inconsistent. With only read repair: Hot data (900K keys of inconsistent set): Fixed quickly (within hours as users read them). Cold data (99.1K keys): May never be read. Remain inconsistent for months! ❌. User eventually reads cold key (rare): Gets stale data. Read repair fixes it, but damage done (user saw wrong data). With only anti-entropy: All data eventually fixed (weekly scan finds all 100K inconsistent keys). But hot data remains inconsistent for up to a week! Millions of users see stale data during this week ❌. Poor user experience (frequently accessed data should be consistent quickly). With both (production approach): Read repair: Hot 900K keys fixed within hours (as users access them). Frequent reads ensure consistency for important data. Anti-entropy: Runs weekly, catches remaining 99.1K cold keys. Ensures no data left behind indefinitely. Result: Best of both worlds. Hot data: Fast repair (hours via read repair). Cold data: Complete coverage (caught by weekly anti-entropy). Resource efficiency: Read repair cheap (piggybacks on reads), anti-entropy expensive but infrequent. Concrete production example - Cassandra: Read repair (probabilistic, 10%): Frequent reads trigger repair. Popular data automatically consistent. Anti-entropy (nodetool repair): Run every 10 days (gc_grace_seconds). Catches cold data, tombstones, edge cases. Together: Most inconsistencies caught by read repair (fast, automatic). Anti-entropy safety net (slow, comprehensive). Monitoring validates: read_repair_count high (hot data), anti_entropy_discrepancies low (catch rare cold data). Another perspective - Cost-benefit: Read repair: Low cost (piggyback on reads), high value (fixes data users care about). Anti-entropy: High cost (full scan), low value per key (most already fixed), but essential (guarantees completeness). Analogy: Read repair = spot cleaning (clean what you see dirty). Anti-entropy = deep cleaning (clean everything, scheduled). Household: Spot clean daily (read repair), deep clean weekly (anti-entropy). Production systems that skip anti-entropy: Risk cold data corruption accumulating over time. Risk tombstones not being cleaned up (storage leak). Regulatory/compliance issues (financial systems must prove all data consistent). Systems that skip read repair: Poor user experience (users see stale popular data until next anti-entropy run). Higher anti-entropy cost (more keys to fix, no incremental fixing).`,
        keyPoints: [
            'Read repair: Triggered by reads, fast for hot data, blind to cold data',
            'Anti-entropy: Background scan, slow but comprehensive, catches all data',
            'Both needed: Read repair fixes hot data (hours), anti-entropy catches cold data (weekly)',
            'Production: Cassandra read repair (10% probabilistic) + anti-entropy (every 10 days)',
            'Analogy: Read repair = spot cleaning (daily), anti-entropy = deep cleaning (weekly)',
        ],
    },
    {
        id: 'q2',
        question:
            'Walk through the synchronous and asynchronous read repair processes in Cassandra. How does the probabilistic read_repair_chance parameter affect the trade-off between consistency and latency?',
        sampleAnswer: `Cassandra implements layered read repair: always-on synchronous (blocking) for quorum, probabilistic asynchronous for full coverage. Synchronous (blocking) read repair: Triggered: Every read, always on (100% of reads). Process: (1) Coordinator sends read to quorum (R=2 out of RF=3, for example). (2) Replicas A and B respond quickly: value="Alice", timestamp=100 (both match). (3) Coordinator compares A and B responses: Match → Return to client immediately (fast path, ~10ms). (4) Client receives data (operation complete from client perspective). Asynchronous digest check: (5) In background, coordinator sends digest request to C (remaining replica). (6) C responds with digest (hash): digest=hash("Bob"), timestamp=50. (7) Coordinator detects mismatch (C has stale data). (8) Coordinator sends repair: Update C to value="Alice", timestamp=100. (9) C applies update. (10) Read repair complete (client unaware, already received response). Latency: Client experiences quorum latency only (~10ms). Background repair adds no latency. Consistency: Client guaranteed to receive latest (compared quorum responses). Stale replica (C) fixed asynchronously. Probabilistic (full) read repair: Triggered: read_repair_chance percent of reads (default 10%). Process: (1) 10% of reads: Read from ALL replicas (not just quorum), wait for ALL responses. (2) Compare all responses (A, B, C). (3) If any mismatch: Repair stale replicas immediately. (4) Return latest value to client. (5) Client waits for repair to complete. (6) 90% of reads: Use synchronous process above (fast path). Latency: 10% of reads: Slower (wait for slowest replica + repair time, ~50ms). 90% of reads: Fast (quorum only, ~10ms). Trade-off analysis: High read_repair_chance (e.g., 50%): Pros: Fast inconsistency detection (50% of reads check all replicas). Higher data consistency (more repairs per unit time). Cons: Higher latency for 50% of reads (wait for slowest replica). More network overhead (read from all replicas frequently). More CPU (compare all responses frequently). Lower throughput (slower reads). Low read_repair_chance (e.g., 1%): Pros: Low latency (99% of reads use fast path). Low network overhead (rarely read all replicas). Higher throughput. Cons: Slower inconsistency detection (relies more on anti-entropy). Lower consistency (inconsistencies last longer). Cassandra defaults: dclocal_read_repair_chance = 0.1 (10% within datacenter). read_repair_chance = 0.0 (0% across datacenters, too expensive). Rationale: 10% balances consistency and performance. Within DC: Lower latency, worth checking all replicas occasionally. Cross-DC: High latency, not worth the overhead (rely on anti-entropy). Production tuning examples: High-consistency requirement (financial data): read_repair_chance = 0.3 (30%). Accept 30% of reads being slower for better consistency. Low-latency requirement (ad serving): read_repair_chance = 0.01 (1%). Prioritize latency, rely on anti-entropy for eventual consistency. Frequently updated data: read_repair_chance = 0.2 (20%). More inconsistencies expected, higher repair rate beneficial. Rarely updated data: read_repair_chance = 0.05 (5%). Inconsistencies rare, low repair rate sufficient. Monitoring to tune: Track metrics: read_repair_count (repairs performed). avg_read_latency_p99 (latency impact). stale_replica_rate (% of reads finding stale data). If stale_replica_rate high (>10%): Increase read_repair_chance (more inconsistencies than expected). Or: Increase anti-entropy frequency. If read_latency_p99 exceeds SLA: Decrease read_repair_chance (trading some consistency for latency). Advanced optimization: Some systems use adaptive read repair: If recent repair rate high: Temporarily increase read_repair_chance (inconsistencies detected, boost repair). If repair rate low: Decrease to reduce overhead. Dynamic adjustment based on actual inconsistency rate.`,
        keyPoints: [
            'Synchronous (always on): Read from quorum, compare, repair others async, no client latency',
            'Probabilistic (10% default): Read from ALL replicas, compare all, repair before returning',
            'Trade-off: High % = better consistency but slower reads, low % = faster but relies on anti-entropy',
            'Cassandra defaults: 10% within DC (balanced), 0% cross-DC (too expensive)',
            'Tuning: Financial (30% for consistency), low-latency (1% prioritize speed)',
        ],
    },
    {
        id: 'q3',
        question:
            'Design a read repair system that minimizes false positives from clock skew and handles concurrent writes during repair. What conflict resolution strategy would you use?',
        sampleAnswer: `Designing robust read repair requires handling clock skew, concurrent writes, and conflict resolution through version vectors and careful ordering. Problem 1: Clock skew causing false positives. Scenario: Replica A: value="Alice", timestamp=100, clock accurate. Replica B: value="Alice", timestamp=102, clock 2 seconds fast. Read repair: Coordinator sees B's timestamp > A's timestamp. Incorrectly thinks B is newer, repairs A with B's data(actually same data, wasted work).Worse: Replica C has value = "Bob", timestamp = 99(actual latest write, but clock slow).Coordinator incorrectly repairs C with A's stale data! ❌. Solution: Use version vectors instead of timestamps. Design: Every write includes vector clock (per-replica counters). Example: Write path: Client writes to coordinator. Coordinator increments its counter in vector. Write: (value="Alice", vector={A:5, B:3, C:2}). Replicas store value + vector. Read repair path: Read from replicas A, B, C. A: (value="Alice", vector={A:5, B:3, C:2}). B: (value="Alice", vector={A:5, B:3, C:2}). C: (value="Bob", vector={A:3, B:2, C:2}). Compare vectors: A's vector dominates C's vector (A:5>3, B:3>2, C:2=2). A has causally newer data. Repair C with A's data.No reliance on timestamps, no clock skew issues ✅. Problem 2: Concurrent write during repair.Scenario: T = 0: Read detects C has stale value = "Bob".T = 1: Coordinator decides to repair C with value = "Alice".T = 2: New write arrives: value = "Charlie"(latest!).T = 3: New write goes to A, B, C.C now has value = "Charlie", vector = { A: 6, B: 4, C: 3 }.T = 4: Repair message arrives at C: value = "Alice", vector = { A: 5, B: 3, C: 2 }.T = 5: C compares vectors: Local vector = { A: 6, B: 4, C: 3 } dominates repair vector = { A: 5, B: 3, C: 2 }.Reject repair(local is newer) ✅.Solution: Vector clock comparison at apply time.Implementation: def apply_repair(repair_value, repair_vector): local_vector = get_local_vector() if repair_vector dominates local_vector: # Repair is causally newer apply(repair_value, repair_vector) elif local_vector dominates repair_vector: # Local is causally newer, reject repair reject() elif concurrent(repair_vector, local_vector): # Neither dominates, conflict! handle_conflict(repair_value, local_value) Conflict resolution strategy: When vectors are concurrent(neither dominates), use application - specific merge.Strategy 1: Last - Write - Wins(fallback).Use wall - clock timestamp as tie - breaker(within vectors).Keep value with higher timestamp.Pros: Deterministic, simple.Cons: Loses one write, clock skew affects tie - breaker.Strategy 2: Multi - Value(Riak approach).Keep both values as siblings.Return both to client, let application merge.Example: Shopping cart: {item A } vs {item B } → Return both, client merges to { A, B }.Pros: No data loss.Cons: Application complexity, client must handle conflicts.Strategy 3: CRDTs(Conflict - Free Replicated Data Types).Use data structures with built -in merge semantics.Example: G - Counter(grow - only counter): Merge by taking max of each replica's counter. OR-Set (observed-remove set): Add/remove operations commute, automatic merge. Pros: Automatic conflict resolution, provably correct. Cons: Limited to specific data types, storage overhead. Production design (comprehensive): Layer 1 - Vector clocks: Every write includes vector clock. Read repair compares vectors, not timestamps. Layer 2 - Conditional apply: Repair message includes expected vector ("apply if your vector is X"). Reject if local vector has diverged. Layer 3 - Conflict detection: If vectors concurrent, flag conflict. Log conflict for monitoring (conflict_rate metric). Layer 4 - Application-specific merge: For shopping cart: Union (merge = {A} ∪ {B}). For counter: Sum (merge = sum of both values). For document: Keep both versions, present to user (Google Docs approach). Layer 5 - Idempotency: Ensure repair is idempotent (applying twice has same effect). Use upsert with vector: "Update to value=X if vector < Y." Layer 6 - Ordering guarantees: Use coordinator's logical timestamp to order repairs.Prevent out - of - order application.Layer 7 - Monitoring: Track repair_success, repair_rejected, repair_conflict.High rejection rate = frequent concurrent writes(normal for hot keys).High conflict rate = investigate(too many concurrent writers ?).Testing: Chaos engineering: Simulate clock skew(advance some node clocks).Concurrent writes during read repair.Verify correct resolution.Latency injection: Delay repair messages, ensure concurrent writes handled.Partition recovery: Simulate partition, diverging writes, partition heal, verify merge.Example production system - Riak: Uses vector clocks for all operations.Read repair compares vectors.Concurrent writes → siblings returned to client.Client merges(e.g., shopping cart union).Monitoring: sibling_count metric(track conflict frequency).`,
        keyPoints: [
            'Clock skew solution: Use version vectors instead of timestamps for causality',
            'Concurrent write handling: Compare vectors at apply time, reject if local dominates repair',
            'Conflict resolution: If vectors concurrent, use LWW, multi-value, or CRDTs',
            'Production layers: Vector clocks + conditional apply + conflict detection + app merge + idempotency',
            'Example: Riak uses vector clocks, returns siblings for conflicts, client merges',
        ],
    },
];
