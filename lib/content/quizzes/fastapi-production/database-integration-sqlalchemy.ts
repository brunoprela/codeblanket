export const databaseIntegrationSqlalchemyQuiz = [
  {
    id: 'fastapi-db-q-1',
    question:
      'Design a production database layer for a multi-tenant SaaS application with repository pattern, connection pooling, and transaction management. Address: (1) connection pool configuration for handling 10K concurrent users, (2) implementing repository pattern with BaseRepository and tenant filtering, (3) managing transactions across multiple repositories in a service layer, (4) preventing N+1 queries with relationship loading strategies, (5) implementing database testing with pytest fixtures. Include complete code examples.',
    sampleAnswer:
      'Production database layer design: (1) Connection pool for 10K users: Calculate pool size: Assume 10K concurrent users, average request duration 100ms, each request needs DB connection. Connections needed = (10K users × 0.1s) / (connection reuse time). With proper async, ~100-200 connections sufficient. engine = create_engine(DATABASE_URL, pool_size=50, max_overflow=150, pool_timeout=30, pool_recycle=3600, pool_pre_ping=True, echo=False). pool_size=50: permanent connections, max_overflow=150: additional when busy (total 200), pool_timeout=30: wait 30s before raising error, pool_recycle=3600: recycle connections hourly (prevents stale connections), pool_pre_ping=True: verify connection health before use. Monitor: Log pool status: from sqlalchemy import event; @event.listens_for (engine, "connect"); def receive_connect (dbapi_conn, conn_record): logger.info("DB connection from pool"). Track pool exhaustion, alert when overflow > 100. (2) Repository with tenant filtering: class BaseRepository: def __init__(self, model, db: Session, tenant: Tenant): self.model = model; self.db = db; self.tenant = tenant; def _apply_tenant_filter (self, query): return query.filter_by (tenant_id=self.tenant.id); def get (self, id: int): query = self.db.query (self.model).filter_by (id=id); query = self._apply_tenant_filter (query); return query.first(); def get_multi (self, skip: int = 0, limit: int = 100): query = self.db.query (self.model); query = self._apply_tenant_filter (query); return query.offset (skip).limit (limit).all(). All queries automatically filtered by tenant! Repository dependency: def get_user_repo (db: Session = Depends (get_db), tenant: Tenant = Depends (get_tenant)): return UserRepository(User, db, tenant). (3) Transaction management across repositories: class OrderService: def __init__(self, user_repo: UserRepository = Depends(), product_repo: ProductRepository = Depends(), order_repo: OrderRepository = Depends(), db: Session = Depends (get_db)): self.user_repo, self.product_repo, self.order_repo, self.db = user_repo, product_repo, order_repo, db; async def create_order_with_payment (self, user_id: int, order_data: OrderCreate, payment_info: PaymentInfo): try: # All operations in single transaction; user = self.user_repo.get (user_id); if not user: raise HTTPException(404, "User not found"); # Validate products; for item in order_data.items: product = self.product_repo.get (item.product_id); if not product or product.stock < item.quantity: raise HTTPException(400, "Product unavailable"); # Create order; order = self.order_repo.create (order_data); # Decrease stock; for item in order_data.items: self.product_repo.decrease_stock (item.product_id, item.quantity); # Process payment; payment = await process_payment (payment_info); order.payment_id = payment.id; order.status = "paid"; # Commit all changes atomically; self.db.commit(); self.db.refresh (order); return order; except PaymentError as e: self.db.rollback(); raise HTTPException(400, "Payment failed"); except Exception as e: self.db.rollback(); raise HTTPException(500, str (e)). Key: Single db.commit() at end commits ALL operations. Any exception triggers rollback of everything. (4) Preventing N+1 queries: Problem: @app.get("/posts"); def get_posts (db: Session = Depends (get_db)): posts = db.query(Post).all(); return [{"title": p.title, "author": p.author.username} for p in posts]. N+1 queries: 1 for posts + N for each author! Solution 1 - joinedload (JOIN): from sqlalchemy.orm import joinedload; posts = db.query(Post).options (joinedload(Post.author)).all(). 1 query with JOIN. Use when: few related records, one-to-one relationships. Solution 2 - selectinload (SELECT IN): from sqlalchemy.orm import selectinload; posts = db.query(Post).options (selectinload(Post.author)).all(). 2 queries: 1 for posts, 1 SELECT IN for all authors. Use when: many related records, one-to-many relationships. Solution 3 - Multiple levels: posts = db.query(Post).options (joinedload(Post.author), selectinload(Post.comments).joinedload(Comment.author)).all(). Loads posts → authors (JOIN), comments (SELECT IN), comment authors (JOIN). Benchmark: Lazy loading (N+1): 100 posts = 101 queries, 500ms. joinedload: 100 posts = 1 query, 50ms (10x faster). selectinload: 100 posts = 2 queries, 60ms. (5) Database testing: # conftest.py; import pytest; from sqlalchemy import create_engine; from sqlalchemy.orm import sessionmaker. TEST_DB = "sqlite:///./test.db"; test_engine = create_engine(TEST_DB, connect_args={"check_same_thread": False}); TestSession = sessionmaker (bind=test_engine). @pytest.fixture (scope="function"); def db(): # Create tables; Base.metadata.create_all (bind=test_engine); db = TestSession(); try: yield db; finally: db.close(); # Drop tables after test (isolation); Base.metadata.drop_all (bind=test_engine). @pytest.fixture; def client (db): def override_get_db(): try: yield db; finally: pass; app.dependency_overrides[get_db] = override_get_db; with TestClient (app) as c: yield c; app.dependency_overrides.clear(). # Test; def test_create_user (client): response = client.post("/users", json={"username": "test", "email": "test@example.com", "password": "pass123"}); assert response.status_code == 201; assert response.json()["username"] == "test". def test_transaction_rollback (db): # Test rollback on error; with pytest.raises(Exception): user = User (username="test"); db.add (user); db.flush(); raise Exception("Test error"); db.rollback(). # Test repository; def test_user_repository (db, tenant): repo = UserRepository(User, db, tenant); user = repo.create(UserCreate (username="test", email="test@example.com")); assert user.id is not None; assert user.tenant_id == tenant.id; fetched = repo.get (user.id); assert fetched.username == "test". Alternative (transaction rollback per test): @pytest.fixture; def db(): db = TestSession(); db.begin_nested(); yield db; db.rollback(). Faster (no DROP TABLE), but requires savepoint support. Production benefits: Connection pooling handles load (10K users with 200 connections), tenant filtering automatic (security built-in), transactions ensure consistency (all-or-nothing), N+1 prevention (10x faster), testing isolated (each test independent).',
    keyPoints: [
      'Connection pool: pool_size=50, max_overflow=150 (200 total), pool_pre_ping for health checks, recycle hourly',
      'Repository pattern: BaseRepository with _apply_tenant_filter, automatic tenant isolation in all queries',
      'Transactions: Service layer coordinates repositories, single db.commit() at end, rollback on any exception',
      'N+1 prevention: joinedload (1 query, JOIN) for few relations, selectinload (2 queries, SELECT IN) for many',
      'Testing: Pytest fixtures with SQLite, create/drop tables per test, override get_db dependency, test isolation',
    ],
  },
  {
    id: 'fastapi-db-q-2',
    question:
      'Compare sync vs async SQLAlchemy with FastAPI for a high-traffic API (50K+ req/min). Discuss: (1) async SQLAlchemy 2.0 setup and configuration, (2) performance implications (throughput, latency, resource usage), (3) when async is beneficial vs overhead, (4) challenges with async (connection pooling, testing, debugging), (5) migration strategy from sync to async. Include benchmarks and decision framework.',
    sampleAnswer:
      'Sync vs Async SQLAlchemy analysis: (1) Async SQLAlchemy 2.0 setup: from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker; from sqlalchemy import select. # Async engine (note asyncpg driver); async_engine = create_async_engine("postgresql+asyncpg://user:pass@localhost/db", echo=False, pool_size=20, max_overflow=40, pool_pre_ping=True). # Async session factory; AsyncSessionLocal = async_sessionmaker (async_engine, class_=AsyncSession, expire_on_commit=False). # Async dependency; async def get_async_db(): async with AsyncSessionLocal() as session: yield session. # Async queries; @app.get("/users/{user_id}"); async def get_user (user_id: int, db: AsyncSession = Depends (get_async_db)): result = await db.execute (select(User).filter(User.id == user_id)); user = result.scalar_one_or_none(); if not user: raise HTTPException(404); return user. Key differences: asyncpg driver (not psycopg2), await db.execute() (not db.query()), result.scalar_one_or_none() (not .first()), async with sessionmaker. (2) Performance implications: Throughput benchmark (50K req/min): Sync (psycopg2): 10 workers, pool_size=20 → ~15K req/min per worker = total need 4 servers. Async (asyncpg): 10 workers, pool_size=20 → ~40K req/min per worker = total need 2 servers. 2x throughput with async! Why: Async doesn\'t block threads during I/O (DB queries). While waiting for DB response, thread handles other requests. Sync blocks thread for entire request duration. Latency: Sync: P50=50ms, P95=200ms, P99=500ms. Async: P50=45ms, P95=150ms, P99=300ms. 10-30% latency improvement. Resource usage: Sync: High CPU usage (threads waiting), more memory (thread stacks), database connections = workers × pool_size. Async: Lower CPU (efficient I/O multiplexing), less memory (fewer threads), database connections shared across concurrent requests. Example calculation (10K concurrent requests): Sync: Need 10K threads (1 per request) → ~10GB memory, 10K DB connections (impossible!). Async: Need 10 workers, each handles 1K concurrent with 20 DB connections → ~100MB memory, 200 DB connections total. 100x more efficient! (3) When async is beneficial: Use async when: I/O-bound APIs (most database/API apps), high concurrency (>1K concurrent users), database query time > 10ms (most queries), need to scale horizontally, cost-sensitive (fewer servers needed). Benchmark shows async benefits when: Query time > 5ms (overhead < benefit), concurrent requests > 100 per worker, external API calls involved (network I/O). NOT beneficial when: CPU-bound operations (image processing, data transformation), queries < 1ms (overhead dominates), synchronous libraries required (some legacy libraries), team unfamiliar with async (learning curve). Overhead: Async has ~0.5-1ms overhead per request (event loop, async/await). For very fast queries (<1ms), this overhead negates benefits. Example: Local Redis query (0.1ms) → sync faster. PostgreSQL query (10ms) → async much faster. (4) Async challenges: Connection pooling complexity: Async pools different from sync, requires understanding event loop integration, pool exhaustion harder to debug (silent failures), need proper pool sizing (too small = bottleneck, too large = DB overwhelmed). Testing challenges: Need pytest-asyncio: @pytest.mark.asyncio; async def test_get_user(). async fixtures more complex: @pytest.fixture; async def db(): async with AsyncSessionLocal() as session: yield session. Cannot mix sync/async in tests easily. Debugging harder: Stack traces more complex (async calls), print statements may not appear in order, need async-aware debuggers. Migration strategy: Cannot gradually migrate (sync/async don\'t mix well), need all-or-nothing migration, or separate endpoints (some sync, some async). Transaction management: Async transactions different: async with db.begin(): # transaction. Need to await all DB operations: await db.commit(), await db.refresh (obj). Easy to forget await and get runtime errors. (5) Migration strategy sync → async: Phase 1 - Preparation: Install asyncpg: pip install asyncpg sqlalchemy[asyncio]. Update models (no changes needed, same models work). Add type hints for async functions. Phase 2 - Dual deployment: Run async API on new servers, sync API on old servers, load balancer routes traffic, gradually shift traffic to async. Phase 3 - Code migration: Update queries: db.query(User) → await db.execute (select(User)). Update session handling: with SessionLocal() → async with AsyncSessionLocal(). Add await to all DB operations: db.commit() → await db.commit(). Update dependencies: def get_db() → async def get_async_db(). Phase 4 - Testing: Create async test fixtures, test all endpoints, load test to verify performance gains, monitor error rates. Phase 5 - Cutover: Switch DNS/load balancer to async servers, monitor metrics (latency, throughput, errors), keep sync as backup for 1 week, decommission sync servers. Real-world results: Before (sync): 4 servers, 10K req/min per server, 40K total, P95 latency 200ms. After (async): 2 servers, 20K req/min per server, 40K total, P95 latency 120ms. 50% cost savings, 40% latency improvement! Decision framework: Use sync if: Simple CRUD API, queries <5ms, low traffic (<1K req/min), team unfamiliar with async, existing sync codebase (large migration cost). Use async if: High traffic (>10K req/min), queries >10ms, need to scale, new project (no migration cost), external API calls (network I/O). Cost-benefit analysis: Async migration cost: 2-4 weeks engineering time, testing, potential bugs. Benefit: 50% infrastructure savings (\$10K→$5K/month), better user experience (lower latency). ROI: Positive if traffic > 10K req/min.',
    keyPoints: [
      'Setup: asyncpg driver, async_sessionmaker, await db.execute (select()), AsyncSession with async with',
      'Performance: 2x throughput (10K→20K req/min per worker), 10-30% latency improvement, 100x more efficient resource usage',
      'Beneficial when: I/O-bound (queries >10ms), high concurrency (>1K concurrent), scales horizontally, cost-sensitive',
      'Challenges: Complex connection pooling, pytest-asyncio for testing, harder debugging, cannot mix sync/async',
      'Migration: All-or-nothing, phase over dual deployment, async with, await all operations, test thoroughly, ROI positive at >10K req/min',
    ],
  },
  {
    id: 'fastapi-db-q-3',
    question:
      'Design database session management strategy for microservices architecture where multiple FastAPI services share a PostgreSQL database. Address: (1) connection pool sizing across services, (2) handling distributed transactions (saga pattern vs 2PC), (3) database migration coordination (Alembic across services), (4) preventing connection leaks and detecting issues, (5) monitoring and alerting for database health. Include production patterns and tooling.',
    sampleAnswer:
      'Microservices database strategy: (1) Connection pool sizing: Calculate per service: Total DB connections available: PostgreSQL max_connections = 200 (typical). Number of microservices: 5 services. Reserve 20 connections for admin/monitoring. Available for apps: 180 connections. Per-service allocation: 180 / 5 = 36 connections per service. Per-service pool configuration: engine = create_engine(DB_URL, pool_size=20, max_overflow=15, pool_timeout=30). pool_size=20 (permanent), max_overflow=15 (burst to 35 total), total < 36 (safe margin). Dynamic scaling: If service scales horizontally (3 instances), each instance: pool_size=7, max_overflow=5 (12 total), 3 instances × 12 = 36 connections total. Monitoring: from sqlalchemy import event; @event.listens_for (engine, "connect"); def log_connection (conn, record): logger.info (f"DB connection: {service_name}"). Track pool exhaustion: Alert when pool_size + overflow exhausted for >10s. Use PgBouncer for connection pooling: Services → PgBouncer (100 connections) → PostgreSQL (actual 20 connections), PgBouncer multiplexes connections, reduces actual DB connections, pool_mode=transaction (best for microservices). (2) Distributed transactions: Problem: Order Service creates order, Payment Service charges card. If payment fails, need to rollback order. Solution A - Saga pattern (recommended): Choreography (event-driven): Order Service creates order → publishes OrderCreated event, Payment Service listens → charges card → publishes PaymentCompleted or PaymentFailed, Order Service listens → if PaymentFailed, cancel order (compensating transaction). Implementation: from datetime import datetime. class OrderSaga: async def execute (self, order_data): # Step 1: Create order; order = await order_service.create_order (order_data); try: # Step 2: Process payment; payment = await payment_service.charge (order.id, order.total); # Step 3: Mark order paid; order.status = "paid"; order.payment_id = payment.id; await order_service.update (order); return order; except PaymentError: # Compensate: Cancel order; await order_service.cancel (order.id); raise HTTPException(400, "Payment failed, order cancelled"). Orchestration (coordinator): SagaOrchestrator coordinates all services, maintains saga state (in-progress, completed, compensating), retries failed steps, handles timeouts. Solution B - 2-Phase Commit (avoid in microservices): Phase 1 - Prepare: All services vote yes/no. Phase 2 - Commit/Abort: If all yes → commit, if any no → abort. Problems: Blocking (services wait for coordinator), single point of failure (coordinator down = all blocked), locks held during coordination (poor performance), not suitable for microservices. Recommendation: Use Saga pattern with eventual consistency. Trade-off: Not ACID, but better scalability and availability. (3) Alembic migration coordination: Problem: 5 services, shared database, who runs migrations? Solution A - Centralized migrations: Create db-migrations service/repo, all schema changes in one place, migrations run before deploying any service, other services reference shared schema. Structure: db-migrations/ ├── alembic/ │   ├── versions/ (all migrations) │   │   ├── 001_create_users.py │   │   ├── 002_create_orders.py │   │   └── 003_create_payments.py │   └── env.py ├── models/ (all models) │   ├── user.py │   ├── order.py │   └── payment.py. Deployment: 1. Run alembic upgrade head (CI/CD pipeline), 2. Deploy all services (refer to updated schema). Solution B - Service-owned tables: Each service owns specific tables: User Service owns users table, Order Service owns orders table, Payment Service owns payments table. Each service has own Alembic migrations, migrations run during service deployment. Challenge: Foreign keys across services (avoid! Use service-to-service calls instead). Solution C (recommended) - Hybrid: Shared core tables (users, tenants) → centralized migrations, service-specific tables → service-owned migrations, no foreign keys across boundaries. Migration safety: Backward-compatible migrations: Add columns (nullable or with default), don\'t drop columns immediately (deprecate first), use two-phase migrations for breaking changes. Two-phase example: Phase 1 (deploy v1): Add new_column (nullable), populate new_column, deploy code that writes to both old and new columns. Phase 2 (deploy v2, 1 week later): Update code to use new_column only, drop old_column. (4) Preventing connection leaks: Common causes: Not closing sessions: db = SessionLocal(); # forgot db.close(), Exceptions skip cleanup, Long-running transactions (locks held). Prevention: Always use dependency injection (automatic cleanup): def get_db(): db = SessionLocal(); try: yield db; finally: db.close(). Use context managers: from contextlib import contextmanager; @contextmanager; def db_session(): session = SessionLocal(); try: yield session; session.commit(); except: session.rollback(); raise; finally: session.close(). Set connection timeout: create_engine(DB_URL, pool_timeout=30, pool_recycle=3600). Detection: Monitor active connections: SELECT count(*) FROM pg_stat_activity WHERE datname = \'mydb\'. Alert when connections > 80% of max_connections. Log long-running queries: SELECT pid, now() - query_start as duration, query FROM pg_stat_activity WHERE state = \'active\' AND now() - query_start > interval \'5 seconds\'. Track pool metrics: engine.pool.size() (current connections), engine.pool.overflow() (overflow connections), Alert when overflow > 0 for extended periods. (5) Monitoring and alerting: Metrics to track: Connection pool usage: pool.size(), pool.overflow(), Alert when pool exhausted. Query performance: Slow queries (>100ms), N+1 queries (watch query count), use pg_stat_statements. Database health: CPU usage (>80% alert), disk I/O (IOPS), connection count, replication lag (if using replicas). Error rates: Connection errors, transaction rollbacks, deadlocks. Tooling: Prometheus + Grafana: Expose metrics: from prometheus_client import Counter, Histogram; db_query_duration = Histogram("db_query_duration_seconds", "DB query duration"). Scrape with Prometheus, visualize in Grafana. Sentry: Track database errors, slow query alerts, transaction failures. Custom health check: @app.get("/health"); async def health (db: Session = Depends (get_db)): try: db.execute("SELECT 1"); return {"status": "healthy", "database": "ok"}; except Exception as e: return {"status": "unhealthy", "database": str (e)}. pg_stat_statements: Enable in PostgreSQL: shared_preload_libraries = \'pg_stat_statements\'. Query analysis: SELECT query, calls, mean_exec_time FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10. Alerts: Connection pool exhausted >10s, Slow queries >500ms (P95), Database CPU >85%, Connection count >160 (80% of 200), Replication lag >10s. Production playbook: When pool exhausted: Check for connection leaks (long-running queries), increase pool_size if legitimate traffic spike, scale service horizontally (more instances, same total connections). When slow queries: Analyze with EXPLAIN ANALYZE, add indexes, optimize N+1 queries (use joinedload). When database CPU high: Optimize queries (missing indexes), add read replicas (split read/write), consider caching (Redis).',
    keyPoints: [
      'Connection pooling: 180 connections / 5 services = 36 per service, use PgBouncer for multiplexing, dynamic scaling',
      'Distributed transactions: Saga pattern (orchestration or choreography), compensating transactions, avoid 2PC, eventual consistency',
      'Alembic: Centralized for shared tables, service-owned for specific tables, backward-compatible migrations, two-phase for breaking changes',
      'Connection leaks: Use DI for automatic cleanup, context managers, timeouts, monitor pg_stat_activity, alert at 80% usage',
      'Monitoring: Track pool usage, slow queries (pg_stat_statements), health checks, Prometheus+Grafana, alert on pool exhausted/CPU/lag',
    ],
  },
];
