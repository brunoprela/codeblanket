export const designingMarketDataSystemsQuiz = [
  {
    id: 'dmds-q-1',
    question:
      'Design a market data system capable of ingesting 1M+ messages per second from multiple exchanges (NYSE, NASDAQ, Binance). Address: (1) feed handler architecture for normalizing different protocols (FIX, WebSocket, proprietary), (2) message queuing strategy (Kafka, RabbitMQ, or Redis Streams), (3) storage layer design with tiered approach (hot/warm/cold), (4) handling exchange outages and data gaps, (5) monitoring and alerting for data quality. Include specific technology choices and latency targets.',
    sampleAnswer:
      "Market data system design: (1) Feed Handler Architecture: Deploy separate feed handlers per exchange, each normalizing to common Tick format. Binance handler: WebSocket connection, parse JSON, extract symbol/price/size/timestamp. NYSE handler: FIX 4.4 protocol, parse execution reports, convert to Tick. Each handler runs in separate process for isolation. Normalization: Convert all timestamps to microseconds since epoch, standardize symbol names (AAPL vs AAPL.NASDAQ), convert prices to float64. Target: <100μs normalization latency per message. (2) Message Queuing: Use Apache Kafka for durability and replay. Partition by symbol (AAPL → partition 0, TSLA → partition 1) for parallel consumption. Config: replication_factor=3, acks=1 (balance durability/latency), compression=lz4 (2-3x compression). Throughput: Kafka can handle 1M+ msgs/sec on 3-node cluster. Alternative: Redis Streams for lower latency (<1ms) but less durability. Use Kafka for tick data, Redis for Level 1 quotes needing <1ms latency. (3) Storage Layer: Hot tier (0-7 days): TimescaleDB on NVMe SSD, query latency <10ms. Store all ticks (trades, quotes) with continuous aggregate views for 1m/5m bars. Partitioned by symbol and day. Warm tier (7-90 days): ClickHouse columnar storage on SSD. Materialized views for bar data, raw ticks compressed 10:1. Query latency ~100ms. Cold tier (90+ days): Parquet files on S3, compressed 50:1. Use AWS Athena for ad-hoc queries (latency 5-30s). Lifecycle: Auto-move data between tiers using cron jobs. Cost: Hot $1000/mo, Warm $500/mo, Cold $50/mo. (4) Handling Outages: Detect gap: If no ticks for symbol in 10 seconds, mark gap start. Resume: On reconnection, request historical data from exchange (FIX ExecutionReport resend) to fill gap. If unavailable, mark gap in database (gap_start, gap_end) for visibility in backtests. Multiple sources: Run redundant feed handlers (primary from Polygon, backup from Alpha Vantage). Conflict resolution: If both sources active, take median price. Failover: Automatic switchover if primary silent >5 seconds. (5) Data Quality Monitoring: Price spikes: Reject ticks where price > 5 standard deviations from 10-tick moving average. Log for manual review. Gaps: Track time since last tick per symbol. Alert if >10s during market hours. Tick rate: Baseline normal rate (e.g., AAPL = 1000 ticks/min). Alert if drops <50%. Sequence numbers: Validate increasing sequence numbers from exchange. Alert on gaps. Latency: Measure tick receive time vs tick exchange timestamp. Alert if >1 second lag. Storage growth: Track GB/day. Alert if exceeds 120% of baseline. Dashboard: Grafana showing tick rates per symbol, storage growth, gaps, latencies. Alerting via PagerDuty for critical issues. Critical: Data quality is more important than completeness. Better to have gaps (which you know about) than bad prices (which you don't).",
    keyPoints: [
      'Feed handlers: separate per exchange, normalize to common format, <100μs latency',
      'Message queue: Kafka partitioned by symbol, replication=3, compression=lz4, 1M+ msgs/sec',
      'Storage tiers: Hot (TimescaleDB <10ms), Warm (ClickHouse ~100ms), Cold (S3/Parquet 5-30s)',
      'Outages: detect gaps >10s, request historical backfill, redundant sources with median price',
      'Monitoring: price spikes (5σ), gaps, tick rate baseline, latency, storage growth, Grafana dashboard',
    ],
  },
  {
    id: 'dmds-q-2',
    question:
      'Explain the trade-offs between different time-series databases for tick data storage. Compare: (1) TimescaleDB, (2) ClickHouse, (3) InfluxDB, (4) DynamoDB, (5) raw Parquet on S3. For each, discuss: write throughput, query latency, compression ratio, cost, and appropriate use cases. Which would you choose for (a) real-time trading, (b) backtesting, (c) long-term archival?',
    sampleAnswer:
      'Time-series database comparison: (1) TimescaleDB: PostgreSQL extension. Write throughput: 100K inserts/sec (single node), 1M/sec (clustered). Query latency: <10ms for recent data (indexed queries), ~100ms for aggregations. Compression: 5-10x with native compression. Cost: $500-1000/mo for hot data (requires SSD/RAM). Pros: SQL queries, continuous aggregates (real-time bars), strong consistency, ACID guarantees. Cons: Expensive at scale, vertical scaling limits. Use case: Hot data (last 7 days), real-time queries, transactional consistency needed. (2) ClickHouse: Columnar database. Write throughput: 1M+ rows/sec. Query latency: 10-100ms for analytical queries (GROUP BY, aggregations), sub-second for billions of rows. Compression: 10-100x (columnar compression excellent for repetitive data). Cost: $200-500/mo (cheaper than TimescaleDB). Pros: Extremely fast for analytics, great compression, horizontal scaling. Cons: Eventual consistency, no transactions, SQL syntax differences. Use case: Warm storage (7-90 days), backtesting with aggregations, analytical queries. (3) InfluxDB: Purpose-built time-series DB. Write throughput: 500K points/sec. Query latency: <50ms for recent data. Compression: 10-20x. Cost: $300-700/mo. Pros: Built for time series, InfluxQL query language, downsampling, retention policies. Cons: Limited SQL support, clustering expensive (Enterprise only). Use case: Monitoring/metrics, IoT data. Less common for tick data (TimescaleDB/ClickHouse better). (4) DynamoDB: NoSQL key-value store. Write throughput: Unlimited (with auto-scaling), but expensive at high volume. Query latency: Single-digit ms for key-value lookups, slow for scans. Compression: None (store raw data). Cost: Expensive for bulk storage ($0.25/GB/mo + read/write costs). Pros: Fully managed, infinite scale, low latency for point queries. Cons: Expensive for high-volume writes, poor for analytical queries, no built-in aggregation. Use case: Low-volume data, simple key-value lookups. Not recommended for tick data. (5) Parquet on S3: Columnar file format on object storage. Write throughput: Fast (batch writes), append-only. Query latency: 5-30s (with AWS Athena), faster with Spark (1-5s). Compression: 50-100x (excellent columnar compression). Cost: $0.023/GB/mo (extremely cheap). Pros: Cheapest storage, good for archival, query with Athena/Spark, infinite scale. Cons: High query latency, not suitable for real-time, immutable (hard to update). Use case: Cold storage (90+ days), archival, infrequent access. Best for each use case: (a) Real-time trading: TimescaleDB. Need <10ms query latency, strong consistency, ACID guarantees for risk calculations. Store last 7 days in memory, SSD-backed. Alternative: Redis for Level 1 quotes (<1ms). (b) Backtesting: ClickHouse. Need fast analytical queries (GROUP BY date, symbol), can tolerate 10-100ms latency. Columnar format perfect for OHLCV calculations. Store 7-90 days. Use Parquet/S3 for historical backtests (90+ days), pre-compute bars to reduce query time. (c) Long-term archival: Parquet on S3. Store raw tick data compressed 50:1. Cost = $20-50/month for TBs of data. Query infrequently with Athena. Consider storing pre-computed bars (1m, 5m, 1h, 1d) in separate Parquet files for faster backtest loading. Recommended architecture: Hybrid approach using multiple databases for different access patterns. Hot: TimescaleDB → Warm: ClickHouse → Cold: S3/Parquet. Automatically move data between tiers as it ages.',
    keyPoints: [
      'TimescaleDB: <10ms latency, 100K writes/sec, expensive, best for hot data (0-7 days)',
      'ClickHouse: 10-100ms latency, 1M+ writes/sec, 10-100x compression, best for analytics (7-90 days)',
      'Parquet/S3: 5-30s latency, cheapest ($0.023/GB/mo), 50-100x compression, best for archival (90+ days)',
      'Real-time: TimescaleDB (<10ms); Backtesting: ClickHouse (fast aggregations); Archival: S3/Parquet (cheap)',
      'Hybrid architecture: Use multiple tiers, automatically move data as it ages',
    ],
  },
  {
    id: 'dmds-q-3',
    question:
      'Design a real-time bar aggregator that generates OHLCV bars at multiple timeframes (1m, 5m, 15m, 1h) from a tick stream. Address: (1) handling late-arriving ticks (out-of-order data), (2) aligning bars to exchange time zones (US=ET, Europe=CET, Crypto=UTC), (3) dealing with gaps during market close, (4) publishing bars for downstream consumption, (5) ensuring exactly-once semantics. How would you test this system for correctness?',
    sampleAnswer:
      "Real-time bar aggregator design: (1) Handling late-arriving ticks: Buffer approach: Keep bars open for 5-10 seconds after bar close to accept late ticks. Example: 09:30:00 bar closes at 09:31:00, but accept ticks with timestamp 09:30:00-09:30:59 until 09:31:10. Watermark system: Track watermark = max tick timestamp seen - 10 seconds. Only close bars before watermark. Out-of-order handling: Use priority queue sorted by tick timestamp (not arrival time). Process ticks in timestamp order. Revision mechanism: If very late tick arrives (>10s), publish revised bar with is_revision=true flag. Consumers decide whether to use. Example: Original bar: O=100, H=105, L=99, C=103. Late tick arrives with price=98 at 09:30:15. Revised bar: O=100, H=105, L=98, C=103 (new low). Trade-off: More buffer time = more accurate bars but higher latency. For real-time trading, 5s buffer is reasonable. (2) Time zone alignment: Store bars in exchange local time, not UTC. Reason: Market open/close meaningful in local time (09:30 ET for NYSE). Example: NYSE bars aligned to ET: 09:30:00, 09:31:00, ... XETRA bars aligned to CET: 09:00:00, 09:01:00, ... Binance bars aligned to UTC: 00:00:00, 00:01:00, ... Implementation: Convert tick timestamp to exchange timezone before bucketing. Python: tick_dt = datetime.fromtimestamp (tick.timestamp/1e6, tz=pytz.timezone('US/Eastern')). Bar timestamp stored as timezone-aware datetime. Consumers convert to their local timezone if needed. Daylight saving: Handle DST transitions carefully. Use pytz.localize() not datetime.replace(). DST transition days have 23 or 25 hour days—bar count differs. (3) Handling gaps during market close: Market close gaps (expected): NYSE closed 16:00 ET to 09:30 ET next day (17.5 hours). Don't generate bars during closed hours. Check exchange calendar: if pd.date_range (start, end, freq='1min')[i] not in market_hours: skip. Intraday gaps (unexpected): If no ticks for 5+ minutes during market hours, still publish empty bar with volume=0, close=previous_close, OHLC=previous_close. Flags: Add gap_detected=true to bar metadata. Weekend gaps: Stocks closed Sat-Sun. Crypto 24/7 but lower volume. Handle calendar appropriately per asset class. Gap detection: If tick_timestamp - last_tick_timestamp > 5 minutes during market hours → gap. (4) Publishing bars: Use Kafka topic per timeframe: bars_1m, bars_5m, bars_1h. Partition by symbol for parallel consumption. Message format: JSON or Protobuf with schema. Example: {symbol: AAPL, timestamp: 2024-01-15T09:31:00-05:00, open: 150.0, high: 151.0, low: 149.5, close: 150.5, volume: 125000, trades: 450, is_revision: false, gap_detected: false}. Consumers: Strategy engine, database writer, analytics service. Exactly-once semantics: Use Kafka transactions with idempotent producer. Deduplication: Include bar_id = hash (symbol, timestamp, timeframe). Consumers check if already processed. Checkpointing: Consumers commit Kafka offsets only after successful processing. (5) Testing for correctness: Unit tests: Test single symbol, single timeframe. Feed sorted ticks, verify OHLCV correct. Edge cases: Bar boundary (tick at exactly 09:31:00.000000), single tick in bar, no ticks (empty bar). Out-of-order tests: Feed ticks in random order, verify bar identical to sorted order (with buffer). Integration tests: Multi-symbol, multi-timeframe. Verify bars independent (AAPL 1m doesn't affect TSLA 5m). Gap tests: Inject 10-minute gap, verify empty bars generated and flagged. Compare with reference: Use pandas.resample() on historical data as ground truth. Compare aggregator output to pandas. Should match exactly. Revision tests: Send late tick, verify revised bar published. Consumers see both original and revised. Production validation: Daily reconciliation: Compare bars in database with re-aggregation from ticks. Alert on mismatch. Volumetric checks: Daily volume from 1m bars should equal sum of tick volumes. Check OHLC consistency: open of bar i+1 should equal close of bar i (or close if gap). Critical: Bar aggregator is mission-critical. Bugs here poison entire downstream system (strategies, backtests, analytics).",
    keyPoints: [
      'Late ticks: 5-10s buffer, watermark system, priority queue, revision mechanism for very late ticks',
      'Time zones: Store in exchange local time, handle DST with pytz, different alignment per exchange',
      'Gaps: Check market calendar, publish empty bars (volume=0) with gap_detected=true flag',
      'Publishing: Kafka topics per timeframe, partitioned by symbol, exactly-once with transactions',
      'Testing: unit tests, out-of-order, compare with pandas.resample, daily reconciliation, volume checks',
    ],
  },
];
