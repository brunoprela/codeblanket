export const mlModelServingTradingQuiz = [
  {
    id: 'mmst-q-1',
    question:
      'Design an ML model serving system for high-frequency trading with <1ms end-to-end latency (feature computation + inference + result delivery). Address: (1) model selection and optimization techniques, (2) feature store architecture for real-time features, (3) minimizing inference latency, (4) handling model updates without downtime, (5) monitoring prediction quality. What are the trade-offs between model complexity and latency?',
    sampleAnswer:
      "Sub-millisecond ML serving: (1) Model selection: Fast models (<100μs inference): Linear regression (5-10μs), Logistic regression (5-10μs), LightGBM (20-50μs for 100 trees depth 6), Simple NN (50-100μs for 2 layers 64 units). Slow models (>1ms): Deep NNs (1-10ms for 10+ layers), Transformers (10-100ms), Graph NNs (10-50ms). Optimization: Use ONNX Runtime (2-5x speedup vs native), quantization (INT8 vs FP32, 2-4x faster), model pruning (remove low-importance features/neurons), distillation (train small model to mimic large). Example: LightGBM with 100 trees depth 6: Inference 30μs, accuracy 70%, Sharpe 1.5. Deep NN 10 layers: Inference 5ms (too slow), accuracy 73%, Sharpe 1.6 (not worth latency cost). Trade-off: Simple model 30μs + acceptable accuracy better than complex model 5ms + marginal accuracy gain. (2) Feature store (Redis): Architecture: Market data stream → feature pipeline → Redis (pre-computed features) → model server. Store: Technical indicators (SMA, RSI, Bollinger Bands) computed in batch every 1 second, store in Redis with 60s TTL. Retrieval: Model server reads from Redis (< 1ms network latency if co-located). Format: Binary (numpy array serialized) for speed, not JSON. Key design: features:AAPL → numpy array [sma_20, rsi_14, bb_upper, ...]. Real-time features: Price changes, volume computed on-the-fly (cheap: <10μs). Historical features: Expensive aggregations pre-computed and cached. Latency breakdown: Fetch from Redis: 0.5ms, compute real-time features: 0.05ms, model inference: 0.03ms, total: 0.58ms < 1ms ✓. (3) Minimizing inference latency: Pre-allocate arrays: feature_buffer = np.zeros(100, dtype=np.float32), reuse buffer (avoid malloc). Warm-up: Run 100 dummy predictions at startup (load model into CPU cache). Single-threaded: Avoid thread context switches (pin to CPU core). Batch size 1: Don't batch (batching adds latency waiting for batch to fill). Memory-mapped models: Load model weights via mmap (faster than file I/O). Measure: Use rdtsc (CPU timestamp counter) for nanosecond precision timing. Profile: Identify bottlenecks (feature fetch 80% of latency → optimize fetch). Example optimization: Before: 2ms latency (1.5ms Redis fetch, 0.5ms inference). After: Co-locate Redis on same machine (0.1ms fetch), optimize model (0.03ms inference), total: 0.13ms. (4) Model updates without downtime: Blue-green deployment: Run two models (blue = production, green = new). Load new model to green (production unaffected). Test green in shadow mode (compute predictions, don't use). Switch traffic blue → green (instant). Monitor for 1 hour. Rollback if issues (switch back to blue). Canary deployment: Gradual traffic shift: Day 1: 5% traffic to new model, Day 2: 25%, Day 3: 50%, Day 4: 100%. Monitor metrics per model (latency, Sharpe ratio, prediction distribution). Rollback if new model underperforms. A/B testing: Run two models in parallel (70% traffic model A, 30% model B). Compare Sharpe ratios over 1 week. Winner gets 100% traffic. Zero downtime: Load new model in memory (no disk I/O during serving). Atomic pointer swap (change active_model pointer). Old model kept in memory for 1 hour (quick rollback). (5) Monitoring prediction quality: Latency: Track p50, p95, p99 inference latency. Alert if p99 > 1ms (SLA breach). Throughput: Predictions per second. Alert if drops below baseline. Prediction distribution: Track mean, std of predictions. Alert if shift >3σ (indicates drift). Feature drift: Compare recent feature distribution to training. KL divergence or simple mean/std comparison. Alert if divergence high. Model accuracy: If outcomes observable (did price go up after buy signal?), track win rate. Alert if drops >5% from baseline. Example monitoring: Baseline: p99 latency 100μs, mean prediction 0.05, win rate 55%. Day 30: p99 latency 150μs (↑50%, investigate), mean prediction 0.12 (↑140%, drift detected), win rate 52% (↓3%, acceptable). Action: Feature drift → retrain model on recent data. Latency increase → profile code, optimize hot path. (6) Trade-offs model complexity vs latency: Simple models (linear, shallow trees): Pros: Fast inference (5-50μs), easy to interpret, robust (less overfitting). Cons: Lower accuracy (may miss non-linear patterns), limited capacity. Example: Linear model Sharpe 1.2, latency 10μs. Complex models (deep NNs, transformers): Pros: Higher accuracy (capture complex patterns), better on non-linear data. Cons: Slow inference (1-100ms), black-box, overfit risk. Example: Transformer Sharpe 1.4, latency 50ms. Optimal choice: Depends on trading frequency. HFT (1000+ trades/day): Latency critical → simple models. Sharpe 1.2 with 10μs better than Sharpe 1.4 with 50ms (latency costs exceed accuracy gains). Medium frequency (10-100 trades/day): Can tolerate 10ms → complex models okay. Sharpe improvement offsets latency cost. Low frequency (1-5 trades/day): Latency irrelevant → use most accurate model. Economic analysis: HFT: 10μs advantage = $10K-100K/year value. 1.4 vs 1.2 Sharpe = $50K/year value. Latency advantage more valuable → choose simple model. Conclusion: For trading, simple fast models usually optimal. Diminishing returns from complexity. Key metric: Sharpe per microsecond. Maximize information per unit latency.",
    keyPoints: [
      'Fast models: Linear (5-10μs), LightGBM (20-50μs), Simple NN (50-100μs); Slow: Deep NN (1-10ms)',
      'Feature store: Redis, pre-compute expensive features (SMA, RSI), fetch 0.5ms, compute realtime 0.05ms',
      'Latency optimization: pre-allocate arrays, warm-up, single-threaded, batch size 1, mmap models',
      'Zero downtime: blue-green (instant switch), canary (gradual 5%→100%), A/B test (70/30 split)',
      'Trade-off: Simple fast models (Sharpe 1.2, 10μs) better than complex slow (Sharpe 1.4, 50ms) for HFT',
    ],
  },
  {
    id: 'mmst-q-2',
    question:
      'Compare different model serving frameworks for trading: (1) ONNX Runtime, (2) TensorFlow Serving, (3) Native framework (scikit-learn, LightGBM). For each: explain architecture, latency characteristics, pros/cons, when to use. How would you benchmark to choose the optimal framework for your trading strategy?',
    sampleAnswer:
      'Model serving framework comparison: (1) ONNX Runtime: Architecture: Convert model to ONNX format (open standard), optimized C++ runtime executes model, supports multiple frameworks (PyTorch, TensorFlow, scikit-learn). Latency: 2-5x faster than native framework. Example: scikit-learn RandomForest 200μs → ONNX 50μs. Optimizations: Graph optimization (fuse operations), quantization (FP32 → INT8), multi-threading (if beneficial), operator fusion. Pros: Framework-agnostic (one format, any source), optimized for CPU inference, easy deployment (single binary), good latency. Cons: Conversion required (extra step), not all models supported (some custom ops fail), debugging harder (optimized graph obscured). When to use: Production serving where latency critical, have models from multiple frameworks, want portability. (2) TensorFlow Serving: Architecture: gRPC server serving TensorFlow models, batching requests, multi-model serving, versioning. Latency: Moderate (1-10ms) due to gRPC overhead. Good for GPU, less optimal for CPU. Optimizations: Batching (amortize fixed costs), GPU acceleration, TensorRT integration. Pros: Production-ready (Google scale), model versioning built-in, monitoring/metrics, GPU support excellent. Cons: TensorFlow-only (not PyTorch, scikit-learn), gRPC overhead (milliseconds), heavyweight (complex setup). When to use: TensorFlow models, GPU inference needed, multiple models/versions, okay with 1-10ms latency (not HFT). (3) Native framework (LightGBM, scikit-learn): Architecture: Load model in Python, call predict() directly, no intermediary. Latency: Varies. LightGBM fast (20-100μs), scikit-learn moderate (50-500μs), PyTorch slow (1-10ms). Pros: Simplest (no conversion), full framework features, easy debugging. Cons: Slower than optimized (ONNX 2-5x faster), Python GIL issues (multi-threading limited), deployment harder (Python dependencies). When to use: Prototyping/research, simplicity valued over performance, latency <10ms acceptable. Detailed comparison table: Framework | Latency | Pros | Cons | Use case. ONNX | 20-100μs | Fast, portable | Conversion step | HFT, production. TF Serving | 1-10ms | Scalable, GPU | gRPC overhead | GPU inference, microservices. Native | 50-500μs | Simple, full features | Slower, Python deps | Prototyping, non-critical. Benchmarking methodology: (1) Setup: Same model (e.g., LightGBM 100 trees depth 6), same hardware (8-core CPU, no GPU), same input (100 features). (2) Metrics: Latency: p50, p95, p99 (run 10K predictions), throughput: predictions/sec, memory: RAM usage, cold start: Time to load model. (3) Tests: Single-threaded: Pin to one core (trading typically single-threaded), batching: Test batch sizes 1, 10, 100 (trading usually batch 1), warm vs cold: Measure after warm-up and first prediction. (4) Results (example): ONNX: p50 25μs, p99 45μs, throughput 40K/sec, memory 100MB. TF Serving: p50 2ms, p99 8ms, throughput 500/sec, memory 500MB. Native LightGBM: p50 60μs, p99 120μs, throughput 16K/sec, memory 80MB. (5) Decision criteria: Latency requirement: If need <100μs → ONNX or Native LightGBM (not TF Serving). Throughput: If >10K predictions/sec → ONNX (best throughput). Simplicity: If prototyping → Native (easiest). GPU: If GPU inference → TensorFlow Serving (best GPU support). Example decision: HFT strategy needing <50μs latency → ONNX Runtime (p99 45μs ✓). Medium-frequency needing GPU → TensorFlow Serving (GPU support ✓). Research prototype → Native (simplest ✓). Best practice for trading: Use ONNX Runtime for production (fastest, portable), develop with native framework (easy debugging), convert to ONNX before deployment, benchmark on your hardware (latency varies). Advanced: Hybrid approach: Critical path (price prediction): ONNX Runtime (20μs), non-critical (risk assessment): Native or TF Serving (slower acceptable). Co-locate Redis and model server (reduce network latency 0.5ms → 0.05ms). Custom C++ inference (if ONNX not fast enough): 10-100x faster, but 10x harder to maintain. Conclusion: For HFT trading, ONNX Runtime optimal (best latency + portability). For GPU-based deep learning, TensorFlow Serving better. Always benchmark on your specific hardware and workload—results vary significantly.',
    keyPoints: [
      'ONNX Runtime: 20-100μs, 2-5x faster than native, portable, best for HFT CPU inference',
      'TensorFlow Serving: 1-10ms, gRPC overhead, excellent GPU support, not for HFT',
      'Native (LightGBM): 50-500μs, simplest, good for prototyping, slower than ONNX',
      'Benchmark: measure p50/p99 latency, throughput, memory on YOUR hardware (results vary)',
      'Trading decision: HFT → ONNX Runtime, GPU inference → TF Serving, research → Native',
    ],
  },
  {
    id: 'mmst-q-3',
    question:
      'Design a feature engineering pipeline for real-time trading that ensures training-serving consistency. Address: (1) preventing training-serving skew, (2) handling late-arriving or missing data, (3) feature versioning and reproducibility, (4) testing feature correctness, (5) monitoring feature drift. What are common causes of training-serving skew and how do you prevent them?',
    sampleAnswer:
      "Feature engineering for training-serving consistency: (1) Preventing training-serving skew: Problem: Features computed differently in training vs serving → model fails in production. Example: Training: df['sma_20'] = df['close'].rolling(20).mean() (uses future data at boundaries!). Serving: sma_20 = sum(recent_20_closes) / 20 (correct, but implementation differs). Solution: Shared feature code: Write features once, use in both training and serving. def compute_features(close_prices): sma_20 = close_prices[-20:].mean(). rsi_14 = compute_rsi(close_prices[-14:]). return {'sma_20': sma_20, 'rsi_14': rsi_14}. Training: features = compute_features(historical_prices). Serving: features = compute_features(live_prices). Use feature store: Compute features in pipeline, store in Redis, both training and serving read from same store. Version features: Feature v1.2.3 (semantic versioning), training and serving must use same version. Testing: Unit tests verify training and serving produce identical features for same input. assert abs(training_features - serving_features) < 1e-6. Common skew causes: Lookahead bias: Training uses df.fillna(method='bfill') (future data), serving uses ffill (correct). Time zones: Training uses UTC, serving uses local time → off-by-hours. Floating point: Training uses float64, serving float32 → numerical differences. Libraries: Training uses pandas, serving uses numpy → subtle differences. Prevent: Same code path, same libraries, same dtypes, comprehensive tests. (2) Handling late/missing data: Late-arriving data: Problem: Market data arrives out-of-order. Tick at 10:00:05 arrives after tick at 10:00:06. Solution: Use timestamp from data (not arrival time). Buffer window: Hold data for 5 seconds, sort by timestamp, then process. If very late (>5s), discard or flag for investigation. Missing data: Problem: Exchange down, network issue → missing ticks. Solution: Forward fill (ffill): Use last known value. Interpolation: Linear interpolate for continuous features (price), not for discrete (volume = 0 if missing). Flag: Add is_imputed flag to features (model can learn to handle). Example: if recent_prices.empty: sma_20 = last_known_sma. is_imputed = True. else: sma_20 = recent_prices.mean(). is_imputed = False. Trade-off: Imputation introduces error, but better than no prediction. Test: Simulate missing data in backtest, verify strategy robust. (3) Feature versioning: Why: Features evolve (add new, remove old, change computation). Must track which version model trained on. Versioning scheme: Semantic: v1.2.3 (major.minor.patch). Major: Breaking change (remove feature). Minor: Add feature. Patch: Bug fix. Storage: Store feature version with model metadata. Model trained on features v1.2.3, serving must use v1.2.3. Implementation: class FeatureEngineering: VERSION = \"1.2.3\". def compute(self, data): # Feature logic. return features, self.VERSION. Training: features, version = fe.compute(train_data). model.metadata['feature_version'] = version. Serving: required_version = model.metadata['feature_version']. if fe.VERSION != required_version: raise ValueError(\"Feature version mismatch\"). Migration: When updating features, train new model with new version, deploy model + feature code together (atomic). Reproducibility: Git tag feature code at training time, serving checks out same git tag (exact same code). (4) Testing feature correctness: Unit tests: test_sma_20(): input = [100, 101, 102, ...] (20 values). expected = 100.5. assert compute_sma(input) == expected. Integration tests: Known input → verify all features correct. Use golden dataset (carefully validated). Regression tests: After feature changes, verify output unchanged for same input (if not intentional change). Cross-validation: Compute features in training, save, compute in serving for same data, compare: training_features = compute_features(data). serving_features = compute_features_serving(data). assert np.allclose(training_features, serving_features, rtol=1e-5). Online validation: In production, log features for sample of requests, recompute offline, compare (detect drift/bugs). Example: Every 1000th request, log features, batch job compares with recomputed features. Alert if >1% divergence. (5) Monitoring feature drift: Feature drift: Distribution of features changes over time (market regime change). Detection: Statistical: Compare recent features (last 1000) to training features. Mean/std: If mean shifted >3σ → drift. KL divergence: Measure distribution distance. Example: Training: RSI mean=50, std=20. Recent: RSI mean=65, std=25. Drift = (65-50)/20 = 0.75σ (within normal). Another feature: Training mean=0.05, recent=0.12. Drift = (0.12-0.05)/0.02 = 3.5σ → alert! Response to drift: Retrain model: Use recent data (last 6 months), not all historical. Feature engineering: Add regime-adaptive features (volatility-adjusted). Model ensemble: Blend models trained on different regimes. Monitoring dashboard: Plot feature distributions over time (Grafana). Alert if KL divergence >threshold. Example dashboard: Feature | Training Mean | Current Mean | Drift. SMA_20 | 100.5 | 105.2 | 0.5σ. RSI_14 | 50.0 | 65.0 | 3.5σ ← ALERT. Volume | 1M | 1.5M | 2.1σ. (6) Production best practices: Feature store (Feast, Tecton): Central repository for features, versioning built-in, training and serving use same store. Offline features: Batch-computed (nightly) for training. Online features: Real-time for serving. Consistency guaranteed. Feature validation: Schema validation: Ensure expected features present, correct dtypes. Range checks: price > 0, 0 ≤ RSI ≤ 100. Outlier detection: Flag if feature >5σ from mean. Rollback: If feature pipeline fails, rollback to last known good version (blue-green deployment). Monitoring: Track every stage (data ingestion, feature computation, model inference). Alert on any failure. Example: Feature computation latency: <100ms (alert if >1s). Feature freshness: <5s old (alert if stale). Missing data rate: <1% (alert if >5%). Conclusion: Training-serving consistency critical. Use same code, version features, test extensively, monitor in production. Skew causes production failures—prevent with discipline.",
    keyPoints: [
      'Prevent skew: same code path training/serving, shared feature functions, version features (v1.2.3), unit tests',
      'Late/missing data: use timestamp (not arrival), 5s buffer window, forward fill, flag imputed features',
      'Versioning: semantic versioning, store with model metadata, atomic deploy (model + features), git tag code',
      'Testing: unit tests (golden datasets), integration (known input/output), cross-validation (train vs serve features)',
      'Drift detection: KL divergence, mean/std comparison (>3σ alert), retrain on recent data, monitoring dashboard',
    ],
  },
];
