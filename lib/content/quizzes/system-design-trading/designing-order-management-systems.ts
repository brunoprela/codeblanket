export const designingOrderManagementSystemsQuiz = [
  {
    id: 'doms-q-1',
    question:
      'You are designing an OMS for a high-frequency trading firm with latency requirements of <100μs end-to-end. Compare monolithic vs microservices architecture: (1) analyze the latency impact of each approach with specific estimates, (2) discuss trade-offs for scalability, resilience, and deployment, (3) propose a hybrid architecture that balances latency and scalability, (4) explain how to measure and optimize critical path latency. Include specific optimization techniques.',
    sampleAnswer:
      'Architecture comparison: (1) Monolithic OMS: All components (risk, routing, position tracking) in single process. Latency: Risk check ~5μs (in-memory), routing ~10μs (shared memory), total ~50-80μs achievable. No network overhead, direct function calls, shared memory. Advantages: Lowest latency, simplest deployment, no serialization cost. Disadvantages: Vertical scaling only, single point of failure, difficult to update components independently. (2) Microservices: Separate services for risk, routing, positions. Latency: Network call ~100-500μs (even with low-latency networks), serialization ~20-50μs per hop, total ~500-1000μs. Multiple network hops kill latency. Advantages: Horizontal scaling, independent deployment, failure isolation, technology diversity. Disadvantages: Network latency, complexity, debugging harder. (3) Hybrid architecture: Hot path (order submission, risk, routing) in monolithic core for <100μs latency. Cold path (reporting, analytics, compliance) as microservices for flexibility. Use shared memory or memory-mapped files for zero-copy IPC between critical components. Position service: Separate but co-located on same machine with shared memory. Example: Core OMS process handles orders, publishes events to message bus for downstream services. (4) Optimization techniques: (a) Memory pools: Pre-allocate order objects to avoid malloc/free (saves ~5-10μs). (b) Lock-free queues: Use atomic operations instead of locks (saves ~2-5μs). (c) CPU pinning: Pin threads to specific cores to avoid context switches. (d) DPDK: Kernel bypass for network I/O (saves ~50-100μs). (e) Cython/C++ for hot paths: 10-20x faster than Python. (f) Measurement: Use rdtsc (CPU timestamp counter) for nanosecond precision timing. Measure at each stage: submission → risk → validation → routing. Identify bottlenecks with flame graphs. Target: Risk check <10μs, routing <15μs, total <80μs. Critical: For HFT (<100μs), monolithic or hybrid is required. For institutional trading (1-10ms acceptable), microservices work fine.',
    keyPoints: [
      'Monolithic: <100μs achievable, no network overhead, vertical scaling only',
      'Microservices: 500-1000μs latency, horizontal scaling, failure isolation',
      'Hybrid: Hot path monolithic (<100μs), cold path microservices (flexibility)',
      'Optimizations: memory pools, lock-free queues, CPU pinning, DPDK, Cython',
      'Measure with rdtsc, target breakdowns: risk <10μs, routing <15μs',
    ],
  },
  {
    id: 'doms-q-2',
    question:
      'Design a comprehensive risk management system for an OMS that prevents catastrophic losses. Address: (1) pre-trade risk checks (types and implementation), (2) real-time position tracking with thread-safety, (3) handling the "fat finger" problem (erroneous large orders), (4) implementing circuit breakers for system failures, (5) position reconciliation with brokers. How would you test this system to ensure it never fails?',
    sampleAnswer:
      "Comprehensive risk system: (1) Pre-trade checks (all must pass before order submitted): (a) Position limits: Check absolute position limit (e.g., max 10K shares of AAPL). Use lock-protected position map. (b) Order value limit: quantity * price < max_order_value (e.g., $1M). Prevent single huge order. (c) Price reasonableness: For limit orders, check limit price within 5% of current market price. Catches fat fingers (e.g., buy limit at $1000 when market is $100). (d) Notional exposure: Sum of abs(position * price) across all symbols < max_exposure (e.g., $10M). (e) Concentration limits: No single position > 20% of portfolio. (f) Velocity limits: Max orders per second per account (e.g., 100/sec). Prevent runaway algorithms. (2) Thread-safe position tracking: Use lock-protected dictionary (Python) or concurrent hash map (C++). All position updates acquire lock. Alternative: Actor model with single-threaded position manager. Store positions in Redis for HA recovery. Reconcile positions every 5 minutes against broker reports. (3) Fat finger prevention: (a) Price checks: Reject if limit price > 3 standard deviations from recent average. (b) Size checks: Reject if quantity > 10x average order size for this account. (c) Confirmation step: Orders > $100K require manual confirmation (add confirmation_required flag). (d) Order preview: Show projected P&L impact before submission. (e) Rate limiting: Max 1 large order per minute. (4) Circuit breakers: (a) Detect failures: Count consecutive rejections from broker. If > 10 in 1 minute → OPEN circuit. (b) States: CLOSED (normal) → OPEN (block all orders) → HALF_OPEN (test with 1 order). (c) Timeout: After 60 seconds in OPEN, try HALF_OPEN. If successful → CLOSED. (d) Scope: Per broker/exchange. One exchange down doesn't block others. (e) Kill switch: Manual override to block all trading. Big red button. (5) Position reconciliation: Every 5 minutes, fetch positions from broker API. Compare with internal positions. If mismatch > threshold (e.g., 1% or 10 shares), alert and investigate. Possible causes: Missed fill report, duplicate fill, manual intervention. Resolution: Trust broker positions (source of truth), adjust internal state, log discrepancy. (6) Testing strategy: (a) Unit tests: Mock scenarios (position at limit, fat finger prices, concurrent updates). (b) Chaos testing: Randomly fail components, inject bad data, test recovery. (c) Load testing: Simulate 100K orders/sec, verify all checks complete. (d) Backtesting: Run historical trades through risk system, verify would have caught known incidents. (e) Kill switch drill: Monthly test of emergency shutdown. Time to halt all trading: <10 seconds. (f) Position reconciliation test: Deliberately inject discrepancy, verify alerts fire. Critical: Risk system is the last line of defense. Must be fail-safe (reject if uncertain), not fail-soft. Logging all risk decisions with reasoning for post-incident analysis.",
    keyPoints: [
      'Pre-trade checks: position limits, order value, price reasonableness, concentration, velocity',
      'Thread-safe positions: locks or actor model, reconcile with broker every 5 min',
      'Fat finger: price bounds (3σ), size limits (10x average), manual confirmation >$100K',
      'Circuit breaker: CLOSED→OPEN→HALF_OPEN, 10 failures in 1 min triggers, 60s timeout',
      'Testing: unit tests, chaos, load, backtesting, kill switch drills, fail-safe not fail-soft',
    ],
  },
  {
    id: 'doms-q-3',
    question:
      'Design a high-availability OMS that guarantees zero data loss and <1 second failover time. Address: (1) primary-secondary architecture with state replication, (2) detecting primary failure and promotion logic, (3) handling split-brain scenarios, (4) order state persistence strategy (database, in-memory, hybrid), (5) recovery process after failover. What are the CAP theorem implications, and how do you handle network partitions?',
    sampleAnswer:
      "HA OMS design: (1) Primary-secondary architecture: Run two OMS instances (primary and secondary). Primary handles all requests, writes to shared state (Redis/PostgreSQL). Secondary monitors primary via heartbeat (1s interval). Shared state contains: all active orders, positions, risk limits, pending fills. Use Redis for low-latency (1ms reads) or PostgreSQL for durability. Hybrid: Redis for active orders (hot), PostgreSQL for completed orders (cold). Replication: Primary writes to shared state, secondary reads on startup. Both instances always have access to same data. (2) Failure detection and promotion: Primary sends heartbeat to Redis every 1s with TTL=3s. Secondary checks heartbeat every 1s. If heartbeat expires (no update in 3s) → primary failed. Promotion: Secondary acquires lock in Redis (SET NX) to become primary. First to acquire lock wins (prevents multiple promotions). New primary: (a) Loads active orders from Redis, (b) Resumes order processing, (c) Starts sending heartbeat. Total time: <1s (detect 1s + promote 0.5s). (3) Split-brain prevention: Use distributed lock with fencing tokens. Lock in Redis includes incrementing counter. Example: Primary 1 has token 5, Primary 2 (split brain) has token 6. Broker/exchange only accepts messages with highest token seen. When network partition heals, Primary 1's orders rejected (lower token). Alternative: Use Zookeeper/etcd for leader election with stronger consistency. Only write to broker if hold valid lease. (4) State persistence strategy: Hot data (active orders): Redis for speed. Write-through: Every order state change written to Redis immediately. TTL on completed orders (expire after 1 hour). Cold data (historical orders): PostgreSQL. Async write after order completed. For audit trail, regulatory compliance. Hybrid approach: Write-ahead log (WAL) to disk for durability, then async flush to PostgreSQL. On crash, replay WAL. Backup: Snapshot Redis every 5 minutes. (5) Recovery process: Secondary promotion: (a) Acquire primary lock (SET NX), (b) Load active orders from Redis (orders in status SUBMITTED, ACKNOWLEDGED, PARTIALLY_FILLED), (c) Reconnect to broker/exchange via FIX, (d) Send execution report requests for active orders (check current status), (e) Resume order processing, (f) Start heartbeat. Handle in-flight orders: Mark as UNKNOWN status on failover, query broker for current state, update internal state based on response. Time to full recovery: <10s (includes reconnection to broker). (6) CAP theorem: Trading requires availability and partition tolerance → eventual consistency. During network partition: Continue accepting orders (availability), risk temporary inconsistency. Resolution: After partition heals, reconcile positions with broker (broker is source of truth). Accept that P&L reports may be temporarily incorrect during partition, but all orders eventually reconciled. Trade-off: Prioritize not blocking trading over perfect consistency. Critical: Network partition between OMS and broker is most dangerous. Solution: Co-locate OMS near broker/exchange, use redundant network paths, detect partition quickly and halt trading. Testing: Simulate network partitions monthly, verify failover time <1s, zero order loss.",
    keyPoints: [
      'Primary-secondary: shared state (Redis for speed, PostgreSQL for durability), heartbeat every 1s',
      'Failure detection: heartbeat TTL=3s, promotion via distributed lock (SET NX), <1s failover',
      'Split-brain prevention: fencing tokens, incrementing counter, broker rejects lower tokens',
      'State persistence: Redis for hot (active orders), PostgreSQL for cold (historical), WAL for durability',
      'CAP: prioritize availability + partition tolerance → eventual consistency, reconcile after partition',
    ],
  },
];
