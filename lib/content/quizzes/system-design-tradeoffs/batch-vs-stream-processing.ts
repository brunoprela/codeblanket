/**
 * Quiz questions for Batch Processing vs Stream Processing section
 */

export const batchvsstreamprocessingQuiz = [
  {
    id: 'q1',
    question:
      'You are designing a fraud detection system for credit card transactions. Should you use batch or stream processing? Justify your choice.',
    sampleAnswer:
      'Credit card fraud detection - STREAM PROCESSING: ANSWER: Use STREAM PROCESSING (real-time). REASONING: (1) Time-Sensitive: Fraud must be detected BEFORE transaction completes. Batch processing (daily): Fraud detected next day (too late, money already stolen). Stream processing: Fraud detected within 1 second (can block transaction). (2) User Experience: Legitimate transaction blocked immediately → User can retry. Legitimate transaction approved, then reversed later (batch) → Angry customer, support ticket. (3) Financial Impact: Real-time detection: Block $10,000 fraudulent transaction before it completes. Batch detection: Discover fraud next day, money already transferred, hard to recover. Cost of stream processing ($2,000/month) << Cost of fraud losses ($100,000s/month). (4) Regulatory Requirements: Credit card networks require real-time fraud detection (PCI-DSS compliance). Must respond within 1-2 seconds of transaction. IMPLEMENTATION: Architecture: Transaction → Kafka → Fraud Detection Service → Block/Approve. Kafka: Ingests transactions (1M/sec). Flink/Kafka Streams: Processes each transaction in real-time. Rules: Check transaction amount, location, merchant, recent history. Model: Pre-trained ML model (updated daily via batch). Output: APPROVE or BLOCK within 500ms. Real-time rules: If transaction amount > $5,000 AND location changed by >1,000 miles in 1 hour → BLOCK. If merchant on blacklist → BLOCK. If spending pattern anomalous (ML model) → BLOCK or require 2FA. Historical context (batch + stream): Batch (nightly): Train ML model on past 6 months of transactions. Update fraud patterns, merchant risk scores. Stream (real-time): Maintain last 10 transactions per card (sliding window). Track spending velocity (transactions per hour). LATENCY TARGET: P50: <100ms (median transaction processed in 100ms). P99: <500ms (99% of transactions within 500ms). SCALE: 1M transactions/second (peak, Black Friday). Kafka: 1M events/sec (no problem). Flink cluster: 50 workers, process 20K transactions/sec each. Total capacity: 1M transactions/sec. COST vs BENEFIT: Cost of stream processing: Kafka + Flink cluster: $5,000/month. Ops/monitoring: $2,000/month. Total: $7,000/month. Benefit: Prevent fraud: Assume 0.1% of transactions are fraudulent. 1M transactions/day × 0.1% = 1,000 fraudulent transactions/day. Average fraud amount: $500. Daily fraud prevented: 1,000 × $500 = $500,000/day. Monthly fraud prevented: $15 MILLION. ROI: $15M saved / $7K cost = 2,142x return! CONCLUSION: Stream processing is THE ONLY option for fraud detection. Real-time detection prevents fraud before it happens. Batch processing would discover fraud after the fact (too late). The cost of stream processing is negligible compared to fraud losses prevented.',
    keyPoints: [
      'Stream processing for fraud detection (real-time required)',
      'Must detect fraud within 1 second (before transaction completes)',
      'Batch processing too slow (fraud detected next day, money already stolen)',
      'Implementation: Kafka → Flink/Kafka Streams → Block/Approve',
      'ROI: Stream processing cost ($7K/month) << Fraud prevented ($15M/month)',
    ],
  },
  {
    id: 'q2',
    question:
      "Compare batch and stream processing for generating a company's monthly sales report. Which would you choose and why?",
    sampleAnswer:
      'Monthly sales report - BATCH PROCESSING: ANSWER: Use BATCH PROCESSING. REASONING: (1) Latency Not Critical: Report generated once per month (on last day of month). No need for real-time updates. Can tolerate 24-hour processing time. (2) Complete Dataset Required: Need ALL transactions for entire month. Cannot generate accurate report incrementally. Batch processing: Wait for month to end → Process all data at once (accurate). Stream processing: Process transactions as they arrive, but report still needs full month (no advantage). (3) High Throughput: Process millions of transactions in one go. Batch optimized for high throughput (process billions of records efficiently). Example: 10M transactions/month → Batch job processes in 1 hour. (4) Cost-Effective: Batch: Run once per month (1 hour of compute). Stream: Run 24/7 (720 hours of compute). Batch cost: $10/month. Stream cost: $2,000/month. For monthly report, stream is 200x more expensive with NO benefit! (5) Simplicity: Batch: SQL query, aggregate data, generate report (simple). Stream: Maintain state for entire month, aggregate incrementally, generate report (complex). IMPLEMENTATION: Batch job (Spark): Schedule: Last day of month at 11:59 PM. Input: Read all transactions from database (PostgreSQL) or data warehouse (Snowflake). Processing: Aggregate sales by product, region, salesperson. Calculate: Total revenue, top products, growth vs last month. Output: Generate PDF report, send via email. Duration: 1 hour (for 10M transactions). EXAMPLE SQL: SELECT product_id, SUM(amount) as total_sales, COUNT(*) as num_transactions FROM transactions WHERE date >= "2024-01-01" AND date < "2024-02-01" GROUP BY product_id ORDER BY total_sales DESC LIMIT 100. ALTERNATIVE (Stream): Continuously aggregate transactions into running totals. Maintain state: Total sales per product (updated every transaction). On last day: Generate report from state. Problems: Complex: Must maintain state for entire month. Not faster: Still need to wait for month to end. More expensive: 24/7 compute vs 1-hour batch. No advantage: Latency not critical (report not needed real-time). WHEN STREAM MAKES SENSE (Variation): If requirement changes to "real-time sales dashboard": Show current sales totals (updated every minute). Then: Use stream processing (Kafka Streams). Users can see sales in real-time throughout month. But for end-of-month report: Still use batch (accurate, complete). HYBRID APPROACH: Stream: Real-time dashboard (current sales totals). Batch: Monthly report (comprehensive, accurate). Best of both: Stream for users who want real-time visibility. Batch for official monthly report (accounting, management). CONCLUSION: For monthly sales report, batch processing is clearly superior. No latency requirement (monthly schedule). High throughput, cost-effective, simple. Stream processing would be unnecessary complexity and 200x cost increase with NO benefit.',
    keyPoints: [
      'Batch processing for monthly report (latency not critical)',
      'Report generated once per month (no real-time requirement)',
      'Batch: High throughput, cost-effective ($10 vs $2,000/month)',
      'Simple implementation (SQL query, aggregate, generate report)',
      'Stream only if real-time dashboard needed (different requirement)',
    ],
  },
  {
    id: 'q3',
    question:
      'Explain the Lambda Architecture and when it would be appropriate to use this hybrid approach.',
    sampleAnswer:
      'Lambda Architecture - Hybrid Batch + Stream: DEFINITION: Lambda Architecture combines batch and stream processing to provide both accuracy and low latency. ARCHITECTURE: THREE LAYERS: (1) BATCH LAYER: Process complete historical data (days/months/years). High accuracy, full reprocessing. Output: Comprehensive, immutable views. Example: Spark job processing 1 year of data (nightly). (2) SPEED LAYER (Stream): Process recent data (last few minutes/hours). Low latency, approximate. Output: Real-time incremental updates. Example: Flink processing events from last 5 minutes. (3) SERVING LAYER: Merge batch + stream results. Serve queries combining both. Example: Batch view (historical) + Speed view (recent delta). EXAMPLE (Twitter Follower Count): BATCH LAYER: Daily job (midnight): Count followers for ALL users. Query: SELECT user_id, COUNT(*) FROM follows GROUP BY user_id. Output: User 123 has 10,000 followers (as of midnight). SPEED LAYER: Real-time: Track new follows since midnight. Stream processing: Increment counter on follow event. Output: +5 new followers since midnight. SERVING LAYER: User requests follower count for User 123: Batch view: 10,000 (as of midnight). Speed view: +5 (since midnight). Merged result: 10,005 followers. ADVANTAGES: ✅ Accuracy: Batch layer provides complete, accurate data. ✅ Low latency: Speed layer provides real-time updates. ✅ Fault tolerance: Batch can reprocess if stream fails. ✅ Best of both: Combine batch throughput + stream latency. DISADVANTAGES: ❌ Complexity: Maintain two processing pipelines. ❌ Duplication: Same logic in batch and stream (hard to keep in sync). ❌ Higher cost: Run both batch and stream infrastructure. WHEN TO USE LAMBDA ARCHITECTURE: (1) Real-Time + Historical Analysis: Need both current and historical data. Example: Analytics dashboard (real-time + past trends). (2) High Accuracy Required (Batch) + Low Latency (Stream): Cannot compromise on either. Example: Financial reporting (accuracy) + fraud alerts (latency). (3) Fault Tolerance Critical: If stream fails, batch provides backup. Example: Mission-critical systems (can\'t lose data). REAL-WORLD EXAMPLE (LinkedIn Skills Endorsements): BATCH LAYER: Weekly job: Calculate endorsement counts for all users, all skills. Process 1 billion endorsements. Output: User 123 has 50 endorsements for "Python" (as of Sunday). SPEED LAYER: Real-time: Track endorsements since last batch job. Stream: Increment counter on new endorsement. Output: +3 endorsements since Sunday. SERVING LAYER: User views profile: Batch: 50 endorsements (as of Sunday). Speed: +3 (since Sunday). Total: 53 endorsements. ALTERNATIVE (Kappa Architecture): Simplification: Use ONLY stream processing. Treat batch as reprocessing the stream. Advantage: Single code path (simpler). Disadvantage: Stream processing more complex than batch. WHEN TO USE KAPPA (not Lambda): If stream processing can handle full historical data. If simplicity more important than optimization. Example: Kafka can replay entire history → Reprocess via stream. CONCLUSION: Lambda Architecture appropriate when: Need both real-time and historical accuracy. Willing to accept complexity of two systems. Cannot compromise on either latency or accuracy. For simpler use cases: Choose pure batch (if latency not critical) or pure stream (if can handle full history).',
    keyPoints: [
      'Lambda: Batch layer (accuracy) + Speed layer (latency) + Serving layer (merge)',
      'Batch: Process historical data (comprehensive, accurate)',
      'Stream: Process recent data (real-time, incremental)',
      'Use when: Need both real-time AND historical accuracy',
      'Trade-off: Best of both worlds but higher complexity (two pipelines)',
    ],
  },
];
