/**
 * Quiz questions for Vertical vs Horizontal Scaling section
 */

export const verticalvshorizontalscalingQuiz = [
  {
    id: 'q1',
    question:
      'Your PostgreSQL database is at 80% CPU usage and queries are slow. You currently have 16GB RAM and 8 cores. Should you vertically scale, horizontally scale (add read replicas), or shard? Explain your reasoning.',
    sampleAnswer:
      'Database scaling decision - analysis: CURRENT STATE: PostgreSQL 16GB RAM, 8 cores, 80% CPU. Queries are slow. Need to understand workload: Are queries slow due to CPU (complex queries) or memory (too little cache)? Is this read-heavy or write-heavy workload? STEP 1: PROFILE THE WORKLOAD. Check read:write ratio: If 90%+ reads, 10% writes → Read-heavy. If 50/50 or write-heavy → Different approach. Check slow query log: Are queries doing full table scans (missing indexes)? Are queries complex joins/aggregations (CPU-intensive)? SCENARIO A: READ-HEAVY WORKLOAD (90% reads). SOLUTION: HORIZONTAL SCALING (Add read replicas). Implementation: Keep primary database as-is (handles writes). Add 5-10 read replicas (horizontally scale reads). Route read queries to replicas, write queries to primary. Cost: $100/month (primary) + $50/month each (5 replicas) = $350/month. Result: Primary CPU drops to 10% (only writes). Read queries distributed across replicas. Queries fast (each replica handles 1/5 of read load). PROS: Solves read scaling problem immediately. No code changes (connection string routing). High availability (if replica down, use others). CONS: Replication lag (replicas 100-500ms behind primary). Not solving write scalability (but not needed if write-light). SCENARIO B: WRITE-HEAVY WORKLOAD (50%+ writes). SOLUTION: VERTICAL SCALING first, then consider sharding if still not enough. Step 1: Vertical scale primary: Upgrade to 64GB RAM, 32 cores. Cost: $100/month → $500/month. Expected result: CPU 80% → 20% (4x more cores). Queries faster (more cache in RAM). Step 2: If vertical scaling not enough (hitting limits at 128GB/64 cores): Consider sharding by user_id or tenant_id. Complexity: High (cross-shard queries, transactions). Only do this at massive scale (Instagram, Facebook level). SCENARIO C: SLOW QUERIES DUE TO MISSING INDEXES. SOLUTION: OPTIMIZE FIRST before scaling. Add indexes on frequently queried columns. Rewrite slow queries. Result: 80% CPU → 30% CPU (no hardware upgrade needed!). RECOMMENDATION FOR MOST CASES: (1) Optimize queries and add indexes first (cheapest, fastest). (2) If read-heavy, add read replicas (horizontal scaling for reads). (3) If write-heavy or optimization not enough, vertical scale primary. (4) Only shard if hitting vertical limits at massive scale. For typical application with 80% CPU at 16GB/8 cores: Likely read-heavy → Add 3-5 read replicas for $150-250/month. Problem solved without complexity of sharding.',
    keyPoints: [
      'Profile workload first: Read-heavy vs write-heavy',
      'Read-heavy: Add read replicas (horizontal scaling)',
      'Write-heavy: Vertical scale primary database',
      'Optimize queries/indexes before scaling (often solves problem)',
      'Sharding only needed at massive scale (last resort)',
    ],
  },
  {
    id: 'q2',
    question:
      'Compare the total cost of ownership for vertical vs horizontal scaling for a web application handling 10,000 requests/second. Include infrastructure costs, operational complexity, and long-term scalability.',
    sampleAnswer:
      "Cost comparison: Vertical vs Horizontal scaling for 10,000 req/s web app. VERTICAL SCALING APPROACH: Single powerful server: AWS c5.9xlarge (36 vCPU, 72GB RAM) = $1,200/month. Handles 10,000 req/s comfortably. Load balancer: Not needed (single server) = $0. Total infrastructure: $1,200/month. OPERATIONAL COSTS: Simple deployment (one server). Simple monitoring (one server logs). Simple debugging (no distributed system). Estimated ops cost: 1 engineer @ 10% time = $1,500/month. TOTAL: $2,700/month. RISKS: Single point of failure (no redundancy). Downtime during deployments/upgrades. Cannot handle traffic spikes beyond 10,000 req/s. If traffic doubles, may need to re-architect to horizontal. HORIZONTAL SCALING APPROACH: Fleet of smaller servers: 10x t3.xlarge (4 vCPU, 16GB RAM each) @ $120/month = $1,200/month. Each handles 1,000 req/s. Load balancer: AWS ALB = $30/month. Shared session storage: Redis cluster = $100/month. Total infrastructure: $1,330/month. OPERATIONAL COSTS: More complex deployment (deploy to 10 servers). Centralized logging needed (ELK stack: $200/month). Distributed tracing (Datadog: $100/month). More complex debugging (requests across servers). Estimated ops cost: 1 engineer @ 20% time = $3,000/month. TOTAL: $4,630/month (74% more expensive than vertical). BENEFITS: High availability (1 server down → 9 still serving, 90% capacity). Zero-downtime deployments (rolling deploy across fleet). Can handle traffic spikes (auto-scale to 20 servers = 20,000 req/s). Future-proof (can scale to 100+ servers linearly). LONG-TERM SCALABILITY: VERTICAL: At 20,000 req/s: Need c5.18xlarge (72 vCPU) = $2,400/month (linear cost growth). At 50,000 req/s: Need multiple servers anyway → Must re-architect to horizontal → Wasted investment in vertical approach. HORIZONTAL: At 20,000 req/s: Add 10 more servers = $2,400/month total (linear). At 50,000 req/s: 50 servers = $6,000/month (linear, predictable). At 100,000 req/s: 100 servers = $12,000/month (still scales linearly). COST BREAKDOWN BY SCALE: 10,000 req/s: Vertical $2,700/month < Horizontal $4,630/month → Vertical wins. 20,000 req/s: Vertical $4,800/month < Horizontal $5,800/month → Vertical still cheaper but gap narrows. 50,000 req/s: Vertical can't handle → Must re-architect. Horizontal $9,000/month → Only option. 100,000 req/s: Horizontal $15,000/month → Scales linearly. RECOMMENDATION: For startup/small scale (<20,000 req/s): Use vertical scaling. Simpler, cheaper, faster to market. For growth stage (20,000-50,000 req/s): Transition to horizontal. Plan ahead (6 months lead time for refactor). For large scale (>50,000 req/s): Horizontal scaling required. Accept operational complexity for scale. OPTIMAL STRATEGY: Start vertical (months 1-12): Single server, simple ops, $2,700/month. Refactor to horizontal (months 12-18): When approaching 15,000 req/s. Transition period: Run both architectures, migrate traffic gradually. Horizontal at scale (months 18+): Auto-scaling fleet, high availability, linear cost growth.",
    keyPoints: [
      'Vertical: Cheaper and simpler at small scale (<20,000 req/s)',
      'Horizontal: Higher upfront cost ($4,600 vs $2,700) but scales linearly',
      'Vertical hits hard limits, requires re-architecture at scale',
      'Horizontal enables high availability and zero-downtime deploys',
      'Start vertical, transition to horizontal before hitting limits',
    ],
  },
  {
    id: 'q3',
    question:
      'You have a stateful application storing user session data in server memory. How would you refactor it to enable horizontal scaling?',
    sampleAnswer:
      'Refactoring stateful application for horizontal scaling: PROBLEM: Current architecture: User session data stored in server memory (e.g., Express.js with in-memory sessions). When horizontally scaled: User logs in → Session stored in Server A memory. Next request → Load balancer routes to Server B → Session not found → User appears logged out. SOLUTIONS: SOLUTION 1: STICKY SESSIONS (Quick fix, not recommended). Load balancer always routes same user to same server. Implementation: Load balancer cookie or IP hash. PROS: No code changes. Quick fix. CONS: Uneven load distribution (some servers overloaded). If server crashes, users on that server lose sessions. Cannot truly horizontally scale. Not recommended for production. SOLUTION 2: EXTERNALIZE SESSION STORAGE (Redis - Recommended). Move session data from server memory to shared Redis cache. Architecture: Web servers (stateless, horizontally scaled). Redis cluster (shared session store). All servers read/write sessions to Redis. IMPLEMENTATION: Code changes (Node.js example): Before: app.use(session({ store: new MemoryStore(), secret: "secret" })); After: const RedisStore = require("connect-redis")(session); app.use(session({ store: new RedisStore({ host: "redis.example.com", port: 6379 }), secret: "secret" })); FLOW: User logs in → Session created in Redis (key: session:abc123, value: {user_id: 101}). Load balancer routes to Server A → Server A reads session from Redis. Next request → Load balancer routes to Server B → Server B reads same session from Redis. User stays logged in (session shared across servers). REDIS SETUP: Use Redis Cluster or AWS ElastiCache for high availability. Session data structure: { session_id: "abc123", user_id: 101, username: "alice", permissions: ["read", "write",], expires_at: 1704067200 }. TTL: Set expiration (30 minutes inactive → session deleted). BENEFITS: Stateless web servers (any server can handle any request). True horizontal scaling (add/remove servers freely). High availability (Redis cluster replicated). Fast access (Redis in-memory, <1ms latency). SOLUTION 3: DATABASE SESSION STORAGE (Alternative). Store sessions in database (PostgreSQL, MySQL). PROS: Don\'t need Redis (one less technology). Sessions persist across Redis restarts. CONS: Slower than Redis (10-50ms vs <1ms). More database load (every request queries sessions). Not recommended for high-traffic apps. SOLUTION 4: JWT TOKENS (Stateless authentication). Use JWT tokens instead of server-side sessions. Flow: User logs in → Server generates JWT token with embedded user info. JWT sent to client (stored in cookie/localStorage). Every request → Client sends JWT → Server validates signature → Extracts user info. No server-side session storage needed. PROS: Truly stateless (no Redis needed). Horizontally scales perfectly. CONS: Cannot revoke tokens before expiration (logout doesn\'t work server-side). Larger cookies (JWT 200-500 bytes vs session ID 20 bytes). Security risk if stolen (token valid until expiration). RECOMMENDATION: Use SOLUTION 2 (Redis) for most applications: Balances stateless architecture with ability to revoke sessions. Fast (<1ms), scalable, high availability. Industry standard (used by Facebook, Twitter, etc.). Cost: Redis cluster $50-200/month. MIGRATION PLAN: Week 1: Set up Redis cluster. Week 2: Deploy code changes (use Redis for new sessions). Week 3: Migrate existing sessions from memory to Redis. Week 4: Deploy horizontally scaled servers. Result: Application ready for horizontal scaling, high availability, linear scalability.',
    keyPoints: [
      'Externalize session storage to Redis (recommended solution)',
      'Stateless web servers enable true horizontal scaling',
      "Sticky sessions are quick fix but don't truly scale",
      "JWT tokens are alternative (stateless but can't revoke)",
      'Redis provides fast (<1ms), scalable, highly available session storage',
    ],
  },
];
