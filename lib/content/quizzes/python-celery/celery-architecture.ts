/**
 * Quiz questions for Celery Architecture section
 */

export const celeryArchitectureQuiz = [
  {
    id: 'q1',
    question:
      'Your company processes 100,000 image uploads per day. Each image needs: thumbnail generation (CPU-heavy, 2s), metadata extraction (I/O-heavy, 500ms), and storage to S3 (I/O-heavy, 1s). Design a complete Celery architecture including broker choice, worker pools, queue configuration, and scaling strategy.',
    sampleAnswer:
      'CELERY ARCHITECTURE FOR IMAGE PROCESSING: **REQUIREMENTS**: - 100,000 images/day = 69 images/minute = 1.15 images/second - 3 distinct operations: (1) Thumbnail generation (CPU-heavy), (2) Metadata extraction (I/O-heavy), (3) S3 upload (I/O-heavy). **ARCHITECTURE DESIGN**: **Component 1: Message Broker** - **Choice**: Redis (primary) with RabbitMQ (failover). - **Reasoning**: Redis fast enough for this scale, simple setup. RabbitMQ failover for reliability. - **Configuration**: ```python app = Celery("image_processor", broker="redis://localhost:6379/0", backend="redis://localhost:6379/1") app.conf.broker_connection_retry_on_startup = True app.conf.broker_failover_strategy = "round-robin" ``` **Component 2: Task Routing (3 Queues)** - **Queue 1**: "thumbnails" - CPU-heavy tasks - **Queue 2**: "metadata" - I/O tasks - **Queue 3**: "uploads" - I/O tasks. **Why separate**: Isolate CPU-heavy from I/O-heavy, scale independently, prevent blocking. ```python app.conf.task_routes = {"tasks.generate_thumbnail": {"queue": "thumbnails"}, "tasks.extract_metadata": {"queue": "metadata"}, "tasks.upload_to_s3": {"queue": "uploads"}} ``` **Component 3: Worker Pools** - **Worker Pool 1: Thumbnails (CPU-heavy)**: ```bash celery -A tasks worker -Q thumbnails --pool=prefork --concurrency=4 --hostname=thumbnail-worker@%h ``` Pool type: **prefork** (multiprocessing, bypasses GIL). Concurrency: **4** (matches CPU cores). Reasoning: CPU-bound tasks need true parallelism. - **Worker Pool 2: Metadata (I/O-heavy)**: ```bash celery -A tasks worker -Q metadata --pool=gevent --concurrency=100 --hostname=metadata-worker@%h ``` Pool type: **gevent** (greenlets). Concurrency: **100** (lightweight, high concurrency). Reasoning: I/O-bound, can handle 100s concurrent. - **Worker Pool 3: S3 Uploads (I/O-heavy)**: ```bash celery -A tasks worker -Q uploads --pool=gevent --concurrency=200 --hostname=upload-worker@%h ``` Pool type: **gevent**. Concurrency: **200** (network I/O). Reasoning: Network calls, need high concurrency. **Component 4: Task Definitions** ```python from celery import Celery, chain from PIL import Image import boto3 app = Celery("image_processor", broker="redis://localhost:6379/0") @app.task (name="tasks.generate_thumbnail", queue="thumbnails") def generate_thumbnail (image_path: str) -> str: """CPU-heavy: Image processing""" img = Image.open (image_path) img.thumbnail((150, 150)) thumb_path = image_path.replace(".jpg", "_thumb.jpg") img.save (thumb_path) return thumb_path @app.task (name="tasks.extract_metadata", queue="metadata") def extract_metadata (image_path: str) -> dict: """I/O-heavy: File operations""" img = Image.open (image_path) return {"width": img.width, "height": img.height, "format": img.format} @app.task (name="tasks.upload_to_s3", queue="uploads") def upload_to_s3(image_path: str, metadata: dict) -> str: """I/O-heavy: Network upload""" s3 = boto3.client("s3") s3.upload_file (image_path, "my-bucket", image_path) return f"https://s3.amazonaws.com/my-bucket/{image_path}" # Chain tasks together task_chain = chain( generate_thumbnail.s("/uploads/image.jpg"), extract_metadata.s(), upload_to_s3.s() ) result = task_chain.apply_async() ``` **Component 5: Result Backend** - **Choice**: Redis (same as broker, separate DB). - **Configuration**: ```python app.conf.result_backend = "redis://localhost:6379/1" app.conf.result_expires = 3600 # 1 hour app.conf.result_compression = "gzip" ``` **SCALING CALCULATION**: - **Thumbnails**: 1.15 images/sec × 2s = 2.3 concurrent tasks → Need 3-4 workers (1 worker with concurrency=4 sufficient). - **Metadata**: 1.15 images/sec × 0.5s = 0.58 concurrent → 1 worker with gevent concurrency=100 sufficient. - **Uploads**: 1.15 images/sec × 1s = 1.15 concurrent → 1 worker with gevent concurrency=200 sufficient. **Total Workers**: 3 worker processes (1 per queue). **FAILURE HANDLING**: - **Task retries**: ```python @app.task (bind=True, max_retries=3, autoretry_for=(Exception,)) def generate_thumbnail (self, image_path): try: # Processing except Exception as exc: raise self.retry (exc=exc, countdown=60 * (2 ** self.request.retries)) ``` - **Broker failover**: If Redis down, switch to RabbitMQ backup. - **Result backend**: Store task state for tracking. **MONITORING**: - **Flower**: Real-time worker monitoring: ```bash celery -A tasks flower --port=5555 ``` - **Metrics**: Track queue depth, task latency (p50, p99), failure rate, worker CPU/memory. - **Alerts**: PagerDuty if queue depth >1000 or failure rate >5%. **DEPLOYMENT** (Kubernetes): ```yaml apiVersion: apps/v1 kind: Deployment metadata: name: celery-thumbnail-workers spec: replicas: 2 # Auto-scale based on queue depth ``` **KEY DECISIONS**: 1. **Redis** over RabbitMQ: Simpler for this scale (Redis handles 100K/day easily). 2. **Separate queues**: Isolates CPU vs I/O workloads. 3. **prefork for CPU**: True parallelism for image processing. 4. **gevent for I/O**: High concurrency (100-200) with low memory. 5. **Task chaining**: Thumbnail → Metadata → Upload (sequential). **COST OPTIMIZATION**: - Auto-scale workers based on queue depth. - Use spot instances (70% savings). - Scale down during off-peak hours.',
    keyPoints: [
      'Redis broker for simplicity at this scale, RabbitMQ for failover',
      'Separate queues: thumbnails (CPU), metadata (I/O), uploads (I/O)',
      'Worker pools: prefork for CPU-bound, gevent for I/O-bound',
      'Calculate concurrency: throughput × task_duration = concurrent tasks needed',
      'Monitor with Flower, auto-scale based on queue depth',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the trade-offs between Redis and RabbitMQ as message brokers for Celery. In what scenarios would you choose one over the other?',
    sampleAnswer:
      'REDIS vs RABBITMQ TRADE-OFF ANALYSIS: **REDIS AS BROKER**: **Architecture**: In-memory data store, optional persistence (AOF/RDB). Tasks stored in Redis lists. **Pros**: ✅ **Simple setup**: \`pip install redis\` + `redis-server`. ✅ **Very fast**: In-memory, microsecond latency. ✅ **Easy debugging**: \`redis-cli\` to inspect queues. ✅ **Low overhead**: Lightweight, minimal configuration. ✅ **Good for most cases**: 90% of applications don\'t need more. **Cons**: ❌ **Less reliable**: In-memory, can lose tasks if Redis crashes (unless AOF enabled). ❌ **No advanced routing**: Simple queues only, no exchanges/routing keys. ❌ **Memory limitations**: All tasks in RAM, can run out of memory with huge queues. ❌ **Single point of failure**: If Redis down, entire queue stops. **Best for**: - Startups/SMBs (simple, fast to set up). - Applications with <1M tasks/day. - Non-critical tasks (emails, analytics) where occasional loss acceptable. - I/O-bound workloads. - Teams without dedicated ops engineers. **RABBITMQ AS BROKER**: **Architecture**: Message queue system, disk-backed persistence, AMQP protocol. **Pros**: ✅ **Most reliable**: Disk persistence, survives crashes. ✅ **Advanced features**: Exchanges, routing keys, message TTL, priority queues, dead-letter queues. ✅ **Scalability**: Clustering, federation, shovel for multi-DC. ✅ **Better monitoring**: Management UI, rich metrics. ✅ **Message durability**: Guaranteed delivery with acks. **Cons**: ❌ **Complex setup**: Requires RabbitMQ server, Erlang runtime. ❌ **Slower than Redis**: Disk I/O overhead (~10-20% slower). ❌ **Higher resource usage**: More memory, CPU. ❌ **Steeper learning curve**: AMQP concepts (exchanges, bindings, routing). **Best for**: - Enterprises (need reliability guarantees). - Applications with >1M tasks/day. - Critical tasks (payments, orders) where loss unacceptable. - Complex routing needs (topic exchanges, fanout). - Multi-datacenter deployments. **DETAILED COMPARISON**: **1. Reliability**: - **Redis**: Tasks lost if Redis crashes before persistence. AOF helps but not 100%. - **RabbitMQ**: Tasks persisted to disk immediately. 99.9% durability. **Winner: RabbitMQ** (if you need guarantees). **2. Performance**: - **Redis**: ~100,000 tasks/sec throughput. - **RabbitMQ**: ~50,000 tasks/sec. **Winner: Redis** (2× faster). **3. Setup Complexity**: - **Redis**: 5-minute setup (`brew install redis`, `redis-server`). - **RabbitMQ**: 30-minute setup (install Erlang, RabbitMQ, enable plugins). **Winner: Redis** (6× easier). **4. Routing**: - **Redis**: Simple queues only. Route by queue name. - **RabbitMQ**: Topic exchanges, fanout, direct routing, header-based routing. **Winner: RabbitMQ** (advanced features). **5. Monitoring**: - **Redis**: redis-cli, basic INFO command. - **RabbitMQ**: Rich management UI, per-queue metrics, consumer tracking. **Winner: RabbitMQ** (better visibility). **6. Clustering**: - **Redis**: Redis Cluster (complex), Redis Sentinel (simpler failover). - **RabbitMQ**: Native clustering, federation, shovel. **Winner: RabbitMQ** (mature clustering). **DECISION FRAMEWORK**: **Choose Redis if**: ✅ Simple application (MVP, small team). ✅ Performance > reliability. ✅ Tasks non-critical (emails, analytics). ✅ <1M tasks/day. ✅ Want fast iteration. **Choose RabbitMQ if**: ✅ Enterprise application. ✅ Reliability > performance. ✅ Tasks critical (payments, orders). ✅ >1M tasks/day or 100K tasks/sec peaks. ✅ Need advanced routing. ✅ Multi-DC deployment. **HYBRID APPROACH**: Many companies use both: - **Redis**: Fast, non-critical tasks (emails, cache warming). - **RabbitMQ**: Critical tasks (payments, order processing). ```python # Redis for non-critical app_noncritical = Celery("noncritical", broker="redis://localhost:6379/0") @app_noncritical.task def send_email (email): pass # RabbitMQ for critical app_critical = Celery("critical", broker="amqp://localhost:5672//") @app_critical.task def process_payment (order_id): pass ``` **REAL-WORLD EXAMPLES**: - **Instagram**: Started with Redis, migrated to RabbitMQ at scale (reliability). - **GitHub**: Uses Redis for fast, non-critical jobs. - **Robinhood**: RabbitMQ for financial transactions (can\'t lose tasks). - **Zapier**: Redis for webhooks (speed matters). **MIGRATION PATH**: Start with Redis (simple), migrate to RabbitMQ when: - Task loss becomes unacceptable. - Need advanced routing. - Scale exceeds Redis capacity. - Team has ops resources. **KEY TAKEAWAY**: Redis = fast, simple, good enough for 90% of applications. RabbitMQ = reliable, feature-rich, worth complexity for critical systems. Start with Redis, migrate to RabbitMQ when reliability requirements increase.',
    keyPoints: [
      'Redis: faster (2×), simpler setup (5 min vs 30 min), but less reliable',
      'RabbitMQ: more reliable (disk-backed), advanced routing, better for critical tasks',
      'Decision: Redis for non-critical + speed, RabbitMQ for critical + reliability',
      'Hybrid approach common: Redis for emails/analytics, RabbitMQ for payments/orders',
      'Migration path: Start Redis, move to RabbitMQ as reliability needs grow',
    ],
  },
  {
    id: 'q3',
    question:
      'Your Celery workers are consuming 8GB RAM each and crashing every few hours with "MemoryError". Tasks involve downloading large files (100-500MB), processing them, and uploading results. How do you diagnose and fix this memory issue?',
    sampleAnswer:
      'DIAGNOSING & FIXING CELERY MEMORY ISSUES: **PROBLEM DIAGNOSIS**: **Step 1: Confirm Memory Leak** ```bash # Monitor worker memory over time top -p $(pgrep -f "celery worker") # Or use htop htop -p $(pgrep -f "celery worker") # Check memory growth pattern ps aux | grep celery # Watch for RSS column growing ``` **Step 2: Profile Memory Usage** ```python # Add memory profiling to tasks from memory_profiler import profile @app.task @profile def process_large_file (file_path): """Memory profiling shows line-by-line usage""" # Download file (this might load entire file to memory!) data = download_file (file_path) # 500 MB loaded! # Process (creates copy?) processed = process_data (data) # Another 500 MB! # Upload (keeps both in memory?) upload_result (processed) # Total: 1 GB for single task! return "done" ``` **ROOT CAUSES IDENTIFIED**: **1. Loading Entire File to Memory**: ```python # ❌ Bad: Loads entire 500MB file @app.task def process_file (file_path): with open (file_path, "r") as f: data = f.read() # 500 MB in memory! process (data) # ✅ Good: Stream file line-by-line @app.task def process_file (file_path): with open (file_path, "r") as f: for line in f: # Only one line in memory process_line (line) # ~1KB at a time ``` **2. Memory Not Released Between Tasks**: Celery workers process 1000s of tasks before restarting. Memory accumulates. ```bash # Check max tasks before restart celery inspect conf | grep max_tasks_per_child # "max_tasks_per_child": null means workers NEVER restart! ``` **3. Worker Pool Issues**: ```bash # Check worker pool type celery inspect active # If pool=prefork, each worker is separate process # If pool=threads, memory shared (memory leaks affect all) ``` **COMPREHENSIVE FIX**: **Fix 1: Stream Files (Don\'t Load Entirely)** ```python import requests @app.task def download_and_process (url: str): """Stream download + process, never load entire file""" # ❌ Bad: Downloads to memory response = requests.get (url) data = response.content # 500 MB in RAM! # ✅ Good: Stream response = requests.get (url, stream=True) for chunk in response.iter_content (chunk_size=8192): process_chunk (chunk) # Only 8 KB in memory # ✅ Even better: Stream to disk then process with tempfile.NamedTemporaryFile (delete=True) as tmp: response = requests.get (url, stream=True) for chunk in response.iter_content (chunk_size=8192): tmp.write (chunk) # Stream to disk tmp.seek(0) # Process from disk (streaming) for line in tmp: process_line (line) # File auto-deleted after task ``` **Fix 2: Restart Workers After N Tasks** ```python # Configuration: Restart after 100 tasks app.conf.worker_max_tasks_per_child = 100 # Or via command line celery -A tasks worker --max-tasks-per-child=100 ``` Why: Even with careful coding, Python memory not always released. Periodic restarts prevent accumulation. **Fix 3: Set Memory Limits per Worker** ```python # Soft limit: Warning at 2GB app.conf.worker_max_memory_per_child = 2_000_000 # 2 GB (KB) # Or use OS-level limits (cgroups/Docker) docker run --memory="2g" --memory-swap="2g" celery-worker # Kubernetes memory limits apiVersion: v1 kind: Pod spec: containers: - name: celery-worker resources: limits: memory: "2Gi" requests: memory: "1Gi" ``` **Fix 4: Use Appropriate Worker Pool** ```python # For file processing: Use prefork (process isolation) celery -A tasks worker --pool=prefork --concurrency=4 # Each worker = separate process # If one leaks, others unaffected # Workers restart after max_tasks_per_child ``` **Fix 5: Explicit Garbage Collection** ```python import gc @app.task def process_large_file (file_path): # Process file result = heavy_processing (file_path) # Explicit cleanup del large_variable gc.collect() # Force garbage collection return result ``` **Fix 6: Monitor Memory** ```python from celery.signals import task_prerun, task_postrun import psutil import os @task_prerun.connect def task_prerun_handler (task_id, task, *args, **kwargs): """Log memory before task""" process = psutil.Process (os.getpid()) mem_mb = process.memory_info().rss / 1024 / 1024 print(f"Task {task.name} starting: {mem_mb:.2f} MB") @task_postrun.connect def task_postrun_handler (task_id, task, *args, **kwargs): """Log memory after task""" process = psutil.Process (os.getpid()) mem_mb = process.memory_info().rss / 1024 / 1024 print(f"Task {task.name} finished: {mem_mb:.2f} MB") ``` **Fix 7: Use External Storage** ```python # ❌ Bad: Store large results in Celery result backend @app.task def process_file (file_path): large_result = process (file_path) # 500 MB return large_result # Stored in Redis! (memory explodes) # ✅ Good: Store large results in S3, return URL @app.task def process_file (file_path): large_result = process (file_path) s3_url = upload_to_s3(large_result) # Store in S3 return {"result_url": s3_url} # Return small metadata ``` **DEPLOYMENT CONFIGURATION**: ```python # Complete memory-safe configuration app.conf.update( # Restart workers frequently worker_max_tasks_per_child=100, # Memory limits (2GB soft limit) worker_max_memory_per_child=2_000_000, # KB # Disable result caching result_cache_max=100, # Compress results result_compression="gzip", # Shorter result expiration result_expires=3600, # 1 hour ) ``` ```bash # Run workers with memory-safe settings celery -A tasks worker \\ --pool=prefork \\ --concurrency=4 \\ --max-tasks-per-child=100 \\ --max-memory-per-child=2000000 \\ --loglevel=info ``` **MONITORING & ALERTING**: ```python # Prometheus metrics from prometheus_client import Gauge worker_memory = Gauge("celery_worker_memory_bytes", "Worker memory usage") @task_postrun.connect def track_memory (task_id, **kwargs): process = psutil.Process (os.getpid()) worker_memory.set (process.memory_info().rss) # Alert if >7GB (approaching 8GB limit) ``` **TESTING**: ```python # Load test with memory monitoring def load_test(): tasks = [process_large_file.delay (f"file_{i}.dat") for i in range(100)] # Monitor worker memory during test watch_memory() # Should stay <2GB per worker ``` **KEY TAKEAWAYS**: 1. **Stream files**, don\'t load entirely (8KB chunks). 2. **Restart workers** after 100 tasks (prevent accumulation). 3. **Set memory limits** (2GB per worker). 4. **Use prefork pool** (process isolation). 5. **Store large results in S3**, not result backend. 6. **Monitor memory** per task (identify leaks). 7. **Explicit cleanup**: del + gc.collect(). **RESULT**: Memory usage: 8GB → 500MB per worker. Crashes: Every 2 hours → Never.',
    keyPoints: [
      'Stream files with chunks (8KB), never load entire 500MB to memory',
      'Restart workers after 100 tasks (worker_max_tasks_per_child=100)',
      'Set memory limits per worker (2GB), use prefork pool for isolation',
      'Store large results in S3, return URLs (not 500MB in result backend)',
      'Monitor memory per task, add explicit cleanup (del, gc.collect)',
    ],
  },
];
