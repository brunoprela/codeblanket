/**
 * Quiz questions for Task Queue Fundamentals section
 */

export const taskQueueFundamentalsQuiz = [
  {
    id: 'q1',
    question:
      'Your team is building a SaaS application where users upload CSV files (up to 100MB) for data processing. Currently, users upload files via a REST API endpoint that processes the file synchronously, causing 30-60 second response times and frequent timeout errors. Propose a complete architecture using task queues to solve this problem, including how you handle file uploads, progress tracking, and result delivery.',
    sampleAnswer:
      'COMPLETE TASK QUEUE ARCHITECTURE FOR CSV PROCESSING: **PROBLEM ANALYSIS**: Current issues: (1) 30-60s response times (poor UX), (2) Timeout errors (requests fail), (3) No progress visibility, (4) Server resources blocked during processing. **PROPOSED ARCHITECTURE**: **Component 1: File Upload Endpoint (Synchronous)**: POST /api/uploads → Upload file to S3 (fast, streaming, ~2-5s for 100MB) → Create database record (upload_id, user_id, status="pending", s3_key) → Queue processing task with upload_id → Return {upload_id, status: "queued"} immediately (total: ~3-5s). **Component 2: Message Broker**: Redis or RabbitMQ stores task queue. **Component 3: Celery Worker Pool**: 10-50 workers processing tasks in parallel. Workers pick up "process_csv" tasks from queue. **Component 4: Processing Task**: ```python @app.task (bind=True) def process_csv (self, upload_id: int): upload = Upload.query.get (upload_id) upload.status = "processing"; upload.save() file = download_from_s3(upload.s3_key) total_rows = count_rows (file) for i, row in enumerate (file): process_row (row) if i % 1000 == 0: progress = (i / total_rows) * 100 self.update_state (state="PROGRESS", meta={"progress": progress}) upload.progress = progress; upload.save() results = finalize_processing() upload.status = "complete"; upload.results = results; upload.save() send_email (upload.user_id, "Processing complete") ``` **Component 5: Progress Tracking API**: GET /api/uploads/:upload_id/status → Returns {upload_id, status: "processing", progress: 45, eta: 120} → Frontend polls every 2 seconds to update UI progress bar. **Component 6: Result Delivery**: Option A: Polling: Frontend polls /api/uploads/:upload_id/results until status="complete". Option B: WebSocket: Server pushes completion event to connected client. Option C: Email: Send email with download link when complete. **USER EXPERIENCE FLOW**: (1) User uploads 100MB CSV → 5s response with upload_id. (2) UI shows "Processing... 0%" with progress bar. (3) Frontend polls every 2s: "Processing... 15%", "Processing... 45%", "Processing... 89%". (4) Task completes in background (~5 min). (5) UI updates: "Complete! Download results". (6) User downloads processed file from S3. **BENEFITS**: (1) No timeouts (upload completes in 5s). (2) Users see progress (transparency). (3) Horizontal scaling (add workers for parallel processing). (4) Failure handling (retry failed tasks). (5) No blocked server resources. **FAILURE HANDLING**: File upload fails: Return error immediately, don\'t queue task. Processing fails: Celery retries 3 times with exponential backoff. If still fails: Mark upload.status = "failed", send notification, store error. S3 download fails: Retry with backoff (transient network issue). Database write fails: Retry transaction. Worker crashes: Task requeued automatically (Celery acks after completion). **SCALABILITY**: 10 concurrent uploads: Queue 10 tasks, 10 workers process in parallel. 1000 concurrent uploads: Queue 1000 tasks, 50 workers process (takes ~2 hours). Peak traffic: Auto-scale workers (Kubernetes HPA based on queue depth). **MONITORING**: Track metrics: (1) Queue depth (alert if >1000), (2) Average processing time, (3) Failure rate, (4) Worker utilization. Dashboards: Grafana + Prometheus for real-time monitoring. Alerts: PagerDuty if queue depth >5000 or failure rate >10%. **COST OPTIMIZATION**: Use spot instances for workers (70% cost savings). Auto-scale workers down during off-hours. Store processed results for 7 days, then delete. **TECHNOLOGY STACK**: - Upload: FastAPI with streaming file upload - Storage: S3 for raw + processed files - Queue: Redis (simple, fast) - Workers: Celery with 20 workers (prefork pool) - Database: PostgreSQL for upload metadata - Frontend: React with polling every 2s - Monitoring: Flower for Celery, Prometheus + Grafana **IMPLEMENTATION PRIORITY**: Phase 1: Basic queuing (2 weeks). Phase 2: Progress tracking (1 week). Phase 3: Email notifications (1 week). Phase 4: WebSocket real-time updates (2 weeks). Phase 5: Auto-scaling + monitoring (2 weeks). **KEY TAKEAWAY**: Task queues transform 60s blocking operation into 5s async operation with progress visibility, horizontal scalability, and automatic retry handling. Users get instant feedback, and system scales to 1000s of concurrent uploads.',
    keyPoints: [
      'Upload files to S3 (fast, streaming), queue processing task, return immediately',
      'Workers process in background with progress updates stored in database',
      'Polling API endpoint provides real-time progress (0%, 25%, 50%, 100%)',
      'Handle failures with automatic retries and clear error states',
      'Scale horizontally by adding workers for parallel processing',
    ],
  },
  {
    id: 'q2',
    question:
      'When should you NOT use a task queue? Provide specific examples where synchronous processing is better than asynchronous task queues, and explain the trade-offs.',
    sampleAnswer:
      "WHEN NOT TO USE TASK QUEUES: **PRINCIPLE**: Use task queues for operations that are: (1) Slow (>1s), (2) Non-critical to response, (3) Can be eventually consistent. Don't use task queues when operation is: (1) Required for response, (2) Very fast (<100ms), (3) Must be transactionally atomic. **SCENARIO 1: User Authentication** ❌ Bad: Queue login verification ```python @app.route('/login', methods=['POST']) def login(): email = request.json['email'] password = request.json['password'] verify_login.delay (email, password) # ❌ Queue task return {'message': 'Login queued'}, 202 ``` Why bad: (1) User needs to know NOW if credentials are correct, (2) Can't issue JWT token asynchronously, (3) User expects immediate response (not \"check back later\"). ✅ Good: Synchronous verification ```python @app.route('/login', methods=['POST']) def login(): email = request.json['email'] password = request.json['password'] user = User.query.filter_by (email=email).first() if user and user.verify_password (password): token = generate_jwt (user.id) return {'token': token}, 200 else: return {'error': 'Invalid credentials'}, 401 # Total: 50-100ms (fast enough, must be synchronous) ``` **SCENARIO 2: Payment Authorization** ❌ Bad: Queue payment processing ```python @app.route('/checkout', methods=['POST']) def checkout(): order_id = create_order() charge_payment.delay (order_id) # ❌ Queue payment return {'message': 'Order placed', 'order_id': order_id}, 201 ``` Why bad: (1) What if payment fails? Order already created! (2) User needs to know NOW if payment succeeded, (3) Creates inconsistent state (order exists but no payment), (4) Refund complexity if payment fails later. ✅ Good: Synchronous payment authorization ```python @app.route('/checkout', methods=['POST']) def checkout(): charge_result = stripe.Charge.create(...) # Wait for result (~500ms) if charge_result.success: order_id = create_order() queue_fulfillment_tasks (order_id) # ✅ NOW queue non-critical tasks return {'order_id': order_id, 'status': 'paid'}, 201 else: return {'error': 'Payment failed'}, 402 ``` Critical operation (payment) is synchronous, non-critical operations (email, fulfillment) are asynchronous. **SCENARIO 3: Search Results** ❌ Bad: Queue search operation ```python @app.route('/search', methods=['GET']) def search(): query = request.args.get('q') search_task = perform_search.delay (query) # ❌ Queue search return {'task_id': search_task.id, 'message': 'Search queued'}, 202 ``` Why bad: (1) User expects results NOW, (2) Adding polling adds complexity + latency, (3) Search is typically fast (<100ms with Elasticsearch), (4) Queuing overhead (10-50ms) > search time. ✅ Good: Synchronous search ```python @app.route('/search', methods=['GET']) def search(): query = request.args.get('q') results = elasticsearch.search (query) # Fast: 20-100ms return {'results': results}, 200 ``` **SCENARIO 4: Transactional Atomicity** ❌ Bad: Queue balance transfer ```python @app.route('/transfer', methods=['POST']) def transfer_money(): from_account = request.json['from'] to_account = request.json['to'] amount = request.json['amount'] # ❌ Queue as 2 separate tasks debit_account.delay (from_account, amount) credit_account.delay (to_account, amount) return {'message': 'Transfer queued'}, 202 ``` Why bad: (1) What if debit succeeds but credit fails? Money lost! (2) No transactional guarantee (atomicity broken), (3) Race conditions possible. ✅ Good: Synchronous database transaction ```python @app.route('/transfer', methods=['POST']) def transfer_money(): with db.transaction(): # Atomic: both succeed or both fail Account.debit (from_account, amount) Account.credit (to_account, amount) return {'message': 'Transfer complete'}, 200 ``` **SCENARIO 5: Very Fast Operations** ❌ Bad: Queue cache lookup ```python @app.route('/user/:id', methods=['GET']) def get_user (id): result = fetch_user.delay (id) # ❌ Queue task return {'task_id': result.id}, 202 @app.task def fetch_user (user_id): return cache.get (f'user:{user_id}') # 1-5ms operation ``` Why bad: (1) Queuing overhead (10-50ms) > operation time (1-5ms), (2) Adds complexity (polling), (3) No benefit. ✅ Good: Synchronous cache read ```python @app.route('/user/:id', methods=['GET']) def get_user (id): user = cache.get (f'user:{id}') # 1-5ms return {'user': user}, 200 ``` **DECISION FRAMEWORK**: Use SYNCHRONOUS processing when: ✅ Operation required for response (auth, payment, search) ✅ Very fast (<100ms) ✅ Must be transactionally atomic ✅ User expects immediate result ✅ Real-time feedback critical Use ASYNCHRONOUS task queue when: ✅ Operation slow (>1s) ✅ Non-critical to response (email, analytics) ✅ Can be eventually consistent ✅ Batch processing ✅ Scheduled/periodic jobs **HYBRID APPROACH**: Often best solution is hybrid: Synchronous for critical path, asynchronous for non-critical: ```python @app.route('/order', methods=['POST']) def create_order(): # Synchronous (must wait): payment_result = charge_payment() # 500ms if not payment_result.success: return {'error': 'Payment failed'}, 402 order = create_order_in_db() # 50ms # Asynchronous (don't wait): send_confirmation_email.delay (order.id) notify_warehouse.delay (order.id) update_analytics.delay (order.id) return {'order_id': order.id}, 201 ``` **KEY TAKEAWAY**: Task queues are powerful but not universal. Use them judiciously: critical operations stay synchronous, non-critical operations go asynchronous. The best systems combine both patterns strategically.",
    keyPoints: [
      "Don't queue operations required for response (auth, payment, search)",
      "Don't queue very fast operations (<100ms) - queuing overhead exceeds operation time",
      "Don't queue operations requiring transactional atomicity",
      'Hybrid approach: synchronous for critical path, asynchronous for non-critical tasks',
      'Decision framework: required + fast + atomic = synchronous, slow + non-critical = asynchronous',
    ],
  },
  {
    id: 'q3',
    question:
      'Design a task queue system for a social media platform where users post content that needs moderation. The system must handle 10,000 posts/minute, moderate each post using an AI model (takes 2-5 seconds per post), and ensure no inappropriate content goes live. Explain your architecture, queue design, worker configuration, and how you handle the trade-off between speed and safety.',
    sampleAnswer:
      "CONTENT MODERATION TASK QUEUE ARCHITECTURE: **REQUIREMENTS ANALYSIS**: - **Scale**: 10,000 posts/min = ~167 posts/sec - **Latency**: Moderation takes 2-5s per post - **Safety**: No inappropriate content can go live (zero false negatives acceptable) - **Throughput**: Need to process 167 posts/sec = need significant parallelism **ARCHITECTURE DESIGN**: **Approach 1: Block Until Moderated (Safest)** ```mermaid User → API → Create Post (status=\"pending\") → Queue Moderation Task → Return \"Post under review\" → (User waits) ↓ Worker → AI Moderation (2-5s) → If safe: status=\"published\", notify user → If unsafe: status=\"rejected\", notify user ``` Pros: 100% safe (nothing goes live before moderation). Cons: 2-5s wait time (poor UX), users don't see posts immediately. **Approach 2: Publish Immediately + Retroactive Moderation (Fastest but Risky)** ```mermaid User → API → Create Post (status=\"published\") → Return immediately → Post visible to users! ↓ Queue Moderation Task (async) → Worker moderates → If unsafe: Unpublish + notify ``` Pros: Instant publishing (great UX). Cons: Unsafe content briefly visible (1-5s window), violates \"no inappropriate content goes live\" requirement ❌. **Approach 3: Hybrid - Pre-publish + Priority Moderation (CHOSEN)** ```mermaid User → API → Create Post (status=\"pending\") → Run quick sync filters (100ms): (1) Banned words regex check (2) Image hash against known bad content (3) User reputation check → If clearly safe (95% confidence): status=\"published\", return immediately (UX win!) → If uncertain: Queue for AI moderation, return \"under review\" ↓ Worker → AI Moderation (2-5s) → If safe: status=\"published\" → If unsafe: status=\"rejected\" ``` **DETAILED ARCHITECTURE**: **1. API Endpoint (Producer)** ```python from flask import Flask, request from celery import Celery import re app = Flask(__name__) celery_app = Celery('moderation', broker='redis://localhost:6379/0') @app.route('/posts', methods=['POST']) def create_post(): user_id = request.json['user_id'] content = request.json['content'] image_url = request.json.get('image_url') # Step 1: Create post (status=\"pending\") post = Post.create (user_id=user_id, content=content, image_url=image_url, status='pending') # Step 2: Quick synchronous filters (100ms total) risk_score = quick_moderation_check (post) if risk_score < 0.05: # Very low risk (95%+ safe) post.status = 'published' post.save() return {'post_id': post.id, 'status': 'published'}, 201 elif risk_score > 0.8: # Very high risk (80%+ unsafe) post.status = 'rejected' post.save() return {'error': 'Content violates policies'}, 400 else: # Uncertain: Queue for AI moderation moderate_post.apply_async (args=[post.id], priority=get_priority (user_id)) return {'post_id': post.id, 'status': 'under_review'}, 202 def quick_moderation_check (post) -> float: \"\"\"Fast synchronous checks (100ms)\"\"\" risk = 0.0 # Check 1: Banned words (10ms) banned_words = ['badword1', 'badword2', ...] if any (word in post.content.lower() for word in banned_words): risk += 0.5 # Check 2: Image hash against known bad content (20ms) if post.image_url: image_hash = compute_hash (download_image (post.image_url)) if image_hash in known_bad_hashes: risk += 0.5 # Check 3: User reputation (10ms) user = User.get (post.user_id) if user.violation_count > 5: risk += 0.3 if user.account_age_days < 7: risk += 0.2 return min (risk, 1.0) def get_priority (user_id): \"\"\"VIP users get higher priority\"\"\" user = User.get (user_id) if user.is_verified: return 9 # High priority elif user.follower_count > 10000: return 7 else: return 5 # Normal priority ``` **2. Message Broker: Redis with Priority Queues** ```python celery_app.conf.task_routes = { 'moderate_post': { 'queue': 'moderation', 'routing_key': 'moderation.priority', }, } # Priority queue: High-priority tasks processed first ``` **3. Celery Workers (Consumers)** ```python @celery_app.task (bind=True, max_retries=3) def moderate_post (self, post_id: int): \"\"\"AI-powered moderation (2-5s)\"\"\" post = Post.get (post_id) try: # Call AI moderation model (2-5s) result = ai_moderate( text=post.content, image_url=post.image_url, user_context=get_user_context (post.user_id) ) if result.is_safe: post.status = 'published' post.moderation_score = result.score notify_user (post.user_id, f\"Post {post_id} published\") elif result.is_unsafe: post.status = 'rejected' post.rejection_reason = result.reason notify_user (post.user_id, f\"Post {post_id} rejected: {result.reason}\") User.increment_violations (post.user_id) else: # Uncertain: Escalate to human moderator post.status = 'escalated' post.human_review_queue.add (post.id) post.save() except Exception as exc: # Retry with exponential backoff raise self.retry (exc=exc, countdown=2 ** self.request.retries) ``` **4. Worker Configuration for 10,000 posts/min** Calculation: - 10,000 posts/min = 167 posts/sec - Quick filter: 95% published immediately (158 posts/sec) - Remaining: 5% queued for AI moderation (8.3 posts/sec) - AI moderation: 2-5s per post (avg 3.5s) - Workers needed: 8.3 posts/sec × 3.5s/post = ~29 workers Deployment: ```bash # 30 workers (29 + 1 buffer) celery -A tasks worker \\ --concurrency=30 \\ --pool=prefork \\ --queues=moderation \\ --loglevel=info \\ --max-tasks-per-child=100 # Auto-scaling (Kubernetes HPA): kubectl autoscale deployment celery-workers \\ --cpu-percent=70 \\ --min=30 \\ --max=200 ``` **5. Queue Monitoring** ```python # Monitor queue depth @celery_app.task def check_queue_health(): from celery import current_app i = current_app.control.inspect() active = i.active() scheduled = i.scheduled() queue_depth = len (scheduled.get('moderation', [])) if queue_depth > 1000: alert_ops_team(\"Queue depth exceeded threshold\") if queue_depth > 5000: # Emergency: Auto-scale workers scale_workers (target=queue_depth // 30) ``` **TRADE-OFF ANALYSIS: Speed vs Safety** **Our Hybrid Approach**: - **95% posts**: Published instantly (100ms) - **5% posts**: Queued for AI moderation (2-5s wait) - **Safety**: Zero inappropriate content goes live (quick filters + AI catch all) **Metrics**: - **Average latency**: 0.95 × 0.1s + 0.05 × 3.5s = 0.27s - **Throughput**: 167 posts/sec sustained - **False negative rate**: <0.01% (very safe) **Alternative Approaches Comparison**: | Approach | Latency | Safety | Throughput | UX | |----------|---------|--------|------------|----| | Synchronous AI for all | 3.5s | 100% | Requires 580 workers | Poor | | Publish immediately | 0.1s | 95% (5% bad content visible) | Unlimited | Great but unsafe | | Hybrid (ours) | 0.27s avg | 99.99% | 167/sec with 30 workers | Good + Safe | **FAILURE HANDLING**: - AI model down: Fallback to rule-based filters + human review queue - Redis down: Failover to backup Redis, alert ops team - Worker crashes: Tasks requeued automatically (Celery acknowledgment) - Queue overflow: Auto-scale workers, throttle new posts temporarily **MONITORING & ALERTS**: - Queue depth (alert if >1000) - Moderation latency (p50, p95, p99) - False positive/negative rates - Worker CPU/memory utilization - AI model response time **COST OPTIMIZATION**: - Use GPU instances for AI workers (faster inference: 2s → 0.5s) - Cache AI results for similar content (de-duplication) - Batch processing: Process 10 posts per AI call (10× throughput) - Scale workers based on time of day (peak vs off-peak) **KEY TAKEAWAY**: Hybrid approach balances speed and safety: 95% instant publishing (great UX) + 5% AI moderation (perfect safety). Quick synchronous filters eliminate obvious cases, AI handles nuanced moderation. System scales horizontally by adding workers. Zero unsafe content goes live while maintaining sub-second latency for most users.",
    keyPoints: [
      'Hybrid approach: Quick sync filters (100ms) + async AI moderation (2-5s) for uncertain cases',
      '95% of posts published instantly (low-risk), 5% queued for AI moderation',
      'Priority queues: VIP users get faster moderation',
      'Worker calculation: 8.3 posts/sec × 3.5s = 29 workers needed',
      'Safety: Zero false negatives (no inappropriate content goes live)',
    ],
  },
];
