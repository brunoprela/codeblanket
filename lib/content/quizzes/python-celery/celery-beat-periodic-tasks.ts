/**
 * Quiz questions for Celery Beat (Periodic Tasks) section
 */

export const celeryBeatPeriodicTasksQuiz = [
  {
    id: 'q1',
    question:
      'Your production system accidentally started 3 Celery Beat instances simultaneously. What problems will this cause, and how do you prevent/detect/fix it?',
    sampleAnswer: `MULTIPLE CELERY BEAT INSTANCES PROBLEM: **PROBLEM**: Running 3 Beat instances simultaneously causes **DUPLICATE TASK EXECUTION**. **What happens**: Each Beat instance independently checks schedule and queues tasks. Timeline example (task scheduled every 5 minutes): T+0min: Beat1 queues "cleanup" task → Worker1 executes. T+0min: Beat2 queues "cleanup" task → Worker2 executes (DUPLICATE!). T+0min: Beat3 queues "cleanup" task → Worker3 executes (DUPLICATE!). Result: Task runs 3× instead of 1×! **CONSEQUENCES**: (1) **Data corruption**: Cleanup task deletes same records 3×, DB backup runs 3×, Email sent 3×. (2) **Resource waste**: 3× CPU/memory/API calls. (3) **Race conditions**: Multiple tasks modifying same data. (4) **Billing issues**: 3× external API costs. (5) **User complaints**: 3× notification emails. **REAL-WORLD EXAMPLE**: Daily email digest scheduled midnight: Beat1, Beat2, Beat3 all queue "send_daily_digest" → Users receive 3 identical emails! Angry users, support tickets, brand damage. **DETECTION**: **Method 1: Log Analysis**: \\`\\`\\`bash # Check Beat logs - should see only 1 instance grep "beat: Starting..." /var/log/celery/beat.log # If you see multiple "Starting" messages → Problem! \\`\\`\\` **Method 2: Process Monitoring**: \\`\\`\\`bash # Check running Beat processes ps aux | grep "celery.*beat" # Should see exactly 1 process! # If multiple → Kill extras \\`\\`\\` **Method 3: PID File**: \\`\\`\\`bash # Beat creates PID file cat /var/run/celerybeat.pid # Only 1 file should exist # Multiple PIDs → Problem! \\`\\`\\` **Method 4: Monitoring Metrics**: \\`\\`\\`python # Track task execution counts @app.task def daily_backup(): """Should run once per day""" metrics.increment("task.daily_backup.execution") # Alert if > 1 execution per day # If metric shows 3 executions → 3 Beat instances! \\`\\`\\` **Method 5: Database Check** (with django-celery-beat): \\`\\`\\`python from django_celery_beat.models import PeriodicTask task = PeriodicTask.objects.get(name="daily-backup") # Check last_run_at timestamp # If multiple executions within seconds → Duplicate Beats \\`\\`\\` **PREVENTION**: **Solution 1: Deployment Constraints**: \\`\\`\\`yaml # Kubernetes: Force 1 replica apiVersion: apps/v1 kind: Deployment metadata: name: celery-beat spec: replicas: 1 # MUST be 1! selector: matchLabels: app: celery-beat # Add anti-affinity to prevent multiple pods affinity: podAntiAffinity: requiredDuringSchedulingIgnoredDuringExecution: - labelSelector: matchExpressions: - key: app operator: In values: - celery-beat topologyKey: kubernetes.io/hostname \\`\\`\\` **Solution 2: Locking Mechanism**: \\`\\`\\`python # Use Redis lock to ensure only 1 Beat runs import redis from celery import Celery redis_client = redis.Redis(host="localhost", port=6379) app = Celery("myapp", broker="redis://localhost:6379/0") # Acquire lock before starting Beat def start_beat_with_lock(): lock = redis_client.lock("celery-beat-lock", timeout=300) # 5 min if lock.acquire(blocking=False): try: # Start Beat (only if lock acquired) app.Beat().run() finally: lock.release() else: print("Another Beat instance running, exiting") sys.exit(1) \\`\\`\\` **Solution 3: Systemd (One Per Server)**: \\`\\`\\`ini # /etc/systemd/system/celerybeat.service [Unit] Description=Celery Beat After=network.target [Service] Type=simple User=celery PIDFile=/var/run/celery/beat.pid ExecStart=/opt/myapp/venv/bin/celery -A tasks beat --pidfile=/var/run/celery/beat.pid Restart=always # Only 1 service per server [Install]

 WantedBy=multi - user.target\\`\\`\\` **Solution 4: Docker Compose (replicas: 1)**: \\`\\`\\`yaml services: celery - beat: image: myapp command: celery - A tasks beat deploy: replicas: 1 # Exactly 1 instance restart_policy: condition: on- failure\\`\\`\\` **FIX (If Already Running Multiple)**: **Step 1: Identify all Beat processes**: \\`\\`\\`bash ps aux | grep "celery.*beat" # Output: celery 1234 ...celery - A tasks beat # celery 5678 ...celery - A tasks beat # celery 9012 ...celery - A tasks beat\\`\\`\\` **Step 2: Kill all but one**: \\`\\`\\`bash # Kill by PID kill 5678 9012 # Or kill all and restart 1 pkill - f "celery.*beat" celery - A tasks beat--detach\\`\\`\\` **Step 3: Verify only 1 running**: \\`\\`\\`bash ps aux | grep "celery.*beat" # Should see exactly 1 process\\`\\`\\` **Step 4: Check task logs**: \\`\\`\\`bash # Verify no more duplicate executions tail - f /var/log/celery / worker.log # Should see each task execute once\\`\\`\\` **IDEMPOTENT TASKS (Defense in Depth)**: Even with 1 Beat, make tasks idempotent to handle accidental duplicates: \\`\\`\\`python @app.task def daily_backup(): """Idempotent: Safe even if run multiple times""" # Check if backup already done today if backup_exists_today(): logger.info("Backup already done today, skipping") return # Perform backup perform_backup() # Mark as done mark_backup_done_today()\\`\\`\\` **MONITORING ALERTS**: \\`\\`\\`python # Prometheus alert alert: CeleryBeatMultipleInstances expr: count(up{ job="celery-beat" }) > 1 for: 1m labels: severity: critical annotations: summary: "Multiple Celery Beat instances detected" # Or custom metric @app.task def monitor_task_duplicates(): """Periodic task to check for duplicates""" # Check if this task ran multiple times in last period executions = count_task_executions(task_name = "daily_backup", period = "1h") if executions > 1: alert_ops_team(f"Task ran {executions} times (expected 1)!")\\`\\`\\` **KEY TAKEAWAY**: Multiple Beat instances = Duplicate task execution. Prevent: (1) Deployment constraints (replicas: 1), (2) Locking mechanism (Redis lock), (3) Process monitoring, (4) Idempotent tasks (defense). Detect: Log analysis, process count, metrics. Fix: Kill all but one Beat instance.`,
    keyPoints: [
      'Multiple Beat instances → duplicate task execution (task runs N times for N instances)',
      'Prevention: Deployment replicas=1, Redis locking, systemd single service',
      'Detection: ps aux | grep beat (should see 1 process), log analysis, metrics',
      'Fix: Kill all Beat processes except 1, verify with ps aux',
      'Defense: Make tasks idempotent to handle accidental duplicates',
    ],
  },
  {
    id: 'q2',
    question:
      'Design a comprehensive periodic task schedule for an e-commerce platform including: daily sales reports, hourly inventory sync, weekly customer emails, monthly billing, and real-time health checks. Include crontab expressions, task routing, and production considerations.',
    sampleAnswer: `E-COMMERCE PERIODIC TASK SCHEDULE: **REQUIREMENTS ANALYSIS**: (1) Daily sales reports: 2 AM (off-peak). (2) Hourly inventory sync: Every hour :00. (3) Weekly customer emails: Sunday 9 AM. (4) Monthly billing: 1st of month, midnight. (5) Health checks: Every 5 minutes. **COMPREHENSIVE SCHEDULE**: \\`\\`\\`python from celery import Celery from celery.schedules import crontab app = Celery("ecommerce", broker = "redis://localhost:6379/0") app.conf.timezone = "UTC" app.conf.enable_utc = True app.conf.beat_schedule = { # 1. Health Check(Every 5 minutes, critical) "health-check": { "task": "tasks.system_health_check", "schedule": 300.0, # 5 minutes "options": { "queue": "monitoring", "priority": 9, # Highest priority "expires": 240, # Expire if not run within 4 min }, }, # 2. Inventory Sync(Every hour at:00) "hourly-inventory-sync": { "task": "tasks.sync_inventory", "schedule": crontab(minute = 0), # Every hour "options": { "queue": "inventory", "priority": 8, "expires": 3000, # 50 minutes }, "kwargs": { "full_sync": False }, # Incremental }, # 3. Daily Sales Report(2 AM UTC) "daily-sales-report": { "task": "tasks.generate_daily_sales_report", "schedule": crontab(hour = 2, minute = 0), "options": { "queue": "reports", "priority": 7, "expires": 82800, # 23 hours(must run before next) }, "kwargs": { "email_recipients": ["sales@company.com"] }, }, # 4. Weekly Customer Email(Sunday 9 AM) "weekly-customer-digest": { "task": "tasks.send_weekly_customer_digest", "schedule": crontab(hour = 9, minute = 0, day_of_week = 0), # Sunday "options": { "queue": "emails", "priority": 6, "expires": 3600 * 6, # 6 hours }, }, # 5. Monthly Billing(1st of month, midnight) "monthly-billing": { "task": "tasks.process_monthly_billing", "schedule": crontab(hour = 0, minute = 0, day_of_month = 1), "options": { "queue": "billing", "priority": 9, # Critical! "expires": 3600 * 12, # 12 hours }, "kwargs": { "send_invoices": True }, }, # Additional tasks # 6. Abandoned Cart Reminder(Every 30 minutes) "abandoned-cart-reminder": { "task": "tasks.send_abandoned_cart_reminders", "schedule": crontab(minute = "*/30"), "options": { "queue": "emails", "priority": 5 }, }, # 7. Database Backup(Daily 3 AM) "database-backup": { "task": "tasks.backup_database", "schedule": crontab(hour = 3, minute = 0), "options": { "queue": "maintenance", "priority": 9, "expires": 3600 * 3, # 3 hours }, "kwargs": { "backup_type": "full", "upload_to_s3": True }, }, # 8. Cleanup Old Sessions(Daily 4 AM) "cleanup-old-sessions": { "task": "tasks.cleanup_old_sessions", "schedule": crontab(hour = 4, minute = 0), "options": { "queue": "maintenance" }, "kwargs": { "days_old": 30 }, }, # 9. Update Product Recommendations(Every 6 hours) "update-recommendations": { "task": "tasks.compute_product_recommendations", "schedule": crontab(minute = 0, hour = "*/6"), # 0, 6, 12, 18 "options": { "queue": "analytics", "priority": 4 }, }, # 10. Check Low Stock(Every 2 hours during business hours) "check-low-stock": { "task": "tasks.check_low_stock_alerts", "schedule": crontab(minute = 0, hour = "9-17/2"), # 9, 11, 13, 15, 17 "options": { "queue": "inventory" }, }, # 11. Update Search Index(Every 15 minutes) "update-search-index": { "task": "tasks.update_elasticsearch_index", "schedule": crontab(minute = "*/15"), "options": { "queue": "search", "expires": 840 }, # 14 min }, # 12. Weekly Analytics Report(Monday 8 AM) "weekly-analytics": { "task": "tasks.generate_weekly_analytics", "schedule": crontab(hour = 8, minute = 0, day_of_week = 1), # Monday "options": { "queue": "reports" }, }, } \\`\\`\\` **TASK IMPLEMENTATIONS**: \\`\\`\\`python @app.task(bind = True) def system_health_check(self): """Health check (every 5 min)""" try: # Check database db.execute("SELECT 1") # Check Redis redis_client.ping() # Check queue depth depth = get_queue_depth() if depth > 10000: alert_ops("Queue depth high: {depth}") # Check worker count workers = app.control.inspect().active_queues() if len(workers) < 3: alert_ops("Only {len(workers)} workers active!") logger.info("Health check passed") except Exception as e: logger.error(f"Health check failed: {e}") alert_ops(f"CRITICAL: Health check failed: {e}") raise @app.task def sync_inventory(): """Hourly inventory sync""" # Check if already synced this hour if is_synced_this_hour(): return # Sync from warehouse API warehouse_data = fetch_warehouse_inventory() update_local_inventory(warehouse_data) mark_synced_this_hour() @app.task def generate_daily_sales_report(): """Daily sales report (2 AM)""" # Idempotent check date = datetime.utcnow().date() if report_exists(date): return # Generate report sales_data = fetch_sales_data(date) report = generate_pdf_report(sales_data) # Upload to S3 s3_url = upload_to_s3(report, f"reports/sales-{date}.pdf") # Email stakeholders send_email(to = ["sales@company.com"], subject = f"Daily Sales Report - {date}", body = f"Report: {s3_url}") mark_report_generated(date) @app.task def send_weekly_customer_digest(): """Weekly customer email (Sunday 9 AM)""" # Get active customers users = User.objects.filter(active = True, subscribed = True) for user in users.iterator(): # Send personalized digest send_email(to = user.email, subject = "Your Weekly Digest", body = generate_digest_content(user)) # Rate limit: 100 emails / sec time.sleep(0.01) @app.task def process_monthly_billing(): """Monthly billing (1st of month)""" # Idempotent: Check if already processed month = datetime.utcnow().strftime("%Y-%m") if billing_processed(month): return # Get subscriptions subscriptions = Subscription.objects.filter(active = True) for sub in subscriptions: # Charge customer charge_result = stripe.charge(sub.customer, sub.amount) # Generate invoice invoice = generate_invoice(sub, charge_result) send_invoice_email(sub.customer, invoice) mark_billing_processed(month)\\`\\`\\` **WORKER DEPLOYMENT**: \\`\\`\\`bash # Worker 1: Monitoring queue(high priority, fast) celery - A tasks worker - Q monitoring--concurrency = 4 --pool = prefork # Worker 2 - 3: Email queue(I / O - heavy, high concurrency) celery - A tasks worker - Q emails--concurrency = 200 --pool = gevent # Worker 4 - 5: Inventory / Reports(CPU - heavy) celery - A tasks worker - Q inventory, reports--concurrency = 8 --pool = prefork # Worker 6: Billing queue(critical, dedicated) celery - A tasks worker - Q billing--concurrency = 2 --pool = prefork # Worker 7: Maintenance(low priority, off - peak) celery - A tasks worker - Q maintenance--concurrency = 2 # Beat scheduler(ONLY 1 instance!) celery - A tasks beat--loglevel = info\\`\\`\\` **PRODUCTION CONSIDERATIONS**: **1. Timezone Handling**: All times in UTC, convert to user timezone when displaying. **2. Idempotency**: All tasks check if already executed (prevent duplicates). **3. Expiration**: Tasks expire if not executed within reasonable time. **4. Priority**: Critical tasks (billing, health) get highest priority. **5. Queue Isolation**: Slow reports don't block fast health checks. **6. Error Handling**: All tasks have try/except and alerting. **7. Monitoring**: Track execution time, success rate, queue depth. **8. Scaling**: Add workers during peak times (business hours). **MONITORING DASHBOARD**: \\`\\`\\`python # Key metrics to track metrics = { "health_check": { "frequency": "5min", "p99_latency": "<1s", "success_rate": ">99.9%" }, "inventory_sync": { "frequency": "1h", "p99_latency": "<30s", "success_rate": ">99%" }, "daily_sales_report": { "frequency": "1d", "p99_latency": "<5min", "success_rate": "100%" }, "weekly_digest": { "frequency": "1w", "emails_sent": ">10k", "bounce_rate": "<2%" }, "monthly_billing": { "frequency": "1mo", "success_rate": "100%", "revenue": ">$100k" }, } # Alerts alert_rules = ["health_check fails 3 times in 15 min → Page on-call", "daily_sales_report not generated by 6 AM → Alert", "monthly_billing fails → CRITICAL alert + SMS", "queue_depth > 10k for 5 min → Auto-scale workers",]\\`\\`\\` **KEY TAKEAWAY**: Comprehensive schedule with: (1) Appropriate crontab expressions, (2) Queue routing for isolation, (3) Priority for critical tasks, (4) Expiration to prevent pile-up, (5) Idempotency for safety, (6) Monitoring and alerting.`,
    keyPoints: [
      'Schedule design: Health checks (5min), hourly sync, daily reports (2 AM), weekly emails (Sunday), monthly billing (1st)',
      'Queue routing: monitoring, emails, inventory, reports, billing (isolation + specialization)',
      'Priority: Critical tasks (billing, health) get priority 9, reports get priority 7',
      'Idempotency: All tasks check if already executed (date-based checks)',
      'Production: 1 Beat instance, multiple workers per queue, monitoring + alerting',
    ],
  },
  {
    id: 'q3',
    question:
      "Your Celery Beat schedule file got corrupted and tasks stopped running. Explain what the schedule file is, why it's needed, how to detect corruption, and how to recover.",
    sampleAnswer: `CELERY BEAT SCHEDULE FILE CORRUPTION: **WHAT IS THE SCHEDULE FILE?**: Celery Beat stores scheduling state in a file (default: celerybeat-schedule): (1) Last run timestamp for each task, (2) Next run timestamp, (3) Schedule information. Purpose: Remember when tasks last ran (persist across Beat restarts). **WHY IT EXISTS**: Without schedule file: Every Beat restart would run all tasks immediately! Example: Daily backup scheduled for 2 AM. Beat starts at 3 PM. Without file: Beat thinks "backup never ran, run it NOW!" (wrong). With file: Beat knows "backup already ran at 2 AM today, skip until tomorrow 2 AM" (correct). **FILE LOCATION**: \\`\\`\\`bash # Default location./ celerybeat - schedule # Custom location(recommended) celery - A tasks beat--schedule = /var/run / celery / celerybeat - schedule\\`\\`\\` **FILE CONTENTS** (binary format): \\`\\`\\`python # Schedule file structure(simplified) { "entries": { "daily-backup": { "last_run_at": datetime(2024, 1, 15, 2, 0, 0), "total_run_count": 365, "schedule": <crontab: 0 2 * * *> }, "hourly-sync": { "last_run_at": datetime(2024, 1, 15, 14, 0, 0), "total_run_count": 8760, "schedule": <crontab: 0 * * * *> } } } \\`\\`\\` **CORRUPTION SYMPTOMS**: **Symptom 1: Tasks Running Too Frequently**: Daily backup runs every hour (should be daily). Hourly sync runs every minute (should be hourly). **Symptom 2: Tasks Not Running At All**: Scheduled tasks never execute. Beat logs show schedule loaded but no task queuing. **Symptom 3: Beat Won't Start**: \\`\\`\\`bash celery - A tasks beat ERROR: Corrupt schedule file\\`\\`\\` **Symptom 4: Beat Logs Show Errors**: \\`\\`\\`bash[ERROR] Failed to load schedule file ValueError: corrupt schedule file\\`\\`\\` **DETECTION**: **Method 1: Check Beat Logs**: \\`\\`\\`bash tail - f /var/log/celery / beat.log # Look for: [ERROR] Failed to load schedule[WARNING] Schedule file corrupt, using default \\`\\`\\` **Method 2: Inspect Schedule File**: \\`\\`\\`bash ls - lh /var/run/celery / celerybeat - schedule # If size is 0 bytes or very small → Corrupt # If modification time is old(days ago) → Stale\\`\\`\\` **Method 3: Monitor Task Execution**: \\`\\`\\`python # Check if tasks running as scheduled from django_celery_beat.models import PeriodicTask task = PeriodicTask.objects.get(name = "daily-backup") # Check last_run_at timestamp if task.last_run_at: time_since_last_run = datetime.now() - task.last_run_at if time_since_last_run > timedelta(hours = 25): # Daily task, 25h threshold alert_ops("daily-backup hasn't run in 25+ hours!")\\`\\`\\` **CAUSES OF CORRUPTION**: (1) **Disk full**: No space to write schedule updates. (2) **Permissions**: Beat can't write to schedule file location. (3) **Crash during write**: Beat killed mid-write. (4) **Multiple Beat instances**: Two Beats writing to same file (race condition). (5) **Manual edit**: Someone edited binary file. **RECOVERY**: **Step 1: Stop Beat**: \\`\\`\\`bash # Find Beat process ps aux | grep "celery.*beat" # Kill it kill < PID > # Or if systemd sudo systemctl stop celerybeat\\`\\`\\` **Step 2: Backup Corrupt File**: \\`\\`\\`bash # Keep for debugging cp /var/run/celery / celerybeat - schedule / tmp / celerybeat - schedule.corrupt\\`\\`\\` **Step 3: Delete Corrupt File**: \\`\\`\\`bash rm /var/run/celery / celerybeat - schedule\\`\\`\\` **Step 4: Restart Beat** (Creates fresh schedule): \\`\\`\\`bash celery - A tasks beat--loglevel = info--schedule = /var/run / celery / celerybeat - schedule # Beat creates new, clean schedule file\\`\\`\\` **Step 5: Verify**: \\`\\`\\`bash # Check file created and growing ls - lh /var/run/celery / celerybeat - schedule # Check Beat logs tail - f /var/log/celery / beat.log # Should see: [INFO] beat: Starting...[INFO] Scheduler: Sending due task ... \\`\\`\\` **Step 6: Monitor Task Execution**: \\`\\`\\`bash # Watch worker logs for scheduled tasks tail - f /var/log/celery / worker.log # Should see tasks executing on schedule\\`\\`\\` **PREVENTION**: **Solution 1: Separate Partition** (Prevent disk full): \\`\\`\\`bash # Mount separate partition for schedule file df - h /var/run/celery # Ensure space available\\`\\`\\` **Solution 2: Proper Permissions**: \\`\\`\\`bash # Ensure Beat user can write chown celery: celery /var/run/celery chmod 755 /var/run/celery \\`\\`\\` **Solution 3: Monitoring & Alerts**: \\`\\`\\`python # Monitor schedule file @app.task def monitor_beat_health(): """Periodic task to check Beat health""" schedule_file = "/var/run/celery/celerybeat-schedule" # Check file exists if not os.path.exists(schedule_file): alert_ops("Beat schedule file missing!") return # Check file size size = os.path.getsize(schedule_file) if size == 0: alert_ops("Beat schedule file is empty!") # Check file age age = time.time() - os.path.getmtime(schedule_file) if age > 3600: # Not updated in 1 hour alert_ops("Beat schedule file stale!")\\`\\`\\` **Solution 4: Systemd Restart on Failure**: \\`\\`\\`ini # / etc / systemd / system / celerybeat.service[Service] Restart = always RestartSec = 10 # Restart if crashes\\`\\`\\` **Solution 5: Database Scheduler** (No file!): \\`\\`\\`python # Use django - celery - beat(stores in database) app.conf.beat_scheduler = "django_celery_beat.schedulers:DatabaseScheduler" # No schedule file needed! # Schedule stored in database(more reliable)\\`\\`\\` **ALTERNATIVE: DATABASE SCHEDULER**: Best for production (no file corruption risk): \\`\\`\\`bash # Install pip install django - celery - beat # settings.py INSTALLED_APPS = ["django_celery_beat"] # Migrate python manage.py migrate django_celery_beat # celeryconfig.py beat_scheduler = "django_celery_beat.schedulers:DatabaseScheduler" # Now schedule stored in database(no file!)\\`\\`\\` **MONITORING SCRIPT**: \\`\\`\\`python # monitor_beat.py import os import sys import time schedule_file = "/var/run/celery/celerybeat-schedule" while True: if not os.path.exists(schedule_file): print("ERROR: Schedule file missing!") sys.exit(1) size = os.path.getsize(schedule_file) if size == 0: print("ERROR: Schedule file empty!") sys.exit(1) age = time.time() - os.path.getmtime(schedule_file) if age > 3600: print(f"WARNING: Schedule file stale ({age/3600:.1f} hours)") print(f"✓ Beat healthy (file size: {size} bytes, age: {age:.0f}s)") time.sleep(300) # Check every 5 min\\`\\`\\` **KEY TAKEAWAY**: Schedule file stores "last run" timestamps for tasks. Corruption causes tasks to run wrong schedule or not at all. Recovery: Stop Beat, delete file, restart (creates fresh file). Prevention: Database scheduler (django-celery-beat), monitoring, proper permissions, disk space.`,
    keyPoints: [
      'Schedule file stores last_run_at timestamps (persist across restarts)',
      'Corruption causes: disk full, permissions, crash during write, multiple Beats',
      "Symptoms: tasks run too frequently/never, Beat won't start, errors in logs",
      'Recovery: Stop Beat, delete corrupt file, restart (creates fresh file)',
      'Prevention: Use database scheduler (django-celery-beat), monitoring, proper permissions',
    ],
  },
];
