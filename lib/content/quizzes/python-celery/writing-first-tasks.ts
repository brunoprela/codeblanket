/**
 * Quiz questions for Writing Your First Tasks section
 */

export const writingFirstTasksQuiz = [
  {
    id: 'q1',
    question:
      'You need to pass user profile data (including profile photo as bytes, 5MB) to a Celery task for processing. How should you design this to follow best practices?',
    sampleAnswer:
      'PASSING LARGE DATA TO CELERY TASKS: **PROBLEM**: Passing 5MB photo bytes as task argument is bad practice: (1) Serialize 5MB → Redis (memory bloat), (2) Deserialize 5MB in worker (CPU cost), (3) Every retry = another 5MB copy, (4) Result backend stores 5MB result. **ANTI-PATTERN** (❌ Don\'t do this): ```python @app.task def process_profile(user_id: int, photo_bytes: bytes): """❌ BAD: Passing 5MB bytes""" img = Image.open(io.BytesIO(photo_bytes)) # Process... # Called like: with open("photo.jpg", "rb") as f: photo_data = f.read() # 5MB process_profile.delay(user_id=123, photo_bytes=photo_data) # ❌ 5MB through queue! ``` **Why bad**: Queue bloat (5MB per task), Memory waste (5MB × concurrent tasks), Slow serialization/deserialization, Redis memory explosion with 1000s of tasks. **BEST PRACTICE** (✅ Do this): Pass reference (URL, S3 key, file path), not data itself: ```python @app.task def process_profile_reference(user_id: int, photo_s3_key: str): """✅ GOOD: Pass S3 reference (tiny string)""" import boto3 s3 = boto3.client("s3") # Download in worker (only when needed) obj = s3.get_object(Bucket="my-bucket", Key=photo_s3_key) photo_bytes = obj["Body"].read() # Process img = Image.open(io.BytesIO(photo_bytes)) processed = process_image(img) # Upload result result_key = f"processed/{user_id}.jpg" s3.put_object(Bucket="my-bucket", Key=result_key, Body=processed) # Return reference (not data!) return {"result_s3_key": result_key} # Called like: s3.upload_file("photo.jpg", "my-bucket", f"uploads/{user_id}.jpg") process_profile_reference.delay(user_id=123, photo_s3_key=f"uploads/{user_id}.jpg") # ✅ Tiny argument! ``` **Benefits**: Argument size: 5MB → 50 bytes (100,000× smaller!), Queue memory: Minimal, Worker memory: Only during processing, Retry cost: Just re-download if needed, Scalability: Can handle millions of tasks. **ALTERNATIVE APPROACHES**: **Option 1: S3 Pre-signed URLs**: ```python # Generate pre-signed URL (expires in 1 hour) photo_url = s3.generate_presigned_url("get_object", Params={"Bucket": "my-bucket", "Key": f"uploads/{user_id}.jpg"}, ExpiresIn=3600) process_profile_url.delay(user_id=123, photo_url=photo_url) ``` **Option 2: Database Reference**: ```python # Store photo in database, pass ID photo_id = Photo.create(user_id=user_id, data=photo_bytes) process_profile_db.delay(user_id=123, photo_id=photo_id) @app.task def process_profile_db(user_id: int, photo_id: int): photo = Photo.get(photo_id) photo_bytes = photo.data # Process... ``` **Option 3: Shared Filesystem** (if workers on same machine): ```python # Save to shared temp directory temp_path = f"/shared/tmp/photo_{user_id}.jpg" with open(temp_path, "wb") as f: f.write(photo_bytes) process_profile_file.delay(user_id=123, file_path=temp_path) @app.task def process_profile_file(user_id: int, file_path: str): with open(file_path, "rb") as f: photo_bytes = f.read() # Process... os.remove(file_path) # Clean up ``` **COMPARISON**: | Approach | Argument Size | Pros | Cons | |----------|---------------|------|------| | Pass bytes (❌) | 5MB | Simple | Queue bloat, memory waste | | S3 reference (✅) | 50 bytes | Scalable, clean | Need S3 | | Pre-signed URL (✅) | 200 bytes | No S3 client in worker | URL expires | | DB reference (✅) | 4 bytes | Simple | DB load | | File path (⚠️) | 50 bytes | Fast | Workers must share filesystem | **GENERAL RULE**: - Arguments <1KB: Pass directly (IDs, strings, small dicts) - Arguments 1KB-100KB: Consider passing reference - Arguments >100KB: Always pass reference (S3 key, URL, DB ID) - Return values: Same rules apply! **OTHER SERIALIZATION ISSUES**: **Non-serializable types** (datetime, Decimal, custom objects): ```python # ❌ Bad @app.task def process_order_bad(order_date: datetime.datetime, price: decimal.Decimal): """datetime/Decimal not JSON serializable!""" pass # ✅ Good @app.task def process_order_good(order_date_iso: str, price_float: float): """Convert to serializable types""" order_date = datetime.datetime.fromisoformat(order_date_iso) price = decimal.Decimal(str(price_float)) # Called: process_order_good.delay( order_date_iso=datetime.datetime.now().isoformat(), price_float=19.99 ) ``` **KEY TAKEAWAY**: Never pass large data (>100KB) as task arguments. Pass reference (ID, URL, S3 key). Worker downloads/fetches when needed. Same for return values: return reference, not 5MB data. This pattern is foundation of scalable task queues.',
    keyPoints: [
      'Never pass large data (>100KB) as task arguments - pass reference instead',
      'Use S3 key, pre-signed URL, database ID, or file path (not raw bytes)',
      'Worker downloads data only when processing (not stored in queue)',
      'Return values: same rule - return S3 key for large results, not data itself',
      'General rule: args <1KB pass directly, >100KB always pass reference',
    ],
  },
  {
    id: 'q2',
    question:
      'Explain the difference between .delay() and .apply_async(). When would you use each? Provide specific scenarios.',
    sampleAnswer:
      '.delay() VS .apply_async() COMPARISON: **SYNTAX COMPARISON**: ```python from tasks import send_email # .delay() - Simple result = send_email.delay("user@example.com", "Welcome", "Hi!") # .apply_async() - Equivalent result = send_email.apply_async(args=("user@example.com", "Welcome", "Hi!")) ``` They do the same thing by default! `.delay()` is syntactic sugar for `.apply_async()`. **UNDER THE HOOD**: ```python # .delay() implementation (simplified) def delay(self, *args, **kwargs): return self.apply_async(args, kwargs) ``` `.delay()` is just a convenience wrapper! **WHEN TO USE .delay()**: **Scenario 1: Simple Task Queuing** (90% of cases): ```python # No special options needed result = send_email.delay("user@example.com", "Hi", "Hello!") result = process_data.delay(user_id=12345) result = generate_report.delay(company_id=999) ``` **Why .delay()**: Clean syntax, Pythonic (feels like function call), No boilerplate, Perfect for most use cases. **WHEN TO USE .apply_async()**: **Scenario 1: Delayed Execution (countdown)**: ```python # Send email after 1 hour result = send_reminder.apply_async( kwargs={"user_id": 12345, "message": "Don\'t forget!"}, countdown=3600 # 1 hour ) ``` Can\'t do this with `.delay()`! **Scenario 2: Scheduled Execution (eta)**: ```python from datetime import datetime, timedelta # Send at specific time eta = datetime.utcnow() + timedelta(days=1) result = send_scheduled_email.apply_async( kwargs={"email": "user@example.com"}, eta=eta # Tomorrow ) ``` **Scenario 3: Task Expiration**: ```python # Task expires if not executed within 1 hour result = process_urgent_data.apply_async( kwargs={"data_id": 12345}, expires=3600 # Expire after 1 hour ) ``` Useful for time-sensitive tasks (stock prices, real-time data). **Scenario 4: Priority Queues**: ```python # High priority (VIP user) result = process_order.apply_async( kwargs={"order_id": 12345}, priority=9 # Highest priority (0-9) ) # Low priority (bulk processing) result = generate_analytics.apply_async( kwargs={"date": "2024-01-01"}, priority=1 # Low priority ) ``` Requires RabbitMQ (Redis has limited priority support). **Scenario 5: Custom Queue Routing**: ```python # Send to specific queue result = process_video.apply_async( kwargs={"video_id": 12345}, queue="gpu-workers" # Route to GPU queue ) result = send_email.apply_async( kwargs={"email": "user@example.com"}, queue="email-queue" # Route to email queue ) ``` **Scenario 6: Retry Configuration**: ```python result = call_external_api.apply_async( kwargs={"endpoint": "https://api.example.com/data"}, retry=True, retry_policy={ "max_retries": 5, "interval_start": 0, "interval_step": 60, # 0s, 60s, 120s, 180s, 240s "interval_max": 600 } ) ``` **Scenario 7: Custom Task ID (Idempotency)**: ```python # Prevent duplicate tasks task_id = f"daily-report-{company_id}-{date}" result = generate_daily_report.apply_async( kwargs={"company_id": 123, "date": "2024-01-01"}, task_id=task_id ) # If called again with same ID, won\'t create duplicate ``` **Scenario 8: Linking Tasks (Callbacks)**: ```python # Call another task when this completes result = process_data.apply_async( kwargs={"data_id": 12345}, link=send_notification.s(user_id=999) # Callback ) ``` When `process_data` completes, `send_notification` runs automatically. **Scenario 9: Serializer Override**: ```python # Use msgpack for large binary data result = process_binary_data.apply_async( kwargs={"data": binary_data}, serializer="msgpack" # Faster than JSON for binary ) ``` **COMPLETE COMPARISON TABLE**: | Feature | .delay() | .apply_async() | |---------|----------|----------------| | **Syntax** | Clean, simple | Verbose | | **Countdown** | ❌ No | ✅ Yes | | **ETA (scheduled)** | ❌ No | ✅ Yes | | **Expires** | ❌ No | ✅ Yes | | **Priority** | ❌ No | ✅ Yes | | **Queue routing** | ❌ No | ✅ Yes | | **Custom task ID** | ❌ No | ✅ Yes | | **Retry policy** | ❌ No | ✅ Yes | | **Callbacks (link)** | ❌ No | ✅ Yes | | **Serializer** | ❌ No | ✅ Yes | | **Use case** | 90% of tasks | Advanced control | **REAL-WORLD DECISION FLOW**: ```python # Do you need any of: countdown, eta, expires, priority, queue, custom ID, retries, callbacks? if any_advanced_features_needed: use_apply_async() else: use_delay() # Cleaner! ``` **EXAMPLES FROM PRODUCTION**: **Example 1: E-commerce Order Processing**: ```python # Step 1: Charge payment (immediate, high priority) charge_result = charge_payment.apply_async( kwargs={"order_id": order_id}, queue="payment-queue", priority=9 # High priority ) # Step 2: Send confirmation (immediate) send_email.delay("user@example.com", "Order confirmed", "Thanks!") # Step 3: Generate invoice (delayed, low priority) generate_invoice.apply_async( kwargs={"order_id": order_id}, countdown=300, # 5 minutes later priority=3 # Low priority ) # Step 4: Request review (scheduled, 7 days later) request_review.apply_async( kwargs={"order_id": order_id}, eta=datetime.utcnow() + timedelta(days=7) ) ``` Mixed usage based on requirements! **Example 2: Social Media Scheduler**: ```python # Post immediately result = post_to_twitter.delay(tweet_id=12345) # Schedule post for later result = post_to_twitter.apply_async( kwargs={"tweet_id": 67890}, eta=datetime(2024, 12, 25, 9, 0, 0) # Christmas morning ) ``` **Example 3: API Rate Limiting**: ```python # Urgent API call (high priority) result = call_api.apply_async( kwargs={"endpoint": "/urgent"}, queue="api-high-priority", priority=9 ) # Bulk API calls (low priority, spaced out) for i in range(100): call_api.apply_async( kwargs={"endpoint": f"/data/{i}"}, queue="api-low-priority", countdown=i * 10, # Space out by 10s priority=1 ) ``` **KEY TAKEAWAY**: Use `.delay()` for 90% of tasks (simple, clean). Use `.apply_async()` when you need: Delayed/scheduled execution, Priority control, Queue routing, Custom task IDs, Retry policies, Task callbacks. Both queue tasks identically - `.apply_async()` just has more options.',
    keyPoints: [
      '.delay() is syntactic sugar for .apply_async() with no options (90% of cases)',
      'Use .delay() when: Simple task queuing, no special requirements',
      'Use .apply_async() when: countdown/eta, priority, queue routing, retries, custom ID',
      '.apply_async() examples: Scheduled tasks (eta), delayed execution (countdown), priority queues',
      'Decision: If you need advanced options → .apply_async(), else → .delay() (cleaner)',
    ],
  },
  {
    id: 'q3',
    question:
      'Design an idempotent task for sending welcome emails. The task should be safe to call multiple times without sending duplicate emails, even if workers crash or tasks are retried.',
    sampleAnswer:
      'IDEMPOTENT WELCOME EMAIL TASK DESIGN: **WHAT IS IDEMPOTENCE?**: Operation is idempotent if calling it N times has the same result as calling it once. **Why needed**: Tasks can be retried (failures, timeouts), Workers can crash mid-task (task requeued), Same task queued multiple times (race conditions), Network issues cause re-execution. **NON-IDEMPOTENT** (❌ Bad - sends multiple emails): ```python @app.task def send_welcome_email_bad(user_id: int): """❌ NOT idempotent - can send multiple emails""" user = User.get(user_id) send_email(user.email, "Welcome!", "Thanks for joining!") # Problem: If task retried, user gets multiple welcome emails! # Problem: If queued twice, user gets 2 emails! # Problem: No check if already sent! ``` **IDEMPOTENT SOLUTION 1: Database Flag**: ```python from celery import Celery from sqlalchemy import Column, Integer, String, Boolean, DateTime from sqlalchemy.ext.declarative import declarative_base import datetime app = Celery("myapp", broker="redis://localhost:6379/0") Base = declarative_base() class User(Base): __tablename__ = "users" id = Column(Integer, primary_key=True) email = Column(String) welcome_email_sent = Column(Boolean, default=False) # ✅ Idempotency flag welcome_email_sent_at = Column(DateTime, nullable=True) @app.task def send_welcome_email_idempotent_v1(user_id: int): """✅ Idempotent: Check database flag""" from sqlalchemy.orm import sessionmaker Session = sessionmaker(bind=engine) session = Session() try: user = session.query(User).filter_by(id=user_id).first() # Check if already sent if user.welcome_email_sent: logger.info(f"Welcome email already sent to user {user_id}") return {"status": "already_sent", "sent_at": user.welcome_email_sent_at} # Send email send_email(user.email, "Welcome!", "Thanks for joining!") # Mark as sent (atomic!) user.welcome_email_sent = True user.welcome_email_sent_at = datetime.datetime.utcnow() session.commit() logger.info(f"Welcome email sent to user {user_id}") return {"status": "sent", "sent_at": user.welcome_email_sent_at} except Exception as exc: session.rollback() raise finally: session.close() ``` **Benefits**: ✅ Database-backed (persistent across crashes), ✅ Queryable (can see who got emails), ✅ Atomic (transaction ensures consistency), ✅ Works across multiple workers. **IDEMPOTENT SOLUTION 2: Separate Tracking Table**: ```python class EmailLog(Base): """Track all sent emails""" __tablename__ = "email_logs" id = Column(Integer, primary_key=True) user_id = Column(Integer, index=True) email_type = Column(String, index=True) # "welcome", "reset_password", etc. sent_at = Column(DateTime, default=datetime.datetime.utcnow) # Unique constraint: can\'t send same email type to same user twice __table_args__ = ( UniqueConstraint("user_id", "email_type", name="unique_user_email_type"), ) @app.task def send_welcome_email_idempotent_v2(user_id: int): """✅ Idempotent: Separate email log table""" from sqlalchemy.exc import IntegrityError Session = sessionmaker(bind=engine) session = Session() try: # Check if already sent existing = session.query(EmailLog).filter_by( user_id=user_id, email_type="welcome" ).first() if existing: logger.info(f"Welcome email already sent to user {user_id} at {existing.sent_at}") return {"status": "already_sent", "sent_at": existing.sent_at} # Get user user = session.query(User).filter_by(id=user_id).first() # Send email send_email(user.email, "Welcome!", "Thanks for joining!") # Log email (atomic!) email_log = EmailLog(user_id=user_id, email_type="welcome") session.add(email_log) session.commit() logger.info(f"Welcome email sent and logged for user {user_id}") return {"status": "sent", "sent_at": email_log.sent_at} except IntegrityError: # Race condition: another worker sent email at same time session.rollback() logger.warning(f"Race condition: welcome email already sent to user {user_id}") return {"status": "already_sent"} except Exception as exc: session.rollback() raise finally: session.close() ``` **Benefits**: ✅ Audit trail (all emails logged), ✅ Handles race conditions (unique constraint), ✅ Scalable (doesn\'t modify user table), ✅ Can track multiple email types. **IDEMPOTENT SOLUTION 3: Redis Cache**: ```python import redis redis_client = redis.Redis(host="localhost", port=6379, db=0) @app.task def send_welcome_email_idempotent_v3(user_id: int): """✅ Idempotent: Redis cache for fast check""" cache_key = f"email_sent:welcome:{user_id}" # Check if already sent (fast!) if redis_client.exists(cache_key): logger.info(f"Welcome email already sent to user {user_id}") return {"status": "already_sent"} # Get user user = User.get(user_id) # Send email send_email(user.email, "Welcome!", "Thanks for joining!") # Mark as sent in cache (with expiration) redis_client.setex( cache_key, 86400 * 365, # 1 year "sent" ) logger.info(f"Welcome email sent to user {user_id}") return {"status": "sent"} ``` **Benefits**: ✅ Very fast check (<1ms), ✅ Simple implementation, ✅ Automatic expiration. **Drawbacks**: ❌ Lost if Redis crashes (unless AOF), ❌ No audit trail. **IDEMPOTENT SOLUTION 4: Custom Task ID**: ```python @app.task def send_welcome_email_idempotent_v4(user_id: int): """✅ Idempotent: Custom task ID prevents duplicates""" user = User.get(user_id) send_email(user.email, "Welcome!", "Thanks for joining!") return {"status": "sent"} # Queue with custom ID (prevents duplicates) def queue_welcome_email(user_id: int): """Queue welcome email (idempotent at queue level)""" task_id = f"welcome-email-{user_id}" from celery.result import AsyncResult result = AsyncResult(task_id, app=app) # Check if already queued/processing/complete if result.state in ["PENDING", "STARTED", "SUCCESS"]: logger.info(f"Welcome email already queued/sent for user {user_id}") return result # Not queued yet, queue it result = send_welcome_email_idempotent_v4.apply_async( kwargs={"user_id": user_id}, task_id=task_id ) logger.info(f"Welcome email queued for user {user_id}") return result ``` **Benefits**: ✅ Prevents duplicate queue entries, ✅ Simple at task level. **Drawbacks**: ❌ Doesn\'t prevent duplicate sends (if task_id changes), ❌ State not persistent (Celery result backend expiration). **COMPREHENSIVE SOLUTION** (Best Practice): Combine multiple approaches: ```python from celery import Celery from typing import Dict, Any import logging logger = logging.getLogger(__name__) app = Celery("myapp", broker="redis://localhost:6379/0") @app.task(bind=True, max_retries=3) def send_welcome_email_production( self, user_id: int ) -> Dict[str, Any]: """ Production-grade idempotent welcome email task Features: - Database tracking (persistent) - Redis cache (fast check) - Retry logic - Comprehensive logging - Error handling """ session = Session() try: # Fast check: Redis cache cache_key = f"email_sent:welcome:{user_id}" if redis_client.exists(cache_key): logger.info(f"Welcome email already sent (cache hit) for user {user_id}") return {"status": "already_sent", "source": "cache"} # Persistent check: Database existing = session.query(EmailLog).filter_by( user_id=user_id, email_type="welcome" ).first() if existing: logger.info(f"Welcome email already sent (DB) for user {user_id} at {existing.sent_at}") # Repopulate cache redis_client.setex(cache_key, 86400 * 365, "sent") return {"status": "already_sent", "sent_at": existing.sent_at, "source": "database"} # Get user user = session.query(User).filter_by(id=user_id).first() if not user: raise ValueError(f"User {user_id} not found") # Send email (with retry on failure) try: send_email(user.email, "Welcome to MyApp!", "Thanks for joining us!") except SMTPException as exc: # Retry on SMTP errors logger.error(f"SMTP error sending welcome email to user {user_id}: {exc}") raise self.retry(exc=exc, countdown=60 * (2 ** self.request.retries)) # Log to database (atomic) email_log = EmailLog( user_id=user_id, email_type="welcome", sent_at=datetime.datetime.utcnow() ) session.add(email_log) session.commit() # Cache for fast future checks redis_client.setex(cache_key, 86400 * 365, "sent") logger.info(f"Welcome email sent successfully to user {user_id}") return { "status": "sent", "sent_at": email_log.sent_at, "email": user.email } except IntegrityError: # Race condition: another worker sent email session.rollback() logger.warning(f"Race condition detected for user {user_id}, email already sent") redis_client.setex(cache_key, 86400 * 365, "sent") return {"status": "already_sent", "source": "race_condition"} except Exception as exc: session.rollback() logger.exception(f"Failed to send welcome email to user {user_id}: {exc}") raise finally: session.close() # Queue with custom ID for extra safety def queue_welcome_email(user_id: int): """Queue welcome email with custom task ID""" task_id = f"welcome-email-{user_id}" return send_welcome_email_production.apply_async( kwargs={"user_id": user_id}, task_id=task_id ) ``` **This solution provides**: ✅ Fast check (Redis cache), ✅ Persistent tracking (database), ✅ Race condition handling (unique constraint), ✅ Retry logic (SMTP failures), ✅ Comprehensive logging, ✅ Custom task ID (prevent queue duplicates), ✅ Audit trail (EmailLog table). **TESTING IDEMPOTENCE**: ```python # Test 1: Call twice, should send once queue_welcome_email(user_id=123) queue_welcome_email(user_id=123) # Second call returns "already_sent" # Test 2: Simulate crash during task # Worker crashes after send but before database commit # Task requeued → Database check prevents duplicate send # Test 3: Concurrent calls (race condition) from threading import Thread Thread(target=lambda: queue_welcome_email(123)).start() Thread(target=lambda: queue_welcome_email(123)).start() # Only one email sent (database unique constraint) ``` **KEY TAKEAWAY**: Idempotent tasks safe to call multiple times. Achieve via: Database flags, Unique constraints (race conditions), Redis cache (fast checks), Custom task IDs (queue-level), Comprehensive logging. Production tasks should combine multiple techniques for robustness.',
    keyPoints: [
      'Idempotent = safe to call multiple times (same result as calling once)',
      'Database tracking: email_sent flag or separate EmailLog table with unique constraint',
      'Redis cache: fast check (milliseconds), but database is source of truth',
      'Custom task ID prevents duplicate queue entries (task_id=f"welcome-email-{user_id}")',
      'Production: Combine database (persistence) + Redis (speed) + unique constraints (races)',
    ],
  },
];
