export const featureImportanceInterpretationQuiz = {
  title: 'Feature Importance & Interpretation - Discussion Questions',
  questions: [
    {
      id: 1,
      question: `Compare and contrast three methods for feature importance: (1) built-in importance from tree models, (2) permutation importance, and (3) SHAP values. For each method, explain how it works, its advantages/disadvantages, and when you would prefer it. Include computational complexity considerations.`,
      expectedAnswer: `**1. Built-in Tree Importance**: **How**: Measures total reduction in impurity (Gini/entropy) from each feature's splits. **Pros**: Fast (computed during training), no extra computation, native to trees. **Cons**: Biased toward high-cardinality features, can't use for non-tree models, doesn't account for feature interactions, can be misleading with correlated features. **Use When**: Quick exploration with RF/GB, computational constraints, preliminary analysis. **Complexity**: O(1) after training. **2. Permutation Importance**: **How**: Shuffle each feature, measure performance drop. Large drop = important feature. **Pros**: Model-agnostic (works for any model), accounts for interactions, unbiased. **Cons**: Computationally expensive (N_features × N_permutations evaluations), can be unreliable with correlated features, requires validation set. **Use When**: Need model-agnostic importance, want to understand feature impact on specific metric, have compute budget. **Complexity**: O(n_features × n_samples × model_predict). **3. SHAP Values**: **How**: Game-theoretic approach, Shapley values show each feature's contribution to each prediction. **Pros**: Theoretically grounded, shows direction of effect, individual prediction explanations, handles interactions and correlations. **Cons**: Computationally very expensive (exponential in features without approximations), complex to explain to non-technical users. **Use When**: Need rigorous explanations, regulatory requirements, debugging individual predictions, explaining to stakeholders. **Complexity**: O(2^n_features) exact, O(n_features²) with approximations (TreeSHAP). **Recommendation**: Start with built-in (fast overview), use permutation (validation), use SHAP (deep analysis/production explanations).`,
      difficulty: 'advanced' as const,
      category: 'Methods',
    },
    {
      id: 2,
      question: `Your Random Forest model shows "account_age" as the most important feature for credit default prediction, but domain experts insist "debt_to_income_ratio" should be more important. How would you investigate this discrepancy? What could cause it, and how would you communicate your findings to both technical and non-technical stakeholders?`,
      expectedAnswer: `**Investigation Steps**: **1. Validate Importance**: 1) **Permutation importance**: Re-calculate with permutation (model-agnostic), 2) **SHAP analysis**: Get SHAP values to see directional effects, 3) **Partial dependence**: Plot how predictions change with each feature, 4) **Different metrics**: Check if importance consistent across precision, recall, F1. **2. Identify Causes**: **Possible Explanations**: 1) **Proxy Effect**: account_age might be correlated with debt_to_income (older accounts = established income), model using account_age as proxy, 2) **Data Quality**: debt_to_income might have missing values/errors, model relies on clean account_age, 3) **Non-linear Relationships**: account_age captures complex patterns debt_to_income doesn't, 4) **Feature Engineering**: account_age combined with other features creates powerful interactions, 5) **Domain Insight Missing**: Account_age captures stability/responsibility not in debt_to_income. **3. Correlation Analysis**: Check correlation between features - if highly correlated (>0.7), model may arbitrarily choose one. **4. Ablation Study**: Train two models: one with account_age only, one with debt_to_income only. Compare performance. **Communication to Stakeholders**: **Technical**: "SHAP analysis shows account_age contributes 0.15 to log-odds on average, while debt_to_income contributes 0.09. Permutation importance confirms: 8% performance drop vs 5%. However, features are correlated (r=0.6), suggesting they capture related information." **Non-Technical**: "Think of account_age as capturing financial maturity and stability over time - something debt_to_income snapshot misses. Both matter, but historical behavior (account_age) predicts better than current snapshot (debt_to_income). It\'s like judging someone's reliability by their 5-year track record vs today's finances."`,
      difficulty: 'advanced' as const,
      category: 'Analysis',
    },
    {
      id: 3,
      question: `Design a comprehensive model interpretation strategy for a production medical diagnosis system that must satisfy regulatory requirements (GDPR Article 22 - right to explanation). What methods would you implement, how would you present explanations to different audiences (doctors, patients, regulators), and what safety checks would you include?`,
      expectedAnswer: `**Regulatory Compliance Strategy**: **1. Multi-Layer Interpretation**: **Global**: Overall model behavior: Feature importance rankings (permutation), Partial dependence plots (feature effects), Decision rules extraction. **Local**: Individual predictions: SHAP force plots (feature contributions per case), LIME explanations (local linear approximations), Counterfactual explanations ("if lab value X was Y, diagnosis would change"). **2. Audience-Specific Presentations**: **For Doctors (Clinical Decision Support)**: Show: Top 3 contributing factors with confidence scores, Comparison to typical cases, Alert when model uncertain, Link to relevant medical literature. Format: "Diagnosis: Condition X (85% confidence). Key factors: Lab A (high), Symptom B (present), History C. Similar to 127 past cases, 89% confirmed." **For Patients (Transparency)**: Show: Simple language explanation, What factors influenced diagnosis, What they can control/discuss with doctor. Format: "Based on your test results and symptoms, system suggests further testing for X. Main factors: your recent lab results and family history. Please discuss with your doctor." **For Regulators (Compliance)**: Provide: Complete SHAP analysis, Model card with limitations, Validation on diverse populations, Bias/fairness metrics, Audit logs of predictions. **3. Safety Checks**: 1) **Sanity Checks**: Verify important features make medical sense, Flag if unexpected features dominate, 2) **Explanation Validation**: Test if explanations align with doctor expectations on 100 cases, 3) **Adversarial Testing**: Ensure explanations robust to small input changes, 4) **Human-in-Loop**: Always present as decision support, not final decision, Require doctor confirmation for high-stakes diagnoses. **4. Documentation**: Maintain explanation audit trail, Version control for interpretation methods, Regular review with medical board.`,
      difficulty: 'advanced' as const,
      category: 'Production',
    },
  ],
};
