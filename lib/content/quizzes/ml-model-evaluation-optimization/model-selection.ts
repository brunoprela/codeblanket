export const modelSelectionQuiz = {
  title: 'Model Selection - Discussion Questions',
  questions: [
    {
      id: 1,
      question: `You're comparing five models for a binary classification problem: Logistic Regression (Acc=0.82, Time=0.1s), Random Forest (Acc=0.87, Time=2s), Gradient Boosting (Acc=0.88, Time=10s), Neural Network (Acc=0.89, Time=15s), Ensemble (Acc=0.90, Time=30s). Your application requires <1 second response time and will serve 1M predictions/day. Which model would you deploy and why? What if interpretability was also required?`,
      expectedAnswer: `**Analysis**: **Hard Constraint**: <1s response time eliminates GB (10s), NN (15s), Ensemble (30s). **Remaining**: LogReg (0.82, 0.1s) vs RF (0.87, 2s). RF violates 1s constraint. **Answer**: **Deploy Logistic Regression** (only option meeting constraint). **Optimization Path**: 1) **Optimize RF**: Try feature selection, reduce trees, smaller depth → might get to 0.5s with 0.85 accuracy, 2) **Batch Predictions**: If real-time not strictly required, batch process with RF, 3) **Model Distillation**: Train small neural net to mimic RF (knowledge distillation), might get 0.86 accuracy in 0.3s, 4) **Hybrid**: Use LogReg for real-time, RF for batch/offline analysis. **With Interpretability Requirement**: **Definitely Logistic Regression**: 1) Linear model = coefficients directly interpretable, 2) Can explain each prediction, 3) Regulatory compliance (GDPR right to explanation), 4) Stakeholder trust. **Alternative**: Small decision tree (max_depth=5) if slightly better accuracy needed and still interpretable. **Trade-off**: Sacrificed 7-8% accuracy for 20x speedup and interpretability - often right choice for production. **Next Steps**: Invest in better features to improve LogReg accuracy while maintaining speed.`,
      difficulty: 'advanced' as const,
      category: 'Production',
    },
    {
      id: 2,
      question: `Explain the "No Free Lunch" theorem and its practical implications for model selection. Provide three different problem scenarios where different algorithms would be optimal, and explain why no single algorithm dominates across all problems.`,
      expectedAnswer: `**No Free Lunch Theorem**: Averaged over ALL possible problems, every optimization algorithm performs equally well. Mathematically proven that no algorithm is universally superior. **Practical Implications**: 1) Must try multiple algorithms - can't assume favorite will work, 2) Algorithm performance depends on problem structure, 3) Domain knowledge crucial for model selection, 4) Always benchmark against multiple baselines. **Scenario 1 - Linear Relationships**: **Problem**: Housing prices based on sqft, bedrooms, location (strong linear relationships). **Best Algorithm**: Linear Regression or Logistic Regression. **Why**: Simple, fast, interpretable, statistically efficient when relationships are truly linear. Complex models (RF, GB) waste capacity on non-existent non-linearity. **Scenario 2 - Non-linear Boundaries**: **Problem**: Image classification, complex decision boundaries. **Best Algorithm**: Neural Networks or Gradient Boosting. **Why**: Can learn arbitrary non-linear patterns, hierarchical features (NN), complex interactions (GB). Linear models completely fail here. **Scenario 3 - Small Tabular Data with Missing Values**: **Problem**: 1000 samples, 50 features, 20% missing values. **Best Algorithm**: XGBoost or Random Forest. **Why**: Handle missing values natively, work well with small data, capture interactions, resistant to overfitting with proper tuning. Neural networks would overfit badly. **Key Insight**: Algorithm success depends on match between algorithm assumptions and problem structure.`,
      difficulty: 'intermediate' as const,
      category: 'Theory',
    },
    {
      id: 3,
      question: `You've trained three models with similar validation performance (F1: 0.85, 0.86, 0.84) but you can only deploy one. Design a comprehensive decision framework that goes beyond just accuracy metrics. What additional factors would you consider and how would you weigh them? Include a scoring system or decision matrix.`,
      expectedAnswer: `**Comprehensive Decision Framework**: **1. Performance Dimensions**: 1) **Accuracy**: F1, precision, recall (weight: 30%), 2) **Consistency**: Std across CV folds (weight: 10%), 3) **Edge cases**: Performance on rare but important cases. **2. Operational Metrics**: 1) **Latency**: Prediction time (weight: 20%) - <100ms = 5pts, <1s = 3pts, >1s = 1pt, 2) **Throughput**: Predictions/second, 3) **Resource Usage**: Memory, CPU, GPU needs (weight: 10%), 4) **Cost**: Inference cost per 1M predictions. **3. Development/Maintenance**: 1) **Training Time** (weight: 5%): How often retrain? Daily vs monthly matters, 2) **Complexity**: Lines of code, dependencies (weight: 5%), 3) **Team Expertise**: Can team maintain it?, 4) **Debug-ability**: Easy to diagnose failures? **4. Business Alignment**: 1) **Interpretability** (weight: 15%): Required for regulatory? Stakeholder trust?, 2) **Robustness**: Performance under distribution shift (weight: 5%), 3) **Scalability**: Growth plan for next 2 years. **Scoring Matrix**: Model | Accuracy | Latency | Interp | Maintain | Total. A: 28 + 20 + 5 + 8 = 61. B: 30 + 15 + 15 + 10 = 70 ← Deploy B. **Additional Tests**: 1) **A/B Test**: Deploy to 5% traffic, compare real-world performance, 2) **Shadow Mode**: Run alongside current system, 3) **Stakeholder Review**: Present to product, eng, compliance teams.`,
      difficulty: 'advanced' as const,
      category: 'Decision Making',
    },
  ],
};
