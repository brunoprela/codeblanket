export const modelDebuggingQuiz = {
  title: 'Model Debugging - Discussion Questions',
  questions: [
    {
      id: 1,
      question: `Your model shows excellent training accuracy (98%) but poor test accuracy (65%). Walk through a systematic debugging process with at least 8 specific checks you would perform, explaining what each check reveals and how you would fix common issues discovered.`,
      expectedAnswer: `**Systematic Debugging Process**: **1. Data Leakage Check**: **What**: Are features using future information or target data? **How**: Review feature engineering, check for target leakage, verify preprocessing order. **Fix**: Remove leaky features, ensure proper time-based splits. **2. Preprocessing Leakage**: **What**: Was scaling/normalization fitted on all data including test? **How**: Check if StandardScaler fitted before split. **Fix**: Fit only on training, transform test. **3. Train-Test Distribution**: **What**: Are train and test from same distribution? **How**: Plot feature distributions, run statistical tests (KS test). **Fix**: Check data collection process, ensure representative sampling. **4. Label Quality**: **What**: Are training labels noisy or incorrect? **How**: Manually inspect confident predictions, check label distribution. **Fix**: Clean labels, use noise-robust methods. **5. Overfitting Diagnosis**: **What**: Model too complex for data? **How**: Plot learning curves, check model capacity vs data size. **Fix**: Reduce complexity, add regularization, get more data. **6. Feature Quality**: **What**: Are features informative? **How**: Check feature importance, correlation with target. **Fix**: Remove uninformative features, engineer better ones. **7. Evaluation Metric**: **What**: Is metric appropriate for problem? **How**: For imbalanced data, accuracy misleading - check precision/recall. **Fix**: Use appropriate metric. **8. Implementation Bugs**: **What**: Bugs in data pipeline or model? **How**: Verify shapes, check for NaN, test on toy example. **Fix**: Debug code, add assertions. **9. Cross-Validation**: **What**: Is this consistent or one-time fluke? **How**: Run 5-fold CV, check variance. **Fix**: If high variance, need more data or simpler model.`,
      difficulty: 'advanced' as const,
      category: 'Debugging',
    },
    {
      id: 2,
      question: `A trading model performs well in backtests (Sharpe=2.0) but loses money in live trading. This is a common and critical problem. What are the most likely causes of backtest-to-live degradation in trading systems, and how would you prevent each issue during development?`,
      expectedAnswer: `**Critical Issues in Trading Models**: **1. Lookahead Bias**: **Problem**: Using future information in features (e.g., using day-close price for day-open trades). **Prevention**: Strict temporal ordering, verify all features use only past data, add delays to simulate real-world latency. **2. Survivorship Bias**: **Problem**: Backtesting only on stocks still trading (excludes bankruptcies). **Prevention**: Use complete historical universe including delisted stocks, point-in-time data. **3. Transaction Costs**: **Problem**: Ignoring commissions, slippage, market impact. **Prevention**: Model realistic costs (0.1% round-trip + 0.1% slippage), reduce trading frequency, account for bid-ask spread. **4. Overfitting to Historical Data**: **Problem**: Strategy works on specific historical period, doesn't generalize. **Prevention**: Walk-forward validation, test across multiple market regimes (2008 crash, 2020 COVID, etc.), require 5+ years of data. **5. Execution Assumptions**: **Problem**: Assuming fills at exact prices in backtest. **Prevention**: Simulate order book, model limit order fills, account for partial fills, add latency delays. **6. Market Impact**: **Problem**: Large orders move market price. **Prevention**: Position size limits, volume-based constraints, model slippage as function of order size. **7. Data Quality**: **Problem**: Survivorship-free database has errors, corporate actions not adjusted. **Prevention**: Validate data against multiple sources, check for unrealistic returns. **8. Regime Change**: **Problem**: Market dynamics changed. **Prevention**: Recent data weighted higher, monitor live performance vs backtest, have kill switch. **9. Implementation Errors**: **Problem**: Backtest code ≠ live code. **Prevention**: Use same codebase, extensive testing, paper trading before live. **Best Practice**: Start with paper trading, scale up gradually, assume backtest Sharpe will halve in live trading.`,
      difficulty: 'advanced' as const,
      category: 'Trading',
    },
    {
      id: 3,
      question: `Design a comprehensive monitoring and debugging dashboard for a production ML model. What metrics would you track in real-time, what alerts would you set up, and how would you detect the three main failure modes: (1) data drift, (2) model drift, and (3) implementation bugs?`,
      expectedAnswer: `**Production ML Monitoring Dashboard**: **Real-Time Metrics**: **1. Performance Metrics**: Predictions/second, p50/p95/p99 latency, Error rate, Cache hit rate. **Alerts**: Latency >100ms, errors >1%, throughput drops >20%. **2. Model Metrics**: Prediction distribution (histogram), Confidence scores (mean, variance), Feature statistics (mean, std per feature), Output class balance (for classification). **Alerts**: Prediction distribution shift >2σ, confidence drops >10%, feature out of expected range. **3. Business Metrics**: Conversion rate, Revenue impact, User engagement, A/B test comparison. **Alerts**: Business metric drops >5%, statistically significant degradation. **Detecting Failure Modes**: **1. Data Drift Detection**: **Method**: Track input feature distributions over time, **Technical**: Kolmogorov-Smirnov test weekly, Population Stability Index (PSI), **Visualization**: Feature distribution plots (train vs current week), **Alert**: PSI >0.25 or KS test p<0.05, **Action**: Retrain model or investigate cause of drift. **2. Model Drift Detection**: **Method**: Track model performance on labeled data, **Technical**: Compare predictions to ground truth (when available), shadow predictions vs current production, precision/recall monitoring. **Visualization**: Performance metrics over time, confusion matrix weekly, **Alert**: F1 score drops >5%, precision/recall imbalance, **Action**: Retrain with recent data, investigate feature importance changes. **3. Implementation Bugs**: **Method**: Sanity checks and invariant monitoring, **Checks**: Predictions in valid range, no NaN outputs, feature preprocessing consistent, model version tracking, **Visualization**: Exception rates, prediction anomalies, **Alert**: Any NaN, predictions out of bounds, unexpected exceptions, **Action**: Rollback to previous version, investigate logs. **Dashboard Layout**: **Page 1**: Health at-a-glance (traffic light: green/yellow/red), **Page 2**: Performance deep-dive (latency, throughput), **Page 3**: Model quality (drift, performance), **Page 4**: Business impact (conversions, revenue).`,
      difficulty: 'advanced' as const,
      category: 'Production',
    },
  ],
};
