export const finalProjectForecastingSystemDiscussionQuestions = [
    {
        id: 1,
        question:
            "Your forecasting system runs in production serving 100 traders. Over 3 months, you observe: ARIMA consistently outperforms in stable periods, GARCH excels during volatile periods, ML models perform poorly overall. The ensemble averages all models equally. Design an intelligent ensemble weighting system that: (1) Detects current market regime (stable/volatile/trending), (2) Dynamically adjusts model weights based on regime and recent performance, (3) Handles model failures gracefully, (4) Provides explainability to traders, and (5) Monitors for degradation requiring human intervention.",
        answer: `[Comprehensive framework: (1) Regime detection via rolling volatility (VIX analog) + trend strength (ADX): Stable = low vol + no trend, Volatile = high vol, Trending = low vol + strong trend; (2) Weight optimization: per-regime historical performance window (30 days), exponential decay on errors, constrain weights [0.1, 0.6] per model for diversification; (3) Failure handling: if model forecast NaN or >3σ from others → set weight to 0, alert ops team, graceful degradation to remaining models; (4) Explainability: dashboard shows "Current regime: Volatile → GARCH weight 60%, ARIMA 25%, ML 15%", historical regime-conditional performance charts; (5) Monitoring: if ensemble RMSE > 1.5× historical avg for 5 consecutive days → trigger retraining, if all models fail → emergency fallback to historical mean + alert. Implementation uses state machine for regimes, Bayesian updating for weights, comprehensive logging for post-mortem analysis.]`,
    },
    {
        id: 2,
        question:
            "Design a comprehensive backtesting framework for the forecasting system that avoids common pitfalls: look-ahead bias, survivorship bias, data snooping, and overfitting. The framework must support: walk-forward validation across multiple assets and time periods, realistic transaction costs and slippage, regime-aware evaluation, and statistical significance testing. How would you structure the code, what metrics would you track, and how would you ensure the backtest results are reliable predictors of live performance?",
        answer: `[Framework structure: (1) Data management: Point-in-time database ensuring no look-ahead (store data with 'as_of_date'), survivorship-free universe (include delisted stocks), (2) Walk-forward engine: expanding or rolling window with retraining frequency parameter, strict temporal ordering, out-of-sample forecasting only, (3) Cost model: bid-ask spread (0.01-0.05%), market impact (f(order_size, ADV)), financing costs for shorts (borrow rate), (4) Regime labeling: ex-post regime classification, evaluate performance separately by regime, (5) Metrics: RMSE/MAE for accuracy, direction accuracy for trading signal, Sharpe/Sortino for risk-adjusted returns, max drawdown, hit rate, profit factor; (6) Statistical tests: Diebold-Mariano for forecast comparison, bootstrap confidence intervals (1000 iterations), multiple testing correction (Bonferroni) if testing many models; (7) Validation: out-of-sample test set never touched until final evaluation, cross-asset validation (if works on stocks, test on FX/commodities), crisis period analysis (2008, 2020); (8) Documentation: version control all code, log all parameters, track data provenance, maintain audit trail. Key principle: conservative assumptions, multiple validation approaches, transparency in methodology.]`,
    },
    {
        id: 3,
        question:
            "After deploying the forecasting system, you discover that model performance degrades significantly during the first hour after market open (9:30-10:30 AM) but is strong rest of day. Investigation shows: high forecast errors, increased volatility, larger bid-ask spreads. Diagnose the root causes and propose solutions addressing: data quality issues, model specifications, timing of retraining, and whether to disable forecasts during this period. How would you test your hypothesis and implement the solution in production?",
        answer: `[Root cause analysis: (1) Opening volatility: News overnight, earnings releases, economic data → structural break from close-to-open, models trained on intraday patterns fail; (2) Liquidity: Lower liquidity at open → wider spreads, more noise, microstructure effects dominate; (3) Data staleness: Models retrained EOD yesterday, 16+ hours stale by market open, overnight information not incorporated; (4) Model assumption violations: Volatility clustering parameters estimated on full-day data don't apply to open; Solutions: (A) Separate open-period model: Train GARCH on first-hour data specifically, use higher volatility prior, incorporate overnight returns/gap as feature; (B) Real-time model updating: Retrain incrementally at 9:45, 10:00, 10:15 as new data arrives, Kalman filter approach for parameter adaptation; (C) Confidence-adjusted forecasting: Wider prediction intervals during first hour, reduce position sizes or require higher signal threshold, explicitly model time-of-day volatility pattern; (D) Alternative data integration: Pre-market futures, European market close, news sentiment to bridge overnight gap; Testing hypothesis: (1) Separate backtest for 9:30-10:30 vs 10:30-16:00 periods, (2) Compare model residuals by time of day, (3) A/B test in paper trading: old system vs new approach; Implementation: (1) Time-aware model selection in production, (2) Dynamic confidence intervals, (3) Hourly model performance dashboard with alerts, (4) Document as known limitation with mitigation strategy. Decision: DON'T disable entirely (still predictive power), instead adapt models and position sizing for this regime.]`,
    },
];

