export const conversationalTradingAssistantsQuiz = {
  title: 'Conversational Trading Assistants Discussion',
  id: 'conversational-trading-assistants-quiz',
  sectionId: 'conversational-trading-assistants',
  questions: [
    {
      id: 1,
      question:
        'What safety mechanisms are essential for voice-activated trading systems to prevent catastrophic errors from misheard commands or unintended activations? Discuss multi-layer verification approaches and how to balance safety with usability.',
      expectedAnswer: `Should discuss: voice trading creates unique risks—misheard "sell" vs "sell all," background conversation triggering unintended trades, unclear or ambiguous instructions, and inability to recover from executed trades. Essential safety mechanisms: explicit confirmation required before execution ("I heard sell 100 shares AAPL at market, say yes to confirm"), numerical and ticker repetition to catch errors, maximum position size limits that require additional authentication for larger trades, blackout periods during high-volatility when voice trading disabled, audio quality checks that reject unclear commands, context-aware validation (if you don\'t own stock, can\'t sell it), and "undo" window of 5-10 seconds post-confirmation where trade can be canceled. Multi-layer approach: voice recognition confidence thresholds, semantic understanding validation (does parsed intent match what was said), portfolio/risk validation (can account support this trade), confirmation loop back to user, and execution-level checks. Balance safety vs usability: too much friction defeats purpose of voice convenience, but too little risks catastrophic losses. Tiered approach: small routine trades (buy 10 shares) with streamlined confirmation, large or unusual trades require multi-factor verification, and first-time actions (trading new symbol) require enhanced confirmation. Additional: voice biometrics for speaker verification, time/location restrictions, and transaction logging with audio retention for dispute resolution.`,
    },
    {
      id: 2,
      question:
        'How should conversational trading assistants handle emotionally-charged situations where users might make poor decisions during market volatility or portfolio losses? What is the appropriate role for the AI in these situations?',
      expectedAnswer: `Should cover: trading assistants will encounter users experiencing fear (market crash), greed (FOMO), anger (losses), or panic (volatility), situations where emotional decisions destroy long-term wealth. AI challenges: preventing harm while respecting user autonomy, providing appropriate guidance without overstepping, and detecting emotional state from text/voice. Appropriate roles: acknowledge emotional state ("I understand the losses are concerning"), provide factual context (market history, typical volatility ranges), remind of long-term plan and goals, introduce friction for potentially harmful emotional trades ("given unusual volatility, require 1-hour cooling off before executing"), offer alternative actions (review plan, talk to human advisor, set alert to reconsider tomorrow), and escalate to human advisors for severe situations. Should NOT: make judgments about user\'s emotional state in condescending way, refuse to execute legal instructions (user\'s money, their decisions), or generate false comfort ("everything will be fine"). Specific approaches: when detecting emotional indicators (urgent language, all-caps, revenge trading patterns, instructions contradicting long-term goals), implement graduated responses—first provide information and pause, if user confirms intent after information, add time delay, if user still insists after delay, execute but document emotional state. For extreme situations: "I\'m concerned about this decision, connecting you with an advisor" (require human conversation for very large emotional trades). Balance: user autonomy vs duty of care, regulation may require intervening in some cases, some users will resent AI questioning them while others appreciate protection from themselves. Track outcomes: did interventions prevent poor decisions or just annoy users? Build trust: users who experience helpful intervention in volatile periods may trust system more.`,
    },
    {
      id: 3,
      question:
        'What are the regulatory and compliance implications of automated trading assistants executing trades based on conversational commands? How should systems maintain audit trails, ensure suitability, and handle disputes about what was said or meant?',
      expectedAnswer: `Should analyze: regulators require knowing why trades executed, ensuring suitability, preventing market manipulation, and protecting customers from AI errors. Compliance challenges: proving user authorized trade if they claim they didn\'t say it, demonstrating suitability analysis occurred, showing AI didn\'t generate unsuitable recommendations, and maintaining audit trail linking conversation to execution. Required systems: complete audio/text logging of all interactions with timestamps, transcription of voice commands with confidence scores, record of parsed intent and validation results, screenshot or data log of portfolio state at trade time, IP address and device information, and biometric verification results. Suitability: AI must verify trade consistent with user\'s risk profile, investment objectives, and financial situation before executing—can\'t enable unsuitable trades just because user requests them. For complex trades or new strategies, may require enhanced suitability documentation. Dispute handling: customer claims "I never said sell everything, I said sell some"—audit trail must clearly show what was said, what AI understood, what confirmation provided, and what user confirmed. Ambiguity resolution: when command unclear, AI should ask clarifying questions and log them (demonstrated attempt to understand intent). Regulatory examination: systems must be able to demonstrate controls preventing unauthorized trading, erroneous executions, market manipulation, and unsuitable recommendations. Additional: regular review of AI decisions by compliance, testing with adversarial prompts (could user trick AI into unsuitable trade?), and clear terms of service establishing user responsibility for voice commands. Consider: some regulators may require human review before execution, others may accept AI with robust controls, vary by jurisdiction and trade type. Documentation proving system prevents rather than enables problems is essential.`,
    },
  ],
};
