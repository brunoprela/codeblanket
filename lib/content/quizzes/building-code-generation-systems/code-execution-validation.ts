/**
 * Quiz questions for Code Execution & Validation section
 */

export const codeexecutionvalidationQuiz = [
  {
    id: 'bcgs-codeexec-q-1',
    question:
      'Design a complete sandboxing strategy for executing untrusted generated code. What resources must be limited, what capabilities must be blocked, and how would you detect/prevent sandbox escapes?',
    hint: 'Consider CPU, memory, I/O, network, file system, and system calls.',
    sampleAnswer:
      '**Complete Sandboxing Strategy:** **1) Resource Limits** - **CPU**: Limit to percentage of one core (e.g., 25%), Implement timeout (kill after N seconds), Prevent infinite loops. **Memory**: Hard limit (e.g., 256MB), Monitor allocation rate, Kill if exceeds limit. **Disk I/O**: Read-only file system OR restricted directory, Limit disk space if writes allowed. **Network**: Disabled by default (network_mode=none), If needed, whitelist specific IPs/ports only. **2) Capability Restrictions (Docker)** - ```python\\ndocker.containers.run(\\n    image="python:3.11-slim",\\n    command="python /sandbox/script.py",\\n    mem_limit="256m",\\n    cpu_quota=25000,  # 25% CPU\\n    network_disabled=True,\\n    security_opt=["no-new-privileges",],\\n    cap_drop=["ALL",],  # Drop all capabilities\\n    read_only=True,  # Read-only filesystem\\n    tmpfs={"/tmp": "size=50m"},  # Temp space only\\n)``` **3) Blocked Capabilities** - System calls: fork/exec (no child processes), mknod (no device creation), mount (no filesystem changes). **Dangerous Python Features**: eval/exec/compile blocked or monitored, __import__ restricted, File access limited to sandbox dir, socket module unavailable (no network). **4) Prevent Sandbox Escapes** - **Container Escape Prevention**: Use unprivileged containers (no root), Drop all capabilities, Use AppArmor/SELinux profiles, Keep Docker daemon updated. **Code-Level Escapes**: Parse code AST before execution: ```python\\ndef check_for_escapes (code):\\n    tree = ast.parse (code)\\n    dangerous = ["eval", "exec", "__import__", "compile",]\\n    \\n    for node in ast.walk (tree):\\n        if isinstance (node, ast.Call):\\n            if isinstance (node.func, ast.Name):\\n                if node.func.id in dangerous:\\n                    raise SecurityError (f"Dangerous function: {node.func.id}")``` **Monitor execution**: Watch for: Excessive syscalls, Attempts to access /proc, /sys, Attempts to bind ports, Suspicious error patterns. **5) Defense in Depth** - Layer 1: AST analysis (prevent dangerous code), Layer 2: Container isolation (limit blast radius), Layer 3: Resource limits (prevent DoS), Layer 4: Monitoring (detect escape attempts), Layer 5: Kill switch (terminate if suspicious). **Example Attack Prevention:** Attack: Use /proc to read host info → Blocked by read-only FS + dropped capabilities. Attack: Fork bomb → Blocked by CPU limit + no fork capability. Attack: Network exfiltration → Blocked by network_disabled.',
    keyPoints: [
      'Limit CPU, memory, disk, network resources strictly',
      'Drop all container capabilities, use unprivileged containers',
      'Block dangerous functions (eval, exec, __import__) in code',
      'Defense in depth: AST check + container + limits + monitoring',
    ],
  },
  {
    id: 'bcgs-codeexec-q-2',
    question:
      'You need to validate generated code by running tests, but the tests themselves might be buggy or infinite. How would you design a robust test execution system that handles test failures, timeouts, and resource exhaustion?',
    hint: 'Consider test isolation, timeout strategies, failure classification, and reporting.',
    sampleAnswer:
      '**Robust Test Execution System:** **1) Test Isolation** - Run each test in separate process/container: ```python\\nclass TestExecutor:\\n    def run_test_isolated (self, test_code, timeout=5):\\n        # Separate process per test\\n        with multiprocessing.Pool(1) as pool:\\n            try:\\n                result = pool.apply_async(\\n                    execute_test,\\n                    args=(test_code,)\\n                )\\n                return result.get (timeout=timeout)\\n            except multiprocessing.TimeoutError:\\n                pool.terminate()\\n                return TestResult (status="timeout")\\n            except Exception as e:\\n                return TestResult (status="error", error=str (e))``` Benefits: One hanging test doesn\'t block others, Memory leaks contained, Can kill cleanly. **2) Tiered Timeout Strategy** - **Per-Test Timeout**: Individual test max time (e.g., 5s), Prevents infinite loops in one test. **Suite Timeout**: All tests max time (e.g., 30s), Prevents death by 1000 cuts (many slow tests). **Progressive Timeout**: ```python\\ndef run_tests_with_progressive_timeout (tests):\\n    results = []\\n    time_budget = 30  # Total budget\\n    per_test_timeout = time_budget / len (tests)\\n    \\n    for i, test in enumerate (tests):\\n        remaining = time_budget - sum (r.time for r in results)\\n        timeout = min (per_test_timeout, remaining)\\n        \\n        result = run_test (test, timeout=timeout)\\n        results.append (result)\\n        \\n        if result.time > timeout:\\n            break  # Out of time``` **3) Failure Classification** - Classify WHY test failed: ```python\\nclass TestFailureType:\\n    SYNTAX_ERROR = "syntax"  # Test code itself is broken\\n    TIMEOUT = "timeout"  # Took too long\\n    ASSERTION_FAILED = "assertion"  # Actual test failure\\n    IMPORT_ERROR = "import"  # Missing dependencies\\n    RUNTIME_ERROR = "runtime"  # Unexpected exception\\n    RESOURCE_EXCEEDED = "resource"  # Memory/CPU limit\\n\\ndef classify_failure (exception, runtime_info):\\n    if isinstance (exception, SyntaxError):\\n        return TestFailureType.SYNTAX_ERROR\\n    elif runtime_info.time > timeout:\\n        return TestFailureType.TIMEOUT\\n    elif isinstance (exception, AssertionError):\\n        return TestFailureType.ASSERTION_FAILED\\n    # ...``` **4) Resource Monitoring** - Track resources per test: ```python\\nclass ResourceMonitor:\\n    def monitor_test (self, test_func):\\n        start_mem = get_memory_usage()\\n        start_time = time.time()\\n        \\n        try: result = test_func() \\n        finally: \\n            end_time = time.time() \\n            end_mem = get_memory_usage() \\n            \\n        return TestResult(\\n            result = result, \\n            time = end_time - start_time, \\n            memory = end_mem - start_mem\\n)``` Kill test if: Memory > 512MB, CPU > 80% for > 3s, Disk writes > 100MB. **5) Graceful Degradation** - If tests timeout/fail: Still validate syntax, Still check imports, Still run static analysis, Report what WAS validated. **6) Smart Test Ordering** - Run fast tests first: Syntax tests (instant), Unit tests (fast), Integration tests (slower). **Example:** Test suite has 10 tests, budget 30s. Test 1: 2s ✓, Test 2: 5s ✓, Test 3: ∞ (killed at 8s timeout), Tests 4-10: Skipped (out of time), Report: 2/3 passed, 1 timeout, 7 skipped due to timeout.',
    keyPoints: [
      'Isolate each test in separate process/container',
      'Use tiered timeouts: per-test, suite-wide, progressive',
      'Classify failures: syntax, timeout, assertion, resource',
      'Monitor resources and kill if exceeded',
    ],
  },
  {
    id: 'bcgs-codeexec-q-3',
    question:
      'Explain how to measure and validate the performance of generated code. What metrics matter, how would you detect performance regressions, and when should you reject code for being too slow?',
    hint: 'Consider algorithmic complexity, resource usage, real-world impact, and baseline comparisons.',
    sampleAnswer:
      '**Performance Validation System:** **1) Key Metrics to Measure** - **Execution Time**: Average, Min, Max, P95, P99 (percentiles important!), **Algorithmic Complexity**: Detected O(n), O(n²), O(2^n) patterns, **Memory Usage**: Peak memory, Memory allocation rate, Memory leaks (growing over iterations), **I/O Operations**: File reads/writes, Database queries, Network calls. **2) Complexity Detection** - Analyze code structure: ```python\\ndef detect_complexity (code): \\n    tree = ast.parse (code) \\n    \\n    nested_loops = 0\\n    for node in ast.walk (tree): \\n        if isinstance (node, (ast.For, ast.While)): \\n            # Check if loop contains another loop\\n            for child in ast.walk (node): \\n                if child != node and isinstance (child, (ast.For, ast.While)): \\n                    nested_loops += 1\\n    \\n    if nested_loops >= 2: \\n        return "O(n^2) or worse"  # Red flag``` **3) Benchmark Against Baseline** - Compare to: Previous version (detect regression), Acceptable threshold, Alternative implementations. ```python\\nclass PerformanceTester: \\n    def benchmark (self, code, test_inputs, baseline_code = None): \\n        # Run new code\\n        new_times = []\\n        for input in test_inputs: \\n            time = measure_execution_time (code, input) \\n            new_times.append (time) \\n        \\n        new_avg = statistics.mean (new_times) \\n        \\n        # Compare to baseline\\n        if baseline_code: \\n            baseline_times = [measure_execution_time (baseline_code, inp) for inp in test_inputs]\\n            baseline_avg = statistics.mean (baseline_times) \\n            \\n            regression = (new_avg - baseline_avg) / baseline_avg\\n            \\n            if regression > 0.5:  # 50 % slower\\n                return PerformanceResult(\\n                    status = "rejected", \\n                    reason = f"50% slower than baseline ({new_avg:.3f}s vs {baseline_avg:.3f}s)"\\n)``` **4) Scaling Tests** - Test with increasing input sizes: ```python\\ndef test_scaling (code): \\n    inputs = [10, 100, 1000, 10000]\\n    times = []\\n    \\n    for size in inputs: \\n        test_data = generate_input (size) \\n        time = measure_execution_time (code, test_data) \\n        times.append (time) \\n    \\n    # Analyze growth rate\\n    # If time quadruples when size doubles → O(n²) \\n    ratios = [times[i + 1] / times[i] for i in range (len (times) - 1)]\\n    \\n    if all (r > 3.5 for r in ratios):  # Growing faster than O(n log n) \\n        return "WARNING: Possibly O(n²) or worse"``` **5) Rejection Criteria** - **Absolute**: >1s for simple operations, >10s for batch operations. **Relative**: >2x slower than baseline, >50% slower than alternative implementation. **Complexity**: O(n²) when O(n) solution exists, O(2^n) (exponential - almost always reject). **Resource**: >1GB memory for small dataset, Memory grows unbounded. **6) Context-Aware Decisions** - Consider: Is this prototype or production?, Is performance critical for this function?, What\'s the expected input size? **Example Decision:** ```\\nFunction: sort_items\\nGenerated: O(n²) bubble sort, 2.5s for 10K items\\nBaseline: O(n log n) quicksort, 0.02s for 10K items\\n\\nDecision: REJECT - 125x slower, worse complexity\\n\\nFunction: validate_email_format  \\nGenerated: Complex regex, 0.5ms\\nBaseline: Simple regex, 0.1ms\\n\\nDecision: ACCEPT - 5x slower but still < 1ms, acceptable\\n```',
    keyPoints: [
      'Measure: execution time (percentiles), memory, complexity',
      'Detect complexity from code structure (nested loops = O(n²))',
      'Benchmark against baseline, reject if >2x slower',
      'Test scaling with increasing input sizes',
    ],
  },
];
