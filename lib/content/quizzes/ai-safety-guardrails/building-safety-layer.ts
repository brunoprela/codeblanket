/**
 * Quiz questions for Building a Safety Layer section
 */

export const buildingsafetylayerQuiz = [
  {
    id: 'build-safety-q-1',
    question:
      'Design a complete production-ready safety layer for an LLM application. Your system must handle 10,000 requests/minute with <100ms added latency while catching: prompt injection, PII, harmful content, hallucinations. How do you architect this system for speed, reliability, and comprehensive safety?',
    hint: 'Consider parallel checks, caching, fallbacks, and monitoring.',
    sampleAnswer:
      '**Requirements:** Throughput: 10,000 req/min = 167 req/sec. Latency: <100ms added. Coverage: Injection, PII, harmful content, hallucinations. **Architecture: Multi-Stage Pipeline** **Stage 1: Input Validation (Pre-Processing)** - Runs before LLM. Checks: (1) Prompt injection detection (20ms). (2) PII detection (30ms). (3) Content moderation (40ms, OpenAI API). Run in parallel: Total: max(20, 30, 40) = 40ms ✓. Implementation: async def validate_input(prompt): results = await asyncio.gather(detect_injection(prompt), detect_pii(prompt), moderate_content(prompt)). if any(result.blocked for result in results): raise SafetyViolation(results). return prompt. **Stage 2: LLM Generation** - If input passed: Generate response. **Stage 3: Output Validation (Post-Processing)** - Runs after LLM. Checks: (1) Output schema validation (5ms). (2) PII in output (30ms). (3) Hallucination detection (15ms, confidence-based). (4) Quality checks (10ms). Run in parallel: Total: max(5, 30, 15, 10) = 30ms ✓. **Stage 4: Rate Limiting (Parallel)** - Runs alongside validation: Check user rate limits (2ms, Redis). Check reputation (2ms, Redis). Total: ~2ms ✓. **Total Latency:** Input: 40ms. Output: 30ms. Rate limit: 2ms (parallel). Total: 40 + 30 = 70ms ✓ (under 100ms). **Optimization: Caching** - Cache validation results: cache_key = hash(prompt). if cache_key in validation_cache: return cached_result  # 2ms. Cache hit rate: 60%. Effective latency: 0.6 × 2ms + 0.4 × 70ms = 29ms. **Reliability: Fallbacks** - If validation service fails: Use cached results (stale ok). Degrade gracefully: Skip non-critical checks. Alert on-call team. Never block user without validation (fail-open for non-critical). Always block for critical (fail-closed): PII, harmful content. **Scalability:** Horizontal scaling: Run validation services in Kubernetes. Auto-scale based on load: If latency > 80ms: Add more pods. Load balancing: Distribute across multiple instances. **Monitoring:** Track: P50, P95, P99 latency. Blocked requests per type. False positive rate. Service health. Alerts: Latency > 100ms for 5 minutes. Error rate > 1%. Service down. **Code Structure:** class SafetyLayer: def __init__(self): self.input_validator = InputValidator(). self.output_validator = OutputValidator(). self.rate_limiter = RateLimiter(). async def validate_and_generate(self, prompt, user): # Stage 1, await self.input_validator.validate(prompt). # Rate limiting, await self.rate_limiter.check(user). # Stage 2, response = await self.llm.generate(prompt). # Stage 3, await self.output_validator.validate(response). return response. **Result:** Latency: 29ms (with cache) ✓. Throughput: Scales horizontally. Safety: Comprehensive coverage.',
    keyPoints: [
      'Multi-stage pipeline: input validation → generation → output validation',
      'Parallel execution within stages to minimize latency',
      'Caching for frequently seen inputs',
      'Fallbacks and graceful degradation for reliability',
    ],
  },
  {
    id: 'build-safety-q-2',
    question:
      'Your safety layer blocks 5% of requests. Analysis shows: 3% true positives (actual violations), 2% false positives (incorrectly blocked). Design a human review system to handle these cases. How do you prioritize review, minimize user friction, and improve the safety layer over time?',
    hint: 'Consider async review, feedback loops, and continuous improvement.',
    sampleAnswer:
      '**Current State:** Blocked requests: 5% of total. True positives (correctly blocked): 3%. False positives (incorrectly blocked): 2%. **Goal:** Reduce false positives: 2% → 0.5%. Maintain true positive rate: 3% (catch all violations). **Immediate User Experience:** Don\'t just show "Blocked". Provide: Reason: "This request was flagged for safety review." Option: "Request human review". Estimated time: "Review within 2 hours." Alternative: "Try rephrasing your request." **Human Review Queue:** Priority levels: P0 (Critical): Paid users, high-value accounts. Review within 15 minutes. P1 (High): Verified users. Review within 2 hours. P2 (Normal): Free users. Review within 24 hours. Auto-review: Low confidence blocks (score 0.5-0.7): Auto-review with ML. High confidence blocks (score > 0.7): Human review. **Review Interface:** Show reviewer: Original prompt. Block reason (which check failed). Safety score (confidence). User history. Decision options: Approve (false positive). Block (true positive). Escalate (uncertain). **Feedback Loop:** For false positives: Add to training data: prompt, label=safe. Update filters: If pattern detected, adjust threshold. For true positives: Reinforce: Pattern is correctly blocked. Track: Which attacks are most common. **Metrics to Track:** False positive rate: 2% → 0.5% (target). Time to review: P0 within 15 min, P1 within 2 hours. Reviewer agreement: Inter-reviewer agreement > 90%. User satisfaction: Post-review survey. **Automated Improvements:** Weekly analysis: Top 10 false positive patterns. Retrain models. Monthly review: Safety layer accuracy. Update thresholds. Quarterly: Red team testing. Security audit. **Implementation:** class ReviewSystem: async def request_review(self, prompt, block_reason, user): priority = self.get_priority(user). review = Review(prompt=prompt, reason=block_reason, priority=priority, created_at=now()). await self.review_queue.add(review). # Notify user, await self.notify_user(user, "Under review, estimated: {estimate}"). # Async review, result = await self.wait_for_review(review.id). return result. def get_priority(self, user): if user.is_paid: return "P0". elif user.is_verified: return "P1". else: return "P2". **Cost Optimization:** Not all blocks need review: Clear violations (e.g., PII detected): Auto-block, no review. Borderline cases: Review. Users can opt-in to review: "Was this blocked incorrectly? Request review." **Result Over Time:** False positives: 2% → 1% (month 1) → 0.5% (month 3). User satisfaction: 60% → 85%. Safety maintained: 3% true positives caught.',
    keyPoints: [
      'Priority-based review queue (paid users first)',
      'Provide feedback to users, not just "Blocked"',
      'Feedback loop: False positives improve safety layer',
      'Track metrics: false positive rate, review time',
    ],
  },
  {
    id: 'build-safety-q-3',
    question:
      'A new attack vector bypasses your safety layer. You discover it from user reports 3 days after deployment. Design an incident response process and continuous monitoring system to detect novel attacks faster (target: <1 hour). How do you respond to active incidents and prevent future bypasses?',
    hint: 'Consider anomaly detection, alerts, rapid deployment, and post-incident analysis.',
    sampleAnswer:
      '**Current Problem:** Attack detected: 3 days after start. Damage: 3 days of unsafe responses. Target: Detection within 1 hour. **Continuous Monitoring:** **Metric 1: Block Rate Anomalies** - Track: Block rate per hour (baseline: 5%). Alert if: Sudden drop: 5% → 1% (attacks bypassing filters). Sudden spike: 5% → 15% (false positives or new attack pattern). def check_block_rate_anomaly(): current_rate = get_block_rate(last_hour). baseline = get_baseline_block_rate(). if abs(current_rate - baseline) > 2 * stddev: ALERT("Block rate anomaly detected"). **Metric 2: Output Quality Anomalies** - Track: Average hallucination score. Average user satisfaction. Response length. Alert if: Hallucination score increases 20%. User feedback drops below 70% positive. **Metric 3: New Attack Patterns** - Use anomaly detection: Cluster prompts by embedding. New cluster appears: Potential new attack. Prompt length distribution changes. **Metric 4: User Reports** - Integrate with monitoring: User clicks "Report unsafe content". Immediate alert if 3+ reports in 1 hour. Auto-review: Manually check reported content. **Real-Time Alerts:** Slack: "@security-team New attack pattern detected: [details]". PagerDuty: For critical issues. Dashboard: Real-time metrics visible. **Incident Response Process:** (1) Detection (<1 hour): Automated: Monitoring detects anomaly. Manual: User report reaches threshold. (2) Immediate Action (5 minutes): Block: If clear attack, block pattern immediately. Alert: Notify on-call engineer. Isolate: Identify affected users/requests. (3) Analysis (30 minutes): What: What attack vector? How: How did it bypass? Impact: How many requests affected? (4) Mitigation (1 hour): Deploy fix: Add new detection pattern. Test: Verify fix works on captured examples. Monitor: Ensure attack is stopped. (5) Communication (2 hours): Internal: Notify team of incident and fix. External: If user data compromised, notify users. (6) Post-Incident (1 week): Root cause: Why did attack succeed? Prevention: How to prevent similar attacks? Update: Add to test suite. Red team: Test similar attack vectors. **Rapid Deployment:** Hot-fix pipeline: Deploy safety updates without full app deployment. Canary: Deploy to 10% traffic first. Rollback: Auto-rollback if error rate increases. **Prevention of Future Bypasses:** Red team: Weekly security testing. Adversarial training: Train on attack examples. Defense in depth: Multiple layers (not single point of failure). Bug bounty: Reward researchers who find vulnerabilities. **Implementation:** class ContinuousMonitoring: def __init__(self): self.baseline_metrics = self.calculate_baselines(). async def monitor(self): while True: await asyncio.sleep(60)  # Every minute, metrics = self.get_current_metrics(). if self.is_anomaly(metrics): await self.alert_team(metrics). await self.auto_mitigate(metrics). def is_anomaly(self, metrics): if abs(metrics.block_rate - self.baseline_metrics.block_rate) > 2 * self.baseline_metrics.stddev: return True. if metrics.user_reports_per_hour > 3: return True. return False. **Result:** Detection time: 3 days → <1 hour ✓. Response time: Deploy fix within 2 hours. Prevention: Continuous monitoring catches novel attacks.',
    keyPoints: [
      'Continuous monitoring: block rates, quality metrics, user reports',
      'Anomaly detection for new attack patterns',
      'Rapid incident response: detect → analyze → mitigate within 2 hours',
      'Post-incident analysis and prevention improvements',
    ],
  },
];
