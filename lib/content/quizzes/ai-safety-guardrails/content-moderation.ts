/**
 * Quiz questions for Content Moderation section
 */

export const contentmoderationQuiz = [
    {
        id: 'content-mod-q-1',
        question:
            "You're building content moderation for a global social platform.The OpenAI Moderation API blocks 8% of legitimate posts in your testing.Design a multi- layered moderation system that reduces false positives while maintaining safety.What layers would you implement and how would you balance precision vs recall ? ",
        hint: 'Consider combining multiple detection methods and using confidence thresholds.',
        sampleAnswer:
            "**Problem:** OpenAI Moderation API: 8% false positive rate (blocks legitimate content). Need: Reduce false positives while catching real violations. **Multi-Layered Moderation System:** **Layer 1: Fast Pre-Filter (Keyword Blocklist)** - Purpose: Catch obvious violations quickly before expensive API calls. Implementation: Exact match blocklist: Racial slurs, explicit profanity, spam patterns. Decision: If match → Block immediately (high precision). Cost: Free, latency: 1ms. False positive rate: ~0.1% (very precise list). **Layer 2: ML-Based Toxicity Detection** - Model: Toxicity-BERT or Detoxify model. Threshold: 0.85 (high confidence). Decision: Score > 0.85 → Block. Score 0.7-0.85 → Flag for review. Score < 0.7 → Pass to next layer. Cost: $0.0001 per request (self-hosted). Latency: 50ms. False positive rate: ~2% at 0.85 threshold. **Layer 3: OpenAI Moderation API** - Use for borderline cases that passed Layer 2. Custom thresholds per category: Hate: 0.8 (strict), Sexual: 0.7 (context-dependent), Violence: 0.75. Decision: If flagged by API AND score > threshold → Block or review. Cost: $0.0002 per request. Latency: 200ms. False positive rate: 8% → But only runs on ~30% of content (rest caught by earlier layers). **Layer 4: Context-Aware Review** - For content flagged by Layer 2 or 3 but not meeting block threshold. Consider: User history (first offense vs repeat violator), Content context (educational vs hateful), Community norms (gaming community vs professional network). Decision: New user + borderline content → Block. Trusted user + borderline content → Allow with warning. **Layer 5: Human Review Queue** - For: High-value content (posts with many likes), Appeals from users (\"my post was wrongly blocked\"), Uncertain cases (multiple layers disagree). SLA: Review within 4 hours for appeals, 24 hours for uncertain cases. **Decision Logic:** def moderate_content(text, user): # Layer 1: Keyword check, if keyword_match(text): return BlockDecision(\"keyword_match\", confidence=1.0). # Layer 2: ML toxicity, toxicity_score = toxicity_model(text). if toxicity_score > 0.85: return BlockDecision(\"high_toxicity\", confidence=toxicity_score). elif toxicity_score > 0.7: # Flag for review, return ReviewDecision(\"medium_toxicity\", confidence=toxicity_score). # Layer 3: OpenAI API (only for borderline), openai_result = openai.Moderation.create(text). if openai_result.flagged: flagged_categories = get_flagged_categories(openai_result). if any(score > THRESHOLDS[cat] for cat, score in flagged_categories): # Layer 4: Context check, if should_block_with_context(user, flagged_categories): return BlockDecision(\"moderation_api\", confidence=0.9). else: return ReviewDecision(\"context_dependent\", confidence=0.7). return AllowDecision(confidence=0.95). **Balancing Precision vs Recall:** Precision = (True positives) / (True positives + False positives). Recall = (True positives) / (True positives + False negatives). High precision = Few false positives (don\'t block legitimate content). High recall = Few false negatives (catch all violations). **Current Results:** Layer 1 (Keywords): Precision 99.9%, Recall 20% (catches obvious). Layer 2 (ML): Precision 98%, Recall 60% (catches most). Layer 3 (OpenAI): Precision 92%, Recall 85% (catches remainder). Combined: Precision 96%, Recall 88%. False positive rate: 4% (down from 8%). False negative rate: 12% (acceptable for safety). **Optimization Strategies:** (1) **Adjust Thresholds by Context:** Educational content: Higher thresholds (more lenient). Direct messages: Lower thresholds (stricter). Public posts: Medium thresholds. (2) **User Reputation:** Trusted users (good history): +0.1 to all thresholds (more lenient). New users: -0.1 to all thresholds (stricter). Repeat violators: -0.2 to all thresholds (very strict). (3) **Feedback Loop:** Track user appeals: \"This was wrongly blocked\". If appeal rate > 10%, thresholds too strict. If safety incidents increase, thresholds too loose. A/B test threshold changes. (4) **Cost Optimization:** OpenAI API only runs on ~30% of content (borderline cases). Other 70% handled by cheaper layers. Average cost: $0.00006 per post (vs $0.0002 if API on everything). **Monitoring Dashboard:** Metrics: (1) False positive rate (target: < 5%). (2) False negative rate (target: < 15%). (3) Review queue size (target: < 100 pending). (4) Average moderation latency (target: < 300ms). (5) Cost per moderation (target: < $0.0001). Alerts: If false positive rate > 7%, investigate. If false negative rate > 20%, tighten thresholds.",
        keyPoints: [
            'Use multiple layers: keywords, ML models, API, context, human review',
            'Set different thresholds per layer and context',
            'Balance precision (few false positives) and recall (catch violations)',
            'Use user reputation and feedback loops to optimize',
        ],
    },
    {
        id: 'content-mod-q-2',
        question:
            "Your content moderation system has a 200ms average latency, but users complain the app feels slow. You need to reduce latency to <100ms while maintaining safety. What caching, parallelization, and optimization strategies would you implement?",
        hint: 'Consider semantic caching, pre-filtering, and async processing.',
        sampleAnswer:
            "**Problem:** Current: 200ms avg latency. Target: <100ms. Must maintain safety. **Latency Breakdown:** Measure current system: Keyword check: 5ms. ML toxicity model: 80ms. OpenAI Moderation API: 120ms. Total: 205ms. **Optimization Strategies:** **Strategy 1: Semantic Caching** - Cache moderation results for similar content. Implementation: Generate embedding of content, Check if similar content (cosine similarity > 0.95) was recently moderated, If yes, return cached result. Example: \"This is spam\" → Hash: abc123 → Cached: BLOCK. \"This is spam!\" (same content, different punctuation) → Hash: abc456 → Similar to abc123 → Return cached BLOCK. Savings: 80% of requests are similar to recent content → 80% cached. Cached requests: 2ms (Redis lookup). New requests: Still 205ms. Average: 0.8 × 2ms + 0.2 × 205ms = 42.6ms. ✅ Under 100ms target! Considerations: Cache TTL: 1 hour (moderation policies stable). Cache invalidation: When policies update, clear cache. False matches: Cosine similarity > 0.95 = very similar, safe threshold. **Strategy 2: Parallel Execution** - Run multiple checks in parallel instead of serially. Current (Serial): Keyword (5ms) → ML (80ms) → API (120ms) = 205ms. Optimized (Parallel): All 3 run simultaneously → max(5ms, 80ms, 120ms) = 120ms. Implementation: import asyncio. async def moderate_parallel(text): keyword_task = asyncio.create_task(keyword_check(text)). ml_task = asyncio.create_task(ml_toxicity_check(text)). api_task = asyncio.create_task(openai_moderation(text)). keyword_result, ml_result, api_result = await asyncio.gather(keyword_task, ml_task, api_task). return combine_results(keyword_result, ml_result, api_result). Savings: 205ms → 120ms (42% faster). Combined with caching: 0.8 × 2ms + 0.2 × 120ms = 25.6ms. ✅ Well under 100ms! **Strategy 3: Early Termination** - If high-confidence decision reached, skip remaining checks. Logic: If keyword match (BLOCK with 100% confidence) → Return immediately, skip ML and API. If ML score > 0.95 (BLOCK with 95% confidence) → Skip API. Only run API for uncertain cases (ML score 0.7-0.85). Savings: 60% of requests blocked by keyword or high-confidence ML → Skip API (save 120ms). Average: 0.6 × (5ms + 80ms) + 0.4 × (5ms + 80ms + 120ms) = 51ms + 82ms = 133ms. Combined with caching and parallel: 0.8 × 2ms + 0.2 × 133ms = 28.2ms. ✅ Even better! **Strategy 4: Async Post-Processing** - For non-blocking moderation: Allow content immediately (optimistic), Run moderation in background, If violation detected, remove content retroactively (within 5 seconds). User experience: Instant post (0ms perceived latency). Background moderation: 200ms. If violation: Content removed + user notified. Trade-off: Brief window (5s) where violating content is visible. Acceptable for: Low-risk content (comments, not financial transactions). Not acceptable for: High-risk content (payments, legal documents). **Strategy 5: Model Optimization** - Use faster ML models: Current: Toxicity-BERT (80ms). Alternative: DistilBERT-toxicity (40ms, 95% accuracy of BERT). Savings: 80ms → 40ms. Trade-off: Slightly lower accuracy (acceptable for first-pass). Quantization: Run model in INT8 instead of FP32 → 2x faster. Batch processing: Process multiple requests together (for high traffic). **Strategy 6: Pre-Filter with Bloom Filters** - Use Bloom filter for known-safe content: If content hash in Bloom filter → Allow immediately (0.1ms). Only moderate new/suspicious content. False positive rate: 0.1% (Bloom filter might incorrectly mark new content as safe). Savings: 90% of repeat content → 0.1ms. 10% new content → Full moderation. Average: 0.9 × 0.1ms + 0.1 × 120ms = 12.09ms. **Final Optimized System:** Step 1: Check cache (2ms for 80% of requests). Step 2: For cache misses (20% of requests): (a) Check Bloom filter (0.1ms for 90% of misses = 18% of total). (b) For Bloom misses (2% of total): Parallel: Keyword + DistilBERT + OpenAI API. Early termination if high confidence. Average latency: 0.8 × 2ms + 0.18 × 0.1ms + 0.02 × 60ms = 1.6ms + 0.018ms + 1.2ms = 2.82ms. ✅ Under 100ms by a huge margin! **Safety Verification:** Test false negative rate: Run full moderation suite on 10,000 test cases. Compare optimized vs original. Ensure false negative rate increase < 2%. Monitor production: If safety incidents increase, investigate. **Rollout Plan:** Week 1: Deploy caching only (lowest risk). Week 2: Add parallel execution. Week 3: Add early termination. Week 4: Add async post-processing for low-risk content. Monitor each step, rollback if issues.",
        keyPoints: [
            'Semantic caching can reduce latency by 80%',
            'Parallel execution instead of serial saves significant time',
            'Early termination skips unnecessary checks',
            'Balance latency optimization with safety verification',
        ],
    },
    {
        id: 'content-mod-q-3',
        question:
            "You discover your content moderation system has a 15% higher false positive rate for non-English content. Investigate the root cause and design a solution that achieves fairness across all languages while maintaining safety.",
        hint: 'Consider translation, multilingual models, and per-language thresholds.',
        sampleAnswer:
            "**Problem:** English content: 5% false positive rate. Non-English content: 20% false positive rate (4x higher). Why? **Root Cause Investigation:** (1) **ML Model Training Bias:** Toxicity model trained primarily on English text → Performs poorly on other languages. Test: Run model on non-English text, check confidence scores. Result: Confidence scores 0.2-0.4 lower for non-English (model uncertain). (2) **Translation Issues:** System translates non-English to English, then moderates. Translation errors introduce false positives. Example: Spanish \"Estoy caliente\" = \"I am hot\" (temperature). Translated: \"I am horny\" (sexual) → Falsely flagged. Cultural context lost in translation. (3) **Keyword Blocklist:** Blocklist contains English words. Non-English words that are innocent in original language might translate to blocked words. Example: German \"Gift\" = \"poison\" in English → Blocked by keyword \"gift\". But \"Gift\" is common word in German. (4) **Context Collapse:** Idioms and cultural references don\'t translate well. Example: Japanese \"死ぬほど\" (literally \"to death\") = \"extremely\" in context. Translated: \"to death\" → Flagged as violence. **Solution: Multi-Language Fair Moderation System:** **Approach 1: Multilingual Models** - Use models trained on multiple languages: Replace: English-only toxicity model. With: XLM-RoBERTa-toxicity (trained on 100+ languages). Advantage: No translation needed, preserves context. Performance: Spanish false positive rate: 20% → 6% (much better). Japanese: 20% → 7%. English: 5% → 5% (no degradation). **Approach 2: Per-Language Thresholds** - Different languages have different baseline scores. Calibrate thresholds per language: English: Block if score > 0.85. Spanish: Block if score > 0.75 (model more confident on Spanish). Japanese: Block if score > 0.70 (model has more training data for Japanese). Rationale: Adjust thresholds to achieve similar false positive rates across languages. Implementation: THRESHOLDS = {\'en\': 0.85, \'es\': 0.75, \'ja\': 0.70, \'default\': 0.80}. def moderate(text, language): score = model(text). threshold = THRESHOLDS.get(language, THRESHOLDS[\'default\']). return score > threshold. **Approach 3: Language-Specific Keyword Lists** - Separate blocklists per language, curated by native speakers: English blocklist: [racial slurs, profanity]. Spanish blocklist: [insultos, palabrotas]. Japanese blocklist: [罵倒語]. Avoid: Translating English blocklist to other languages (loses context). **Approach 4: Native-Language Moderation** - Don\'t translate → Moderate in original language. Use: OpenAI Moderation API (supports multiple languages natively). Anthropic Claude (multilingual). For languages not supported: Fall back to translation, but add warning: "Moderation confidence lower for this language." **Approach 5: Cultural Context** - Add cultural context to moderation: Example: In India, "bloody" is mild, common word. In US, "bloody" is profanity. Solution: Geo-locate user → Apply culture-specific rules. Implementation: if user_country == "India": profanity_threshold *= 1.2  # More lenient for "bloody". **Validation & Testing:** (1) **Test Dataset per Language:** Create test sets: 1000 toxic posts per language (labeled by native speakers). 1000 non-toxic posts per language. Measure false positive/negative rates. (2) **Native Speaker Review:** Hire moderators fluent in each language. Review flagged content for false positives. Target: < 8% false positive rate across all languages. (3) **A/B Testing:** 50% of users: Old system (translation-based). 50% of users: New system (multilingual model). Measure: False positive rate by language, User complaints, True positive rate (safety maintained). **Results:** After implementing multilingual model + per-language thresholds: English: 5% false positive rate (same). Spanish: 20% → 7% (71% improvement). Japanese: 20% → 8% (60% improvement). German: 20% → 6% (70% improvement). Overall fairness: Max deviation now 3% (vs 15% before). **Ongoing Monitoring:** Dashboard: False positive rate per language (daily). Alert if any language > 10% or > 2x average. Feedback: "Was this correctly moderated?" button (in user\'s language). Monthly review of per-language performance. **Key Lessons:** (1) English-centric systems inherently unfair to non-English users. (2) Translation loses cultural context and introduces errors. (3) Multilingual models + per-language thresholds achieve fairness. (4) Continuous monitoring essential as language usage evolves.',
        keyPoints: [
            'English-centric models perform poorly on non-English content',
            'Use multilingual models trained on diverse languages',
            'Set per-language thresholds to achieve fairness',
            'Avoid translation when possible; preserve cultural context',
        ],
    },
];

