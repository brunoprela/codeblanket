/**
 * Discussion Questions for Logistic Regression
 */

import { QuizQuestion } from '../../../types';

export const logisticregressionQuiz: QuizQuestion[] = [
  {
    id: 'logistic-regression-q1',
    question:
      'Explain why binary cross-entropy loss is used for logistic regression instead of mean squared error. What problems would arise if we used MSE for classification?',
    hint: 'Consider the properties of the loss function, gradient behavior, and the nature of classification problems.',
    sampleAnswer:
      "Binary cross-entropy (log loss) is specifically designed for probabilistic classification, while MSE is fundamentally unsuited for this task. Cross-entropy measures the difference between two probability distributions - the predicted probability and the true distribution (0 or 1). For correct predictions, the loss smoothly approaches zero as confidence increases; for incorrect predictions, the loss grows rapidly, heavily penalizing confident wrong predictions. The gradient ∂L/∂w = (predicted - actual) * x provides clear, strong signals regardless of prediction confidence, enabling efficient learning throughout training.\n\nUsing MSE for classification creates multiple serious problems: (1) **Non-convex optimization** - The composition of sigmoid with squared error creates a non-convex loss landscape with many local minima and flat regions where gradients vanish, making optimization difficult and unreliable. (2) **Vanishing gradients** - When the model makes very confident wrong predictions (sigmoid outputs near 0 or 1 but opposite to truth), the sigmoid derivative σ'(z) = σ(z)(1-σ(z)) approaches zero, and MSE gradient also approaches zero, causing learning to stall precisely when the model needs strongest correction signals. (3) **Inappropriate penalties** - MSE penalizes the squared distance from the true binary value (0 or 1), but this doesn't align with what matters in classification: we care about probabilities and decision boundaries, not numeric distances. A prediction of 0.6 for class 1 should incur much lower loss than 0.4, but MSE treats these somewhat symmetrically.\n\nCross-entropy avoids these issues through its logarithmic form: -[y*log(p) + (1-y)*log(1-p)]. The logarithm amplifies errors when the model is confidently wrong, providing strong gradients that persist even at extreme predictions. The loss is convex when combined with the logistic sigmoid, guaranteeing gradient descent finds the global minimum. It directly optimizes what matters: the probability assigned to the correct class. Mathematically, cross-entropy also arises naturally from maximum likelihood estimation under a Bernoulli distribution, giving it strong theoretical justification.\n\nPractical example: Consider a model predicting spam. For an actual spam email, if the model predicts p=0.01 (very confident it's not spam - very wrong), cross-entropy loss is -log(0.01) ≈ 4.6 with a large gradient pushing toward correction. MSE loss is (1-0.01)² ≈ 0.98 with a tiny gradient due to the sigmoid saturation, failing to provide learning signal. This is why MSE doesn't work for classification.",
    keyPoints: [
      'Cross-entropy measures difference between predicted and true probability distributions',
      'MSE with sigmoid creates non-convex optimization landscape with local minima',
      'Vanishing gradients in MSE: wrong confident predictions have near-zero gradients',
      'Cross-entropy gradient ∂L/∂w = (p - y)x provides consistent strong signals',
      'Logarithmic form amplifies errors for confidently wrong predictions',
      'MSE penalizes numeric distance from 0/1, inappropriate for probability-based classification',
      'Cross-entropy convex with logistic sigmoid, guarantees global minimum',
      'Theoretical justification: arises from maximum likelihood under Bernoulli distribution',
    ],
  },
  {
    id: 'logistic-regression-q2',
    question:
      'Discuss the interpretation of logistic regression coefficients in terms of odds ratios. How do you explain these to non-technical stakeholders? Provide a concrete example.',
    hint: 'Think about what odds and log-odds mean, and how to translate statistical concepts into business language.',
    sampleAnswer:
      'Logistic regression coefficients represent changes in log-odds, which requires careful translation for non-technical audiences. The log-odds (logit) is the logarithm of the odds: log(P(yes)/P(no)). While statisticians find this natural, it\'s abstract for stakeholders. The key insight is that exponentiating the coefficient gives the **odds ratio** - a multiplicative factor showing how odds change with the feature, which is more intuitive.\n\nConcrete example: Credit card default prediction with feature "months_delinquent" having coefficient β = 0.5. The interpretation works through three levels:\n\n**Statistical (precise but technical):** "A one-month increase in delinquency increases the log-odds of default by 0.5." Most stakeholders won\'t understand this.\n\n**Odds ratio (better):** "The odds ratio is e^0.5 ≈ 1.65. For each additional month delinquent, the odds of default multiply by 1.65." This means if someone currently has 2:1 odds of default (67% probability), one more delinquent month changes it to 3.3:1 odds (77% probability). Better, but "odds" still confuses some.\n\n**Business language (best):** "Each additional month of delinquency increases the likelihood of default by approximately 65%. Someone with 3 months delinquency is substantially more likely to default than someone with 1 month." For stakeholders, emphasize direction and magnitude without technical jargon. Use phrases like "strongly associated with," "increases risk," or "protective factor" (for negative coefficients).\n\nFor negative coefficients, interpretation flips: If "credit_score" has β = -0.03, then e^-0.03 ≈ 0.97. "For each 1-point increase in credit score, the odds of default multiply by 0.97 (decrease by 3%). A 100-point increase decreases odds by (0.97^100 ≈ 0.05) or 95%." In business terms: "Higher credit scores strongly protect against default. Customers with 100 points higher score are much less likely to default."\n\nCaveats to communicate: (1) These are **associations, not causation** - just because delinquency predicts default doesn\'t mean reducing delinquency reports will reduce defaults if underlying financial issues remain. (2) Effects are **multiplicative and compounding** - two months delinquent doesn\'t double risk, it squares the odds ratio (1.65² ≈ 2.7x increase). (3) **All else equal assumption** - the coefficient applies when other features are held constant, but in reality features correlate. (4) **Scale matters** - interpretation depends on feature units; a one-month change is very different from a one-year change.\n\nPractical presentation: Use visualizations showing predicted probabilities at different feature values (e.g., default probability vs. credit score curve), which are more intuitive than coefficients. Combine with real examples: "Consider two customers identical except one has been delinquent one month longer - that customer has a 65% higher chance of defaulting." This makes the impact concrete.',
    keyPoints: [
      'Coefficients represent change in log-odds: technical and unintuitive for stakeholders',
      'Odds ratio e^β is multiplicative factor: more interpretable than log-odds',
      'Example: β=0.5 means odds multiply by 1.65 (65% increase) per unit increase',
      'Business language: "increases likelihood by X%" rather than "log-odds" or "odds"',
      'Negative coefficients: odds ratio < 1, protective effect (reduces probability)',
      'Emphasize direction and magnitude, avoid jargon ("strongly associated with")',
      'Important caveats: association not causation, multiplicative effects compound',
      'Visualizations more effective: plot probability vs. feature value curves',
    ],
  },
  {
    id: 'logistic-regression-q3',
    question:
      'Compare logistic regression with linear regression for classification tasks. In what scenarios might someone mistakenly use linear regression for classification? What goes wrong, and how does logistic regression address these issues?',
    hint: 'Think about output constraints, probability interpretation, decision boundaries, and practical consequences.',
    sampleAnswer:
      'Using linear regression for classification is a common beginner mistake that appears to work superficially but fails fundamentally. The confusion arises because linear regression can produce numerical outputs, and with binary classification (0 and 1), one might reasonably think to threshold the output (predict class 1 if output > 0.5, else class 0). However, this approach violates probability theory and creates severe practical problems.\n\n**Why people try it:** Linear regression is simpler conceptually and computationally. For well-separated classes, it might yield similar decision boundaries to logistic regression in the middle of the feature space. With few samples and clean data, predictions might accidentally cluster near 0 and 1, appearing to work. Some older methods (linear discriminant analysis) do use linear functions, creating further confusion.\n\n**What goes wrong:**\n\n1. **Invalid probabilities:** Linear regression produces unbounded outputs (-∞ to +∞), but probabilities must be in [0, 1]. A prediction of -0.3 or 1.7 is meaningless as a probability. You can\'t threshold these reasonably - what does it mean that 1.7 > 0.5? The model has no notion of "confidence" beyond arbitrary thresholds.\n\n2. **Non-linear decision boundaries needed:** The relationship between features and class probability is fundamentally non-linear (S-shaped). Linear regression forces a straight-line relationship, failing to model how probability should saturate at 0 and 1 as feature values become extreme. This causes poor predictions far from the training distribution.\n\n3. **Inappropriate loss function:** MSE penalizes squared distance from 0 or 1. A prediction of 0.6 for class 1 has MSE = (1-0.6)² = 0.16, while prediction 0.55 has MSE = 0.20. But from a classification perspective, both are on the correct side of 0.5! MSE wastes effort making confident predictions more confident rather than fixing wrong predictions. It also creates non-convex optimization landscapes with the sigmoid transformation.\n\n4. **Outlier sensitivity:** Linear regression is highly sensitive to outliers. A single mislabeled point or extreme feature value can drastically shift the regression line, destroying decision boundaries for all other points. Classification should focus on getting the boundary right, not fitting every point\'s exact value.\n\n5. **No probabilistic interpretation:** Linear regression coefficients can\'t be interpreted probabilistically. You can\'t say "this feature increases the probability of class 1 by X%" because the model doesn\'t output probabilities.\n\n**How logistic regression fixes these:**\n\nLogistic regression applies the sigmoid function σ(wᵀx) = 1/(1 + e^(-wᵀx)) to force outputs into [0, 1], providing valid probabilities. The S-curve naturally models how probability transitions from 0 to 1 as the linear combination wᵀx moves from negative to positive. Cross-entropy loss directly optimizes probability assignments, strongly penalizing confident wrong predictions while allowing confident correct predictions. The model becomes much more robust to outliers because extreme predictions are bounded. Coefficients gain probabilistic interpretation through odds ratios.\n\n**Real-world consequence:** In medical diagnosis, using linear regression might predict "probability" of 1.3 for disease (meaningless) or provide over-confident predictions in regions far from training data. Logistic regression correctly models uncertainty, predicting probabilities approaching 1 asymptotically as evidence accumulates, never exceeding bounds. This prevents dangerous overconfidence in automated decision systems.\n\nBottom line: Binary classification is fundamentally a probability estimation problem, requiring tools designed for probabilities (logistic regression), not tools designed for continuous numeric prediction (linear regression).',
    keyPoints: [
      'Linear regression outputs unbounded values (-∞, +∞); probabilities require [0, 1]',
      'Linear relationship inappropriate: probability should follow S-curve, not straight line',
      'MSE loss wrong for classification: penalizes numeric distance, not classification errors',
      'Linear regression highly sensitive to outliers, breaks decision boundaries',
      'No probabilistic interpretation of coefficients possible with linear regression',
      'Logistic regression uses sigmoid to bound outputs in [0, 1]',
      'Cross-entropy loss optimizes probability assignments directly',
      'Logistic regression robust to outliers, coefficients interpretable as odds ratios',
      'Real-world: invalid probabilities (>1 or <0) cause serious issues in automated systems',
    ],
  },
];
