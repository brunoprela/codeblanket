/**
 * Discussion Questions for k-Nearest Neighbors
 */

import { QuizQuestion } from '../../../types';

export const knnQuiz: QuizQuestion[] = [
  {
    id: 'knn-q1',
    question:
      'Explain the "curse of dimensionality" in the context of kNN. Why does kNN performance degrade in high-dimensional spaces? Provide specific mathematical intuition and practical implications.',
    hint: 'Consider how distances behave as dimensionality increases and what this means for nearest neighbor identification.',
    sampleAnswer:
      "The curse of dimensionality severely impacts kNN because the algorithm relies on meaningful distance measurements to identify similar instances. As dimensionality increases, several catastrophic problems emerge. First, distances lose discriminative power - in high dimensions, all points become roughly equidistant. The ratio of the distance to the nearest neighbor vs. the farthest neighbor approaches 1 as dimensions increase, making it impossible to distinguish \"close\" from \"far.\" Mathematically, for random points in d dimensions, the expected distance grows as √d, but the standard deviation grows slower, so relative differences shrink.\n\nSecond, data becomes exponentially sparse. To maintain the same density of points, you need exponentially more samples as dimensions increase. In a unit hypercube, if you have 100 points uniformly distributed in 2D, you'd need 100^5 = 10 billion points to achieve the same density in 10D! Most real datasets don't have this luxury, so in practice, all points are isolated with large gaps between them. When finding k-nearest neighbors, you're often selecting from points that aren't actually similar at all - they're just the \"least far\" among equally distant points.\n\nThird, irrelevant features dominate. If you have 100 features but only 5 are relevant for your task, the 95 irrelevant dimensions add noise to distance calculations, drowning out the signal from relevant features. Distance in 100D is dominated by these noise dimensions. Without feature selection or dimensionality reduction, kNN can't focus on what matters.\n\nPractical implications: (1) kNN requires massive datasets in high dimensions, often infeasible; (2) Feature selection/reduction becomes critical - use PCA, feature selection, or domain knowledge to reduce dimensions before applying kNN; (3) Alternative algorithms that don't rely on distances (tree-based methods, neural networks) often outperform kNN in high dimensions; (4) Approximate nearest neighbor methods (LSH, Annoy) become necessary for computational feasibility but don't solve the fundamental problem of meaningless distances.\n\nReal example: In genomics with 20,000 gene features predicting disease, kNN performs poorly because distances are meaningless across so many dimensions. Reducing to 50 principal components or selecting 100 most predictive genes dramatically improves performance, as distances in this reduced space actually capture relevant similarity.",
    keyPoints: [
      'High dimensions: all points become roughly equidistant, distances lose meaning',
      "Distance ratio (nearest/farthest) approaches 1, can't distinguish close vs. far",
      'Data sparsity: need exponentially more samples to maintain density',
      'Irrelevant features dominate distance calculations, drowning out signal',
      'Practical fix: dimensionality reduction (PCA) or feature selection before kNN',
      'Alternative algorithms (trees, neural nets) often better in high dimensions',
      'Example: genomics with 20k features → reduce to 50-100 dimensions for kNN',
    ],
  },
  {
    id: 'knn-q2',
    question:
      'Why is feature scaling absolutely critical for kNN? Explain what happens without scaling using a concrete example, and discuss the appropriate scaling techniques.',
    hint: 'Think about how distances are calculated and how features with different scales affect those distances.',
    sampleAnswer:
      "Feature scaling is absolutely critical for kNN because the algorithm computes distances, and distance calculations are directly affected by feature magnitudes. Without scaling, features with larger numeric ranges dominate the distance metric, rendering smaller-scale features irrelevant regardless of their actual predictive importance.\n\nConcrete example: Predicting house prices with features [square_footage, num_bedrooms]. Square footage ranges [1000, 5000], bedrooms range [1, 5]. Computing Euclidean distance between two houses: House A (2000 sqft, 3 bedrooms) and House B (2100 sqft, 4 bedrooms). Distance = √[(2000-2100)² + (3-4)²] = √[10000 + 1] ≈ 100.005. The bedroom difference (1) contributes only 0.005% to the total distance, while square footage difference (100) contributes 99.995%. Bedrooms are effectively ignored!\n\nIf bedrooms are actually more predictive than square footage (which could be true - luxury apartments vs. suburban houses), kNN will still base decisions almost entirely on square footage due to the scale difference. The algorithm has no way to know that \"1 bedroom difference\" should matter more than \"100 sqft difference\" - it only sees raw numbers.\n\nAppropriate scaling techniques:\n\n**StandardScaler (z-score normalization):** x' = (x - μ)/σ. Transforms features to mean=0, std=1. Best for most cases as it preserves the shape of the distribution and handles outliers reasonably. After standardization, a 1-unit change in any feature represents 1 standard deviation, making all features comparable.\n\n**MinMaxScaler:** x' = (x - min)/(max - min). Scales to [0, 1]. Good when you need bounded values and the distribution doesn't have significant outliers. However, very sensitive to outliers - one extreme value can compress all other values.\n\n**RobustScaler:** Uses median and IQR instead of mean and std. Best when data has outliers, as it's not affected by extreme values.\n\n**MaxAbsScaler:** Scales by maximum absolute value, preserving sparsity. Good for sparse data where many values are zero.\n\nImportant considerations: (1) **Fit on training data only**, then transform both train and test - fitting on all data would be data leakage; (2) Apply same transformation to new prediction data; (3) Scale after train-test split, never before; (4) Don't scale categorical binary features (0/1) if they're already on meaningful scales.\n\nFor the house example, after StandardScaler: square footage might map to [-1.5, 1.5] and bedrooms to [-1.2, 1.3], both on comparable scales. Now a 1-unit difference in standardized bedrooms contributes equally to distance as 1-unit difference in standardized square footage, allowing the model to learn which truly matters based on their relationship with the target, not their arbitrary measurement units.",
    keyPoints: [
      'kNN uses distances; features with larger scales dominate distance calculations',
      'Example: sqft [1000-5000] vs bedrooms [1-5], sqft contributes 99.995% to distance',
      'Small-scale but important features become irrelevant without scaling',
      'StandardScaler (z-score): most common, transforms to mean=0 std=1',
      'MinMaxScaler: scales to [0,1], but sensitive to outliers',
      'RobustScaler: uses median/IQR, good for outliers',
      'Critical: fit scaler on training data only, transform train and test',
      'After scaling, all features contribute equally to distances based on actual importance',
    ],
  },
  {
    id: 'knn-q3',
    question:
      'Compare kNN with logistic regression for binary classification. In what scenarios would you prefer one over the other? Discuss computational complexity, interpretability, and performance characteristics.',
    hint: 'Think about parametric vs. non-parametric, training vs. prediction time, and decision boundary complexity.',
    sampleAnswer:
      'kNN and logistic regression represent fundamentally different approaches to classification, each with distinct strengths and appropriate use cases.\n\n**Parametric vs. Non-parametric:** Logistic regression is parametric - it assumes a specific functional form (linear decision boundary in feature space) and learns a fixed set of parameters. kNN is non-parametric - it makes no assumptions about data distribution and stores all training examples. This means kNN can capture arbitrarily complex decision boundaries (any shape), while logistic regression is limited to linear separations (or polynomial with feature engineering).\n\n**Computational characteristics:** Logistic regression has O(n·d) training complexity (iterative optimization over n samples, d features) but O(d) prediction (just compute wᵀx + b). kNN has O(1) training (just store data) but O(n·d) prediction (must compute distance to all n training points). For large datasets with many predictions, logistic regression is vastly more efficient. For small datasets with few predictions, kNN\'s slow prediction isn\'t problematic.\n\n**Interpretability:** Logistic regression coefficients have clear interpretable meaning - odds ratios showing how each feature affects class probability. You can explain to stakeholders: "Each unit increase in feature X multiplies odds by e^β." kNN has no coefficients or parameters to interpret - predictions are just "these 5 training examples were similar and mostly class 1." This lack of interpretability makes kNN unsuitable for regulated industries requiring explainable decisions.\n\n**Decision boundaries:** Logistic regression creates linear boundaries. For linearly separable or approximately linear problems, it works excellently. For highly non-linear problems (XOR-like patterns, circular clusters), logistic regression fails without extensive feature engineering. kNN naturally handles any boundary shape - it creates piecewise linear boundaries that approximate any complex shape given enough data.\n\n**When to prefer logistic regression:** (1) Large datasets where prediction speed matters; (2) Need interpretable results (healthcare, finance, legal); (3) Data is approximately linearly separable; (4) High-dimensional data (kNN suffers from curse of dimensionality); (5) Online learning needed (can update parameters); (6) Want probabilistic predictions calibrated to confidence; (7) Limited memory (just store parameters, not data).\n\n**When to prefer kNN:** (1) Small to medium datasets; (2) Highly non-linear decision boundaries; (3) Training time is critical (need immediate deployment); (4) Low-dimensional data; (5) Prototype/baseline needed quickly; (6) Instance-based reasoning desired (find similar past cases); (7) Don\'t want to make distributional assumptions.\n\n**Real-world scenarios:**\n\n*Medical diagnosis from symptoms (prefer logistic regression):* Need explainable decisions ("Patient has 73% probability of disease because high fever (odds ratio 2.5) and elevated white blood cell count (odds ratio 1.8)"). Regulators require this interpretability. Large patient databases make training cost acceptable, and millions of daily diagnoses require fast prediction.\n\n*Recommender system finding similar users (prefer kNN):* "Users who liked items A, B, C also liked D" is naturally instance-based. Decision boundaries are highly non-linear (user preferences are complex and multi-modal). Interpretability less critical than accuracy. Prediction volume manageable with approximate nearest neighbor methods.\n\n*Fraud detection (prefer logistic regression):* Need real-time predictions on millions of transactions (kNN too slow). Must explain flagged transactions to customers and regulators. Many features (high-dimensional), where kNN suffers. Can retrain model periodically with new fraud patterns.\n\n**Hybrid approach:** Often practical to use both - logistic regression as primary model (interpretable, fast) and kNN for confidence calibration or hard cases where logistic regression is uncertain. Or use logistic regression on top of kNN distances as features.',
    keyPoints: [
      'Logistic regression: parametric (linear boundaries), kNN: non-parametric (any shape)',
      'Training: logistic O(n·d), kNN O(1). Prediction: logistic O(d), kNN O(n·d)',
      'Logistic regression: interpretable coefficients (odds ratios), kNN: no interpretation',
      'Logistic regression: linear boundaries, kNN: complex piecewise boundaries',
      'Prefer logistic: large data, need speed, interpretability, high dimensions',
      'Prefer kNN: small data, non-linear, quick deployment, low dimensions',
      'Medical/finance: logistic for explainability and speed',
      'Recommender systems: kNN for instance-based similarity',
      'Can combine both in hybrid approaches for best of both worlds',
    ],
  },
];
