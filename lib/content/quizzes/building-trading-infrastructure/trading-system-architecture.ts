export const tradingSystemArchitectureQuiz = [
    {
        id: 'trading-system-architecture-q-1',
        question:
            'Design a production-grade "event-driven trading system architecture" that handles 1 million events per second with sub-5ms latency. Your design should include: (1) Event bus implementation (pub-sub pattern), (2) Component isolation (market data, strategy, risk, OMS, EMS), (3) State management (shared state across services), (4) Error handling (circuit breakers, retries), (5) Performance monitoring (latency tracking), (6) Explain trade-offs between monolithic vs microservices architecture for trading systems.',
        sampleAnswer:
            'Production Event-Driven Trading System: **Event Bus Design**: Use Kafka for inter-service messaging (high throughput: 1M+ msg/sec), Topic structure: market-data, signals, orders, fills, positions, risk-events. Consumer groups for load balancing: Multiple strategy instances consume from market-data topic in parallel. Exactly-once semantics for critical paths (orders, fills). In-memory queue (asyncio.Queue) for intra-process events (sub-millisecond). Trade-off: Kafka adds ~1-2ms latency but provides persistence and scalability. **Component Isolation**: Market Data Service: C++/Rust for low latency (<100μs), normalizes data from exchanges, publishes to Kafka. Strategy Service: Python for flexibility, subscribes to market data, generates signals. Risk Service: Go for concurrent risk checks, pre-trade checks in <1ms, position limit enforcement. OMS Service: Java for enterprise features, manages order lifecycle, audit trail. EMS Service: C++ for exchange connectivity, FIX protocol, execution algorithms. Each service in separate container (Docker), independent scaling. Communication ONLY via event bus (no direct calls). **State Management**: Redis cluster for shared state (positions, orders), <1ms read/write latency. PostgreSQL for persistent storage (order history, trades), asynchronous writes (don\'t block trading). Event sourcing: All state changes are events, can replay to rebuild state. Example: Order state stored in Redis with TTL, archived to PostgreSQL. **Error Handling**: Circuit breaker per external service (broker, exchange): Open after 5 consecutive failures, Close after 60s timeout, Half-open to test recovery. Retry with exponential backoff: Max 3 retries, 1s → 2s → 4s delays. Dead letter queues for failed messages (manual review). Example: Broker connection fails → circuit opens → reject new orders gracefully. **Performance Monitoring**: Instrument every component with latency tracking: Market data: Receive from exchange → Publish to Kafka (target <200μs), Strategy: Receive data → Generate signal (target <1ms), Risk: Receive signal → Approve/reject (target <500μs), OMS: Receive order → Send to EMS (target <100μs). Use Prometheus for metrics, Grafana for dashboards. Alert on p99 latency >5ms. **Monolithic vs Microservices**: Monolithic Pros: Lower latency (no network hops), simpler deployment, easier debugging. Cons: Hard to scale (must scale everything), blast radius (one bug kills all), hard to use optimal language per component. Microservices Pros: Independent scaling (scale market data 10×, strategy 2×), fault isolation (strategy crash doesn\'t kill data feed), technology flexibility (C++ for speed, Python for logic). Cons: Network latency (+1-2ms per hop), operational complexity (service discovery, monitoring), harder debugging (distributed tracing needed). **Recommendation**: Start monolithic for MVP (faster time-to-market), migrate to microservices at scale (>100K events/sec). Use hybrid: Hot path monolithic (market data → signal), cold path microservices (reporting, analytics).',
        keyPoints: [
            'Event bus: Kafka for inter-service (1M+ msg/sec), asyncio.Queue for intra-process; trade-off is 1-2ms latency vs scalability',
            'Component isolation: Each service in separate container, communicate only via event bus; enables independent scaling and fault isolation',
            'State management: Redis for real-time state (<1ms), PostgreSQL for persistence, event sourcing for reproducibility',
            'Error handling: Circuit breakers (5 failures → open 60s), retry with exponential backoff (1s → 2s → 4s), dead letter queues',
            'Monolithic vs microservices: Monolithic for MVP (<100K evt/sec), microservices at scale; hybrid approach for hot/cold paths',
        ],
    },
    {
        id: 'trading-system-architecture-q-2',
        question:
            'You are building a trading system that must handle market data from 10 exchanges, execute strategies on 5000 symbols, and manage 50,000 orders per day. Design the "data flow architecture" including: (1) Market data ingestion and normalization, (2) Strategy signal generation pipeline, (3) Order routing and execution flow, (4) Position and P&L update flow, (5) Identify bottlenecks and optimization strategies, (6) Calculate required infrastructure (CPU, memory, network bandwidth).',
        sampleAnswer:
            'Multi-Exchange Trading System Architecture: **Market Data Ingestion**: 10 exchanges × 5000 symbols = 50,000 feeds. Each exchange: WebSocket connection (quotes at 1Hz to 100Hz). Normalization layer: Exchange-specific format → standard format. Example: Binance: {"s":"BTCUSDT","b":"50000","a":"50001"} → Standard: {symbol:"BTC/USD",bid:50000,ask:50001,exchange:"binance"}. Throughput: 50K symbols × 10 quotes/sec = 500K quotes/sec. Optimization: Drop duplicate quotes (price unchanged), only publish on change. Result: ~100K unique quotes/sec after dedup. **Strategy Pipeline**: Input: 100K quotes/sec. Processing: 5000 symbols × 10 strategies = 50K strategy instances. Each strategy: Receive quote → Calculate indicators (EMA, RSI) → Generate signal. Latency target: <10ms per strategy run. Throughput: 100K quotes → 50K strategy evaluations → ~5K signals/hour (1% signal rate). Optimization: Batch processing (process multiple symbols together), use vectorized operations (NumPy), cache indicator state (don\'t recalculate). **Order Flow**: Input: 5K signals/hour = 1.4 signals/sec (avg), 50K orders/day = 35 orders/min (avg), peaks at 500 orders/min. Flow: Signal → Risk Check → OMS → EMS → Exchange. Risk check: Position limits, capital limits (<1ms). OMS: Create order, assign ID, persist to DB (<5ms). EMS: Route to exchange, FIX protocol (<10ms). Total latency: Signal to exchange = 16ms (acceptable for non-HFT). Optimization: Pre-allocate order IDs (avoid DB write blocking), batch risk checks (check 10 orders together). **Position & P&L Updates**: Input: Order fills (50K/day = 35/min avg). Flow: Fill from exchange → EMS → OMS → Portfolio Manager → Redis + PostgreSQL. Portfolio Manager: Update position (add/subtract quantity), recalculate P&L (mark-to-market), publish to Redis (<1ms). PostgreSQL: Async write for audit trail (doesn\'t block). Real-time P&L: Subscribe to market data + positions → calculate unrealized P&L. Optimization: Batch P&L calculations (every 1 second instead of per fill), use Redis for fast reads. **Bottleneck Analysis**: Bottleneck 1: Market data dedup (CPU-bound at 500K quotes/sec). Solution: Horizontal scaling (10 workers, 50K quotes/sec each), use Redis for last price cache. Bottleneck 2: Strategy execution (50K evaluations/sec). Solution: Distribute strategies across 10 machines (5K each), use message queue for load balancing. Bottleneck 3: Database writes (50K orders + fills/day). Solution: Async writes, batch inserts (1000 at a time), use TimescaleDB for time-series data. Bottleneck 4: Network bandwidth (100K quotes × 100 bytes = 10 MB/sec). Solution: Acceptable, but compress data for cross-region replication. **Infrastructure Requirements**: Market Data Servers: 10 machines × 4 vCPU × 8GB RAM = 40 vCPU, 80GB RAM. Handle 50K symbols each, ~$200/month each = $2K/month. Strategy Servers: 10 machines × 8 vCPU × 16GB RAM = 80 vCPU, 160GB RAM. Handle 5K strategies each, ~$400/month each = $4K/month. Order/Execution Servers: 2 machines × 4 vCPU × 8GB RAM (active-passive failover). Handle 500 orders/min, ~$200/month each = $400/month. Database: PostgreSQL (4 vCPU, 16GB, 500GB SSD) = $300/month. Redis Cluster (3 nodes, 8GB each) = $150/month. Total: ~$7K/month for infrastructure. Network: 10 MB/sec × 2 (in+out) × 2.6M sec/month = 52 TB/month. AWS: ~$5K/month. Grand Total: ~$12K/month operating cost.',
        keyPoints: [
            'Market data: 500K quotes/sec from 10 exchanges → dedup to 100K/sec; normalize to standard format; Redis cache for dedup',
            'Strategy pipeline: 50K strategy instances generate ~5K signals/hour; optimize with batching, vectorization, state caching',
            'Order flow: 35 orders/min avg, 500/min peak; latency <16ms total; pre-allocate IDs, batch risk checks for performance',
            'Bottlenecks: Market data CPU (scale to 10 workers), strategy CPU (distribute across machines), DB writes (async batching)',
            'Infrastructure: $7K/month compute (120 vCPU, 240GB RAM), $5K/month network (52TB/month); total $12K/month operating cost',
        ],
    },
    {
        id: 'trading-system-architecture-q-3',
        question:
            'Design a "disaster recovery and failover architecture" for a trading system that must maintain 99.99% uptime during market hours. Include: (1) Active-active vs active-passive trade-offs for each component, (2) Data replication strategy (synchronous vs asynchronous), (3) Failover detection and automatic recovery, (4) State consistency during failover, (5) Testing strategy to ensure failover works, (6) Regulatory requirements (audit trail, no order loss).',
        sampleAnswer:
            'Trading System DR/Failover Architecture: **Component Failover Strategy**: Market Data Feed: Active-active (both regions consume data simultaneously). Reason: No state, idempotent, can run multiple instances. Failover: DNS-based, instant. Strategy Service: Active-active (both run same strategies). Reason: Stateless (state in Redis), signals are idempotent. Conflict: Both may generate same signal (dedup by order ID). Failover: Load balancer detects health check failure, routes to other region. Risk Service: Active-active with distributed locks. Reason: Pre-trade checks must be centralized to prevent double-counting. Use Redis distributed lock (Redlock algorithm). Failover: Lock automatically released on node failure (TTL). OMS: Active-passive (primary/secondary). Reason: Order IDs must be sequential, order state must be consistent. Failover: Raft consensus (3-node cluster), automatic leader election. EMS: Active-active (route to exchange from both regions). Reason: Exchanges accept orders from multiple IPs. Use different source IPs to identify. Failover: Instant (already active). Database: Active-active (multi-master PostgreSQL). Reason: No single point of failure. Conflict resolution: Last-write-wins with timestamp. Failover: Read from any replica. **Data Replication**: Real-time Data (Redis): Synchronous replication to 2 regions (Redis cluster with replicas). Latency penalty: +1-2ms. Consistency: Strong (can\'t proceed until replicated). Reason: Position data must be consistent. Historical Data (PostgreSQL): Asynchronous replication (streaming replication). Lag: <100ms. Consistency: Eventual (acceptable for historical data). Reason: Don\'t block trading on slow DB writes. Order Data (OMS): Synchronous replication (Raft consensus). Latency penalty: +5ms. Consistency: Strong (majority quorum). Reason: Can\'t lose orders (regulatory requirement). **Failover Detection**: Health checks every 1 second (HTTP endpoint per service). Failure threshold: 3 consecutive failures = 3 seconds. Automatic failover: Service mesh (Istio/Linkerd) detects failure → routes to healthy instance. Database failover: Raft election (< 5 seconds), new leader elected automatically. Network partition: Split-brain protection (require majority quorum). Example: 3-node cluster, 1 node isolated → remaining 2 continue (have majority). **State Consistency**: Pre-failover: All state in Redis (positions, open orders, last prices). Continuously replicated to DR region (sync replication). Post-failover: DR region has up-to-date state (max 2ms lag). Order state: Replayed from OMS (event sourcing), all orders in PostgreSQL, on recovery: read last 1000 orders → rebuild state. Position reconciliation: Compare positions with broker (EOD reconciliation), correct any discrepancies (manual review if > 0.01%). Idempotency: All operations idempotent (can replay events safely), order submissions have unique ID (prevent duplicates). **Testing Strategy**: Monthly DR drills (planned failover): Announce maintenance window, trigger failover to DR region, run for 1 hour, verify: positions match, orders execute correctly, P&L accurate, failover back to primary. Chaos engineering (random failures): Randomly kill services (1 per day), verify auto-recovery, measure MTTR (mean time to recovery), target: <30 seconds. Simulated disasters: Network partition (split-brain), entire region failure (AWS outage), database corruption. Canary testing: Route 1% of traffic to DR region continuously, detect issues before full failover. **Regulatory Requirements**: Order Audit Trail: Every order event (new, modify, cancel, fill) logged to immutable storage (S3), includes: timestamp (microsecond precision), user ID, order details, reason for action. Retention: 7 years (SEC requirement). No Order Loss: Synchronous replication for order data (Raft consensus), Order ID assigned before sending to exchange, On crash: replay from last checkpoint, confirm with exchange (query open orders). Maximum data loss: 0 orders (guaranteed by Raft). Best Execution: Log order routing decisions (why exchange X chosen), include: venue, price, liquidity, latency. Prove best execution in case of audit. **Failover SLA**: Target: 99.99% uptime = 52 minutes downtime per year = 4.3 minutes per month. Achieved: Automatic failover in <30 seconds (RTO), no data loss (RPO = 0 for orders), monthly DR drills ensure confidence. Cost: 2× infrastructure (active-active) = $24K/month (vs $12K single region). Benefit: Peace of mind, regulatory compliance, customer trust.',
        keyPoints: [
            'Active-active for stateless (market data, strategy), active-passive for stateful (OMS); use distributed locks for risk service',
            'Synchronous replication for critical data (orders, positions, +1-2ms latency), asynchronous for historical (<100ms lag)',
            'Failover detection: Health checks every 1s, 3 failures = failover; Raft election <5s for OMS; split-brain protection with quorum',
            'State consistency: Redis sync replication (2ms lag), event sourcing for orders, idempotent operations for safe replay',
            'Testing: Monthly DR drills, chaos engineering (random failures), canary testing (1% traffic); regulatory: audit trail (7 years), no order loss',
        ],
    },
];

