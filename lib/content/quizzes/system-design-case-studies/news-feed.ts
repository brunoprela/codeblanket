/**
 * Design News Feed Quiz
 */

export const newsFeedQuiz = [
    {
        question: "Compare push (fanout-on-write) and pull (fanout-on-read) models for news feed generation. A celebrity with 100 million followers posts a photo. Calculate the fanout cost for push model assuming 100ms per batch of 1000 writes, and explain why a hybrid approach is necessary. Then design the specific threshold and logic for determining when to use push vs pull.",
        sampleAnswer: "In the push model, when the celebrity posts, the system must write the post_id to all 100 million followers' feeds. Assuming batch writes of 1000 followers at a time (Redis pipeline), we need 100,000 batches. At 100ms per batch, total fanout time is 100,000 × 100ms = 10,000 seconds = 2.78 hours. This is unacceptable—the post wouldn't appear in followers' feeds for almost 3 hours, and during this time, the fanout service is blocked processing this single post. Additionally, at 100M writes, if we're writing 8-byte post IDs to Redis lists, that's 800 MB of cache updates for a single post. In contrast, the pull model requires no fanout—the celebrity's post is simply stored in their timeline. When followers load their feed, they query the celebrity's recent posts (SELECT * FROM posts WHERE user_id = celebrity_id ORDER BY created_at DESC LIMIT 20). With proper indexing and caching (celebrity timelines cached in Redis), this query takes ~5-10ms per follower. However, if 10 million followers load their feed simultaneously (typical for celebrity posts), that's 10M queries—still expensive but distributed over time. The hybrid approach uses a follower count threshold: users with <10,000 followers use push model (fanout completes in seconds), users with ≥10,000 followers use pull model (no fanout). For followers, the feed generation logic becomes: get pre-computed feed from Redis (push-model posts from normal users) + query timelines of celebrities being followed (pull model) + merge and rank. This balances latency: normal users get instant feed updates (push), celebrities don't block the fanout system (pull), and follower feeds are still fast (<500ms) because celebrity timeline queries are cached and parallelized.",
        keyPoints: [
            "Push model for 100M followers: 100,000 batches × 100ms = 2.78 hours fanout time (unacceptable latency)",
            "Pull model avoids fanout but requires querying celebrity timelines on every follower feed load",
            "Hybrid threshold: <10K followers = push (seconds to fanout), ≥10K followers = pull (no fanout bottleneck)",
            "Follower feed = pre-computed feed (push) + celebrity timelines (pull), merged and ranked in real-time",
            "Celebrity timeline queries cached in Redis with 5-minute TTL, handling 10M concurrent requests efficiently"
        ]
    },
    {
        question: "Design a ranking algorithm for a news feed that balances recency, engagement (likes/comments/shares), user affinity, and content type. Provide specific formulas with weights, explain how to calculate affinity between users, and discuss how to avoid echo chambers where users only see content they already agree with.",
        sampleAnswer: "The ranking formula is: score = base × recency_factor × (1 + engagement_score) × affinity × content_boost × diversity_factor. Let me break down each component: (1) Recency: recency_factor = e^(-0.05h) where h = hours since post. This gives 95% weight at 1 hour, 60% at 10 hours, 37% at 20 hours—exponential decay favoring recent posts. (2) Engagement: engagement_score = (likes × 1 + comments × 2 + shares × 3) / 100. Comments weighted higher than likes (deeper engagement), shares highest (strongest signal). Divided by 100 to normalize. (3) Affinity: tracks historical interactions between users. Calculate as: affinity = (interactions_with_author / total_interactions_last_30_days). For example, if user viewed 500 posts in the last month, and 50 were from close friend Alice, affinity(user, Alice) = 50/500 = 0.1. Update affinity weekly based on likes, comments, profile visits, messages. (4) Content type: videos get 1.5× boost (highest engagement), images 1.2×, text 1.0×. (5) Diversity factor: to avoid echo chambers, track the 'viewpoint diversity' of user's recent feed using a topic clustering model (e.g., politics-left, politics-right, sports, entertainment). If user's last 20 posts were 90% politics-left, apply diversity_factor = 0.7 to future politics-left posts and 1.3× to other viewpoints. This gentle nudge (not forcing) exposes users to diverse content without feeling manipulated. Finally, add explicit 'Explore' feed with zero personalization—purely engagement-ranked posts from across the network. ML optimization: use a gradient boosted trees model (XGBoost) trained on historical data (features: recency, engagement, affinity, content type; label: did user engage with post?) to learn optimal weights instead of hand-tuning.",
        keyPoints: [
            "Recency: exponential decay (e^(-0.05h)) gives 60% weight at 10 hours, balancing fresh and evergreen content",
            "Engagement: weighted sum (likes×1 + comments×2 + shares×3) prioritizes deep engagement over passive likes",
            "Affinity: calculated as (interactions_with_author / total_interactions_last_30_days), updated weekly",
            "Diversity factor: 0.7× penalty for over-represented viewpoints, 1.3× boost for under-represented, prevents echo chambers",
            "ML optimization: XGBoost model learns optimal weights from user engagement data, adapting to individual preferences"
        ]
    },
    {
        question: "Your news feed system experiences a 10× traffic spike during a major global event (e.g., World Cup final). Feed generation latency increases from 200ms (p95) to 2 seconds (p95). Diagnose the likely bottlenecks (Redis, database, ranking service, fanout lag) and propose specific scaling strategies with capacity calculations. Additionally, explain how to implement graceful degradation if some services remain overloaded.",
        sampleAnswer: "First, diagnose bottlenecks by monitoring key metrics: (1) Redis latency: if p95 increases from 2ms to 50ms, Redis is overloaded (too many concurrent reads). (2) Database query time: if queries go from 20ms to 500ms, database is bottleneck (celebrity timeline queries overwhelming replicas). (3) Ranking service CPU: if CPU hits 90%+, scoring calculations are the bottleneck. (4) Fanout backlog: if Kafka consumer lag grows (posts stuck in queue for minutes), fanout workers are overwhelmed. For the World Cup scenario, the likely bottleneck is celebrity timeline queries—millions of users following the same sports stars refresh their feeds simultaneously, causing 10M concurrent queries to celebrity timelines. Solution: (1) Scale database read replicas: add 10× replicas (100 → 1000) using auto-scaling (RDS Aurora, or Kubernetes horizontal pod autoscaler). (2) Aggressive caching: celebrity timelines cached in Redis with 1-minute TTL (normally 5 minutes), accept slight staleness. Pre-warm caches before event (predict which celebrities will be active). (3) CDN edge caching: cache feed responses for 30 seconds at edge locations (CloudFlare, CloudFront)—during rapid-refresh periods, serve cached feeds instead of regenerating. (4) Ranking service: scale horizontally (10× instances), simplify ranking formula (skip ML model, use simple recency × engagement). Graceful degradation if overload persists: (1) Disable real-time WebSocket updates—users must refresh manually (save 50% server load). (2) Increase feed cache TTL from 1 hour to 6 hours—trade freshness for capacity. (3) Limit pull-model queries: if celebrity timeline queries fail (timeout), skip celebrity posts temporarily and show only push-model posts (degraded feed but functional). (4) Shed load: rate limit feed requests per user (max 10 refreshes/minute), return HTTP 503 for excess requests with Retry-After header. (5) Static feed: serve pre-computed 'Top World Cup Moments' feed (batch-generated every 5 minutes) to all users if personalization fails. Monitor recovery: once traffic drops to 3× normal, re-enable WebSocket, ranking ML, and normal cache TTLs. Post-event analysis: calculate cost (1000 replicas × $5/hour × 4 hours = $20K for the event) and determine if worth the UX benefit.",
        keyPoints: [
            "Diagnose via metrics: Redis latency, database query time, ranking CPU, Kafka consumer lag identify bottleneck",
            "Celebrity timeline queries likely bottleneck during global events—scale read replicas 10×, cache aggressively (1-min TTL)",
            "Graceful degradation: disable WebSocket, extend cache TTL 6×, skip celebrity posts if queries timeout",
            "Load shedding: rate limit refreshes (10/min per user), return 503 for excess, serve static 'Top Moments' feed as fallback",
            "Cost analysis: 1000 replicas for 4 hours = $20K, trade-off between UX (low latency) and cost (auto-scaling)"
        ]
    }
];

