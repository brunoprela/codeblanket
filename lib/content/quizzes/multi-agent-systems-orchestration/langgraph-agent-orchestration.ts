/**
 * Quiz questions for LangGraph for Agent Orchestration section
 */

export const langgraphagentorchestrationQuiz = [
  {
    id: 'maas-langgraph-q-1',
    question:
      "Explain how LangGraph's state management differs from manual state passing between agents. What are the benefits and potential pitfalls of LangGraph's approach?",
    hint: 'Think about how state updates are merged and accessed.',
    sampleAnswer:
      '**Manual State Passing:** Agent A returns dict, pass to Agent B as input. Agent B returns dict, pass to Agent C. State is explicit at each step. Example: state1 = agentA(input). state2 = agentB(state1). state3 = agentC(state2). **LangGraph Approach:** Defines StateGraph with typed state (TypedDict). Each node (agent) receives full state, returns partial updates. LangGraph merges updates into state. Example: class State(TypedDict): messages: list, result: str. def agentA(state): return {"messages": state["messages"] + ["A done"]}. def agentB(state): return {"result": "B result"}. LangGraph maintains state, passes to each node, merges returns. **Benefits:** (1) **Accumulated State:** Use operator.add to accumulate (messages list grows across agents). Each agent adds to messages without explicitly passing entire list. (2) **Partial Updates:** Agent only returns fields it modifies. Don\'t need to return entire state. Cleaner code. (3) **Type Safety:** TypedDict ensures all agents use same state structure. Catches errors at definition time. (4) **Automatic Persistence:** LangGraph can checkpoint state between steps. Easy to pause/resume workflows. (5) **Simplifies Complex Flows:** For 10+ agents, not manually threading state through each. LangGraph handles it. **Pitfalls:** (1) **Implicit Data Flow:** Less obvious where data comes from. Agent B reads state["research"] - who set it? Must trace back through graph. Manual passing is explicit: agentB(research). (2) **State Bloat:** Easy to keep adding to state. State grows large with unused data. Manual: Only pass what\'s needed. (3) **Harder Debugging:** State is "magic" - LangGraph manages it. If state is wrong, harder to find which agent caused it. Manual: Clear at each step. (4) **Accidental Overwrites:** If two agents return same key, last one wins. Easy to accidentally overwrite. Example: agentA returns {"result": "A"}. agentB returns {"result": "B"}. Final state has "B", "A" lost. **Best Practices:** (1) Use Annotated with operator.add for lists: Annotated[list, operator.add] (accumulates, doesn\'t overwrite). (2) Clear naming: "research_results" not just "results". (3) Document state structure clearly. (4) Limit state to only what\'s shared. Agent-private data stays in agent. **When to Use:** LangGraph state: Complex workflows (5+ nodes), need checkpointing, multiple agents sharing data. Manual state: Simple workflows (2-3 steps), need explicit control, debugging is priority.',
    keyPoints: [
      'LangGraph: Centralized state with automatic merging',
      'Benefits: Accumulated state, partial updates, type safety',
      'Pitfalls: Implicit data flow, state bloat, accidental overwrites',
      'Best for complex workflows with many agents sharing state',
    ],
  },
  {
    id: 'maas-langgraph-q-2',
    question:
      'Design a LangGraph workflow for content generation with quality-based iteration: generate → evaluate → (if quality >= 0.8: end, else: revise → evaluate). How do you prevent infinite loops while maximizing quality?',
    hint: 'Consider max iterations and quality improvement tracking.',
    sampleAnswer:
      '**State Definition:** class ContentState(TypedDict): content: str. quality_score: float. iteration: int. max_iterations: int. quality_history: Annotated[list, operator.add]. feedback: str. **Nodes:** (1) generate_node: Creates or revises content. If iteration == 0: Generate from scratch. Else: Revise based on feedback. Returns: {content, iteration + 1}. (2) evaluate_node: Scores quality (0-1). Returns: {quality_score, quality_history + [score], feedback}. **Conditional Edge Function:** def should_continue(state) -> str: if state["quality_score"] >= 0.8: return "end". # Good enough, if state["iteration"] >= state["max_iterations"]: return "end". # Max iterations reached, if state["iteration"] >= 2 and not improving(state["quality_history"]): return "end". # Not making progress, return "revise". # Try again, def improving(history) -> bool: if len(history) < 2: return True. # Not enough data, last_two = history[-2:]. return last_two[1] > last_two[0] + 0.05. # Improved by at least 0.05. **Graph Structure:** workflow = StateGraph(ContentState). workflow.add_node("generate", generate_node). workflow.add_node("evaluate", evaluate_node). workflow.set_entry_point("generate"). workflow.add_edge("generate", "evaluate"). workflow.add_conditional_edges("evaluate", should_continue, {"revise": "generate", "end": END}). **Execution:** app = workflow.compile(). result = await app.ainvoke({content: "", quality_score: 0, iteration: 0, max_iterations: 5, quality_history: [], feedback: ""}). **Loop Prevention Strategies:** (1) **Max Iterations:** Hard limit (5). Prevents infinite loops regardless of quality. (2) **Improvement Requirement:** If quality doesn\'t improve by 0.05 between iterations, stop. Not making progress → further iterations wasteful. (3) **Quality Threshold:** Accept >=0.8. Could lower if max iterations reached and quality is "acceptable" (e.g., >=0.6). (4) **Time Limit:** Could add timestamp and timeout after 5 minutes. (5) **Cost Limit:** Track API costs, stop if exceeding budget. **Handling Edge Cases:** All iterations fail (quality stays <0.5): Return best attempt with warning: {content: best_content, quality: 0.47, warning: "Did not meet quality threshold"}. Quality decreases: Stop immediately (revision made it worse). First iteration already >=0.8: Stop immediately (got lucky). **Optimization:** Start with temperature=0.7 for generation. If quality <0.6 after 2 iterations: Increase temperature to 0.9 (more creative). Or: Try different prompting approach. **Result Structure:** If ended by quality: {success: True, content, quality: 0.85, iterations: 3}. If ended by max iterations: {success: False, content, quality: 0.72, iterations: 5, message: "Max iterations"}. If ended by no improvement: {success: False, content, quality: 0.68, iterations: 3, message: "No improvement"}.',
    keyPoints: [
      'Prevent infinite loops with max iterations',
      'Require improvement between iterations (not just quality threshold)',
      'Track quality history to detect stagnation',
      'Have multiple exit conditions: quality reached, max iterations, no improvement',
    ],
  },
  {
    id: 'maas-langgraph-q-3',
    question:
      "Compare LangGraph's checkpointing feature to building your own state persistence. When is checkpointing essential, and how would you implement resume-from-checkpoint in a long-running workflow?",
    hint: 'Think about failure recovery and human-in-the-loop scenarios.',
    sampleAnswer:
      '**LangGraph Checkpointing:** Save state after each node execution. Can resume from any checkpoint. Implemented with: from langgraph.checkpoint.memory import MemorySaver. checkpointer = MemorySaver(). app = workflow.compile(checkpointer=checkpointer). **Manual State Persistence:** After each agent: save_state_to_db(state). On failure: state = load_state_from_db(), resume from last agent. More control but more code. **When Checkpointing Essential:** (1) **Long-Running Workflows:** 30+ minute workflows. Failure midway (API timeout, crash) would lose all progress. With checkpoints: Resume from last successful node. (2) **Human-in-the-Loop:** Workflow pauses for human approval. System might restart before approval received. Need to restore exact state where it paused. (3) **Costly Operations:** If nodes call expensive APIs ($0.50 per call). Don\'t want to re-run successful nodes on failure. (4) **Multi-Step Research:** Agent 1 researches (10 min), Agent 2 analyzes (10 min). If Agent 2 fails, don\'t re-do Agent 1\'s research. (5) **Debugging:** Can inspect state at each checkpoint. See exactly what each node produced. **Resume-from-Checkpoint Implementation:** # Save checkpoints, config = {"configurable": {"thread_id": "workflow_123"}}. try: result = await app.ainvoke(initial_state, config=config). except Exception as e: print(f"Workflow failed at checkpoint: {e}"). # Later, resume, result = await app.ainvoke(None, config=config). # LangGraph loads last checkpoint. **Custom Checkpoint Manager:** class CheckpointManager: def save_checkpoint(self, workflow_id, node_id, state): db.insert({workflow_id, node_id, state, timestamp: now()}). def load_checkpoint(self, workflow_id): return db.query(workflow_id).order_by(timestamp desc).first(). def resume_workflow(self, workflow_id): checkpoint = load_checkpoint(workflow_id). # Find which node to resume from, last_completed_node = checkpoint.node_id. next_node = workflow.get_next_node(last_completed_node). # Execute from next node, await execute_from_node(next_node, checkpoint.state). **Checkpoint Storage Options:** (1) Memory: Fast, but lost on process restart. Only for testing. (2) SQLite: Persistent, local. Good for single-machine. (3) Postgres/Redis: Persistent, distributed. Good for production. (4) S3: Very durable but slower. Good for archival. **Checkpoint Granularity:** Fine-grained (after every node): Pros: Can resume from exact point. Cons: More storage, slower. Coarse-grained (after expensive nodes only): Pros: Less overhead. Cons: Might re-run some work on resume. **LangGraph vs Manual - When to Choose:** **Use LangGraph Checkpointing:** Complex graph (10+ nodes), need automatic state management, human-in-the-loop, standard use case. **Use Manual Persistence:** Simple workflow (3-5 nodes), need custom resume logic, specific storage requirements, want full control. **Example - Human Approval Workflow:** workflow pauses at "approval_needed". Save checkpoint: {state: {proposal: "..."}, waiting_for: "human_approval", checkpoint_id: "ckpt_123"}. Hours later, human approves. Resume: load checkpoint ckpt_123, continue from "approval_received" node. **Best Practices:** (1) Include timestamp in checkpoints (for expiry). (2) Clean up old checkpoints (workflows completed >7 days ago). (3) Store metadata: {workflow_id, node_id, state, timestamp, workflow_version}. (4) Version checkpoints: Handle workflow updates gracefully.',
    keyPoints: [
      'Checkpointing: Save state after each node for resume',
      'Essential for long-running workflows and human-in-the-loop',
      'LangGraph: Built-in with MemorySaver or custom storage',
      'Resume: Load checkpoint, continue from last successful node',
    ],
  },
];
