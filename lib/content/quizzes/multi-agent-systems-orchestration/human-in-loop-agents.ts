/**
 * Quiz questions for Human-in-the-Loop Agents section
 */

export const humaninloopagentsQuiz = [
  {
    id: 'maas-hitl-q-1',
    question:
      'Design a confidence-based approval system where agents automatically proceed if confidence >= 0.9, ask for human approval if 0.5 <= confidence < 0.9, and automatically reject if confidence < 0.5. How do you calibrate agent confidence scores to be reliable?',
    hint: 'Think about how to train agents to be well-calibrated on their confidence.',
    sampleAnswer:
      "**System Design:** if confidence >= 0.9: auto_approve(). elif confidence >= 0.5: request_human_approval(). else: auto_reject(). **Challenge:** Agent confidence might be miscalibrated. Says 0.95 but actually wrong. **Calibration Problem:** Overconfident agent: Always outputs 0.95 confidence, even when uncertain. Underconfident: Always outputs 0.6, even when correct. **Calibration Strategy:** (1) **Historical Analysis:** Track: agent_predictions with confidence, actual_outcomes (approved/rejected by human). Calculate: For confidence bucket 0.9-1.0, what % were actually approved? If 95% approved → well calibrated. If only 60% approved → overconfident. (2) **Calibration Curve:** Plot: x-axis = predicted confidence, y-axis = actual approval rate. Ideal: Diagonal line (predicted=actual). Reality: Might be S-curve or other shape. Apply calibration function: calibrated_confidence = calibration_function (raw_confidence). Example: If agent says 0.95 but historically only 0.80 approval rate, calibrate to 0.80. (3) **Confidence Prompting:** Explicitly prompt agent to explain confidence: \"Rate your confidence 0-1 and explain reasoning. Consider: complexity, ambiguity, prior experience with similar cases.\" Agent forced to think about confidence, not just guess. (4) **Multi-Agent Voting:** Run 3 agents independently. If all agree → high confidence (0.95). If 2/3 agree → medium (0.7). If split → low (0.4). Aggregate confidence more reliable than single agent. (5) **Temperature-Based Confidence:** Generate multiple samples at different temperatures. If all similar → high confidence. If vary wildly → low confidence. confidence = 1 - variance (samples). (6) **Active Learning Loop:** Start conservative (require human approval for more cases). As system learns (humans approve 90%+ of agent's 0.8 confidence calls), gradually trust agent more. Raise threshold: Initially require human for <0.95. After 100 good decisions, require human for <0.90. (7) **Feature-Based Confidence:** Confidence based on task features: Simple task (seen 100x before): +0.2 confidence. Complex task (new type): -0.2 confidence. High stakes (affects many users): Require human regardless. **Implementation:** class CalibratedAgent: def __init__(self): self.history = []. # (confidence, approved). def get_calibrated_confidence (self, raw_confidence): # Apply calibration based on history, bucket = int (raw_confidence * 10) / 10. # Round to 0.1. historical = [h for h in self.history if bucket <= h[0] < bucket + 0.1]. if len (historical) < 10: return raw_confidence * 0.8. # Be conservative with little data, approval_rate = sum (h[1] for h in historical) / len (historical). return approval_rate. # Use actual approval rate as calibrated confidence. def record_outcome (self, confidence, approved): self.history.append((confidence, approved)). **Handling Edge Cases:** Agent says 0.91 (auto-approve) but human would reject: Record as miscalibration. Lower agent's confidence scores. Agent says 0.49 (auto-reject) but human would approve: Record as underconfidence. Increase agent's scores. **Monitoring:** Track per week: Auto-approve rate, auto-reject rate, human approval rate, calibration error (|predicted - actual|). Alert if calibration error > 0.15 (need recalibration). **Starting Point:** Start with conservative thresholds: auto_approve >= 0.95, human_required < 0.95 and >= 0.3, auto_reject < 0.3. After 1000 decisions, analyze data and adjust thresholds.",
    keyPoints: [
      'Agent confidence needs calibration based on historical accuracy',
      'Compare predicted confidence to actual approval rates',
      'Start conservative, gradually trust agent as it proves reliable',
      'Use multiple agents or sampling to get better confidence estimates',
    ],
  },
  {
    id: 'maas-hitl-q-2',
    question:
      "Your approval system has a 5-minute timeout for human responses, but critical workflows can't wait that long. Design a system that handles human unavailability gracefully with fallback strategies.",
    hint: 'Consider alternative approvers and temporary decisions.',
    sampleAnswer:
      '**Problem:** Workflow blocks waiting for human approval. Human unavailable or slow. Workflow can\'t proceed. **Fallback Strategies:** (1) **Escalation Chain:** Don\'t wait for one person. Request approval from: Primary approver (2 min timeout). If no response → Secondary approver (2 min). If no response → Manager (1 min). If still no response → Apply fallback decision. (2) **Provisional Approval:** Make temporary decision based on: Agent recommendation (if confidence >= 0.8). Historical patterns (if similar past decisions exist). Proceed provisionally. When human returns, they review provisional decision. Can rollback if disapproved. (3) **Degraded Mode:** Define two modes: Full mode: Human approval required. Degraded mode: Auto-proceed with logging. If humans consistently unavailable, switch to degraded mode temporarily. Send urgent alerts. (4) **Quorum Voting:** Instead of single human, ask multiple humans (3-5). Proceed if majority (2/3) approve within timeout. More resilient to individual unavailability. (5) **Time-Based Fallback:** Low-stakes decisions: 1 min timeout → proceed automatically. Medium-stakes: 5 min timeout → escalate to manager. High-stakes: 30 min timeout → abort and retry later. **Implementation:** class ResilientApprovalGate: async def request_approval (self, request, approvers, timeout=300): # Try primary approver, result = await self.try_approver (approvers[0], request, timeout=120). if result: return result. # Try secondary, result = await self.try_approver (approvers[1], request, timeout=120). if result: return result. # Fallback decision, return await self.make_fallback_decision (request). async def make_fallback_decision (self, request): if request.confidence >= 0.8: # High confidence → provisional approval, return ProvisionalApproval (approved=True, reason="High confidence, human unavailable"). elif request.stakes == "low": # Low stakes → auto-proceed, return AutoApproval (approved=True, reason="Low stakes"). else: # High stakes, low confidence → defer, return Deferred (reason="Requires human approval"). **Provisional Approval Handling:** Mark workflow as "provisionally approved". Continue execution but with flag: provisional=True. Log all actions taken under provisional approval. When human returns: Display provisional decisions. Human can: Approve retroactively (no action). Reject → Trigger rollback procedure. **Rollback Procedure:** If workflow already completed under provisional: Can\'t undo (e.g., email sent). Log as "approved provisionally, rejected retroactively". Alert for manual cleanup. If workflow still in progress: Pause, make corrections, resume. **Monitoring:** Track: Average human response time. Timeout rate (% of requests that timeout). Provisional approval rate. Retroactive rejection rate. If timeout rate > 20% → adjust: Increase timeout, or lower threshold for auto-proceed, or add more approvers. **User Experience:** Don\'t make humans feel rushed. Show: "Your approval needed (2 minutes remaining)". After 1 minute: Send reminder notification. After 2 minutes: Escalate, but still allow response. **Priority Queue:** If human has 10 pending approvals: Show highest priority first. Low-stakes auto-proceed without human. Batch similar approvals: "Approve all 5 similar requests?"',
    keyPoints: [
      'Escalation chain: Try multiple approvers in sequence',
      'Provisional approval: Proceed tentatively, allow retroactive review',
      'Time-based fallback: Different timeouts for different stakes',
      'Track metrics and adjust timeouts based on human availability',
    ],
  },
  {
    id: 'maas-hitl-q-3',
    question:
      'Design a progressive automation system that gradually reduces human involvement as the system proves reliable. How do you measure reliability, and what are the risks of automating too quickly?',
    hint: 'Think about learning curves and safety thresholds.',
    sampleAnswer:
      "**Goal:** Start with heavy human oversight. Gradually automate as system proves reliable. **Reliability Metrics:** (1) **Approval Rate:** % of agent proposals approved by humans. If 95%+ approved → agent is reliable. (2) **Error Rate:** % of automated decisions later found to be wrong. If <5% errors → safe to automate more. (3) **Confidence Calibration:** How often agent's confidence matches actual outcome. Well-calibrated confidence → can trust confidence-based automation. **Progressive Automation Levels:** Level 0 (Bootstrap): All decisions require human approval. Agent proposes, human decides. Collect data: 100+ decisions. Level 1 (Assisted): Agent can auto-proceed on simple cases only. Simple = seen 50+ similar cases, 95%+ historical approval. Still need human for new/complex cases. Collect data: 500+ decisions. Level 2 (Supervised): Agent auto-proceeds if confidence >= 0.85. Human approval if confidence < 0.85. Collect data: 1000+ decisions. Level 3 (Autonomous): Agent auto-proceeds if confidence >= 0.7. Human spot-checks 5% randomly. Collect data: Ongoing. Level 4 (Trusted): Agent auto-proceeds on everything. Human reviews only on errors/complaints. **Advancement Criteria:** Level 0 → 1: 100 decisions, 90%+ approved, no critical errors. Level 1 → 2: 500 decisions, 95%+ approved, <3% errors. Level 2 → 3: 1000 decisions, 97%+ approved, <2% errors, well-calibrated confidence. Level 3 → 4: 5000 decisions, 98%+ approved, <1% errors, zero critical errors. **Risks of Automating Too Quickly:** (1) **Overfitting to Early Data:** First 100 decisions might be easy cases. Agent looks good. Next 100 are harder → agent fails. Mitigation: Require diverse cases before advancing. (2) **Drift:** Agent reliable initially, but environment changes (new product features, user behaviors). Agent becomes less reliable over time. Mitigation: Continuous monitoring, auto-downgrade if metrics worsen. (3) **Catastrophic Errors:** Agent makes one horrible decision that wasn't caught. Damages reputation/trust. Mitigation: Keep human oversight for high-stakes decisions regardless of level. Critical decisions never fully automated. (4) **Gaming Metrics:** Agent learns to optimize approval rate, not correctness. Might propose overly conservative decisions to avoid rejection. Mitigation: Track outcome metrics, not just approval rate. Measure actual business impact. **Implementation:** class ProgressiveAutomation: def __init__(self): self.level = 0. self.decision_history = []. def get_automation_level (self): # Check if ready to advance, if self.level == 0 and self.can_advance_to_1(): self.level = 1. elif self.level == 1 and self.can_advance_to_2(): self.level = 2. return self.level. def can_advance_to_1(self): if len (self.decision_history) < 100: return False. approval_rate = self.calculate_approval_rate(). return approval_rate >= 0.90. def should_auto_proceed (self, decision): level = self.get_automation_level(). if level == 0: return False. # Always need human, elif level == 1: return decision.is_simple and decision.has_precedent. elif level == 2: return decision.confidence >= 0.85. elif level == 3: return decision.confidence >= 0.70. elif level == 4: return True. # Always auto-proceed. **Downgrade Triggers:** If error rate spikes to >10%: Downgrade one level immediately. Alert team. If approval rate drops below 80%: Downgrade one level. If single critical error occurs: Drop to Level 0 (full human oversight) until reviewed. **Monitoring Dashboard:** Show: Current automation level, decisions per level (how many auto, how many human), reliability metrics over time, advancement progress (80/100 decisions for Level 1). **Gradual Rollout:** Don't advance globally. Test new level on: 10% of traffic for 1 week. If metrics good, expand to 50%. If still good, expand to 100%. If any issues, rollback.",
    keyPoints: [
      'Start with full human oversight, gradually increase automation',
      'Define clear advancement criteria based on approval rate and errors',
      'Risks: Overfitting, drift, catastrophic errors from too-fast automation',
      'Implement downgrades and continuous monitoring to catch regressions',
    ],
  },
];
