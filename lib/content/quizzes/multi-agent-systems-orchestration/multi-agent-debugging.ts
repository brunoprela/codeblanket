/**
 * Quiz questions for Multi-Agent Debugging section
 */

export const multiagentdebuggingQuiz = [
  {
    id: 'maas-debug-q-1',
    question:
      'You have a 5-agent system where Agent C frequently fails, but only when it receives output from Agent B (not when tested independently). Design a debugging strategy to identify the root cause. What logging, tracing, and testing would you implement?',
    hint: 'Consider input validation and comparing working vs failing cases.',
    sampleAnswer:
      '**Problem:** Agent C works in isolation, fails after Agent B. Issue is in interaction, not Agent C itself. **Debugging Strategy:** (1) **Logging - Capture Inputs:** Log exact input Agent C receives from B: log.info(f"Agent C received from B: {input[:200]}"). Compare input when C succeeds vs fails. Look for: Format differences, unexpected values, missing fields, data corruption. (2) **Distributed Tracing:** Add trace ID flowing through entire chain: A → B → C. Log trace ID at each step. When C fails, trace back through B and A with same trace ID. See full context of what happened before failure. (3) **Input Validation:** Add explicit validation in C: def validate_input(input): assert isinstance(input, dict), "Input not dict". assert "required_field" in input, "Missing field". If validation fails, log exactly what was wrong and who sent it. (4) **Differential Testing:** Test C with: (a) Known-good input (from test suite) → passes. (b) Actual input from B (captured from logs) → fails. Compare inputs character-by-character. Find difference. (5) **Reproduce in Isolation:** Extract exact input from failed run. Test C in isolation with that input: test_agent_c(input_from_failing_run). Should reproduce failure without needing full system. Now can debug C with debugger. (6) **Check Assumptions:** What does C assume about B\'s output? C expects dict with key "research_results". B might sometimes return string or None. Add assertion: assert input is not None and "research_results" in input. (7) **Version Mismatch:** Check if B\'s output format changed but C wasn\'t updated. Add schema versioning: output = {"version": "1.0", "data": {...}}. (8) **Timing Issues:** Maybe B occasionally returns incomplete data due to timeout. Check if B\'s output has "status": "complete". **Specific Logging Implementation:** class LoggedAgent: async def execute(self, input): log.info(f"[{self.name}] INPUT: {json.dumps(input, indent=2)}"). try: output = await self._do_work(input). log.info(f"[{self.name}] SUCCESS: {json.dumps(output, indent=2)}"). return output. except Exception as e: log.error(f"[{self.name}] FAILED: {e}"). log.error(f"[{self.name}] INPUT was: {input}"). raise. **Analysis Process:** (1) Run system until C fails. (2) Get trace ID from failure log. (3) Search logs for that trace ID. (4) See: Agent B output: {"result": "text with \\\\"quotes\\\\""}. Agent C input: {"result": "text with "quotes""}. (5) Aha! JSON escaping issue. B outputs valid JSON, but somewhere in transmission quotes get unescaped. C tries to parse, fails. (6) Fix: Ensure proper JSON serialization/deserialization between agents. **Prevention:** Add integration tests: test_a_to_b_to_c() runs full chain, validates each handoff.',
    keyPoints: [
      'Capture exact inputs/outputs at each agent',
      'Use distributed tracing to follow execution chain',
      'Reproduce issue in isolation with captured input',
      'Compare working vs failing inputs to find differences',
    ],
  },
  {
    id: 'maas-debug-q-2',
    question:
      'Your multi-agent system is 2x slower than expected. Design a profiling approach to identify bottlenecks. How do you distinguish between: (1) slow LLM calls, (2) coordination overhead, (3) unnecessary serial execution that could be parallelized?',
    hint: 'Think about timing each component and analyzing dependencies.',
    sampleAnswer:
      '**Expected:** 5 agents × 10 seconds each = 50 seconds. **Actual:** 100 seconds. Where is extra 50 seconds? **Profiling Approach:** (1) **Time Each Agent:** Record start and end time for each agent execution. Agent A: 12 seconds. Agent B: 25 seconds. Agent C: 10 seconds. Agent D: 11 seconds. Agent E: 9 seconds. Total agent time: 67 seconds. But total time is 100 seconds. Missing 33 seconds = coordination overhead. (2) **Time Coordination:** Log time between agent completions: Agent A completes at t=12. Agent B starts at t=15. Gap = 3 seconds (coordination). Breakdown: Message serialization (0.1s), message queue (0.5s), deserialization (0.1s), agent startup (2.3s). Biggest: Agent startup. Optimize: Keep agents warm, don\'t create new instance each time. (3) **Identify Slow LLM Calls:** Within Agent B (25 seconds): LLM call 1: 15 seconds. LLM call 2: 8 seconds. Internal processing: 2 seconds. Agent B is slow because of long LLM calls (expected 10s, actual 23s). Why? Check: Prompt size (Agent B has 5K token prompt vs others 1K). Model used (Agent B using gpt-4 instead of gpt-3.5). Temperature (high temperature = longer generation). Fix: Reduce prompt size, use faster model if possible, lower temperature. (4) **Check Serialization:** All agents execute sequentially: A → B → C → D → E. But: B depends on A. C, D, E don\'t depend on B. Can parallelize C, D, E while B runs! Current: A(12) → B(25) → C(10) → D(11) → E(9) = 67 seconds. Optimal: A(12) → [B(25) || C(10) || D(11) || E(9)] = 12 + max(25, 10, 11, 9) = 12 + 25 = 37 seconds. **Profiling Implementation:** import time. class ProfiledAgent: async def execute(self, input): start = time.time(). self.log(f"Started"). llm_start = time.time(). result = await llm_call(). llm_time = time.time() - llm_start. self.log(f"LLM took {llm_time:.2f}s"). processing_start = time.time(). processed = process(result). processing_time = time.time() - processing_start. total_time = time.time() - start. self.log(f"Total {total_time:.2f}s: LLM={llm_time:.2f}s, Processing={processing_time:.2f}s"). return processed. **Analysis Results:** (1) Agent B\'s LLM calls: 23s (too slow) → Reduce prompt from 5K to 2K tokens → Now 12s. Savings: 11s. (2) Coordination overhead: 3s per agent × 5 agents = 15s → Keep agents warm (don\'t recreate) → Now 0.5s each = 2.5s. Savings: 12.5s. (3) Unnecessary serialization → Parallelize C, D, E → Saves: 10+11+9=30s, now they run during B (25s), saves 5s. **New Total:** 37s (agent time) + 2.5s (coordination) = 39.5s. Was 100s, now 39.5s → 2.5x speedup! **Monitoring Dashboard:** Show: Per-agent execution time, LLM call duration, coordination overhead, parallelization opportunities. Alert if any agent >2x expected time.',
    keyPoints: [
      'Time each component: agents, LLM calls, coordination',
      'Identify coordination overhead between agents',
      'Find unnecessary serialization and parallelize',
      'Optimize largest bottlenecks first for maximum impact',
    ],
  },
  {
    id: 'maas-debug-q-3',
    question:
      'Design a replay system that can reproduce bugs by re-executing a failed multi-agent workflow with the exact same inputs and conditions. What do you need to capture, and what are the challenges?',
    hint: 'Consider non-deterministic LLM outputs and external dependencies.',
    sampleAnswer:
      '**Goal:** Reproduce bug by replaying failed workflow. **What to Capture:** (1) **Initial Input:** Exact input that started workflow. (2) **Agent Inputs:** Input to each agent at each step. (3) **LLM Responses:** Exact output from each LLM call (non-deterministic!). (4) **External API Calls:** Responses from web searches, databases, etc. (5) **Timestamps:** When each event occurred. (6) **Randomness Seeds:** Any random number generation. (7) **Environment:** Agent versions, model versions, configuration. **Challenges:** (1) **Non-Deterministic LLMs:** Same prompt to GPT-4 can give different outputs. Can\'t reliably reproduce LLM output. Solution: Record LLM responses, replay with recorded responses (not actual LLM). (2) **External Dependencies:** Web search, database queries return different results over time. Solution: Record external call responses at capture time, replay with recorded responses (mock mode). (3) **Timing:** Some bugs only appear under specific timing (race conditions). Solution: Record exact timing, replay with same delays. (4) **State Mutations:** Agents might mutate shared state. Order matters. Solution: Record all state mutations with timestamps. **Capture Implementation:** class ReplayRecorder: def __init__(self, workflow_id): self.workflow_id = workflow_id. self.events = []. async def record_agent(self, agent_name, input, output): self.events.append({type: "agent", agent: agent_name, input: input, output: output, timestamp: time.time()}). async def record_llm_call(self, prompt, response): self.events.append({type: "llm", prompt: prompt, response: response, timestamp: time.time()}). async def record_external(self, api, request, response): self.events.append({type: "external", api: api, request: request, response: response, timestamp: time.time()}). def save(self): with open(f"replay_{self.workflow_id}.json", "w") as f: json.dump({workflow_id: self.workflow_id, events: self.events, environment: get_environment()}, f). **Replay Implementation:** class ReplayPlayer: def __init__(self, replay_file): self.events = json.load(open(replay_file)). self.event_index = 0. async def get_next_llm_response(self): event = self.events[self.event_index]. assert event["type",] == "llm". self.event_index += 1. return event["response",]. # Return recorded response, don\'t call actual LLM. async def get_next_external_response(self, api): event = self.events[self.event_index]. assert event["type",] == "external" and event["api",] == api. self.event_index += 1. return event["response",]. **Replay Execution:** def replay_workflow(replay_file): player = ReplayPlayer(replay_file). # Monkey-patch LLM calls to use replay, original_llm_call = llm_call. def mocked_llm_call(prompt): return player.get_next_llm_response(). llm_call = mocked_llm_call. # Run workflow, result = await run_workflow(player.events[0]["input",]). **Verification:** After replay, compare: (1) Agent outputs match recorded outputs? (2) Bug reproduced (same error)? (3) Execution order matches? **Limitations:** (1) Only works if bug is deterministic given same inputs/responses. (2) If bug is in LLM response itself, can\'t replay (that response is recorded as-is). Would need to regenerate with LLM. (3) If environment changed (agent code updated), replay might not match. **Use Cases:** (1) Reproduce coordination bugs (agent ordering, message passing). (2) Reproduce integration bugs (how agents interact). (3) Test fixes: Replay failed workflow with fix applied. Should succeed. (4) Performance analysis: Replay to profile without API costs.',
    keyPoints: [
      'Capture all inputs, outputs, and external responses',
      'Record LLM responses to handle non-determinism',
      'Mock external dependencies during replay',
      'Useful for coordination bugs and testing fixes',
    ],
  },
];
