/**
 * Quiz questions for CrewAI & Agent Frameworks section
 */

export const crewaiagentframeworksQuiz = [
  {
    id: 'maas-crewai-q-1',
    question:
      'Compare building a multi-agent system from scratch vs using CrewAI. What are the trade-offs in terms of flexibility, development speed, and learning curve? When would you choose each approach?',
    hint: 'Consider customization needs and time constraints.',
    sampleAnswer:
      "**From Scratch:** Full control, custom everything. **CrewAI:** Abstractions for agents, tasks, crews. **Development Speed:** From Scratch: Weeks. Must build: agent execution engine, task queue, communication protocol, error handling, memory management. CrewAI: Days/Hours. Define agents and tasks, built-in coordination. Example: 3-agent system - From scratch: 2-3 weeks. CrewAI: 2-3 days. **Flexibility:** From Scratch: Total flexibility. Custom coordination strategies, unique communication patterns, specific optimizations. CrewAI: Limited to framework patterns. Sequential or hierarchical process. Can't easily do complex DAG workflows or custom state machines without fighting framework. **Learning Curve:** From Scratch: Steep. Must understand multi-agent concepts, coordination, error handling, state management. CrewAI: Moderate. Learn CrewAI API (agents, tasks, crews, tools). Still need multi-agent concepts but framework handles complexity. **Debugging:** From Scratch: Full visibility. Add logging wherever needed. Step through own code. CrewAI: Some black box. Framework handles coordination - harder to debug interactions. Need to understand framework internals. **Dependencies:** From Scratch: Minimal. Only need LLM SDK. CrewAI: Framework dependency. If CrewAI changes API or has bugs, affected. Must stay on supported versions. **When to Choose From Scratch:** (1) Unique requirements not supported by frameworks (custom coordination patterns). (2) Need maximum performance optimization. (3) Don't want framework lock-in. (4) Learning exercise (understand deeply). (5) Production system needing complete control. **When to Choose CrewAI:** (1) Standard use case (research, write, review workflows). (2) Prototype/MVP - speed matters. (3) Small team (leverage community examples). (4) Role-based agents fit naturally (product manager, engineer, QA). (5) Don't want to maintain coordination infrastructure. **Hybrid Approach:** Start with CrewAI for MVP. If hit limitations, gradually migrate critical parts to custom code. Or: Use CrewAI for standard workflows, custom code for specialized needs. **Real Example:** Building content creation pipeline (research → write → edit). CrewAI: Perfect fit. Role-based agents, sequential process. Done in a day. Building complex software development simulation with branching, loops, human approvals. From scratch: Framework can't handle complexity. Need custom DAG workflow.",
    keyPoints: [
      'CrewAI: Faster development but less flexibility',
      'From scratch: Full control but more time and complexity',
      'Choose CrewAI for standard workflows and MVPs',
      'Choose custom for unique requirements and maximum control',
    ],
  },
  {
    id: 'maas-crewai-q-2',
    question:
      'Design a CrewAI crew for a software development workflow with proper role assignments, task dependencies, and tool usage. Include Product Manager, Developer, and QA roles. How do you ensure tasks are properly sequenced?',
    hint: 'Think about what each role needs from previous roles.',
    sampleAnswer:
      '**Agents with Roles:** pm = Agent (role="Product Manager", goal="Define clear requirements and user stories", backstory="Experienced PM who translates business needs into technical requirements", tools=[web_search, document_reader], allow_delegation=False). dev = Agent (role="Senior Developer", goal="Implement features following requirements", backstory="Expert developer who writes clean, tested code", tools=[code_generator, file_writer], allow_delegation=False). qa = Agent (role="QA Engineer", goal="Ensure quality through comprehensive testing", backstory="Thorough QA who catches all issues", tools=[test_runner, bug_tracker], allow_delegation=True). # Can delegate test execution. **Tasks with Dependencies:** task1_requirements = Task (description="Create requirements document for {feature}", agent=pm, expected_output="Requirements doc with acceptance criteria"). task2_implementation = Task (description="Implement {feature} following requirements from previous task", agent=dev, expected_output="Source code with comments", context=[task1_requirements]). # Depends on task1. task3_testing = Task (description="Test implementation and report any bugs", agent=qa, expected_output="Test report with pass/fail status", context=[task2_implementation]). # Depends on task2. **Crew with Proper Sequencing:** crew = Crew (agents=[pm, dev, qa], tasks=[task1_requirements, task2_implementation, task3_testing], process=Process.sequential, verbose=True). result = crew.kickoff (inputs={"feature": "user authentication"}). **How Sequencing Works:** CrewAI executes tasks in order of list. task1 completes → output saved. task2 starts with task1\'s output available via context. task2 completes → output saved. task3 starts with task2\'s output. **Ensuring Proper Dependencies:** (1) **Context Parameter:** task2 includes context=[task1]. CrewAI passes task1\'s output to task2\'s prompt. (2) **Task Order:** List tasks in dependency order: [requirements, implementation, testing]. (3) **Expected Output:** Define clear expected_output. Next task knows what format to expect. (4) **Agent Constraints:** allow_delegation=False for PM and Dev. They must do their own work. allow_delegation=True for QA. Can delegate test execution to other testers. **Adding Conditional Logic:** If QA finds bugs, loop back to Dev: Create task4_bug_fix: Task (description="Fix bugs from QA report", agent=dev, context=[task3_testing]). Create task5_retest: Task (description="Retest fixes", agent=qa, context=[task4_bug_fix]). But: CrewAI sequential doesn\'t support loops easily. Would need hierarchical process or custom logic. **Hierarchical Alternative:** crew = Crew (agents=[pm, dev, qa], tasks=[high_level_task], process=Process.hierarchical, manager_llm="gpt-4"). Manager agent (created automatically) will: Decompose high_level_task. Assign to appropriate agents. Handle iterations if needed. **Tools Benefit:** PM uses web_search to research similar features. Dev uses code_generator to scaffold code. QA uses test_runner to execute automated tests. **Output Example:** After kickoff: {task1: "Requirements: User auth with email/password, JWT tokens...", task2: "Code: [implementation]...", task3: "Test Report: 18/20 tests passed. 2 failures: [details]..."}. **Next Steps:** If QA finds bugs, could: (1) Re-run crew with task "fix bugs from previous run". (2) Use hierarchical mode for automatic iteration. (3) Manual loop: while not all_tests_pass: run crew.',
    keyPoints: [
      'Define agents with clear roles, goals, and appropriate tools',
      'Use context parameter to pass outputs between tasks',
      'Order tasks list by dependency (requirements before implementation)',
      'Sequential process for linear workflows, hierarchical for iteration',
    ],
  },
  {
    id: 'maas-crewai-q-3',
    question:
      'Your CrewAI crew is executing slowly because all agents are using the expensive gpt-4 model. Design an optimization strategy that uses cheaper models where appropriate while maintaining quality. Which agents/tasks can use gpt-3.5?',
    hint: 'Consider complexity of each task and required reasoning.',
    sampleAnswer:
      '**Current Setup (All GPT-4):** 3 agents × GPT-4 @ $0.03/1K tokens input, $0.06/1K output. Typical run: 10K input, 5K output per agent. Cost per run: 3 agents × (10 × $0.03 + 5 × $0.06) = 3 × $0.60 = $1.80. **Optimization Analysis:** Evaluate complexity of each agent\'s task. **Product Manager (Requirements):** Complexity: High. Needs to understand business context, identify edge cases, write clear requirements. Requires strong reasoning. Keep on GPT-4. Justification: Requirements errors cascade to all downstream work. Worth the cost. **Developer (Implementation):** Complexity: High for architecture decisions. Medium for straightforward implementation. Strategy: Use GPT-4 for initial design/architecture decisions. Use GPT-3.5-turbo for routine implementation after design done. Or: Use GPT-4 for complex features (authentication, payments). Use GPT-3.5-turbo for simple features (CRUD endpoints, basic UI). Potential savings: 50% of dev work can use GPT-3.5. GPT-3.5-turbo: $0.001/1K input, $0.002/1K output. Complex dev (GPT-4): 10K input, 5K output = $0.60. Simple dev (GPT-3.5): 10K input, 5K output = $0.01 + $0.01 = $0.02. If 50/50 split: Average = ($0.60 + $0.02) / 2 = $0.31 (48% savings). **QA Engineer (Testing):** Complexity: Medium-Low. Writing test cases is somewhat formulaic. Test execution is automated (no LLM). Strategy: Use GPT-3.5-turbo for test generation. Tests follow patterns. GPT-3.5 is sufficient. Use GPT-4 only for complex test scenario reasoning (security tests, edge cases). Savings: 80% of QA work can use GPT-3.5. Mostly GPT-3.5: $0.02. **Optimized Configuration:** pm_agent = Agent(..., llm="gpt-4"). # High complexity. dev_agent = Agent(..., llm="gpt-3.5-turbo"). # Mostly routine. qa_agent = Agent(..., llm="gpt-3.5-turbo"). # Formulaic. **New Cost:** PM (GPT-4): $0.60. Dev (GPT-3.5): $0.02. QA (GPT-3.5): $0.02. Total: $0.64 vs original $1.80 = 64% reduction! **Maintaining Quality:** (1) **Validate Agent Outputs:** Add validation tasks that check if requirements are clear, code compiles, tests are comprehensive. If validation fails, escalate to GPT-4. (2) **Hybrid Approach:** First pass with GPT-3.5. Review pass with GPT-4 (only reviews, doesn\'t regenerate). If issues found, regenerate with GPT-4. (3) **Task-Specific Models:** Split Developer into: "Architect" (GPT-4) for design. "Implementer" (GPT-3.5) for coding. (4) **Quality Metrics:** Track defect rates. If GPT-3.5 produces more bugs, revert to GPT-4. **Further Optimizations:** (1) **Prompt Optimization:** Shorter, more focused prompts reduce token usage. (2) **Caching:** Cache common completions (test boilerplate). (3) **Fine-Tuning:** Fine-tune GPT-3.5 on your specific tasks. Might match GPT-4 quality at 1/30th cost. (4) **Local Models:** For very simple tasks (formatting, basic validation), use local models (free). **Monitoring:** Track per-agent: Cost, output quality, success rate. If quality drops, upgrade that agent to GPT-4.',
    keyPoints: [
      'Not all tasks need most expensive model',
      'Use GPT-4 for high-complexity reasoning (requirements, architecture)',
      'Use GPT-3.5-turbo for formulaic tasks (routine coding, test generation)',
      'Monitor quality metrics to ensure cheaper models maintain standards',
    ],
  },
];
