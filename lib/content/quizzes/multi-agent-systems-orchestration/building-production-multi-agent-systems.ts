/**
 * Quiz questions for Building Production Multi-Agent Systems section
 */

export const buildingproductionmultiagentsystemsQuiz = [
  {
    id: 'maas-prod-q-1',
    question:
      'Design a production deployment strategy for a multi-agent system that needs zero downtime during updates. Compare blue-green deployment vs canary deployment for this use case, including rollback procedures.',
    hint: 'Consider how to handle in-flight agent workflows during deployment.',
    sampleAnswer:
      '**Blue-Green Deployment:** Two identical environments: Blue (current production), Green (new version). **Process:** (1) Deploy new version to Green environment. (2) Test Green thoroughly (health checks, smoke tests). (3) Switch traffic from Blue to Green instantly (DNS/load balancer change). (4) Keep Blue running for quick rollback if issues found. (5) After 24-48 hours of Green being stable, decommission Blue. **Pros:** Instant cutover (seconds). Easy rollback (switch back to Blue). Simple to understand. **Cons:** Requires 2x infrastructure (both environments running). What about in-flight workflows? Agent A running on Blue, switch happens, Agent B runs on Green (might be incompatible). **Handling In-Flight Workflows:** Strategy 1: Drain Blue. Stop new workflows on Blue. Wait for all in-flight workflows to complete (up to 30 min). Then switch to Green. Start new workflows on Green. Con: Workflows delayed during drain. Strategy 2: Workflow-aware routing. Tag each workflow with version. Workflow started on Blue continues on Blue until completion. New workflows go to Green. Requires routing logic: route(workflow_id) → if workflow.version == "blue": send to Blue. else: send to Green. **Canary Deployment:** Gradually shift traffic from old to new. **Process:** (1) Deploy new version to canary servers (10% of fleet). (2) Route 10% of traffic to canary. (3) Monitor metrics (error rate, latency, agent success rate). (4) If healthy after 1 hour: Expand to 25%. (5) If healthy after another hour: Expand to 50%. (6) If healthy: Expand to 100%. (7) If any issues: Rollback (route traffic back to old version). **Pros:** Limits blast radius (only 10% of users affected initially). Gradual validation in production. Can catch issues before full rollout. **Cons:** Longer deployment time (hours vs minutes). More complex routing logic. Still need strategy for in-flight workflows. **Handling In-Flight Workflows in Canary:** Option 1: Sticky routing. Once workflow starts on a version, all agents in that workflow use same version. Prevents mixed-version workflows. Implementation: workflow_sessions table: {workflow_id, version}. Route all agents in workflow to same version. Option 2: Compatible versions. Ensure new version is backward compatible. Mixed workflows (some agents on old, some on new) work correctly. Requires: versioned message formats, careful testing. **Rollback Procedures:** Blue-Green Rollback: (1) Detect issue (health check fails, error rate spike). (2) Switch traffic back to Blue (same load balancer change). (3) Takes seconds. (4) Investigate issue in Green while Blue handles traffic. Canary Rollback: (1) Detect issue in canary. (2) Route 100% traffic back to old version. (3) Takes seconds to minutes. (4) No users affected beyond initial 10%. **Comparison for Multi-Agent Systems:** Blue-Green: Better for: Quick rollback needed, don\'t want gradual rollout, have capacity for 2x infrastructure. Worse for: Limited capacity, want to limit blast radius. Canary: Better for: Want to validate gradually, limit impact of bugs, continuous deployment. Worse for: Need instant rollouts, complex routing logic. **Recommendation:** Use Canary for multi-agent systems. Reason: Agent interactions are complex, hard to catch all bugs in testing. Gradual rollout catches issues early with limited impact. Implement workflow-aware routing to handle in-flight workflows. **Metrics to Monitor During Deployment:** Agent success rates, workflow completion rates, error rates per agent, latency (P50, P95, P99), LLM API error rates, circuit breaker states. **Auto-Rollback:** If error rate > 2x baseline: Auto-rollback. If latency P95 > 1.5x baseline: Alert, manual decision. If agent X failing >20%: Rollback.',
    keyPoints: [
      'Blue-green: Fast cutover but requires 2x infrastructure',
      'Canary: Gradual rollout limits blast radius but takes longer',
      'Must handle in-flight workflows during deployment',
      'Implement workflow-aware routing for consistency',
    ],
  },
  {
    id: 'maas-prod-q-2',
    question:
      'Your production multi-agent system is hitting the $100/hour cost limit due to expensive LLM calls. Design a comprehensive cost optimization strategy that reduces costs by 50% while maintaining quality. Consider caching, model selection, and prompt optimization.',
    hint: 'Think about where costs come from and what can be optimized.',
    sampleAnswer:
      '**Current Costs Breakdown:** Assume: 1000 agent executions/hour. Average: 5K input tokens, 2K output tokens per execution. GPT-4 pricing: $0.03/1K input, $0.06/1K output. Cost per execution: 5 × $0.03 + 2 × $0.06 = $0.27. Total: 1000 × $0.27 = $270/hour. (Over limit!) **Cost Optimization Strategies:** (1) **Aggressive Caching (Target: 40% savings):** Cache LLM responses for: Identical prompts (exact match). Similar prompts (semantic similarity >0.95). Implementation: Before LLM call, check cache with hash(prompt). If hit → return cached response (cost: $0). If miss → call LLM, cache response (TTL: 1 hour). Expected hit rate: 30% (common queries repeated). Savings: 30% × $270 = $81/hour. New cost: $189/hour. (2) **Model Tiering (Target: 30% savings):** Not all tasks need GPT-4. Split agents: Complex reasoning (Requirements, Architecture): GPT-4 (30% of calls). Routine tasks (Code generation, Testing, Formatting): GPT-3.5-turbo (70% of calls). GPT-3.5-turbo: $0.001/1K input, $0.002/1K output. Cost for simple tasks: 5 × $0.001 + 2 × $0.002 = $0.009. Weighted cost: 30% × $0.27 + 70% × $0.009 = $0.081 + $0.0063 = $0.0873. Cost with caching (30% hit rate): 0.7 × $0.0873 × 1000 = $61/hour. Savings from model tiering: $189 - $61 = $128/hour. (3) **Prompt Optimization (Target: 20% savings on tokens):** Reduce prompt size without hurting quality: Remove redundant context. Use shorter variable names in few-shot examples. Compress instructions ("Use \'list format\'" vs "Please provide your answer in a bulleted list format with each item on a separate line"). Example: Reduce average prompt from 5K to 4K tokens (20% reduction). New cost: 4 × cost_factor = 80% of previous. New cost: $61 × 0.8 = $49/hour. (4) **Batch Processing (5% savings):** Group similar requests, process in one LLM call. Instead of: 5 separate "summarize this paragraph" calls. Do: 1 call "summarize these 5 paragraphs, number them 1-5". Reduces overhead (system prompts, etc.). Savings: ~5% = $49 × 0.95 = $46.5/hour. (5) **Response Streaming + Early Termination (5% savings):** Stream responses, stop generation when enough information received. If agent only needs first 500 tokens but LLM generates 2000: Early termination saves 75% of output tokens. Requires: Detecting when enough info received. Savings: ~5% = $46.5 × 0.95 = $44/hour. **Total Cost Reduction:** From $270/hour to $44/hour = 84% reduction! (Target was 50%, we exceeded.) **Quality Safeguards:** (1) **Monitor Quality Metrics:** Track: Agent success rate, human approval rate, error rate. If quality drops below baseline: Revert optimizations one by one. Find which caused degradation. (2) **A/B Testing:** Deploy optimizations gradually: 10% of traffic with optimizations. 90% with original setup. Compare quality metrics. If quality maintained, expand to 100%. (3) **Model Fallback:** If GPT-3.5 task fails (error or low quality): Automatic retry with GPT-4. Count fallback rate. If >10%, that task should default to GPT-4. (4) **Cache Validation:** Periodically re-run cached queries with LLM. Compare responses. If cached response is stale/incorrect: Invalidate cache, lower TTL. **Implementation Priority:** Week 1: Caching (biggest win, low risk). Week 2: Model tiering (test quality carefully). Week 3: Prompt optimization (requires careful testing). Week 4: Batch processing and early termination (minor wins). **Monitoring Dashboard:** Show: Cost per hour (real-time), cost per agent, cache hit rate, model distribution (% GPT-4 vs GPT-3.5), quality metrics side-by-side. Alert if: Cost exceeds $100/hour, quality metrics drop >5%, cache hit rate drops <20%.',
    keyPoints: [
      'Caching: Biggest win (30-40% savings) with minimal risk',
      'Model tiering: Use cheaper models for routine tasks',
      'Prompt optimization: Reduce token count without hurting quality',
      "Monitor quality metrics to ensure optimizations don't degrade output",
    ],
  },
  {
    id: 'maas-prod-q-3',
    question:
      'Design a comprehensive health check system for a production multi-agent system. What metrics do you monitor, what are the alert thresholds, and how do you distinguish between transient issues vs systemic failures?',
    hint: 'Think about what can go wrong and how to detect it early.',
    sampleAnswer:
      '**Health Check Categories:** (1) Infrastructure, (2) Agent Performance, (3) Dependencies, (4) Business Metrics. **1. Infrastructure Metrics:** Agent Pool: Available agents / Total agents. Threshold: Alert if <20% available (high load). Action: Scale up. CPU/Memory: Per-agent resource usage. Threshold: Alert if any agent >80% CPU for >5 min. Action: Restart agent or scale. Queue Depth: Pending tasks in queue. Threshold: Alert if >100 tasks waiting. Action: Scale agents. Circuit Breaker: State (open/closed) + failure count. Threshold: Alert if open. Action: Investigate underlying issue. **2. Agent Performance Metrics:** Success Rate: % of tasks completed successfully. Baseline: 95%. Threshold: Alert if <90% over 5 min. Transient: One-time spike, recovers quickly. Systemic: Sustained <90% for >15 min. Latency: P50, P95, P99 execution time. Baseline: P95 = 10s. Threshold: Alert if P95 >15s for >5 min. Transient: Temporary API slowdown. Systemic: Agent logic issue or dependency failure. Error Rate: Errors per agent. Threshold: Alert if any agent >5% errors. Action: Check agent logs, recent changes. **3. Dependency Metrics:** LLM API: Response time, error rate, rate limits. Threshold: Alert if error rate >1% or latency >5s. External APIs: Availability, response times. Threshold: Alert if any API down or >10s latency. Database: Connection pool, query time. Threshold: Alert if connection pool exhausted or queries >1s. Cache: Hit rate, memory usage. Threshold: Alert if hit rate <10% (cache not working). **4. Business Metrics:** Workflow Completion Rate: % of workflows that complete end-to-end. Threshold: Alert if <85%. Cost per Hour: LLM API costs. Threshold: Alert if exceeds budget ($100/hour). User Satisfaction: If collecting feedback. Threshold: Alert if <80% positive. **Distinguishing Transient vs Systemic:** Transient: Spike for <5 minutes, then recovers. Example: API hiccup, network blip. Action: Log, monitor, no alert if recovers. Systemic: Sustained degradation for >15 minutes. Example: Code bug, dependency failure. Action: Alert immediately, investigate. **Implementation:** class HealthChecker: def check_all(self): results = { "infrastructure": self.check_infrastructure(), "agents": self.check_agents(), "dependencies": self.check_dependencies(), "business": self.check_business(), }. overall = all(r["healthy",] for r in results.values()). return {"healthy": overall, "checks": results}. def check_agents(self): success_rate = self.get_recent_success_rate(window="5min"). if success_rate < 0.90: if self.is_sustained(metric="success_rate", threshold=0.90, duration="15min"): alert("CRITICAL: Agent success rate below 90% for 15+ minutes"). return {"healthy": False, "success_rate": success_rate}. else: log_warning("Transient success rate drop"). return {"healthy": True, "success_rate": success_rate, "warning": "transient drop"}. return {"healthy": True, "success_rate": success_rate}. **Alert Thresholds (by Severity):** INFO: Metrics slightly off baseline (success rate 92%). Action: Log, no immediate action. WARNING: Metrics concerning but not critical (success rate 88%). Action: Notify on-call (non-urgent). CRITICAL: Metrics indicate major issue (success rate <85%, or API down). Action: Page on-call immediately, auto-scale/failover if possible. **Health Check Endpoint:** GET /health → {"status": "healthy", "checks": {...}}. Used by: Load balancers (remove unhealthy instances), monitoring systems (dashboards, alerts), deployment systems (don\'t deploy if unhealthy). **Auto-Recovery Actions:** If agent pool low → Scale up. If circuit breaker open → Switch to fallback dependency. If specific agent failing → Restart that agent. If cost exceeding → Enable aggressive caching, use cheaper models. **Dashboard:** Real-time view: Green/yellow/red status for each category, key metrics (success rate, latency, cost), alerts (recent and active), trend graphs (last 24 hours). **Testing Health Checks:** Chaos engineering: Intentionally break things to test alerts. Kill agent → Should alert and auto-restart. Throttle API → Should detect latency, might open circuit breaker. Fill queue → Should alert and scale. Verify: Alerts fire correctly, auto-recovery works, no false positives.',
    keyPoints: [
      'Monitor infrastructure, agent performance, dependencies, and business metrics',
      'Distinguish transient issues (recover <5 min) from systemic (sustained >15 min)',
      'Set alert thresholds based on severity (INFO, WARNING, CRITICAL)',
      'Implement auto-recovery for common issues (scale up, restart, failover)',
    ],
  },
];
