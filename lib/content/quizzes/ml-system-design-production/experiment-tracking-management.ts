export const experimentTrackingManagementQuiz = [
  {
    id: 'etm-q-1',
    question:
      "You're managing 20 data scientists running hundreds of experiments per week. Design a complete experiment tracking system. Address: (1) experiment metadata to track, (2) artifact storage and versioning, (3) comparison and visualization tools, (4) reproducibility mechanisms, (5) collaboration features. How would you prevent experiment chaos and ensure all experiments are reproducible?",
    sampleAnswer:
      'Complete Experiment Tracking with MLflow: (1) Metadata to Track: Every experiment logs: Hyperparameters (learning_rate, batch_size, model_architecture), Metrics (accuracy, precision, F1, loss curves—log per epoch), Tags (experiment_type="baseline", owner="alice"), Code version (git commit hash), Dataset version (DVC hash or path), Environment (Python 3.9, torch==1.12, requirements.txt), Timestamp, Runtime duration. MLflow auto-logs these with mlflow.sklearn.autolog() or manual mlflow.log_param(), mlflow.log_metric(). (2) Artifact Storage: Models (model.pkl, checkpoint.pth), Plots (confusion matrix, ROC curve, feature importance), Data samples (validation predictions), Config files (config.yaml). Store in centralized S3, versioned by run_id. Example: s3://mlflow/artifacts/run_123/model.pkl. Enable model registry for promotion: Staging → Production. (3) Comparison Tools: MLflow UI: Compare runs side-by-side, filter by metrics, sort by accuracy. Parallel coordinates plot: Visualize hyperparameter impact. Example: learning_rate vs accuracy across 100 runs—identify optimal range. Query API: runs = mlflow.search_runs(filter_string="metrics.accuracy > 0.9", order_by=["metrics.accuracy DESC",]). Programmatic analysis. (4) Reproducibility: Each run stores: git commit (code version), environment (conda.yaml or requirements.txt), dataset version, random seeds. To reproduce: mlflow run . --version <commit_hash>. Recreates environment, runs code at exact version, uses same data. Prevents "worked on my machine" issues. (5) Collaboration: Centralized MLflow server accessible to team. Permissions: All can view, owners can edit/delete runs. Tagging system: Tag experiments with project name, hypothesis tested. Example: project="fraud_detection", hypothesis="adding_graph_features". Notebooks link to MLflow runs. Best Practices: (1) Naming convention: Experiment names hierarchical. Examples: "fraud_detection/baseline", "fraud_detection/xgboost_tuning". (2) Mandatory fields: Enforce logging of git commit, dataset version via CI checks. (3) Regular cleanup: Archive old experiments, delete failed runs. (4) Model registry: Promote model to Production only after review. Stage transitions logged. (5) Integration: Link MLflow to Slack (notify on new best model), Airflow (trigger retraining pipeline). Preventing Chaos: Standardized templates for experiments (Cookiecutter), enforce metadata logging (CI fails if missing git hash), automated cleanup (delete runs older than 6 months with low accuracy), review process for Production promotion.',
    keyPoints: [
      'Track hyperparameters, metrics, code version (git hash), dataset version, environment for reproducibility',
      'Centralized artifact storage (S3), model registry for staging → production promotion',
      'MLflow UI for comparison, parallel coordinates for hyperparameter analysis, programmatic queries',
      'Reproducibility: Store git commit, environment (conda/requirements), dataset, seeds—mlflow run recreates',
      'Collaboration: Centralized server, tagging system, experiment hierarchy, mandatory metadata logging',
    ],
  },
  {
    id: 'etm-q-2',
    question:
      'Design a hyperparameter optimization strategy for a large neural network that takes 6 hours to train. Compare grid search, random search, Bayesian optimization, and Hyperband. For each: (1) search strategy, (2) computational cost, (3) when to use, (4) expected performance. If you have a budget of 50 training runs, which would you choose and why? Include code example.',
    sampleAnswer:
      'Hyperparameter Optimization Strategies: GRID SEARCH: (1) Strategy: Try every combination. Example: learning_rate=[0.001, 0.01, 0.1], batch_size=[32, 64, 128] → 3×3=9 runs. (2) Cost: Grows exponentially. 5 hyperparams × 3 values each = 243 runs. With 6-hour training → 1458 hours (2 months). Prohibitive. (3) Use when: Few hyperparameters (<3), discrete choices, need exhaustive search. (4) Performance: Guarantees finding best in grid, but grid may miss optimal values (e.g., best LR is 0.005, not in grid). RANDOM SEARCH: (1) Strategy: Sample hyperparameters randomly. Example: learning_rate ~ uniform(1e-5, 1e-1), batch_size ~ choice([32, 64, 128]). 50 random samples. (2) Cost: Fixed budget (50 runs × 6 hours = 300 hours = 12.5 days). (3) Use when: Many hyperparameters (>5), continuous spaces, limited budget. (4) Performance: 2-5× more efficient than grid (Bergstra & Bengio). Samples more values per hyperparameter. BAYESIAN OPTIMIZATION: (1) Strategy: Build probabilistic model (Gaussian Process) of performance function. Acquisition function (Expected Improvement) suggests next hyperparameters to try. Balances exploration (uncertain regions) vs exploitation (known good regions). (2) Cost: 50 runs, but smarter—each run informs next choice. (3) Use when: Expensive evaluations, continuous spaces, need sample efficiency. (4) Performance: Best for fixed budget. Typically finds optimum in 20-50% of random search budget. HYPERBAND: (1) Strategy: Early stopping + resource allocation. Train many configs for few epochs, keep best, train longer. Adaptive resource allocation. Example: Train 50 configs for 5 epochs, keep top 10, train to 20 epochs, keep top 3, train to 100 epochs. (2) Cost: Efficiently uses budget by stopping bad configs early. (3) Use when: Training is iterative (epochs), clear validation metric, large search space. (4) Performance: 5-10× speedup over random search for neural networks. RECOMMENDATION for 50 runs (300 hours budget): Use BAYESIAN OPTIMIZATION (Optuna). Reasoning: (1) Sample efficient: Finds near-optimal in 30-40 runs, leaving buffer. (2) Handles continuous and categorical hyperparameters. (3) Parallelizable: Run 5 trials concurrently (10 days vs 12.5). (4) Convergence: Early runs explore, later runs exploit best regions. Example Code (Optuna): import optuna; def objective(trial): lr = trial.suggest_float("learning_rate", 1e-5, 1e-1, log=True); batch_size = trial.suggest_categorical("batch_size", [32, 64, 128]); dropout = trial.suggest_float("dropout", 0.1, 0.5); # Train model with these hyperparams; model = train_model(lr=lr, batch_size=batch_size, dropout=dropout); val_accuracy = evaluate(model); return val_accuracy; study = optuna.create_study(direction="maximize", sampler=optuna.samplers.TPESampler()); study.optimize(objective, n_trials=50, n_jobs=5); # Parallel; print(f"Best hyperparameters: {study.best_params}"); print(f"Best accuracy: {study.best_value}"); # Visualizations; optuna.visualization.plot_optimization_history(study); optuna.visualization.plot_param_importances(study); # Shows which hyperparameters matter most. Enhancements: (1) Pruning: Stop bad trials early with MedianPruner. Saves compute. (2) Multi-objective: Optimize accuracy AND inference latency. (3) Visualization: Plot hyperparameter importance, convergence.',
    keyPoints: [
      'Grid search: Exhaustive but exponential cost, only for <3 hyperparameters',
      'Random search: 2-5× more efficient than grid, good baseline for many hyperparameters',
      'Bayesian optimization: Most sample-efficient, builds surrogate model, 20-50% fewer runs',
      'Hyperband: Early stopping + adaptive resources, best for iterative training (epochs)',
      'Recommend Bayesian (Optuna) for 50 runs: sample-efficient, parallel, handles mixed spaces',
    ],
  },
  {
    id: 'etm-q-3',
    question:
      'Explain how to implement proper experiment versioning to ensure reproducibility. Address: (1) code versioning (git), (2) data versioning (DVC), (3) environment versioning (Docker/conda), (4) model versioning (MLflow registry), (5) handling experiment dependencies. If a model trained 6 months ago needs to be recreated exactly, what information must be tracked and how would you reproduce it?',
    sampleAnswer:
      'Complete Reproducibility System: (1) Code Versioning (Git): Every experiment logs git commit hash: git_commit = subprocess.check_output(["git", "rev-parse", "HEAD",]).decode().strip(); mlflow.set_tag("git_commit", git_commit). Tag commit: git tag experiment-v1.2.3. If code modified but not committed → fail experiment (CI check). Ensures exact code version tracked. (2) Data Versioning (DVC): Track datasets with DVC. DVC stores hash + metadata in git, actual data in S3. Initialize: dvc init, dvc remote add -d storage s3://my-bucket. Track dataset: dvc add data/train.csv, git add data/train.csv.dvc, git commit. Reference in experiment: data_version = hash_of("data/train.csv.dvc"); mlflow.log_param("data_version", data_version). Alternatively: Tag: dvc tag dataset-v1.0 data/train.csv. (3) Environment Versioning: Option A: Docker. Dockerfile with all dependencies: FROM python:3.9, RUN pip install torch==1.12 scikit-learn==1.1. Build: docker build -t ml-experiment:v1.0. Run experiments in container: docker run ml-experiment:v1.0 python train.py. Tag image with git commit. Option B: Conda. Export environment: conda env export > environment.yml. Log with experiment. Recreate: conda env create -f environment.yml. Option C: pip. Pin all versions: pip freeze > requirements.txt. Include transitive dependencies. (4) Model Versioning (MLflow Registry): Register model: mlflow.register_model(f"runs:/{run_id}/model", "fraud_detection"). Versions auto-incremented: v1, v2, v3. Each version stores: artifacts, metrics, hyperparameters, linked to training run. Promote through stages: None → Staging → Production → Archived. Auditlog: Who promoted, when, why (approval process). (5) Handling Dependencies: Config file (YAML) for all experiment parameters. experiment_config.yaml: learning_rate: 0.001, batch_size: 64, model: "resnet50", data_version: "v1.2", git_commit: "abc123". Store config in MLflow: mlflow.log_artifact("experiment_config.yaml"). External dependencies (APIs, databases): Log versions (e.g., "postgres:13", "redis:6.2"). Random seeds: Set and log all seeds (Python, NumPy, PyTorch, CUDA). REPRODUCING MODEL FROM 6 MONTHS AGO: Information needed: (1) Git commit hash (code version), (2) Data version (DVC hash or S3 path with timestamp), (3) Environment (Docker image tag or conda env yaml), (4) Config (hyperparameters, seeds), (5) Hardware (GPU type, CUDA version—affects numerical precision). Reproduction steps: # 1. Restore code; git checkout <commit_hash>; # 2. Restore data; dvc checkout data/train.csv.dvc # pulls exact data from S3; # 3. Restore environment; docker pull ml-experiment:v1.0 # or conda env create -f environment.yml; # 4. Restore config; cp mlflow/artifacts/<run_id>/experiment_config.yaml .; # 5. Re-run training; python train.py --config experiment_config.yaml; # 6. Verify: Compare metrics (should match within floating-point error). Gotchas: (1) Non-determinism: Some ops non-deterministic (GPU atomics). Use torch.backends.cudnn.deterministic=True. (2) Hardware differences: Different GPU → slightly different results. (3) External data: If model uses live API data, can\'t reproduce exactly (log API responses). Best Practices: Automate tracking (fail experiment if git not clean), use DVC for all data, containerize everything (Docker), test reproducibility regularly (reproduce last month\'s best model).',
    keyPoints: [
      'Code: Git commit hash + tag, fail if uncommitted changes, exact code version',
      'Data: DVC for versioning, stores hash in git, data in S3, tags for releases',
      'Environment: Docker (best) or conda env export, pin all dependencies including transitive',
      'Model: MLflow registry with version numbers, stage transitions, links to training run',
      'Reproduce: git checkout + dvc checkout + docker pull + config file, verify metrics match',
    ],
  },
];
