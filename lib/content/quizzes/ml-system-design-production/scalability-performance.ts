export const scalabilityPerformanceQuiz = [
  {
    id: 'sp-q-1',
    question:
      'Design a model serving system that can scale from 1,000 to 100,000 requests per second while maintaining P99 latency under 50ms. Address: (1) horizontal vs vertical scaling strategies, (2) load balancing and traffic distribution, (3) caching at multiple levels, (4) database query optimization, (5) monitoring and auto-scaling triggers. What are the bottlenecks at each scale tier?',
    sampleAnswer:
      "Scalable Model Serving (1K → 100K RPS): TIER 1 (1K RPS): Single server adequate. 1 instance handles 1000 req/sec easily. Bottleneck: None. Cost: $100/month. TIER 2 (10K RPS): Horizontal scaling needed. Deploy 50 replicas (each handles 200 RPS). Load balancer (ALB/nginx) distributes traffic round-robin. Bottleneck: Database queries for features. Solution: Add Redis cache for hot features. 80% cache hit rate → 8K RPS from cache (3ms), 2K RPS hit database (20ms). Cost: $1,500/month. TIER 3 (100K RPS): Further scaling required. 500 replicas (each 200 RPS). Multi-level caching critical. Bottleneck: Load balancer becomes bottleneck. Solution: Multiple load balancers (DNS round-robin to 5 LBs). Network bandwidth (10Gb NICs). Redis cluster (sharded, 10 nodes). Bottleneck: Model inference if not optimized. Solution: Model quantization (INT8), GPU inference, batching (group 32 requests). Cost: $25K/month. (1) Horizontal vs Vertical Scaling: HORIZONTAL: Add more servers (1 → 500 replicas). Pros: Linear scaling, fault tolerance (one server fails, others continue). Cons: Coordination overhead, state synchronization complex. VERTICAL: Bigger servers (2 cores → 64 cores). Pros: Simpler (no coordination), lower network overhead. Cons: Limited scaling (max instance size), single point of failure, expensive at high end. RECOMMENDATION: Horizontal scaling for ML serving. Why: Can scale indefinitely, cost-effective (use many small instances), fault tolerant. (2) Load Balancing: ROUND-ROBIN: Distribute requests evenly across replicas. Simple but ignores server load. LEAST-CONNECTIONS: Route to server with fewest active connections. Better for variable request times. CONSISTENT HASHING: Route based on user_id. Same user always hits same server (enables local caching). Implementation: ALB (AWS Application Load Balancer) or nginx. Health checks: Ping /health every 10s. Remove unhealthy instances from pool. Geographic distribution: Use CDN (CloudFlare) + regional clusters. US East users → us-east cluster. EU users → eu-west cluster. Reduces latency. (3) Multi-Level Caching: L1 (Application): In-memory LRU cache, 10K entries, 1ms lookup. Cache frequently requested items. L2 (Redis): Shared across all replicas, 1M entries, 3ms lookup. Cache user features, popular predictions. L3 (CDN): For cacheable responses (e.g., top-10 recommendations for segment), 20ms lookup. Cache hit rates: L1: 20%, L2: 60%, L3: 10%. Total cache hit: 90% → only 10% requests hit database. Cache invalidation: On model update, clear L2 (Redis). TTL: 5 minutes for predictions (avoid stale). (4) Database Optimization: CONNECTION POOLING: Reuse database connections. Pool size: 20 connections per server. Prevents connection exhaustion. QUERY OPTIMIZATION: Index frequently queried columns (user_id). Use read replicas (5 replicas) for read-heavy workloads. Write to primary, read from replicas. Batch queries: Instead of 1 query per request, batch 100 requests → 1 query with WHERE user_id IN (...). Async queries: Non-critical data fetched async (don't block response). DATABASE CHOICE: PostgreSQL for transactional, Redis for caching, Cassandra for high write throughput. (5) Auto-Scaling: TRIGGERS: CPU utilization >70% for 5 minutes → scale up (+10 replicas). CPU <30% for 15 minutes → scale down (-10 replicas). Request queue depth >100 → scale up immediately (traffic spike). P99 latency >50ms for 3 minutes → scale up (capacity issue). SCALING POLICY: Scale up fast (add 10 replicas in 2 minutes). Scale down slow (remove 10 replicas over 10 minutes). Prevent thrashing. MONITORING (CloudWatch/Prometheus): Request rate (current: 85K RPS, capacity: 100K). Latency histogram (P50, P90, P99). Error rate (<0.1%). Cache hit rate (>80%). Replica count (current: 450, target: 500). Bottlenecks by Tier: 1K: None. 10K: Database queries → Redis cache. 100K: Load balancer → multi-LB, Model inference → quantization/GPU, Network → 10Gb, Redis → cluster.",
    keyPoints: [
      'Horizontal scaling: 1 → 50 → 500 replicas for linear scaling, fault tolerance, cost-effective',
      'Load balancing: Round-robin or least-connections, health checks, geographic distribution (CDN)',
      'Multi-level cache: In-memory (1ms) → Redis (3ms) → CDN, 90% hit rate, invalidate on model update',
      'Database: Connection pooling, indexed queries, read replicas, batch queries, async non-critical',
      'Auto-scaling: CPU >70% scale up, <30% scale down, queue depth triggers, monitor RPS/latency/cache-hit',
    ],
  },
  {
    id: 'sp-q-2',
    question:
      'Explain techniques for optimizing ML model inference latency. Address: (1) model optimization (quantization, pruning, distillation), (2) hardware acceleration (GPUs, TPUs), (3) batching strategies, (4) model compilation (ONNX, TensorRT), (5) serving infrastructure. For a BERT model with 50ms inference time, how would you reduce it to <10ms?',
    sampleAnswer:
      'Inference Latency Optimization: BERT 50ms → <10ms: (1) Model Optimization: QUANTIZATION (INT8): Convert FP32 weights to INT8. Benefits: 4x smaller model, 2-4x faster inference, minimal accuracy loss (<1%). Tools: PyTorch quantization, ONNX quantization. Code: import torch.quantization; model_int8 = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8). Expected speedup: 50ms → 20ms (2.5x). DISTILLATION: Train smaller student model (DistilBERT, 6 layers vs 12) to mimic BERT. Benefits: 60% smaller, 2x faster, 95% accuracy of BERT. Training: Teacher (BERT) generates soft labels. Student learns to match. Expected speedup: 50ms → 15ms (DistilBERT). PRUNING: Remove unimportant weights (magnitude-based). Set 30% smallest weights to zero → sparse model. Benefits: Faster with specialized hardware (sparse tensor cores). Expected speedup: 50ms → 35ms. COMBINE: Distillation (→15ms) + Quantization (→7ms). Aggressive: Achieve <10ms target. (2) Hardware Acceleration: GPU: Parallel matrix ops. BERT inference on GPU: 50ms → 10ms (5x speedup). But: GPU expensive ($1.50/hour vs $0.10 for CPU), only beneficial for batch >32. TPU (Google): Optimized for matrix ops. 10x faster than CPU for BERT. But: Vendor lock-in (Google Cloud only). ONNX Runtime: Optimized CPU inference. 2-3x speedup over PyTorch on CPU. TensorRT (NVIDIA): Optimized GPU inference. Layer fusion, kernel auto-tuning. 2-5x speedup over vanilla GPU. For BERT: Deploy on GPU with TensorRT. 50ms (CPU) → 10ms (GPU) → 5ms (TensorRT). (3) Batching: DYNAMIC BATCHING: Collect requests for 10ms, batch together (e.g., 32 requests), run single inference. TRADE-OFF: Latency increases (wait time), throughput increases (amortize overhead). Example: Single request: 50ms. Batch of 32: 80ms total → 2.5ms per request (20x throughput). But: Each request waits up to 10ms for batch to fill → effective latency 12.5ms. For <10ms target: Batch size <16 (keep wait time <5ms). ADAPTIVE BATCHING: Batch size adapts to load. High traffic: batch size 32 (maximize throughput). Low traffic: batch size 1 (minimize latency). (4) Model Compilation: ONNX: Export PyTorch model to ONNX (framework-agnostic). Run with ONNX Runtime (optimized ops). Code: torch.onnx.export(model, dummy_input, "model.onnx"); import onnxruntime; session = onnxruntime.InferenceSession("model.onnx"); output = session.run(None, {\'input\': input_data}). Expected speedup: 50ms → 25ms (2x). TENSORRT: NVIDIA compiler for GPUs. Fuses layers (e.g., Conv + BatchNorm + ReLU → single kernel). Precision calibration (mixed FP16/INT8). Code: import tensorrt; Build engine, optimize, run. Expected speedup: 50ms (GPU) → 10ms (TensorRT GPU). TORCHSCRIPT: Compile PyTorch model to optimized intermediate representation. Code: traced_model = torch.jit.trace(model, example_input); traced_model.save("model.pt"). Expected speedup: 50ms → 40ms (minor, but removes Python overhead). (5) Serving Infrastructure: MODEL CACHING: Load model once at startup (not per request). Keep in memory (GPU memory for GPU inference). PREPROCESSING OPTIMIZATION: Tokenization can be slow (10ms). Cache tokenized inputs if possible. Use fast tokenizers (Rust-based Hugging Face tokenizers). ASYNCHRONOUS SERVING: Non-blocking I/O. Handle multiple requests concurrently. FastAPI with async: async def predict(text): result = await model.predict(text). CONNECTION REUSE: HTTP keep-alive, connection pooling. Avoid TCP handshake overhead. COMPREHENSIVE STRATEGY for BERT 50ms → <10ms: Step 1: Distillation → DistilBERT (15ms). Step 2: Quantization (INT8) → 7.5ms. Step 3: ONNX Runtime → 5ms. Step 4: TensorRT (if GPU available) → 3ms. Step 5: Batching (size 8) → 2.5ms per request (throughput 320 req/sec). Step 6: Fast tokenizer → save 2ms preprocessing. Final latency: 5ms (meets <10ms target). Cost: Quantization free, distillation one-time training cost, GPU $1.50/hour but serves 320 req/sec (cost per request low).',
    keyPoints: [
      'Model optimization: Quantization (INT8, 4x smaller, 2-4x faster), distillation (DistilBERT, 2x faster, 95% accuracy)',
      'Hardware: GPU (5x faster), TPU (10x), TensorRT (2-5x on GPU), ONNX Runtime (2-3x on CPU)',
      'Batching: Dynamic batching (wait 10ms, batch 32), trade-off latency vs throughput, adaptive batch size',
      'Compilation: ONNX (2x speedup), TensorRT (layer fusion, precision calibration), TorchScript (remove Python overhead)',
      'BERT 50ms → <10ms: DistilBERT + quantization + ONNX Runtime + batching → 5ms target achieved',
    ],
  },
  {
    id: 'sp-q-3',
    question:
      'Design a cost optimization strategy for a production ML system. Address: (1) compute cost reduction (spot instances, right-sizing), (2) storage cost optimization (data lifecycle, compression), (3) API cost management (caching, rate limiting), (4) monitoring and cost attribution, (5) cost-performance trade-offs. For a system spending $50K/month on ML infrastructure, how would you reduce costs by 40% without sacrificing quality?',
    sampleAnswer:
      'Cost Optimization Strategy ($50K → $30K/month): CURRENT BREAKDOWN (assumed): Training: $15K (30%), Inference: $25K (50%), Data storage: $5K (10%), Monitoring/misc: $5K (10%). (1) Compute Cost Reduction: SPOT INSTANCES (Training): Use spot instances for training (70% discount vs on-demand). Risk: Can be interrupted. Mitigation: Checkpointing (resume from last checkpoint). Savings: $15K → $5K (training). $10K saved. RIGHT-SIZING: Analyze utilization. If GPU util 30%, model too small for hardware. Use smaller instance (p3.2xlarge vs p3.8xlarge). Savings: 20% reduction → $5K saved. INFERENCE OPTIMIZATION: Reduce inference cost $25K → $15K. (a) Model quantization: INT8 → 2x faster → 50% fewer instances. Savings: $12.5K. (b) Auto-scaling: Scale down during low traffic (nights/weekends). Average utilization 70% → savings: $3.75K. Combined: $25K → $10K. $15K saved. RESERVED INSTANCES: For baseline load (always-on), use 1-year reserved (40% discount). Savings: $2K. (2) Storage Optimization: DATA LIFECYCLE: Move old data to cheaper tiers. Hot (S3 Standard, <30 days): $0.023/GB. Warm (S3 Infrequent Access, 30-90 days): $0.0125/GB. Cold (Glacier, >90 days): $0.004/GB. Archive training data >90 days old to Glacier. Savings: $2K. COMPRESSION: Compress datasets (Parquet with Snappy). 3-5x compression → 70% storage reduction. Savings: $1.5K. DATA DEDUPLICATION: Remove duplicate/unused datasets. Often 20-30% of data is obsolete. Savings: $1K. (3) API Cost Management: CACHING: Cache API responses (LLM, external APIs). 80% cache hit rate → 80% cost reduction. Example: LLM costs $5K/month. With caching → $1K/month. Savings: $4K. RATE LIMITING: Prevent abuse. Limit: 1000 requests/hour per user. Reduces wasteful traffic. Savings: $500. BATCH API CALLS: Instead of 1000 individual calls, batch into 10 calls. Many APIs charge per call (not per item). Savings: $500. (4) Monitoring & Cost Attribution: COST TAGGING: Tag resources by team/project. Example: project=fraud_detection, team=ml_platform. Analyze: Which projects costly? fraud_detection: $15K, recommendations: $10K. DASHBOARDS: Grafana/CloudWatch showing cost by service. Daily breakdown: EC2, S3, API calls. Alerts: If daily cost >$2K (monthly pace >$60K) → investigate. SHOWBACK: Report costs to teams. Creates accountability. Teams optimize when aware of costs. (5) Cost-Performance Trade-offs: TRADE-OFF 1: Model complexity vs cost. Complex model (BERT): 92% accuracy, $15K/month. Simple model (Logistic Regression): 88% accuracy, $1K/month. Decision: Is 4% accuracy worth $14K/month? Depends on business value (fraud detection: yes, spam filter: maybe not). TRADE-OFF 2: Latency vs cost. GPU inference: P99=10ms, $15K/month. CPU inference: P99=50ms, $5K/month. Decision: If latency SLA <20ms → need GPU. If 100ms acceptable → use CPU. TRADE-OFF 3: Retraining frequency vs cost. Daily retraining: $15K/month, accuracy 92%. Weekly retraining: $5K/month, accuracy 90%. Decision: Depends on data drift rate. Fast drift (fraud) → daily. Slow drift (churn prediction) → weekly. $50K → $30K PLAN (40% reduction): Training: Spot instances $10K saved. Inference: Quantization + auto-scaling $15K saved. Storage: Lifecycle + compression $4K saved. APIs: Caching $4K saved. Reserved instances: $2K saved. Total savings: $35K. Remaining cost: $15K/month. OVERSHOOTING TARGET ($30K): Use remaining $15K as buffer or reinvest in improvements. QUALITY PRESERVATION: Quantization: Tested, <1% accuracy loss. Spot instances: Checkpointing prevents loss. Auto-scaling: Maintains latency SLA. Result: 40% cost reduction, zero quality impact.',
    keyPoints: [
      'Compute: Spot instances for training (70% discount), inference quantization (50% fewer instances), auto-scaling',
      'Storage: Lifecycle policies (S3 → Glacier), compression (Parquet, 70% reduction), dedup (remove 20-30% obsolete)',
      'APIs: Caching (80% hit rate → 80% cost reduction), rate limiting, batch calls',
      'Monitoring: Cost tagging by project, dashboards with alerts, showback for accountability',
      '$50K → $30K: Spot ($10K), quantization+autoscaling ($15K), storage ($4K), API caching ($4K), reserved ($2K)',
    ],
  },
];
