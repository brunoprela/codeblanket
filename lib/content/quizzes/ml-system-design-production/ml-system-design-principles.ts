export const mlSystemDesignPrinciplesQuiz = [
  {
    id: 'mlsdp-q-1',
    question:
      'Design a complete ML system architecture for a real-time fraud detection service processing 100,000 transactions per second. Address: (1) data ingestion and feature engineering pipeline, (2) model serving infrastructure with latency requirements, (3) monitoring and alerting strategy, (4) deployment approach with rollback capability, (5) handling model updates without downtime. What are the key design trade-offs and how would you validate the system meets requirements?',
    sampleAnswer:
      'Complete fraud detection architecture: (1) Data Ingestion: Kafka for streaming transactions, Flink/Spark Streaming for real-time feature computation (user velocity, merchant risk scores, location changes). Pre-compute expensive features (30-day aggregates) in batch jobs, store in Redis for <2ms lookup. Feature store ensures training-serving consistency. (2) Model Serving: Deploy gradient boosting model (XGBoost/LightGBM) for <50ms inference. Why not deep learning: Faster inference, interpretable. Architecture: Load balancer → 50+ model servers, each serving quantized model from memory. Auto-scaling based on request rate. Batch incoming requests (size=32) for throughput. (3) Monitoring: Track model metrics (precision/recall per class, false positive rate by merchant category), system metrics (P99 latency, throughput, error rate), business metrics ($ saved from blocked fraud, customer impact from false positives). Alert if: precision drops >5%, latency >100ms, or error rate >1%. Data drift detection with Evidently on hourly windows. (4) Deployment: Blue-green for zero downtime. Deploy new model to green environment, run shadow mode (log predictions without acting), compare with blue for 1 hour. If metrics match, route 10% traffic (canary), then 50%, then 100%. Automated rollback if metrics degrade. (5) Model Updates: Retrain daily with fresh fraud patterns (adversarial environment). Store model versions in MLflow registry. Use A/B testing to validate new model improves business metrics. Key trade-offs: Latency vs accuracy (simpler model faster), cost vs performance (smaller model cheaper to serve), freshness vs stability (frequent retraining risks regressions). Validation: Load test with 150K TPS (1.5x peak), chaos engineering (kill servers, test failover), shadow mode before production, business metric tracking (revenue impact).',
    keyPoints: [
      'Kafka + Flink for streaming, Redis feature store for low-latency lookups',
      'XGBoost over deep learning: faster inference (<50ms), interpretable for compliance',
      'Monitor model metrics (precision/recall), system (latency/errors), business ($ saved)',
      'Blue-green deployment with shadow mode, canary rollout, automated rollback',
      'Daily retraining for adversarial adaptation, A/B test business impact',
    ],
  },
  {
    id: 'mlsdp-q-2',
    question:
      "You're building a recommendation system for an e-commerce platform with 10 million users and 1 million products. Compare batch prediction vs real-time prediction architectures. For each approach, explain: (1) data flow and infrastructure, (2) latency and freshness trade-offs, (3) cost implications, (4) when to use which approach. How would you design a hybrid system combining both?",
    sampleAnswer:
      'Batch vs Real-time Recommendations: BATCH APPROACH: (1) Data flow: Nightly jobs compute recommendations for all users using Spark. Matrix factorization (ALS) on user-item interactions. Generate top-100 products per user, store in database/cache. API serves pre-computed recommendations. (2) Latency: <10ms (cache lookup). Freshness: 24 hours stale. User buys product at 9am, won\'t affect recommendations until tomorrow. (3) Cost: Lower. One-time daily computation cost. No real-time inference. (4) Use when: Recommendations don\'t need personalization based on current session, cost-sensitive, high throughput needed. Example: Email recommendations, homepage for returning users. REAL-TIME APPROACH: (1) Data flow: Session events (clicks, cart adds) → Kafka → Feature computation (session interest, collaborative filtering) → Model inference → Response. Neural network scoring model running on GPU cluster. (2) Latency: 50-200ms (feature lookup + model inference). Freshness: Current session incorporated immediately. (3) Cost: Higher. Continuous model serving, GPU infrastructure, streaming pipeline. (4) Use when: Personalization critical, user intent changes rapidly, high-value transactions. Example: Product detail page (\\"similar items\\"), search results. HYBRID SYSTEM: Combine both for best of both worlds. Architecture: (1) Batch: Generate candidate set (top-1000 products per user category) nightly. Broad collaborative filtering. (2) Real-time: Rank candidates based on current session. Features: time since last click, category preference this session, cart contents. Lightweight ranking model (XGBoost). (3) Flow: User visits → Retrieve batch candidates from cache (2ms) → Add session features (5ms) → Rank with real-time model (20ms) → Return top-20 → Total latency: 27ms. (4) Benefits: Batch provides coverage (1000 candidates), real-time provides personalization (rank by current intent). Cost-effective: Heavy lifting in batch, fast ranking real-time. Implementation: Cache batch candidates in Redis by user_id, TTL=24h. Real-time ranking service: FastAPI + model in memory. Monitor: Cache hit rate (>95%), ranking latency (P99 <50ms), business metrics (CTR, conversion rate).',
    keyPoints: [
      'Batch: Pre-compute overnight, <10ms serving, 24h stale, low cost, good for static recommendations',
      'Real-time: Inference per request, 50-200ms, current session, high cost, good for dynamic content',
      'Hybrid: Batch generates candidates (1000), real-time ranks by session (top-20), best of both',
      'Trade-offs: latency vs freshness vs cost—choose based on business value and scale',
      'Monitor cache hit rates, latency percentiles, business metrics (CTR, conversion)',
    ],
  },
  {
    id: 'mlsdp-q-3',
    question:
      "Design an ML system that needs to handle both batch and online learning for a financial trading model. Explain: (1) when to use batch retraining vs online learning, (2) how to prevent catastrophic forgetting in online learning, (3) how to validate model updates don't degrade performance, (4) strategies for handling distribution shift and concept drift, (5) rollback mechanism if new model underperforms. What metrics would you monitor to ensure system reliability?",
    sampleAnswer:
      'Hybrid Learning System: (1) Batch vs Online: Use batch retraining (weekly) for major updates: new features, architecture changes, hyperparameter tuning. Train on 1 year of data, cross-validate with walk-forward. Use online learning (hourly) for incremental updates: capture recent market regime, adapt to short-term patterns. Update with last 24 hours of data. Combine: Batch provides stable base, online adapts to recent changes. (2) Preventing Catastrophic Forgetting: Problem: Online learning can forget past patterns. Solutions: (a) Reservoir sampling: Maintain buffer of historical examples (size=10K), sample from buffer + recent data for each update. Ensures old patterns represented. (b) Elastic Weight Consolidation: Regularize important weights from previous training. (c) Ensemble: Keep both batch model (stable) and online model (adaptive), weighted combination. (d) Validation: Test online model on held-out historical data periodically. If performance on old data drops >10%, trigger batch retraining. (3) Validation: A/B testing in paper trading. Run new model alongside production model without real orders. Compare: Sharpe ratio, max drawdown, win rate, average profit per trade. If new model Sharpe > old model Sharpe + 0.1 for 7 days → promote. Shadow mode prevents bad models from losing money. (4) Distribution Shift: Monitor: Feature distributions (Kolmogorov-Smirnov test every hour), prediction distributions (mean/std of model outputs), performance metrics (rolling Sharpe on recent trades). If drift detected: (a) Covariate shift: Retrain with recent data. (b) Concept drift: May need new features or model architecture. Trigger manual review. (c) Extreme markets (flash crash): Pause trading, revert to conservative model. (5) Rollback: Store last 5 model versions in registry with performance metrics. Automated rollback triggers: If online trading Sharpe drops below -0.5 for 1 hour, error rate >5%, or max drawdown exceeds 10%. Rollback: Route traffic back to previous stable version, pause online learning, alert team for investigation. Metrics: Model metrics (accuracy, Sharpe, max drawdown, Calmar ratio), system (latency <10ms, uptime 99.99%), business (daily P&L, trades executed, slippage), drift (KS statistic, performance degradation rate). Critical: Financial models require continuous validation—never deploy without paper trading proof.',
    keyPoints: [
      'Batch retraining (weekly) for major updates, online learning (hourly) for adaptation to recent regime',
      'Prevent forgetting: reservoir sampling, ensemble batch+online, validate on historical data',
      'A/B test in paper trading: compare Sharpe, drawdown, win rate before real deployment',
      'Monitor distribution shift with KS test, automate rollback if Sharpe drops or errors spike',
      'Store last 5 model versions, automated rollback, never deploy without paper trading validation',
    ],
  },
];
