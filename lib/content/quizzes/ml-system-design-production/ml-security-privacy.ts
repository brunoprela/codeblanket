export const mlSecurityPrivacyQuiz = [
  {
    id: 'msp-q-1',
    question:
      'Design a comprehensive security strategy for a production ML system handling sensitive user data. Address: (1) model theft prevention, (2) adversarial attack defenses, (3) data privacy (differential privacy, federated learning), (4) secure model serving, (5) compliance (GDPR, HIPAA). How would you protect a medical diagnosis model from both theft and adversarial attacks?',
    sampleAnswer:
      "ML Security Strategy: (1) Model Theft Prevention: PROBLEM: Attacker queries API repeatedly, steals model by training substitute. DEFENSES: Rate limiting: Max 1000 queries/hour per user. Prevents large-scale extraction. Query monitoring: Track query patterns. Random inputs (uniform distribution) suggest extraction attempt. Alert if detected. Add noise to predictions: prediction_noisy = prediction + N(0, σ²). Makes exact extraction harder. But: Reduces accuracy slightly. Watermarking: Embed secret pattern in model. Example: Model always predicts class A for specific crafted input. If stolen model shows same behavior → proof of theft. Legal: Terms of Service prohibit scraping. API key required (traceable). (2) Adversarial Attack Defense: PROBLEM: Attacker crafts input to fool model. Example: Add imperceptible noise to medical image → misdiagnosis. DEFENSES: Adversarial training: Train on mix of clean and adversarial examples. Generate adversarial: x_adv = x + ε × sign(∇_x loss). Train model on x_adv. Makes model robust. Input transformation: JPEG compression (removes high-frequency adversarial noise), bit-depth reduction, median filtering. Applies before model. Ensemble defense: Use multiple models with different architectures. Adversarial examples often don't transfer. If models disagree → flag for human review. Anomaly detection: Train autoencoder on normal inputs. If reconstruction error high → adversarial input. Reject or flag. Certified defense (for critical systems): Randomized smoothing provides provable robustness guarantee. (3) Data Privacy: DIFFERENTIAL PRIVACY: GOAL: Model doesn't leak individual training examples. METHOD: Add noise to gradients during training (DP-SGD). Clip gradients (bound sensitivity), add Gaussian noise (N(0, σ²)). GUARANTEE: (ε, δ)-differential privacy. Example: ε=1.0, δ=10⁻⁵. Trade-off: Privacy vs accuracy. Stronger privacy (lower ε) → lower accuracy. Medical model: ε=1.0 may be acceptable (strong privacy, small accuracy drop). Code: from opacus import PrivacyEngine; privacy_engine = PrivacyEngine(); model, optimizer, dataloader = privacy_engine.make_private(module=model, optimizer=optimizer, data_loader=dataloader, noise_multiplier=1.0, max_grad_norm=1.0). FEDERATED LEARNING: GOAL: Train without centralizing patient data. METHOD: Each hospital trains locally. Send weight updates (not data) to server. Server aggregates updates. ADVANTAGE: Raw data never leaves hospitals (HIPAA compliance). Code (simplified): # Hospital 1; model_1 = train_locally(local_data_1); weights_1 = model_1.get_weights(); # Hospital 2; model_2 = train_locally(local_data_2); weights_2 = model_2.get_weights(); # Server aggregates; global_weights = average([weights_1, weights_2]); global_model.set_weights(global_weights). (4) Secure Model Serving: ENCRYPTION: TLS for API communication (data in transit). Encrypt model weights at rest (AWS KMS). ACCESS CONTROL: API keys per user. OAuth for authentication. Role-based access: Clinicians can access full API, researchers get limited access. AUDIT LOGGING: Log every prediction request: user_id, timestamp, input (hashed), output. Detect misuse, comply with regulations. SECURE ENCLAVE (advanced): Run model in Intel SGX (trusted execution environment). Model encrypted, decrypted only in secure enclave. Even cloud provider can't access model. (5) Compliance: GDPR (Europe): Right to be forgotten: If patient requests deletion, remove their data from training set, retrain model. Data minimization: Collect only necessary features (don't store PII if not needed). Consent: Explicit consent for data use. HIPAA (US healthcare): Encryption (at rest, in transit). Access control (only authorized personnel). Audit logs (track all data access). Business Associate Agreement (BAA) with cloud provider. MEDICAL DIAGNOSIS MODEL PROTECTION: Theft prevention: Rate limit (100 queries/day per clinician), monitor for extraction patterns, legal ToS. Adversarial defense: Adversarial training (robust to perturbations), ensemble of 3 models (if disagree → human review), input validation (check for anomalies). Data privacy: Differential privacy (ε=1.0) or federated learning (train across hospitals without centralizing data). Secure serving: TLS, API keys, audit logging, encrypt model at rest. Compliance: HIPAA (encryption, access control, audit, BAA), GDPR (consent, right to deletion). Result: Multi-layered defense. Even if one layer bypassed, others provide protection.",
    keyPoints: [
      'Model theft: Rate limiting, query monitoring, add noise to predictions, watermarking, legal ToS',
      'Adversarial defense: Adversarial training, input transformation (JPEG compression), ensemble, anomaly detection',
      'Data privacy: Differential privacy (DP-SGD, ε=1.0), federated learning (hospitals train locally, aggregate)',
      'Secure serving: TLS, API keys, encryption at rest, audit logging, secure enclaves (Intel SGX)',
      'Compliance: GDPR (right to be forgotten, consent), HIPAA (encryption, access control, audit, BAA)',
    ],
  },
  {
    id: 'msp-q-2',
    question:
      'Explain adversarial attacks on ML models and defenses. For each attack type (FGSM, PGD, model inversion, membership inference), describe: (1) how the attack works, (2) what information the attacker gains, (3) effective defenses. For a financial fraud detection model, which attacks are most concerning and how would you defend?',
    sampleAnswer:
      "Adversarial Attacks & Defenses: FGSM (Fast Gradient Sign Method): HOW: Compute gradient of loss w.r.t. input: ∇_x L(x, y). Add small perturbation: x_adv = x + ε × sign(∇_x L). WHAT ATTACKER GAINS: Fool model into misclassification. Example: Fraud (class 1) → Legitimate (class 0). Small ε (0.01) invisible to humans but changes prediction. DEFENSE: Adversarial training. Train on x_adv: for epoch in range(epochs): x_adv = x + ε × sign(∇_x loss(x)); train_step([x, x_adv], [y, y]). PGD (Projected Gradient Descent): HOW: Iterative FGSM. Start: x_adv = x. For t steps: x_adv = x_adv + α × sign(∇_x L(x_adv, y)); Project back to ε-ball: x_adv = clip(x_adv, x - ε, x + ε). WHAT: Stronger attack than FGSM (finds better adversarial examples). DEFENSE: Adversarial training with PGD examples (most robust defense known). Ensemble diversity (PGD less transferable across models). MODEL INVERSION: HOW: Attacker has black-box access. Queries model with various inputs. Reconstructs training data. Example: Face recognition model → reconstruct training faces. WHAT: Privacy breach. Reveals sensitive training data. DEFENSE: Differential privacy (add noise to model), limit queries (rate limiting), add noise to predictions. MEMBERSHIP INFERENCE: HOW: Attacker determines if specific sample was in training set. Train shadow model on similar data. Query target model with sample. High confidence → likely in training set. WHAT: Privacy breach. Example: Patient X's medical record was used to train model (reveals diagnosis). DEFENSE: Differential privacy (strongest defense), regularization (reduce overfitting to training data), model ensembles (average predictions). FINANCIAL FRAUD DETECTION—MOST CONCERNING ATTACKS: (1) ADVERSARIAL EXAMPLES (FGSM/PGD): Fraudster crafts transaction to evade detection. Example: Change transaction amount $501 → $499 (below threshold), or add noise to features. Impact: Fraud goes undetected. Revenue loss. DEFENSE: Adversarial training: Train on adversarial transactions. Code: for batch in data: x_adv = generate_adversarial(x, model); train_model([x, x_adv], [y, y]). Input validation: Reject suspicious inputs (feature values outside normal range). Example: If age=999 or location changes from US to China in 1 minute → reject. Ensemble: Use 3 models (XGBoost, Neural Net, Logistic Regression). If predictions disagree significantly → flag for manual review. (2) MODEL THEFT: Fraudster steals fraud detection model. Reverse-engineer to find blind spots. Exploit weaknesses. Impact: Systematic fraud evasion. DEFENSE: Rate limiting (max 10K transactions/day per merchant), query monitoring (detect systematic probing), add noise to predictions (random noise ±5% to fraud probability). (3) DATA POISONING (less common for fraud but possible): Attacker injects fake data into training set. Example: Fake legitimate transactions from compromised account. Model learns incorrect patterns. Impact: High false positive rate (legitimate transactions blocked) or high false negative rate (fraud undetected). DEFENSE: Data validation (outlier detection), robust training (down-weight suspicious samples), ensemble (harder to poison all models). COMPREHENSIVE FRAUD MODEL DEFENSE: Adversarial training (monthly retraining with adversarial examples), Rate limiting + query monitoring (detect theft attempts), Ensemble (3 diverse models), Input validation (sanity checks on features), Differential privacy (if training on sensitive user data), Audit logging (track all predictions for forensics). Monitor: Adversarial example detection rate (flag suspicious inputs), model disagreement rate (ensemble variance), query patterns (detect extraction). Result: Multi-layered defense reduces attack success rate by 90%+.",
    keyPoints: [
      'FGSM: Add ε×sign(gradient) to input, fool model, defense: adversarial training',
      'PGD: Iterative FGSM, stronger attack, defense: adversarial training with PGD examples',
      'Model inversion: Reconstruct training data from queries, defense: differential privacy, rate limiting',
      'Membership inference: Determine if sample in training set, defense: differential privacy, regularization',
      'Fraud detection: Adversarial examples most concerning, defend with adversarial training + ensemble + input validation',
    ],
  },
  {
    id: 'msp-q-3',
    question:
      'Design a federated learning system for training a model across multiple organizations without sharing raw data. Address: (1) architecture (clients, server, aggregation), (2) communication efficiency, (3) handling non-IID data, (4) privacy guarantees, (5) Byzantine fault tolerance. How would you handle a malicious client sending corrupted updates?',
    sampleAnswer:
      "Federated Learning System: (1) Architecture: CLIENTS: N organizations (hospitals, banks, mobile devices). Each has local data D_i. LOCAL TRAINING: Each client trains model on local data for E epochs. Computes weight update: Δw_i = w_new - w_old. AGGREGATION SERVER: Collects weight updates {Δw_1, ..., Δw_N}. Aggregates: w_global = w_old + (1/N) × Σ Δw_i (FedAvg). Broadcasts w_global back to clients. ITERATION: Repeat for T rounds. Privacy: Raw data D_i never leaves client. Only weight updates sent. (2) Communication Efficiency: PROBLEM: Sending full model weights every round is expensive. 100 clients × 10MB model × 100 rounds = 100GB. SOLUTIONS: GRADIENT COMPRESSION: Quantize gradients (FP32 → INT8). 4x smaller. Top-k sparsification: Send only top 10% largest gradients. Compress with gzip (additional 2-3x). FEDERATED DROPOUT: Each client only updates subset of parameters. Different clients update different subsets. Reduces communication by 90%. LOCAL EPOCHS: Train for E=5 epochs locally before sending update (instead of 1 epoch). Reduces communication rounds (100 → 20 rounds). DIFFERENTIAL PRIVACY: Add noise to updates: Δw_noisy = Δw + N(0, σ²). Privacy guarantee: Even server can't infer individual samples. (3) Handling Non-IID Data: PROBLEM: Clients have different data distributions. Hospital A: mostly healthy patients. Hospital B: mostly sick. Standard FedAvg performs poorly (model biased). SOLUTIONS: FedProx: Add proximal term to loss: L = L_task + (μ/2) × ||w - w_global||². Keeps local models close to global model. Prevents divergence. FedBN: Use separate batch normalization layers per client. Global model shares conv layers, but BN stats remain local (capture local data distribution). Personalization: Global model + local fine-tuning. Train global model (federated), then each client fine-tunes on local data. Best of both: shared knowledge + local adaptation. Data augmentation: Augment local data to make distributions more similar. (4) Privacy Guarantees: SECURE AGGREGATION (Bonawitz et al): Clients encrypt updates. Server can only compute aggregate Σ Δw_i, not individual Δw_i. Uses multi-party computation (MPC). Protocol: Each client generates random mask r_i (sums to 0 across all clients: Σ r_i = 0). Client sends: Δw_i + r_i (masked update). Server computes: Σ (Δw_i + r_i) = Σ Δw_i + Σ r_i = Σ Δw_i (masks cancel). Server learns aggregate but not individual updates. DIFFERENTIAL PRIVACY: Add Gaussian noise to updates: Δw_noisy = Δw + N(0, σ²). Guarantees: (ε, δ)-DP. Example: ε=1.0, δ=10⁻⁵. Trade-off: Privacy vs accuracy. (5) Byzantine Fault Tolerance: PROBLEM: Malicious client sends corrupted update (attack model or poison data). Example: Δw_malicious = 1000 × Δw_i (huge update destroys model). DEFENSES: KRUM: Select most similar updates. Compute pairwise distances between updates. Select update closest to majority (discard outliers). Example: 100 clients, 1 malicious. Malicious update far from others → discarded. MEDIAN AGGREGATION: Instead of mean, use coordinate-wise median: w_global[i] = median(Δw_1[i], ..., Δw_N[i]) for each parameter i. Robust to outliers. TRIMMED MEAN: Remove top and bottom 10% of updates (outliers). Average remaining 80%. Robust to up to 10% Byzantine clients. NORM CLIPPING: Clip updates to max norm: if ||Δw_i|| >threshold, Δw_i = Δw_i × (threshold / ||Δw_i||). Prevents huge malicious updates from dominating. HANDLING MALICIOUS CLIENT: DETECTION: Compute update similarity. If client update far from average (cosine similarity <0.5) → flag as suspicious. Track history: If client repeatedly flagged → blacklist. MITIGATION: Use robust aggregation (Krum, median, trimmed mean). Clip norms (prevent huge updates). Differential privacy (noise masks individual contributions, harder to poison). MONITORING: Track model accuracy after each round. If accuracy drops >5% after round → rollback, investigate which client caused it. EXAMPLE: Federated Learning for Medical Diagnosis across 100 Hospitals: Round 1: Each hospital trains for 5 epochs locally. Computes weight update Δw_i. Hospital 50 is malicious: sends Δw_50 = 100 × Δw_50 (sabotage). Server uses Krum aggregation: Computes pairwise distances. Δw_50 far from others → discarded. Aggregates remaining 99 updates: w_global = w_old + (1/99) × Σ_{i≠50} Δw_i. Round 2: Hospital 50 blacklisted (repeated suspicious behavior). 99 hospitals continue training. Result: Model trained successfully despite malicious participant. Privacy: Hospital data never shared. Secure aggregation + DP. Accuracy: 89% (vs 91% centralized training, acceptable trade-off for privacy).",
    keyPoints: [
      'Architecture: Clients train locally, send weight updates (not data), server aggregates (FedAvg), broadcasts global model',
      'Communication efficiency: Quantization, top-k sparsification, local epochs (5 instead of 1), gradient compression',
      'Non-IID data: FedProx (proximal term), FedBN (local batch norm), personalization (global + local fine-tuning)',
      'Privacy: Secure aggregation (MPC, server sees only aggregate), differential privacy (add noise to updates)',
      'Byzantine: Detection (similarity check), robust aggregation (Krum, median, trimmed mean), norm clipping, blacklist malicious',
    ],
  },
];
