export const modelTrainingPipelineQuiz = [
  {
    id: 'mtp-q-1',
    question:
      'Design a scalable model training pipeline that can train multiple models in parallel, handle failures gracefully, and integrate with your ML platform. Address: (1) pipeline orchestration tool and workflow design, (2) distributed training strategy for large models, (3) checkpointing and resume capability, (4) resource allocation and scheduling, (5) monitoring and alerting. How would you handle a training job that fails 80% through a 24-hour run?',
    sampleAnswer:
      'Scalable Training Pipeline with Airflow + Kubeflow: (1) Orchestration (Airflow): DAG structure: data_validation >> feature_engineering >> [model_1_train, model_2_train, model_3_train] >> model_evaluation >> model_registration. Parallel training: Multiple models train simultaneously. Idempotent tasks: Can re-run safely. Retry logic: @task (retries=3, retry_delay=timedelta (minutes=5)). (2) Distributed Training (Horovod/PyTorch DDP): For large models (BERT, ResNet152). Data parallel: Split batch across GPUs. Each GPU has full model, processes different data. Gradient synchronization via AllReduce. Example: 8 GPUs, batch_size=256 → 32 per GPU. Linear speedup (8x faster). Model parallel: For models too large for single GPU. Split model layers across GPUs. Example: GPT-3 layers on different GPUs. Code: torch.nn.parallel.DistributedDataParallel (model, device_ids=[local_rank]). Initialize: torch.distributed.init_process_group (backend="nccl"). (3) Checkpointing: Save every N epochs (e.g., 5) + best model. Checkpoint includes: model weights, optimizer state, epoch number, random states. On failure: Resume from last checkpoint, not from scratch. Code: if epoch % 5 == 0: torch.save({"epoch": epoch, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict()}, f"checkpoint_epoch_{epoch}.pth"). Resume: checkpoint = torch.load("checkpoint_epoch_20.pth"); model.load_state_dict (checkpoint["model_state",]); optimizer.load_state_dict (checkpoint["optimizer_state",]); start_epoch = checkpoint["epoch",] + 1. (4) Resource Allocation (Kubernetes): Training jobs as K8s pods. Resource requests: GPU: 1 (nvidia.com/gpu: 1), CPU: 8, RAM: 64GB. Node affinity for GPU nodes. Priority classes: High priority for production retraining, low for experiments. Autoscaling: Scale GPU node pool based on pending jobs. Queue system: Kubeflow manages job queue, schedules based on resources. (5) Monitoring: Metrics: Loss curves (TensorBoard), GPU utilization (nvidia-smi), training throughput (samples/sec), ETA. Alerts: If loss NaN → stop training, if GPU util <50% → inefficient, if no progress for 1 hour → stuck. Dashboard: Grafana shows all active training jobs. HANDLING FAILURE AT 80% (19.2 hours): With checkpoints every 5 epochs: Resume from last checkpoint (maybe epoch 75 of 100). Only lose 5 epochs work, not 19 hours. Without checkpoints: Lose entire 19 hours, restart from scratch. Strategy: (1) Airflow detects failure, (2) Re-runs task with retry logic, (3) Training script loads latest checkpoint, (4) Resumes from that epoch. Prevention: Spot instance for cost savings but risky (can be interrupted). Use checkpointing + auto-resume. Preemptible instances cheaper (70% savings) but require fault tolerance.',
    keyPoints: [
      'Airflow for orchestration: DAG with parallel tasks, retries, idempotency for reliability',
      'Distributed training: Horovod/DDP for data parallel (8 GPUs → 8x speedup), model parallel for huge models',
      'Checkpointing every N epochs: save model + optimizer + epoch, resume on failure, critical for long runs',
      'Kubernetes for resources: GPU requests, priority classes, autoscaling, queue management',
      'Monitor loss/GPU/throughput, alert on NaN/stuck/low utilization, checkpoint saves 80% of work',
    ],
  },
  {
    id: 'mtp-q-2',
    question:
      'Explain the trade-offs between training on-premises (own hardware) vs cloud (AWS/GCP/Azure) for a company training ML models continuously. Consider: (1) cost analysis (CapEx vs OpEx), (2) scalability and flexibility, (3) maintenance and operations, (4) performance and latency, (5) data privacy and compliance. For a trading firm training models daily with sensitive data, which would you recommend?',
    sampleAnswer:
      "On-Premises vs Cloud: ON-PREMISES: (1) Cost: High upfront CapEx (\$10K per GPU server × 10 servers = $100K). Depreciate over 3 years. Operating costs: Power ($200/month per server), cooling, IT staff. Break-even: If utilization >60%, cheaper than cloud after 18 months. (2) Scalability: Fixed capacity. Buy 10 servers → stuck with 10. Can't scale up for spike, wasted capacity during low usage. Lead time for new hardware: 2-4 weeks. (3) Maintenance: Need IT team: Hardware failures (GPU/motherboard), software updates, networking. Downtime on failures. (4) Performance: No network latency to data (if data on-prem). Choose exact hardware (A100 GPUs). (5) Privacy: Full control. Data never leaves premises. Good for compliance (GDPR, HIPAA). CLOUD: (1) Cost: Pay-as-you-go OpEx. AWS p3.8xlarge (4x V100 GPUs): $12.24/hour. Train 8 hours/day × 30 days = $2,938/month. No upfront cost. Spot instances: 70% discount but can be interrupted. Reserved instances: 40% discount for 1-year commit. (2) Scalability: Infinite (practically). Need 100 GPUs for 1 day? Launch 100 instances. Scale down after. Elasticity major advantage. (3) Maintenance: Managed by cloud provider. No hardware failures to fix. Focus on models, not infrastructure. (4) Performance: Network latency to data (if data on-prem). Need to transfer data to cloud (slow for TB-scale). Egress costs ($0.09/GB from AWS). (5) Privacy: Data in cloud. Encryption (at rest, in transit). Compliance certifications (AWS: SOC 2, ISO 27001). Some regulations prohibit cloud (e.g., classified data). TRADING FIRM RECOMMENDATION: Hybrid Approach. (1) On-Prem: For production training with sensitive data. PII, trading strategies, proprietary data never leave premises. 10-20 GPU servers for daily retraining. Utilization high (80%+) → cost-effective. (2) Cloud: For research and experimentation. Less sensitive data. Scale up for large experiments (hyperparameter sweep with 100 configs). Use spot instances for cost savings. (3) Data Strategy: Keep sensitive data on-prem. Anonymized/aggregated data can go to cloud for research. Never upload raw PII or trading signals. (4) Cost Analysis: On-prem: $100K CapEx + $24K/year OpEx (power, cooling) = $32K/year effective cost (depreciate CapEx over 3 years). Cloud: $2,938/month × 12 = $35K/year for equivalent compute. Similar cost, but on-prem has privacy advantage. (5) Compliance: Financial regulations (SEC) may require audit trails, data residency. On-prem gives full control. Cloud requires careful configuration (VPC, encryption, audit logs). Implementation: Production pipeline on-prem, research platform on cloud (AWS SageMaker), encrypted data transfer (VPN) only for non-sensitive data. Best of both worlds.",
    keyPoints: [
      'On-prem: High CapEx, fixed capacity, full control, good for compliance, cheaper if utilization >60%',
      'Cloud: Pay-as-you-go OpEx, infinite scale, managed infrastructure, higher cost for continuous use',
      'Trading firm: Hybrid—on-prem for production (sensitive data, compliance), cloud for research/experiments',
      'Cost break-even: ~18 months for on-prem if high utilization, cloud better for variable workloads',
      'Privacy: On-prem mandatory for classified/PII, cloud OK with encryption/compliance certs for less sensitive',
    ],
  },
  {
    id: 'mtp-q-3',
    question:
      'Design a complete GPU utilization monitoring and optimization strategy. Address: (1) metrics to track (GPU memory, compute utilization, throughput), (2) common bottlenecks and how to identify them, (3) optimization techniques (mixed precision, gradient accumulation, data loading), (4) multi-GPU utilization strategies, (5) cost optimization. If GPU utilization is only 30%, how would you diagnose and fix the issue?',
    sampleAnswer:
      "GPU Optimization Strategy: (1) Metrics to Track: GPU compute utilization: nvidia-smi dmon -s u. Target: >80% during training. Low util (<50%) = bottleneck elsewhere. GPU memory: nvidia-smi --query-gpu=memory.used,memory.total --format=csv. Use 80-90% of available memory (maximize batch size). Throughput: Samples/second. Track over epochs to detect slowdowns. Data loading time: Profile with torch.utils.bottleneck or cProfile. High data load time = CPU bottleneck. Power consumption: Watt usage. High power + low compute = inefficiency. (2) Common Bottlenecks: CPU-bound data loading: Data preprocessing slow (image augmentation, tokenization). GPU waits for batches. Symptom: GPU util 20-40%, CPU 100%. Fix: Increase num_workers in DataLoader, prefetch batches. Small batch size: GPU underutilized. Symptom: Low GPU memory usage (20%), low compute util. Fix: Increase batch size. I/O bottleneck: Reading from slow disk (HDD vs SSD). Symptom: High data loading time. Fix: Use SSD, cache data in RAM, pre-load dataset. Model bottleneck: Operations non-parallelizable (sequential RNN). Symptom: GPU util high but slow. Fix: Use parallel operations (Transformer vs RNN). (3) Optimization Techniques: Mixed precision (FP16): 2x faster, 2x memory savings. Code: from torch.cuda.amp import autocast, GradScaler; scaler = GradScaler(); with autocast(): loss = model (input). Backward: scaler.scale (loss).backward(); scaler.step (optimizer). Gradient accumulation: Simulate large batch with small batches (for memory limits). Accumulate gradients over 4 steps, then update. Code: for i, batch in enumerate (loader): loss = model (batch) / accumulate_steps; loss.backward(); if (i + 1) % accumulate_steps == 0: optimizer.step(); optimizer.zero_grad(). Data loading: Use DataLoader with num_workers=4-8 (parallel data loading), pin_memory=True (faster transfer to GPU), persistent_workers=True (reuse workers). Cache small datasets in RAM. Larger batch size: Increase until OOM. Find max batch size with binary search. Larger batches = better GPU utilization. Profiler: torch.profiler to find bottlenecks. Shows time per operation (conv2d, matmul, data loading). (4) Multi-GPU: Data parallel: Batch split across GPUs. Linear speedup if communication overhead low. Use DistributedDataParallel (not DataParallel—faster). Pipeline parallel: For large models. Split layers across GPUs (layer 1-10 on GPU 0, 11-20 on GPU 1). Tensor parallel: Split single layer across GPUs (for massive layers). Monitor: GPU communication time (AllReduce). High communication = bottleneck. Use faster interconnect (NVLink, InfiniBand). (5) Cost Optimization: Right-size instances: Don't use p3.16xlarge (8 GPUs) if training on 1 GPU. Use spot instances: 70% cheaper. Checkpoint frequently to survive interruptions. Batch jobs: Train multiple models sequentially on same instance (reuse without stopping). Profile before scaling: Optimize single-GPU training first, then scale. DIAGNOSING 30% GPU UTILIZATION: Step 1: Profile. Code: python -m torch.utils.bottleneck train.py. Output shows time breakdown: data loading, forward pass, backward pass. Step 2: Check DataLoader. If data loading >50% of time → CPU bottleneck. Fix: Increase num_workers from 2 to 8. Prefetch batches. Step 3: Check batch size. Run nvidia-smi. If GPU memory <50% used → increase batch_size. Binary search: Try batch_size 64 → OOM → try 56 → success. Step 4: Mixed precision. Enable AMP. 2x speedup, higher util. Step 5: Check model operations. Are there sequential bottlenecks (loops, RNNs)? Vectorize operations. Example fix: GPU util 30% → Increased num_workers from 2 to 8 (data loading faster) + enabled mixed precision (FP16) + increased batch_size from 32 to 64 → GPU util 85%.",
    keyPoints: [
      'Track GPU compute (>80% target), memory (use 80-90%), throughput (samples/sec), data load time',
      'Bottlenecks: CPU-bound data loading (fix: num_workers), small batch (fix: increase), I/O (fix: SSD/cache)',
      'Optimizations: Mixed precision (FP16, 2x speed), gradient accumulation (large effective batch), larger batch size',
      'Multi-GPU: DistributedDataParallel for data parallel, pipeline parallel for huge models, monitor communication',
      'Diagnose 30% util: Profile with bottleneck tool, increase num_workers, enable FP16, maximize batch size',
    ],
  },
];
