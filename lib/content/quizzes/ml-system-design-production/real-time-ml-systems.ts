export const realTimeMlSystemsQuiz = [
  {
    id: 'rtms-q-1',
    question:
      'Design a real-time ML system for fraud detection that must approve/decline transactions in <100ms. Address: (1) streaming data pipeline architecture, (2) feature computation (cached vs real-time), (3) model optimization for low latency, (4) fallback strategies if system fails, (5) monitoring drift in real-time. How would you handle a sudden spike in traffic (Black Friday)?',
    sampleAnswer:
      "Real-Time Fraud Detection (<100ms): (1) Streaming Pipeline: ARCHITECTURE: Transaction → Kafka → Feature computation (Flink) → Redis (cache) → Model API → Decision. KAFKA: Ingests transactions (100K/sec). Topics: transactions_raw, features_computed, predictions. Partitioned by user_id (for ordering). FLINK: Stateful stream processing. Computes real-time features: transaction_velocity (count in last 1 hour), amount_vs_avg_ratio. Maintains sliding windows. Updates Redis. Latency: <5ms per event. (2) Feature Computation: CACHED (Redis): Expensive features pre-computed nightly: user_30d_transaction_count, user_30d_avg_amount, merchant_risk_score. Lookup: <2ms. TTL: 24 hours. REAL-TIME (Flink): Cheap features computed per transaction: transaction_hour (time.hour), amount_zscore ((amount - user_avg) / user_std), time_since_last_transaction. Latency: <3ms. HYBRID: Model uses 20 features: 15 cached (historical), 5 real-time. Total feature lookup: 2ms (Redis) + 3ms (compute) = 5ms. (3) Model Optimization: MODEL CHOICE: XGBoost (not deep learning). Inference: <10ms for 50 trees. Why: Faster than neural network, interpretable (for compliance). QUANTIZATION: INT8 quantization. 4x smaller, 2x faster. Accuracy drop: <0.5%. Inference: 10ms → 5ms. CACHING MODEL: Load model in memory at startup (not per request). GPU inference (optional): If traffic spike, use GPU. 10ms → 3ms. But cost ↑. BATCHING: Collect requests for 5ms, batch (size 16), infer together. Throughput ↑, latency: 5ms (wait) + 5ms (batch infer) = 10ms. Effective latency: 10-15ms (acceptable). LATENCY BREAKDOWN: Kafka → service: 5ms. Feature lookup (Redis): 2ms. Feature computation (real-time): 3ms. Model inference (INT8 XGBoost): 5ms. Decision logic: 1ms. Response: 2ms. Total: 18ms. Buffer: 82ms. P99 latency: 40ms (well under 100ms). (4) Fallback Strategies: SCENARIO: Model API down (server crash, network issue). FALLBACKS: Fallback 1 (rule-based): Use simple rules. Example: If amount >$1000 AND new_merchant AND hour=3am → Decline. Else → Approve. Latency: <1ms. Accuracy: 70% (vs 92% for ML model). Fallback 2 (cached predictions): Cache predictions for common scenarios. Example: User 123, amount $50, merchant Starbucks → Approve (cached). Covers 30% of transactions. Fallback 3 (fail-open vs fail-closed): Fail-open: Approve all transactions (risk fraud, but don\'t block revenue). Fail-closed: Decline all (prevent fraud, but block legitimate). Choice: Depends on risk tolerance. Most companies: Fail-open for low amounts (<$100), fail-closed for high amounts (>$1000). CIRCUIT BREAKER: If model error rate >5% for 1 minute → switch to fallback. Prevents cascading failures. Monitor: Try model every 30 seconds. If healthy → switch back. (5) Monitoring Drift: PREDICTION DRIFT: Track distribution of fraud predictions. Baseline: 5% predicted fraud rate. Real-time: If jumps to 15% → drift detected. Alert. FEATURE DRIFT: Track feature distributions (mean, std). Example: transaction_amount mean = $150 (baseline). If shifts to $250 → drift. Use exponentially weighted moving average (EWMA): EWMA_t = α × value_t + (1-α) × EWMA_{t-1}. Alert if |EWMA - baseline| >3σ. PERFORMANCE MONITORING: Ground truth: Fraud outcome known in 24 hours (chargeback). Track: False positive rate (declined legitimate transaction), false negative rate (approved fraud). Daily report: If FPR >2% or FNR >10% → retrain. ONLINE LEARNING: Incremental model updates: Every 1 hour, retrain on last 24 hours of data (with labels). Update model (rolling deployment). Adapt to new fraud patterns quickly. BLACK FRIDAY TRAFFIC SPIKE: NORMAL: 10K transactions/sec. BLACK FRIDAY: 100K transactions/sec (10x spike). HANDLING: AUTO-SCALING: K8s HPA (Horizontal Pod Autoscaler). Trigger: CPU >70% or request queue >100 → scale up. Scale: 50 pods → 500 pods in 5 minutes. Pre-warming: Day before Black Friday, pre-scale to 300 pods (anticipate spike). LOAD BALANCER: ALB distributes traffic across 500 pods. Each pod: 200 req/sec capacity. Total: 100K req/sec. CACHING: Cache hit rate critical. Pre-warm cache (load hot features for 1M users). Hit rate: 90% → only 10K req/sec hit database (manageable). MONITORING: Real-time dashboard: Current RPS, P99 latency, error rate, auto-scaling status. Alerts: If P99 >100ms → page on-call. DATABASE: Read replicas (10 replicas) for feature lookup. Avoid write bottleneck (only write fraud labels, not critical path). FALLBACK: If system overwhelmed (P99 >200ms) → use cached predictions for 50% of traffic (reduce load). Result: System handles Black Friday with P99 latency 60ms (under 100ms). Zero downtime.",
    keyPoints: [
      'Streaming: Kafka (transactions) → Flink (real-time features) → Redis (cache) → Model API, <100ms end-to-end',
      'Features: Cached in Redis (user_30d_stats, 2ms), real-time computed (transaction_velocity, 3ms), hybrid approach',
      'Model optimization: XGBoost (not DL), INT8 quantization (2x faster), load in memory, batching (size 16, 10ms)',
      'Fallbacks: Rule-based (70% accuracy, <1ms), cached predictions (30% coverage), fail-open (<$100) vs fail-closed (>$1000), circuit breaker',
      'Black Friday: Auto-scale (50 → 500 pods), pre-warm cache (90% hit rate), load balancer, read replicas, handle 100K req/sec',
    ],
  },
  {
    id: 'rtms-q-2',
    question:
      'Explain online learning and when to use it. Compare with batch retraining. Address: (1) incremental learning algorithms, (2) handling concept drift, (3) catastrophic forgetting prevention, (4) validation strategies, (5) when online learning is appropriate. For a stock price prediction model, would you use online or batch learning?',
    sampleAnswer:
      "Online Learning vs Batch: ONLINE LEARNING: Train model incrementally on new data as it arrives. Update weights continuously. Example: User clicks ad → update recommendation model immediately. BATCH LEARNING: Train model on full dataset periodically (daily/weekly). Replace old model with new. Example: Fraud model retrained nightly on yesterday\'s data. (1) Incremental Learning Algorithms: STOCHASTIC GRADIENT DESCENT (SGD): Update: w = w - η × ∇L(x_i, y_i). Process one sample at a time. Online by nature. MINI-BATCH SGD: Update: w = w - η × ∇L(X_batch, y_batch). Process small batches (size 32). Balance: Update frequency vs stability. ONLINE ALGORITHMS: Scikit-learn: SGDClassifier, SGDRegressor (partial_fit method). Online Random Forest (Mondrian Forest). Online SVM. Neural networks (naturally online with SGD). Example: from sklearn.linear_model import SGDClassifier; model = SGDClassifier(); for X_batch, y_batch in stream: model.partial_fit(X_batch, y_batch, classes=[0, 1]). (2) Handling Concept Drift: PROBLEM: Data distribution changes over time. Model trained on old distribution becomes stale. Example: User preferences change (summer: light clothes, winter: warm clothes). DETECTION: Monitor: Prediction accuracy over time (rolling window). Feature distributions (KS test). If drift detected → accelerate retraining or increase learning rate. ADAPTIVE LEARNING RATE: Normal: η = 0.01. Drift detected: η = 0.05 (learn faster from new data). Stabilize: η = 0.01 after adapting. SLIDING WINDOW: Only use recent data (last 30 days). Forget old data (concept has shifted). Implementation: Buffer of recent samples (size 10K). Train on buffer. Old samples evicted. (3) Catastrophic Forgetting: PROBLEM: Online learning forgets old patterns. Model trained on A, then trained on B → forgets A. Example: Fraud model sees mostly legitimate transactions → forgets fraud patterns → misses future fraud. SOLUTIONS: EXPERIENCE REPLAY: Maintain buffer of past examples (reservoir sampling). For each update: Train on new sample + random sample from buffer. Ensures old patterns retained. Implementation: buffer = ReservoirSampler(size=10K); for x, y in stream: model.update(x, y); if buffer.size() >0: x_old, y_old = buffer.sample(); model.update(x_old, y_old); buffer.add(x, y). ELASTIC WEIGHT CONSOLIDATION (EWC): Regularize important weights (learned from previous data). L = L_new + λ × Σ F_i × (w_i - w_i^*)². F_i = Fisher information (importance of weight i). Prevents important weights from changing too much. ENSEMBLE: Maintain multiple models: Model A (trained on old data), Model B (online, adapting to new data). Blend: prediction = 0.7 × A + 0.3 × B. Combines stability (A) + adaptation (B). (4) Validation Strategies: PREQUENTIAL EVALUATION (Test-then-Train): For each sample: Test model on sample (compute accuracy). Train model on sample (update weights). Track accuracy over time (sliding window). Simulates real online learning. HOLD-OUT VALIDATION SET: Reserve 10% of stream for validation (don\'t train on it). Track validation accuracy. If drops >5% → adjust learning rate or retrain. WALK-FORWARD: Periodically (weekly), evaluate on last week\'s data. Compare online model vs batch retrained model. Ensure online learning competitive. (5) When to Use Online Learning: USE ONLINE: Fast-changing data: User behavior, stock prices, news sentiment. Shifts daily/hourly. Continuous data stream: IoT sensors, web clicks, transactions. No natural batch boundary. Low latency required: Need immediate adaptation (personalized recommendations). Resource constrained: Can\'t store all data (mobile devices). Online updates with small batches. USE BATCH: Slow-changing data: Fraud patterns, churn prediction. Changes monthly, not daily. Computational resources available: Can afford to retrain full model periodically. Need reproducibility: Batch retraining easier to version and audit. Complex models: Deep learning models often need full dataset for training (batch normalization, etc.). STOCK PRICE PREDICTION: RECOMMENDATION: HYBRID (Batch + Online). REASONING: Batch (weekly): Train complex model (LSTM, XGBoost) on 5 years of data. Captures long-term patterns (seasonality, macro trends). Stable baseline. Online (hourly): Incremental updates on last 24 hours. Adapts to short-term market regime (bull/bear shift, volatility spike). Fast adaptation. ARCHITECTURE: Base model (batch): Trained weekly on 5 years. Predicts based on long-term patterns. Online adjustment: Residual model. Predicts: (actual - base_prediction). Trained online. Final prediction: base_prediction + online_adjustment. VALIDATION: Compare: Batch-only, online-only, hybrid. Metrics: RMSE, Sharpe ratio, max drawdown. Empirically: Hybrid often best (stability + adaptation). EXAMPLE: Week 1: Batch model trained (RMSE: 2.5). Day 1-5: Online model adapts (RMSE: 2.5 → 2.2). Week 2: Market regime shifts (high volatility). Online model adapts quickly (RMSE: 2.2 → 2.3). Batch model stale (RMSE: 2.5 → 3.5). Week 3: Batch retrained with new data (RMSE: 2.4). Online model continues adapting. Result: Hybrid achieves lower error (2.3 vs 3.5 for batch-only). IMPLEMENTATION: # Batch training (weekly); base_model = train_lstm(data_5_years); # Online updates (hourly); online_model = SGDRegressor(); for new_data in hourly_stream: # Predict; base_pred = base_model.predict(new_data); online_adj = online_model.predict(new_data); final_pred = base_pred + online_adj; # Wait for actual price (1 hour later); residual = actual - base_pred; # Update online model; online_model.partial_fit(new_data, residual). Catastrophic forgetting mitigation: Experience replay on historical data.",
    keyPoints: [
      'Online learning: Incremental updates (SGD, partial_fit), continuous adaptation, low latency, for fast-changing data',
      'Concept drift: Monitor accuracy/distributions, adaptive learning rate, sliding window (recent data only)',
      'Catastrophic forgetting: Experience replay (buffer past examples), EWC (regularize important weights), ensemble (old+new)',
      'Validation: Prequential (test-then-train), hold-out validation set, walk-forward comparison with batch',
      'Stock prediction: Hybrid (batch weekly for long-term + online hourly for short-term), best of both, experience replay',
    ],
  },
  {
    id: 'rtms-q-3',
    question:
      'Design a low-latency feature store for real-time ML. Address: (1) architecture (offline vs online stores), (2) feature freshness requirements, (3) point-in-time correctness for training, (4) caching strategies, (5) monitoring. How would you serve features with <5ms latency for 100K requests/sec?',
    sampleAnswer:
      'Low-Latency Feature Store: (1) Architecture: OFFLINE STORE (Training): Storage: S3 Parquet files or BigQuery. Contains: All historical feature values with timestamps. Purpose: Training data retrieval. Point-in-time correct joins (no data leakage). Query: "Get features for user_id=123 at timestamp=2024-01-15 10:00". Returns features computed before 10:00 (not after). Latency: Seconds/minutes (acceptable for training). ONLINE STORE (Serving): Storage: Redis (key-value) or DynamoDB. Contains: Latest feature values only. Key: user_id. Value: {clicks_24h: 45, avg_dwell: 32.5, last_purchase: "2024-01-20"}. Purpose: Real-time model serving. Latency: <5ms. SYNC PIPELINE: Batch job (nightly): Compute historical features (30-day stats). Write to offline store (S3). Materialize to online store (Redis). Streaming (real-time): Kafka → Flink. Compute real-time features (last_1h_clicks). Update online store (Redis). (2) Feature Freshness: BATCH FEATURES: user_30d_transaction_count, user_lifetime_value. Freshness: 24 hours (updated nightly). Acceptable staleness: Up to 1 day old. STREAMING FEATURES: transaction_velocity_1h, clicks_last_hour. Freshness: 5 minutes (updated by Flink every 5 min). Acceptable staleness: 5 minutes. REAL-TIME FEATURES: transaction_amount, time_of_day. Freshness: 0 (computed per request). No staleness. TTL (Time-to-Live): Redis keys have TTL: Batch features: 25 hours (refresh before expiry). Streaming features: 10 minutes (refresh before expiry). Eviction: Stale features evicted automatically. (3) Point-in-Time Correctness: PROBLEM: Training uses features from future (data leakage). Example: Predict conversion at 10:00 using features computed at 11:00. Model learns from future → unrealistic accuracy. SOLUTION (Offline Store): Temporal joins. For each training example (user_id, timestamp, label): Retrieve features valid at timestamp. Query: SELECT * FROM features WHERE user_id=123 AND feature_timestamp <= \'2024-01-15 10:00\' ORDER BY feature_timestamp DESC LIMIT 1. Returns most recent features before prediction time. Implementation (Feast): entity_df = pd.DataFrame({"user_id": [123, 456], "event_timestamp": ["2024-01-15 10:00", "2024-01-16 14:00",]}); training_data = store.get_historical_features(entity_df=entity_df, features=["user_features:clicks_24h", "user_features:avg_dwell",]).to_df(). Feast handles temporal joins automatically. No leakage. (4) Caching Strategies: L1 (Application Cache): In-memory LRU cache (each service instance). Size: 10K entries. TTL: 1 minute. Latency: <1ms. Hit rate: 20% (frequently accessed users). L2 (Redis): Shared cache (all service instances). Size: 10M entries. TTL: 24 hours (batch features), 10 min (streaming features). Latency: <5ms. Hit rate: 70%. L3 (Database/Feature Store): Fallback if cache miss. Latency: 50-100ms. Hit rate: 10% (rare). CACHE WARMING: Pre-load cache with hot users (1M most active users) at startup. Prevents cold start (first request slow). Scheduled refresh: Every hour, refresh top 100K users in cache. CACHE INVALIDATION: On feature update (new computation): Invalidate stale keys in Redis. Write new values. TTL-based: Features expire automatically after TTL. Next request fetches fresh value. (5) Monitoring: LATENCY: Track per feature group: P50, P95, P99. Alert if P99 >5ms. CACHE HIT RATE: L1: 20%, L2: 70%, L3: 10%. Alert if L2 <60% (cache not effective). FRESHNESS: Track: time_since_last_update per feature. Alert if batch features >25 hours old (should update nightly). ERRORS: Missing features (key not in Redis): 0.1% acceptable (new users). Alert if >1%. Feature computation failures (Flink job crashes): Alert immediately. DATA QUALITY: Track feature distributions (mean, std, nulls). Alert if drift detected (PSI >0.25). HANDLING 100K REQ/SEC WITH <5MS LATENCY: REDIS CLUSTER: Shard data across 20 Redis nodes. Each node: 5K req/sec capacity. Total: 100K req/sec. Replication: 3 replicas per node (read scalability + fault tolerance). Read from replicas. PIPELINING: Batch Redis queries. Fetch 10 features in 1 round-trip (not 10 round-trips). Latency: 1 request = 1ms overhead + 0.5ms per feature. Without pipelining: 10 requests = 10ms. With pipelining: 1 request = 1ms + 5ms (fetch 10) = 6ms. Still under 5ms if fetch <8 features. CONNECTIONS: Connection pooling (100 connections per service instance). Avoid connection overhead. NETWORK: 10Gb NICs, low-latency switches. Co-locate services with Redis (same AZ). Reduces network latency. SERVICE INSTANCES: 500 service instances. Each handles 200 req/sec. Auto-scaling based on queue depth. FALLBACK: If Redis slow (P99 >10ms): Use L1 cache only (stale but fast). Degrade gracefully (serve predictions with slightly stale features). LATENCY BREAKDOWN: Application cache lookup (L1): 0.5ms (hit) or 0ms (miss). Redis lookup (L2): 3ms (hit) or 0ms (miss). Feature computation (real-time): 1ms. Total: 0.5ms (L1 hit) or 4ms (L2 hit) or 100ms (L3 hit). P99 latency: 4.5ms (95% L1+L2 hits). Meets <5ms requirement. Result: 100K req/sec, P99 latency 4.5ms, 90% cache hit rate.',
    keyPoints: [
      'Architecture: Offline store (S3/BigQuery, training, point-in-time joins), online store (Redis, serving, <5ms)',
      'Freshness: Batch (24h, nightly), streaming (5 min, Flink), real-time (per request), TTL-based expiry',
      'Point-in-time: Temporal joins (features at timestamp ≤ prediction time), Feast handles automatically, no leakage',
      'Caching: L1 (in-memory, 1ms, 20% hit), L2 (Redis, 3ms, 70% hit), L3 (DB, 100ms, 10% hit), pre-warm hot users',
      '100K req/sec: Redis cluster (20 nodes, 5K req/sec each), pipelining (batch 10 features), connection pooling, P99 4.5ms',
    ],
  },
];
