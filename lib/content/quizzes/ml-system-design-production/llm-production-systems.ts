export const llmProductionSystemsQuiz = [
  {
    id: 'llmps-q-1',
    question:
      'Design a production LLM system for a customer support chatbot handling 10K concurrent users. Address: (1) token streaming for better UX, (2) cost optimization strategies, (3) prompt engineering and validation, (4) monitoring LLM-specific metrics, (5) fallback strategies. How would you keep costs under $10K/month while maintaining quality?',
    sampleAnswer:
      'Production LLM Chatbot System: (1) Token Streaming: USER EXPERIENCE: Without streaming: User waits 5-10 seconds → full response appears. With streaming: Tokens appear incrementally (ChatGPT-style). First token in <1 second. IMPLEMENTATION (FastAPI + OpenAI): from fastapi import FastAPI; from fastapi.responses import StreamingResponse; import openai; async def generate_stream (prompt): response = openai.ChatCompletion.create (model="gpt-3.5-turbo", messages=[{"role": "user", "content": prompt}], stream=True); for chunk in response: if "content" in chunk["choices",][0]["delta",]: token = chunk["choices",][0]["delta",]["content",]; yield f"data: {token}\\\\n\\\\n"; @app.post("/chat"); async def chat (prompt: str): return StreamingResponse (generate_stream (prompt), media_type="text/event-stream"). Frontend: EventSource to consume stream. Benefits: Lower perceived latency, better UX, can cancel mid-generation (save cost). (2) Cost Optimization: CURRENT COST (no optimization): 10K users × 20 messages/day × 365 days = 73M messages/year. Average: 500 input tokens + 200 output tokens per message. GPT-3.5: ($0.0015/1K input + $0.002/1K output) × 700 tokens = $0.00175 per message. Total: 73M × $0.00175 = $127,750/year = $10,646/month. Over budget! OPTIMIZATIONS: (a) Response Caching (Redis): Common questions cached. "What are your business hours?" → cached response (free). Cache hit rate: 40% → save 40% of API calls. Cost: $10,646 × 0.6 = $6,388/month. Savings: $4,258/month. (b) Prompt Optimization: Reduce prompt length. Before: 1000 tokens (full conversation history + system prompt). After: 400 tokens (summarize old messages, concise system prompt). Savings: 60% input tokens. Cost: $6,388 × (1 - 0.6×0.5) = $4,466/month. Savings: $1,922/month. (c) Model Selection: Use GPT-3.5 for simple queries (80%), GPT-4 for complex (20%). GPT-3.5: $0.0015/1K input. GPT-4: $0.03/1K input (20× more expensive). Weighted cost: 0.8 × GPT-3.5 + 0.2 × GPT-4 = 0.8 × $0.0015 + 0.2 × $0.03 = $0.0072/1K. But: Only use GPT-4 when needed. Classify query complexity (simple keyword match). If "refund policy" → GPT-3.5. If "complex technical issue" → GPT-4. Savings: Minimal (most queries simple). (d) Rate Limiting: Limit: 20 messages/user/day (prevent abuse). Reduces wasteful traffic. Savings: 10%. Cost: $4,466 × 0.9 = $4,019/month. Savings: $447/month. (e) Fine-tuned Model (advanced): Fine-tune GPT-3.5 on company FAQs. Shorter prompts needed (no examples). Cost: $0.012/1K tokens (8× base rate) but 50% fewer tokens. Net: 0.5 × 8 = 4× base rate. Not cost-effective unless massive volume. FINAL COST: $4,019/month < $10K target ✓. Total savings: 62% from caching, prompt optimization, rate limiting. (3) Prompt Engineering & Validation: STRUCTURED PROMPTS: Consistent format. Reduces variability, easier to validate. Example: You are a customer support agent for TechCorp. Rules: 1. Be polite and professional. 2. If you don\'t know, say "I don\'t know" (don\'t hallucinate). 3. Output format: {{"answer": "...", "confidence": 0.0-1.0, "needs_human": true/false}}. Customer query: {query}. Response:. OUTPUT VALIDATION: Parse JSON response. Check: "answer" field exists, "confidence" 0-1, "needs_human" boolean. If invalid → retry with clarification prompt (max 2 retries). If still invalid → fallback (return generic response). RETRY LOGIC: def call_llm_with_retry (prompt, max_retries=3): for attempt in range (max_retries): response = llm_api_call (prompt); parsed = validate_response (response); if parsed: return parsed; prompt += "\\\\n\\\\nPlease ensure output is valid JSON."; raise ValueError("Failed after max retries"). PROMPT TESTING: Test suite: 100 curated queries (edge cases, common questions). Run through prompt templates. Measure: Accuracy (% correct answers), hallucination rate (% invented facts), format compliance (% valid JSON). Regression test: New prompt versions must match or beat baseline. (4) Monitoring LLM-Specific Metrics: COST TRACKING: Daily cost: Track tokens used, API calls, $ spent. Alert if daily cost >$350 (monthly pace >$10K). Cost per user: Identify heavy users (abuse detection). LATENCY: Time to first token (TTFT): <1 second (streaming benefit). Total generation time: 3-10 seconds typical. P99: <15 seconds. Alert if TTFT >2 seconds or P99 >20 seconds. TOKEN USAGE: Input tokens: Mean, P95. Track prompt length. Output tokens: Mean, P95. Track response verbosity. Alert if input tokens spike (prompt optimization failed). QUALITY METRICS: User thumbs up/down: Track satisfaction. Target: >80% thumbs up. Regeneration requests: If user clicks "regenerate" → dissatisfied. Target: <10%. Human escalation rate: % conversations escalated to human agent. Target: <5%. HALLUCINATION DETECTION: Fact-checking: Cross-reference LLM answers with knowledge base. Flag mismatches. Confidence threshold: If confidence <0.7 → flag for review. User reports: Allow users to report incorrect answers. Track rate. (5) Fallback Strategies: SCENARIO 1: LLM API down (outage). FALLBACK: Rule-based chatbot (simpler, deterministic). Example: Keyword matching. "refund" → "Our refund policy is...". Covers 60% of queries. Latency: <100ms (fast). Quality: Lower but functional. SCENARIO 2: Rate limit hit (429 error). FALLBACK: Queue requests. Inform user: "High traffic, please wait 30 seconds." Retry with exponential backoff. If still fails → show cached response or "Service temporarily unavailable". SCENARIO 3: Slow response (>15 seconds). FALLBACK: Cancel LLM call, use cached response for similar query. Or: Show partial response (if streaming), append "..." SCENARIO 4: Budget exceeded (\$10K/month). FALLBACK: Switch to cheaper model (GPT-3.5 → text-davinci-003, even cheaper). Or: Reduce functionality (limit messages per user, disable for free users). CIRCUIT BREAKER: If error rate >5% for 5 minutes → open circuit. Stop calling LLM API, use fallback exclusively. Retry LLM every 2 minutes (check if recovered). MONITORING: Dashboard: Current status (LLM healthy / fallback active). Recent error rate, latency, cost. Fallback usage (% requests served by fallback). COMPREHENSIVE SYSTEM: Request → Rate limiter → Cache check (hit: return, miss: proceed) → Complexity classifier (simple: GPT-3.5, complex: GPT-4) → Prompt builder (optimize length) → LLM API (with streaming) → Validation → Response. If API fails → Fallback (rule-based). Monitor: Cost, latency, quality, fallback usage. Result: 10K concurrent users, <$10K/month, 80%+ satisfaction, <1s TTFT, robust fallbacks.',
    keyPoints: [
      'Streaming: FastAPI + OpenAI stream=True, SSE, first token <1s, better UX, can cancel mid-generation',
      'Cost optimization: Caching (40% hit rate, save $4,258), prompt optimization (reduce 60%, save $1,922), rate limiting (save 10%)',
      'Prompts: Structured format (JSON output), validation (parse + retry), testing (100 queries, measure accuracy/hallucination)',
      'Monitoring: Cost (daily, alert >$350), latency (TTFT <1s, P99 <15s), quality (thumbs up >80%, escalation <5%), hallucination detection',
      'Fallbacks: Rule-based chatbot (API down), queue + retry (rate limit), cached response (slow), circuit breaker (error rate >5%)',
    ],
  },
  {
    id: 'llmps-q-2',
    question:
      'Compare different LLM deployment strategies. Address: (1) hosted APIs (OpenAI, Anthropic) vs self-hosted, (2) model selection (GPT-4 vs GPT-3.5 vs open-source), (3) cost-performance trade-offs, (4) latency considerations, (5) when to fine-tune. For a startup with limited resources, what would you recommend?',
    sampleAnswer:
      'LLM Deployment Strategies: (1) Hosted APIs vs Self-Hosted: HOSTED APIs (OpenAI, Anthropic, Google): PROS: Zero infrastructure (no servers, no maintenance). Fast time-to-market (integrate in hours). Auto-scaling (handles any load). Latest models (GPT-4, Claude-3). CONS: Ongoing cost (pay per token). No data control (data sent to API). Vendor lock-in. Rate limits. COST: GPT-3.5: $0.0015/1K input, $0.002/1K output. GPT-4: $0.03/1K input, $0.06/1K output. Example: 1M requests × 700 tokens = $1,750/month (GPT-3.5) or $63,000/month (GPT-4). SELF-HOSTED (LLaMA, Mistral, Falcon): PROS: One-time cost (buy GPUs, pay once). Data privacy (runs on your servers). No rate limits. Customizable (can modify model). CONS: Infrastructure cost (\$30K for 4× A100 GPUs). Maintenance (ML engineers, DevOps). Slower updates (new models require re-deployment). Lower quality than GPT-4 (but improving). COST: Setup: $30K (GPUs) + $10K (servers) = $40K. Ongoing: $2K/month (power, maintenance). Break-even: If API cost >$2K/month, self-hosted cheaper after 20 months. (2) Model Selection: GPT-4: Best quality (reasoning, complex tasks). Expensive ($0.06/1K output). Slow (10-15 seconds). Use for: Complex queries, high-value tasks (legal analysis, medical diagnosis). GPT-3.5-TURBO: Good quality (90% of GPT-4). 20× cheaper ($0.002/1K output). Fast (2-5 seconds). Use for: Most tasks (customer support, content generation). OPEN-SOURCE (LLaMA 2, Mistral, Falcon): Quality: 70-80% of GPT-3.5 (depending on task). Cost: Free (once you have infrastructure). Latency: Fast (self-hosted, no network). Use for: Privacy-sensitive tasks, high volume (cost prohibitive with APIs), customization needed. RECOMMENDATION: Start with GPT-3.5 (cheap, good quality), upgrade to GPT-4 for specific hard queries (hybrid approach). (3) Cost-Performance Trade-offs: SCENARIO: Chatbot with 1M messages/month. GPT-4: Quality: 95/100. Cost: $63K/month. Latency: 10 seconds. GPT-3.5: Quality: 90/100. Cost: $1,750/month. Latency: 3 seconds. LLaMA 2 (self-hosted): Quality: 75/100. Cost: $2K/month (after setup). Latency: 2 seconds. TRADE-OFF ANALYSIS: If quality critical (legal, medical): GPT-4 worth premium. If budget limited: GPT-3.5 best balance (90% quality, 36× cheaper than GPT-4). If privacy critical (healthcare, finance): Self-hosted (LLaMA 2) despite lower quality. HYBRID: 80% queries GPT-3.5 (simple), 20% queries GPT-4 (complex). Weighted cost: 0.8 × $1,750 + 0.2 × $63K = $14K/month. Quality: 92/100 (better than GPT-3.5, cheaper than all-GPT-4). (4) Latency: HOSTED API: Network latency: 50-200ms. Generation: 2-10 seconds (depending on model, output length). Total: 2-10 seconds. Optimization: Use streaming (perceived latency <1 second). SELF-HOSTED: Network latency: 5ms (local). Generation: 1-5 seconds (GPU inference). Total: 1-5 seconds (faster than API). But: Requires GPU infrastructure ($$$). BATCHING: API: Supports batching (multiple requests in parallel). Cost: Same (billed per token). Self-hosted: Batching crucial (amortize GPU load). Cost: More efficient (one inference for batch). (5) When to Fine-Tune: FINE-TUNING: Train LLM on custom dataset (company docs, FAQs). Improves performance on specific domain. COST: OpenAI fine-tuning: $0.008/1K tokens training. GPT-3.5-turbo fine-tuned: $0.012/1K tokens inference (8× base rate). Self-hosted: Free (if you have GPUs). WHEN TO FINE-TUNE: Domain-specific language (medical, legal jargon). LLM struggles without fine-tuning. High volume (millions of requests). Cost offset by shorter prompts (fine-tuned model needs less context). Privacy (can\'t send data to API). Fine-tune open-source model locally. EXAMPLE: Customer support chatbot. Before: 1000-token prompt (includes 50 example FAQs). After fine-tuning: 400-token prompt (model learned FAQs). Cost savings: 60% input tokens. Fine-tuning cost: $800 (one-time). API savings: $1,000/month. ROI: Break-even in <1 month. RECOMMENDATION: Fine-tune if: (1) High volume (>1M requests/month), (2) domain-specific, (3) privacy required. Otherwise: Use base models with good prompts. STARTUP RECOMMENDATION (Limited Resources): PHASE 1 (MVP, <10K users): Use GPT-3.5 API. Cost: ~$500/month. Time-to-market: Days. No infrastructure. Focus: Product-market fit, not infrastructure. PHASE 2 (Growth, 10K-100K users): Hybrid: GPT-3.5 (80%) + GPT-4 (20%). Cost: ~$5K/month. Add: Caching (Redis), prompt optimization, rate limiting. PHASE 3 (Scale, 100K+ users): Evaluate: If cost >$20K/month → consider self-hosted open-source. Fine-tune: If domain-specific and high volume. Setup: Hire ML engineer, buy GPUs, deploy LLaMA 2 fine-tuned. NEVER: Start with self-hosted (premature optimization). Start with GPT-4 everywhere (too expensive). Skip monitoring (cost spirals out of control). IMPLEMENTATION: # Phase 1 (GPT-3.5 API); import openai; response = openai.ChatCompletion.create (model="gpt-3.5-turbo", messages=[...]); # Phase 2 (Hybrid); def get_model (query_complexity): return "gpt-4" if is_complex (query) else "gpt-3.5-turbo"; model = get_model (query); response = openai.ChatCompletion.create (model=model, ...); # Phase 3 (Self-hosted); from transformers import pipeline; model = pipeline("text-generation", model="meta-llama/Llama-2-13b-chat-hf"); response = model (prompt, max_length=200). Result: Start lean (API), scale smart (hybrid), consider self-hosted only at massive scale.',
    keyPoints: [
      'Hosted API: Zero infrastructure, fast time-to-market, auto-scaling, $1,750/month (GPT-3.5) vs $63K/month (GPT-4)',
      'Self-hosted: $40K setup, $2K/month ongoing, data privacy, no rate limits, break-even if API cost >$2K/month for 20 months',
      'Model selection: GPT-4 (best quality, expensive), GPT-3.5 (90% quality, 20× cheaper), LLaMA 2 (70% quality, free with GPUs)',
      'Trade-offs: Quality vs cost, hybrid (80% GPT-3.5 + 20% GPT-4) balances both, latency lower for self-hosted',
      'Startup: Phase 1 (GPT-3.5 API, <$500/month), Phase 2 (hybrid, ~$5K/month), Phase 3 (self-hosted if >$20K/month), never start with self-hosted',
    ],
  },
  {
    id: 'llmps-q-3',
    question:
      'Design a monitoring and alerting system specifically for LLM applications. Address: (1) LLM-specific metrics (token usage, cost, hallucinations), (2) quality monitoring (user feedback, output validation), (3) latency and availability tracking, (4) alert thresholds and escalation, (5) automated actions on anomalies. How would you detect a degradation in LLM response quality?',
    sampleAnswer:
      'LLM Monitoring & Alerting: (1) LLM-Specific Metrics: TOKEN USAGE: Track: Input tokens (mean, P95, total), output tokens (mean, P95, total), tokens per request. Visualize: Daily/hourly trends. Alert: If daily tokens >1M (budget: $2K/day at $0.002/1K tokens). Dashboard: Grafana showing token usage over time. COST: Daily cost: $spent = (input_tokens × $0.0015 + output_tokens × $0.002) / 1000. Monthly pace: daily_cost × 30. Alert: If monthly pace >$10K. Cost per user: Identify expensive users (abuse or complex use case). Top 10 users by cost. HALLUCINATIONS: Detection methods: (a) Fact-checking: Cross-reference LLM output with knowledge base. Flag mismatches. Example: LLM says "refund in 7 days", KB says "14 days" → hallucination. (b) Consistency check: Ask same question twice. If answers differ significantly → unreliable. (c) Confidence: If LLM outputs low confidence (<0.7) → flag for review. (d) User reports: "Report incorrect" button. Track rate. Metric: Hallucination rate = flagged responses / total responses. Target: <2%. Alert if >5%. (2) Quality Monitoring: USER FEEDBACK: Thumbs up/down: Track satisfaction. Satisfaction rate = thumbs up / (thumbs up + thumbs down). Target: >80%. Alert if <70% for 1 hour. Regeneration requests: If user clicks "regenerate" → dissatisfied with first response. Regeneration rate = regenerations / total requests. Target: <10%. Alert if >15%. User comments: Free-text feedback. Analyze with sentiment analysis. Flag negative trends. SESSION METRICS: Conversation length: Mean messages per conversation. Short conversations may indicate failure to help. Abandonment rate: % users who leave mid-conversation. Target: <20%. Alert if >30%. Escalation to human: % conversations escalated. Target: <5%. Alert if >10% (LLM failing). OUTPUT VALIDATION: Format compliance: If expecting JSON, validate schema. Compliance rate: % responses with valid JSON. Target: >95%. Alert if <90%. Content safety: Filter offensive/harmful content. Use moderation API (OpenAI Moderation). Flag violations. Rate: Target <0.1%. Alert if >1%. Response length: Track: Mean, P95, P99 output length. Alert if P95 >500 tokens (too verbose, expensive). (3) Latency & Availability: LATENCY: Time to first token (TTFT): Streaming benefit. Target: <1 second. Alert if P95 >2 seconds. Total generation time: Target: <10 seconds. Alert if P99 >15 seconds. API latency: Track OpenAI API response time. Separate from generation time. Identify API issues. Queue time: If requests queued (rate limiting). Target: <1 second. Alert if >5 seconds. AVAILABILITY: Uptime: % time API healthy. Target: 99.9%. Alert if <99%. Error rate: 4xx (client errors), 5xx (server errors). Target: <0.5%. Alert if >2%. Rate limit hits (429 errors): Track frequency. Alert if >10/hour (need to optimize). Circuit breaker: If error rate >5% for 5 minutes → open circuit, use fallback. (4) Alert Thresholds & Escalation: TIERED ALERTS: INFO: Daily summary. Cost: $150 today. Tokens: 100K. Satisfaction: 85%. Email to team. No action. WARNING: Threshold breached but not critical. Cost: $350 today (monthly pace: $10,500 > $10K budget). Slack notification. Action: Review usage, optimize. CRITICAL: Major issue. Error rate: 10% for 5 minutes. Satisfaction: 60% for 1 hour. PagerDuty alert to on-call engineer. Action: Investigate immediately, consider rollback. EMERGENCY: System down or budget exhausted. API unavailable. Daily cost >$1000 (runaway cost). PagerDuty to on-call + manager. Action: Enable fallback, stop API calls. ESCALATION: Fired → 10 minutes (no ack) → escalate to manager. 30 minutes (no fix) → escalate to VP. EXAMPLE ALERTS: Alert: "Daily cost $400 (monthly pace $12K, budget $10K)". Severity: WARNING. Action: "Review top users, enable caching, optimize prompts". Alert: "Satisfaction rate 65% (threshold 70%) for 2 hours". Severity: CRITICAL. Action: "Check recent prompt changes, review user feedback, consider rollback". Alert: "OpenAI API error rate 15% for 10 minutes". Severity: EMERGENCY. Action: "Fallback to rule-based chatbot, check OpenAI status page". (5) Automated Actions: ON COST OVERAGE: If daily cost >$500 (monthly pace >$15K): Action: Enable aggressive caching (increase cache TTL to 1 hour). Reduce rate limits (15 messages/user/day → 10). Switch some traffic to GPT-3.5 (from GPT-4). Alert team: "Cost mitigation activated". ON QUALITY DEGRADATION: If satisfaction <70% for 2 hours: Action: Rollback to previous prompt version (if recent change). Enable human escalation for all queries (temporarily). Increase LLM temperature (more diverse responses). Alert: "Quality issue detected, mitigation enabled". ON API FAILURE: If error rate >5% for 5 minutes: Action: Open circuit breaker (stop calling OpenAI). Switch to fallback (rule-based chatbot or cached responses). Retry API every 2 minutes (check if recovered). Alert: "LLM API unavailable, fallback active". ON HALLUCINATION SPIKE: If hallucination rate >5% for 1 hour: Action: Increase fact-checking (cross-reference all answers with KB). Lower confidence threshold for human review (0.7 → 0.8). Add disclaimer: "AI-generated, verify important information". Alert: "High hallucination rate, enhanced validation enabled". DETECTING QUALITY DEGRADATION: METHODS: (1) User feedback: Satisfaction drops from 85% to 70% over 3 days. (2) Regeneration rate: Increases from 8% to 18%. (3) Escalation rate: Doubles (3% → 6%). (4) Output validation: JSON format compliance drops (98% → 92%). (5) Response length: P95 increases (200 tokens → 350 tokens, too verbose). (6) Hallucination detection: Fact-check failures increase (1% → 4%). CORRELATION: Investigate: Recent changes (prompt, model version, temperature). External factors (API issues, OpenAI model updates). Data drift (user queries changed). ACTION: If prompt change: Rollback. If model version: Revert to previous version (gpt-3.5-turbo-0613 → gpt-3.5-turbo-0301). If no obvious cause: A/B test (50% old system, 50% new) to isolate issue. COMPREHENSIVE DASHBOARD: Metrics: Cost (daily, monthly pace), tokens (input/output), satisfaction (thumbs up %), latency (TTFT, total), errors (rate, types), hallucinations (rate, examples). Alerts: Active alerts (count, severity), recent escalations. Actions: Automated mitigations active (caching, fallback, circuit breaker). Status: System health (green/yellow/red). Result: Proactive monitoring, automated mitigation, <5 min detection-to-action.',
    keyPoints: [
      'LLM metrics: Token usage (input/output, total), cost (daily, monthly pace, per user), hallucinations (fact-checking, consistency, user reports)',
      'Quality: User feedback (thumbs up >80%, regeneration <10%), session metrics (abandonment <20%, escalation <5%), output validation (format, safety)',
      'Latency: TTFT <1s, total <10s, API latency separate, queue time <1s. Availability: uptime 99.9%, error rate <0.5%, rate limit hits <10/hour',
      'Alerts: Tiered (INFO/WARNING/CRITICAL/EMERGENCY), escalation (10 min → manager), examples (cost overage, satisfaction drop, API failure)',
      'Automated actions: Cost overage (aggressive caching, reduce limits), quality drop (rollback prompt), API failure (circuit breaker, fallback), hallucination spike (fact-check all)',
    ],
  },
];
