export const mlSystemCaseStudiesQuiz = [
  {
    id: 'mscs-q-1',
    question:
      'Analyze Netflix\'s recommendation system architecture. Discuss: (1) candidate generation vs ranking stages, (2) handling 230M+ users at scale, (3) A/B testing infrastructure, (4) personalization (from content to thumbnails), (5) cold start problem for new users/content. Why does Netflix use an ensemble of algorithms instead of one "best" model?',
    sampleAnswer:
      'Netflix Recommendation Architecture: (1) Two-Stage System: CANDIDATE GENERATION: Goal: Narrow 15K titles → ~1000 candidates per user. Algorithms: Collaborative filtering (similar users liked), content-based (similar genres/actors), popularity (trending), continue watching (resume), trending in your region. Fast, broad coverage. Output: 1000 candidates in <100ms. RANKING: Goal: Rank 1000 candidates → personalized order (top 20 shown on homepage). Model: Neural network predicts P(watch | user, title, context). Features: watch history, time of day, device, title metadata. Complex, accurate. Output: Ranked list in <100ms. Why two stages: Can\'t run complex model on all 15K titles (too slow). Generate candidates broadly, rank carefully. (2) Handling Scale (230M users): DATA: 1B+ plays per day. 100s TB of interaction data. Spark for batch processing (overnight). Kafka for real-time events. MODEL SERVING: Microservices architecture. Each service: candidate generation (one for CF, one for content-based), ranking service, page generation service. Kubernetes: 1000s of containers, auto-scaling. Caching: Redis for precomputed recommendations. CDN for popular content. User X visits → check cache (hit: return in 10ms). Miss: compute (100ms). PERSONALIZATION: Recommendation computed per user (230M x). Artwork personalized (show action hero for action fans, romance for romance fans). 1000s A/B tests running concurrently (testing ranking algos, UI layouts). (3) A/B Testing: SCALE: 1000s experiments running simultaneously. Every user in multiple experiments (orthogonal traffic splits). METRICS: Engagement (hours watched), retention (% users return next week), satisfaction (thumbs up rate). Not just accuracy. PLATFORM: Custom experimentation platform. Configure experiment via UI (no code deploy). Automated analysis (p-values, confidence intervals). Collision detection (experiments affecting same metric). EXAMPLE: Experiment "New Ranking Algo v2". 10% users see new algo, 90% control. Run for 2 weeks. Measure: +3% engagement, +1.5% retention → statistically significant → roll out to 100%. (4) Personalization: CONTENT: Recommendations personalized by: watch history, ratings, time of day (short shows at lunch, movies at night), device (mobile → shorter content). Row ordering personalized: Action fan sees "Action Movies" row first. THUMBNAILS: Show different artwork per user. Action fan sees explosion scene. Drama fan sees emotional scene. A/B tested: Personalized thumbn ails → +20% click-through rate. EVERYTHING TESTED: Title metadata, row titles, number of rows, autoplay settings. Culture of experimentation. (5) Cold Start: NEW USER: No watch history. Solutions: Ask for preferences during signup (favorite genres, titles). Show popular content (trending). Rapid learning: After 5 plays, have signal for personalization. Collaborative filtering kicks in (users who watched same 5 titles also watched...). NEW CONTENT: No user interactions yet. Solutions: Content-based: Use metadata (genre, actors, director) to find similar titles. Transfer learning: Similar titles have similar audiences. Promote new content on homepage ("New Releases" row) to collect interactions quickly. After 1000 views, have enough signal for CF. WHY ENSEMBLE: (1) Different strengths: CF good for popular titles (lots of data). Content-based good for long-tail (metadata even if no interactions). Popularity captures trends. (2) Coverage: One model may miss certain users/titles. Ensemble ensures broad coverage. (3) Robustness: If one model fails (e.g., CF during new user surge), others compensate. (4) Diversity: Ensemble promotes diverse recommendations (not just one genre). Better user satisfaction. (5) A/B testing shows ensemble >single model: Ensemble (CF + content + popularity): 8.5 hours watched/week. Best single model (CF): 7.8 hours. Ensemble wins. Implementation: Blending: Combine candidate sets from multiple algos. Candidates = union(CF_candidates, content_candidates, popularity_candidates). Ranking by ensemble: Weighted average of model scores. Final_score = 0.6 × CF_score + 0.3 × content_score + 0.1 × popularity_score. Weights learned via A/B testing. Result: 80% of Netflix viewing from recommendations (not search). $1B+ value from recommendation system.',
    keyPoints: [
      'Two-stage: Candidate generation (15K → 1000, fast/broad), ranking (1000 → 20, complex/accurate)',
      'Scale: Spark (batch), Kafka (real-time), microservices, K8s (1000s containers), Redis caching, CDN',
      'A/B testing: 1000s concurrent experiments, orthogonal splits, engagement/retention metrics, automated analysis',
      'Personalization: Content by history/time/device, artwork per user (+20% CTR), everything A/B tested',
      'Ensemble: Different strengths (CF/content/popularity), coverage, robustness, diversity, empirically better than single model',
    ],
  },
  {
    id: 'mscs-q-2',
    question:
      'Design an algorithmic trading system similar to those used by hedge funds. Address: (1) ultra-low latency requirements (<10ms), (2) real-time feature computation, (3) risk management and kill switches, (4) backtesting infrastructure, (5) regulatory compliance. What are the critical differences between trading ML and typical ML systems?',
    sampleAnswer:
      "Algorithmic Trading System: (1) Ultra-Low Latency (<10ms): REQUIREMENTS: Data ingestion <1ms, feature computation <2ms, model inference <5ms, order placement <2ms. Total: <10ms end-to-end. INFRASTRUCTURE: Co-located servers (near exchange, reduces network latency). 10Gb/40Gb NICs, low-latency switches. C++ for critical path (Python too slow). Kernel bypass (DPDK) for networking. OPTIMIZATIONS: Model: XGBoost/LightGBM (not deep learning). <5ms inference. Quantization: INT8 for faster inference. Feature cache: Redis (sub-ms lookup) for expensive features (30-day stats). Precompute nightly. Real-time features: Order book (bid/ask), last price, volume—computed in <1ms. No database queries in hot path (too slow, 10ms+). Everything in memory. MONITORING: Track latency per component. Alert if any component >threshold. P99 latency dashboard. (2) Real-Time Feature Computation: FEATURES: Technical indicators: MA, RSI, MACD (cached, updated every minute). Order book: Bid-ask spread, order book imbalance, top-of-book changes. Microstructure: Trade flow toxicity, volume-weighted spread. Sentiment: News sentiment (NLP model on headlines, updated every 5 minutes). ARCHITECTURE: Market data (WebSocket) → Feature engine (Flink/C++) → Redis → Model → Order management. Streaming pipeline: Flink processes ticks, computes rolling indicators, updates Redis. Latency: Sub-second feature updates. EXAMPLE: At 10:30:15.234, AAPL tick received (price $150.25). Feature engine updates: last_price, rolling_volume_5m, order_book_imbalance. Redis updated in 0.5ms. Model fetches features (0.3ms), predicts (5ms), places order (2ms). Total: 7.8ms. (3) Risk Management & Kill Switches: PRE-TRADE CHECKS: Position limits: Max $1M per symbol, max $10M total exposure. Order size: Max $100K per order. Price limits: Order price within 1% of last trade (prevent fat finger). Blacklist: Restricted symbols (regulatory reasons). If any check fails → reject order (don't send to market). REAL-TIME MONITORING: P&L tracking: Tick-by-tick. Alert if loss >$50K in 1 hour. Drawdown: Max 15%. If exceeded → stop trading, liquidate positions. Sharpe ratio: Rolling Sharpe <0.5 → pause (strategy not working). KILL SWITCHES: Automated: Triggered by: drawdown >15%, error rate >1%, data quality issues (stale prices), connectivity loss to exchange. Action: Cancel all orders, exit positions (market orders), stop new trades, alert team. Manual: Engineer can hit kill switch (emergency button). Circuit breakers: Pause trading if market halts or extreme volatility (flash crash). (4) Backtesting Infrastructure: REQUIREMENTS: Historical tick data (TB-scale), realistic simulation (slippage, fees, market impact), fast execution (backtest 1 year in <1 hour). ARCHITECTURE: Data: TimescaleDB or InfluxDB for tick data. Partitioned by date/symbol. Simulation engine: Iterate through ticks chronologically. For each tick: Compute features, run model, generate signal, simulate order (apply slippage, fees). Vectorized operations (NumPy/Pandas) for speed. Realism: Slippage model: Buy/sell at mid + spread/2 + impact (volume). Fees: $0.005 per share. Market impact: Large orders move price. Avoid lookahead bias: Features at time t use only data up to t-1. VALIDATION: Walk-forward: Train on Year 1, test on Year 2. Slide forward. Metrics: Sharpe ratio, max drawdown, Calmar ratio, win rate. Paper trading: Run strategy live (no real money) for 1 month. Validate backtest matches live. (5) Regulatory Compliance: REQUIREMENTS: Audit trails (every trade logged), risk limits (hard caps on position sizes), market manipulation prevention (no spoofing, layering). AUDIT LOGS: Log every: Order (timestamp, symbol, price, quantity, order_id). Fill (execution price, quantity). Trade decision (model prediction, features, signal). Store for 7 years (SEC requirement). RISK LIMITS: Enforced pre-trade. Cannot exceed position limits (system rejects). Circuit breakers: Pause if abnormal activity. MARKET MANIPULATION: Spoofing detection: If placing then canceling orders rapidly (fake liquidity) → alert compliance. Prevent algorithmic manipulation (strategies designed to move market). REPORTING: Daily: Trade report to compliance. Monthly: Risk metrics (VaR, position concentration). Annual: Audit by regulators (SEC, FINRA). CRITICAL DIFFERENCES (Trading ML vs Typical ML): LATENCY: Trading <10ms, typical 50-500ms. Requires hardware optimization, simpler models. ADVERSARIAL: Market is adversarial (other traders adapt). Must retrain frequently (daily/weekly). Typical ML: data distribution changes slowly. DATA DRIFT: Extreme. Market regimes change (bull/bear, high/low vol). Model must adapt quickly or fail. COST OF ERROR: Trading error = lose money immediately. False positive in spam filter = minor annoyance. RISK MANAGEMENT: Critical. Kill switches, position limits, drawdown caps. Typical ML: errors less catastrophic. BACKTEST REALISM: Must match live perfectly. Typical ML: test set proxy for production. COMPLIANCE: Heavy regulatory requirements. Audit logs, risk reports. Result: Trading ML is high-stakes, low-latency, adversarial, heavily regulated.",
    keyPoints: [
      'Ultra-low latency: Co-located servers, C++, kernel bypass, XGBoost (<5ms), Redis cache, no DB queries, <10ms total',
      'Real-time features: Streaming (Flink), cached (30-day stats in Redis), order book (bid-ask), sentiment (5 min updates)',
      'Risk management: Pre-trade checks (position/price limits), P&L monitoring, kill switches (auto on drawdown >15%, manual button)',
      'Backtesting: Tick data (TB), realistic simulation (slippage, fees, impact), walk-forward validation, paper trading',
      'Critical differences: Latency (<10ms), adversarial (daily retrain), cost of error (lose money), risk management, compliance',
    ],
  },
  {
    id: 'mscs-q-3',
    question:
      "Compare Uber\'s Michelangelo ML platform with a DIY ML infrastructure. Discuss: (1) feature store benefits, (2) model serving at scale, (3) experiment tracking, (4) monitoring and alerting, (5) cost of building vs buying. For a startup with 10 data scientists, would you recommend building an ML platform or using existing tools?",
    sampleAnswer:
      "Uber Michelangelo vs DIY: MICHELANGELO (Uber's ML Platform): (1) Feature Store: Centralized repository. Features defined once (e.g., user_30d_trips), used by all models. Offline store (Hive) for training, online store (Cassandra) for serving. Benefits: Reusability (don't recompute same features), consistency (training = serving features), discovery (teams browse existing features). Example: 100 models use user_30d_trips. Define once, compute once (nightly batch job), serve from Cassandra (<5ms). (2) Model Serving: Horizontal scaling: Model containers on Kubernetes, auto-scaling. Load balancing: Millions predictions/sec distributed across replicas. Latency: P99 <10ms for ETA prediction (critical for UX). Frameworks supported: TensorFlow, PyTorch, XGBoost. GPU support for heavy models. (3) Experiment Tracking: MLflow for tracking runs (hyperparameters, metrics, artifacts). Model registry: Version control, staging → production promotion. A/B testing framework: Deploy new model to 10% traffic, compare metrics, roll out. (4) Monitoring: Data quality: Feature distribution drift detection (alert if PSI >0.2). Model performance: Accuracy, latency, error rate. Dashboards (Grafana) per model. Business metrics: ETA accuracy (within 5% of actual). Automated retraining: Triggered if drift detected or weekly schedule. (5) Cost: 100+ engineers built Michelangelo over 3 years. Estimated cost: $30M (salaries, infrastructure). Benefits: Powers 1000+ models across Uber (ETA, pricing, fraud, demand forecasting). Amortized cost per model: $30K. ROI: Huge (ML drives billions in revenue). DIY ML INFRASTRUCTURE: (1) Feature Store: Build with: Feast (open-source feature store). Setup: 1 ML engineer × 2 months. Cost: $50K (salary). Limitations: Less mature than Michelangelo. Missing advanced features (auto-discovery, lineage). (2) Model Serving: Options: BentoML, TF Serving, or custom FastAPI + Docker + K8s. Setup: 1 ML engineer × 1 month. Cost: $25K. Limitations: Need to maintain (updates, scaling, monitoring). (3) Experiment Tracking: Use MLflow (open-source). Setup: 1 week. Cost: $5K. Cloud hosting: $200/month. (4) Monitoring: Use Evidently + Grafana + Prometheus. Setup: 1 ML engineer × 1 month. Cost: $25K. Limitations: Manual alert configuration. No automated retraining out-of-box. (5) Total DIY Cost: Setup: $105K (upfront). Maintenance: 0.5 ML engineer ongoing = $75K/year. 5-year cost: $105K + $375K = $480K. EXISTING TOOLS (SaaS): Options: Databricks ML, AWS SageMaker, Google Vertex AI, Weights & Biases. Features: Feature store (Vertex AI Feature Store), model serving (SageMaker endpoints), experiment tracking (W&B), monitoring (AWS Model Monitor). Cost: $50K/year for 10 data scientists (based on usage). Setup: Minimal (weeks, not months). 5-year cost: $250K. STARTUP (10 Data Scientists) RECOMMENDATION: USE EXISTING TOOLS (SaaS). REASONING: (1) Time to value: SaaS operational in weeks. DIY takes months. Startup needs speed. (2) Cost: $250K (SaaS) vs $480K (DIY) over 5 years. SaaS cheaper. (3) Maintenance: SaaS maintained by vendor. DIY needs dedicated engineer (takes away from ML work). (4) Features: SaaS has mature features (auto-scaling, monitoring, A/B testing). DIY requires building. (5) Focus: DS should focus on models, not infrastructure. SaaS enables this. WHEN TO BUILD (like Uber): Scale: 100+ data scientists, 1000+ models. SaaS costs become prohibitive (\$500K+/year). Custom needs: Unique requirements (ultra-low latency, specific compliance). SaaS doesn't meet needs. Strategic: ML is core competency (Uber, Netflix). Platform is competitive advantage. Investment justified. HYBRID APPROACH (Best for Startup): Use SaaS for most (SageMaker for training/serving, W&B for tracking). Build custom components only where needed (e.g., custom feature computation for domain-specific features). Gradually build platform as team grows (10 → 50 → 100 DS). IMPLEMENTATION (Startup): Year 1-2 (10 DS): Use SageMaker (training), BentoML (serving), W&B (experiments), Evidently (monitoring). Cost: $50K/year. Year 3-4 (30 DS): Evaluate needs. If SaaS cost >$150K/year, consider building feature store (Feast). Cost: $100K setup + $75K/year maintenance. Year 5+ (50+ DS): If at scale (1000+ models), consider building full platform (Michelangelo-like). Cost: $1M+ (team of 5 engineers). Result: Startup accelerates ML development with SaaS. Avoids infrastructure distraction. Builds platform only when scale justifies.",
    keyPoints: [
      'Michelangelo: Centralized feature store (reuse, consistency), auto-scaling serving, MLflow tracking, drift monitoring, $30M cost',
      'DIY: Feast (feature store), FastAPI+K8s (serving), MLflow, Evidently monitoring, $480K 5-year cost, ongoing maintenance',
      'SaaS: Databricks/SageMaker/Vertex, $250K 5-year cost, minimal setup, vendor-maintained, mature features',
      'Startup (10 DS): Use SaaS—faster, cheaper, less maintenance, focus on models not infrastructure',
      'Build platform when: 100+ DS, 1000+ models, unique needs, ML is core competency, SaaS costs prohibitive',
    ],
  },
];
