export const dataEngineeringForMlQuiz = [
  {
    id: 'deml-q-1',
    question:
      'Design a complete data pipeline for an ML system that ingests 1TB of data daily from multiple sources (databases, APIs, logs). Address: (1) data ingestion strategy (batch vs streaming), (2) data validation and quality checks, (3) feature store architecture for training-serving consistency, (4) data versioning for reproducibility, (5) handling schema evolution. How would you ensure data quality and prevent training-serving skew?',
    sampleAnswer:
      'Complete Data Pipeline: (1) Ingestion Strategy: Hybrid approach. Batch (Airflow): Daily ETL from databases (PostgreSQL, MongoDB) at 2am. Pull historical features, aggregations (30-day user stats). Store in data lake (S3/GCS). Streaming (Kafka): Real-time events (clicks, transactions) → Kafka topics → Flink for windowed aggregations → Feature store. Use streaming for features that change frequently (last 1 hour activity). (2) Data Validation: Great Expectations framework. Checks at ingestion: Schema validation (expected columns, types), range checks (age 0-120, amount >0), null checks (critical fields non-null), distribution checks (mean/std within 3 sigma of historical). On failure: Alert team, quarantine bad data, use previous day\'s data. Example: If transaction_amount has 50% nulls (historical: 0.1%) → block pipeline. (3) Feature Store (Feast/Tecton): Centralized repository. Offline store (Parquet/BigQuery) for training, online store (Redis/DynamoDB) for serving <10ms. Define features once: @feature_view def user_30d_stats (user_id, timestamp) → materialized for both training and serving. Ensures consistency. Point-in-time correct joins (no data leakage). (4) Data Versioning (DVC/Delta Lake): Track datasets like code. DVC: Store pointers in git, data in S3. Tag releases: "train_v1.2.3". Delta Lake: Time travel (query historical state), ACID transactions. Can reproduce any model by dataset version. Example: model_v2 trained on data_v1.5. (5) Schema Evolution: Use Avro/Protobuf (backward compatible). Add new fields optional, never remove required fields. Schema registry (Confluent) tracks versions. If schema changes → migrate old data, version features in feature store. Preventing Training-Serving Skew: (1) Same feature computation code: feature store uses identical logic. (2) Monitoring: Compare feature distributions in training vs serving (KS test). Alert if drift. (3) Integration tests: Compute features offline and online for same input, assert match. (4) Validation: Before deployment, run model on recent production features, verify performance. Common skew causes: Different preprocessing (training scales data, serving doesn\'t), time zone issues (UTC in training, local in serving), feature computation bugs.',
    keyPoints: [
      'Hybrid ingestion: batch (Airflow) for historical, streaming (Kafka+Flink) for real-time',
      'Great Expectations for validation: schema, ranges, nulls, distributions with alerting',
      'Feature store (Feast): offline for training, online (Redis) for serving, same computation',
      'Data versioning (DVC/Delta Lake): reproducibility, time travel, track dataset-model pairs',
      'Prevent skew: identical feature code, monitor distributions, integration tests, validation',
    ],
  },
  {
    id: 'deml-q-2',
    question:
      'Explain the differences between ETL and ELT in the context of ML data pipelines. For a large-scale recommendation system with petabytes of data, which approach would you choose and why? Discuss: (1) infrastructure requirements, (2) transformation complexity, (3) flexibility for experimentation, (4) cost implications, (5) data quality control. Provide a concrete example.',
    sampleAnswer:
      "ETL vs ELT for ML: ETL (Extract-Transform-Load): (1) Flow: Extract from sources → Transform in pipeline (Spark) → Load into warehouse. Transform happens before storage. (2) Infrastructure: Need powerful transformation cluster (Spark on EMR). (3) Use case: Complex transformations, reduce storage (only keep aggregated data), compliance (PII removal before storage). (4) Cost: Higher compute during pipeline, lower storage (transformed data smaller). (5) Flexibility: Lower—need to re-run pipeline to try new features. ELT (Extract-Load-Transform): (1) Flow: Extract from sources → Load raw into warehouse (Snowflake/BigQuery) → Transform on-demand with SQL. (2) Infrastructure: Leverage warehouse compute power. (3) Use case: Data exploration, rapid experimentation, keep raw data for future features. (4) Cost: Lower compute (warehouse auto-scales), higher storage (raw data larger). (5) Flexibility: Higher—query raw data for any transformation without re-ingestion. RECOMMENDATION SYSTEM: Choose ELT. Reasoning: (1) Experimentation: DS team needs to try many features (click sequences, dwell time, co-views). ELT allows querying raw clickstream without re-running ETL. Example: Initially used last_clicked product. Later discover: dwell_time > 30s better signal. With ELT: Write SQL, test immediately. With ETL: Modify pipeline, wait hours for re-run. (2) Infrastructure: BigQuery/Snowflake handle petabyte-scale queries. Separation of storage (cheap S3) and compute (pay per query). Don't need permanent Spark cluster. (3) Data Quality: Load raw data first (safer), validate in warehouse. Can always go back to raw if transformation wrong. (4) Cost: Storage cheap ($0.02/GB/month for S3). Compute expensive. ELT minimizes upfront compute, pay only for queries used. Example pipeline: (1) Extract: Clickstream logs → S3 (Parquet, partitioned by date). User profiles → BigQuery. (2) Load: Raw events in BigQuery. 10TB/day. (3) Transform: dbt models for feature engineering. user_click_features: SELECT user_id, COUNT(*) clicks_24h, AVG(dwell_time) avg_dwell FROM raw.clickstream WHERE timestamp > NOW() - INTERVAL 24 HOUR GROUP BY user_id. Materialized view, refreshes hourly. (4) Serve: Aggregated features → Feature store → Model training. Flexibility: Add new feature = new dbt model, no pipeline changes. Data quality: Great Expectations tests on raw and transformed tables.",
    keyPoints: [
      'ETL: Transform before load, good for complex transformations, less storage, less flexible',
      'ELT: Load raw first, transform on-demand, more storage, more flexible for experimentation',
      'Recommendation systems benefit from ELT: rapid feature iteration, SQL-based exploration',
      'Modern warehouses (BigQuery/Snowflake) make ELT cost-effective: cheap storage, pay-per-query',
      'ELT better for ML: keeps raw data, enables feature discovery, validates transformations easily',
    ],
  },
  {
    id: 'deml-q-3',
    question:
      'Design a feature store for a production ML system. Explain: (1) architecture for offline (training) and online (serving) stores, (2) how to ensure point-in-time correctness to prevent data leakage, (3) feature freshness and update strategies, (4) handling feature backfilling, (5) monitoring feature quality and drift. Include a code example showing feature definition and retrieval.',
    sampleAnswer:
      'Feature Store Design: (1) Architecture: Offline store (training): S3/BigQuery for historical features. Stores all feature values with timestamps. Query: "Give me features for user_id=123 at timestamp=2024-01-15". Enables time-travel for reproducibility. Online store (serving): Redis/DynamoDB for low-latency (<5ms) lookups. Only latest feature values. Key-value: {user_id: 123 → {clicks_24h: 45, avg_dwell: 32.1}}. Updated by streaming pipeline. (2) Point-in-Time Correctness: Critical: Training uses features available at prediction time, not future data. Example: Predict conversion on 2024-01-15 12:00. Must use features computed before 12:00. BAD: Join by user_id (gets latest features, including after 12:00). GOOD: Temporal join—match user_id AND timestamp <= 12:00. Feast provides .get_historical_features (entity_df, feature_refs) with point-in-time joins. Prevents leakage. (3) Feature Freshness: Batch features (30-day aggregates): Update daily at 2am. Acceptable staleness: 24 hours. Streaming features (clicks last 1 hour): Update every 5 minutes via Kafka. Acceptable staleness: 5 minutes. Configure by feature: @feature_view (ttl=Duration (hours=24)) for batch, ttl=Duration (minutes=5) for streaming. (4) Backfilling: Need: Train model on last year, but feature store only has 3 months. Solution: Backfill script. Read historical raw data, compute features for past timestamps, write to offline store. Feast: feast materialize-incremental 2023-01-01 2024-01-01. Expensive but one-time. (5) Monitoring: Track per feature: (a) Null rate (should be <1%), (b) Distribution (mean/std/percentiles), (c) Drift (KS test vs baseline), (d) Freshness (time since last update). Alert if: nulls spike, distribution shifts (KS p-value < 0.05), staleness > TTL. Example with Feast: from feast import FeatureStore, Entity, FeatureView, Field, FileSource; from feast.types import Float32, Int64; from datetime import timedelta; # Define entity; user = Entity (name="user_id", join_keys=["user_id",]); # Define feature view (data source); user_features_source = FileSource (path="s3://features/user_stats.parquet", timestamp_field="event_timestamp"); user_features = FeatureView (name="user_features", entities=[user], schema=[Field (name="clicks_24h", dtype=Int64), Field (name="avg_dwell_time", dtype=Float32)], source=user_features_source, ttl=timedelta (hours=24)); # Initialize store; store = FeatureStore (repo_path="."); # Training: Get historical features (point-in-time correct); entity_df = pd.DataFrame({"user_id": [123, 456], "event_timestamp": [pd.Timestamp("2024-01-15"), pd.Timestamp("2024-01-16")]}); training_df = store.get_historical_features (entity_df=entity_df, features=["user_features:clicks_24h", "user_features:avg_dwell_time",]).to_df(); # Serving: Get latest features; online_features = store.get_online_features (features=["user_features:clicks_24h", "user_features:avg_dwell_time",], entity_rows=[{"user_id": 123}]).to_dict(); # Returns: {\'clicks_24h\': [45], \'avg_dwell_time\': [32.1]}. Benefits: (1) Consistency: Same features training and serving. (2) Reusability: Define once, use across models. (3) Monitoring: Centralized feature quality tracking. (4) Collaboration: Teams share features.',
    keyPoints: [
      'Dual stores: offline (S3/BigQuery) for training, online (Redis) for serving <5ms',
      'Point-in-time correctness: temporal joins prevent data leakage, critical for valid training',
      'Feature freshness: batch (daily) vs streaming (minutes), configure TTL per feature',
      'Backfilling: Recompute features for historical data, expensive but enables long lookback',
      'Monitor per feature: null rate, distribution, drift (KS test), freshness vs TTL',
    ],
  },
];
