export const modelMonitoringQuiz = [
  {
    id: 'mm-q-1',
    question:
      'Design a comprehensive model monitoring system for a production ML model. Address: (1) what metrics to track (model performance, data drift, system health), (2) how to detect different types of drift, (3) alerting strategy and thresholds, (4) automated actions on drift detection, (5) dashboards and reporting. How would you detect that a model trained on 2023 data is degrading in early 2024?',
    sampleAnswer:
      "Complete Monitoring System: (1) Metrics to Track: MODEL PERFORMANCE: Online metrics: Accuracy, precision, recall, F1 (if ground truth available quickly). Example: Fraud detection knows outcome in hours. Proxy metrics: Click-through rate, conversion rate (business proxies for model quality). Prediction distribution: Mean, std, percentiles of predictions. Example: Fraud model predicts 5% fraud historically, now 15% → investigate. DATA DRIFT: Feature distributions: Track mean, std, min, max per feature. Covariate shift: Input distribution changes. Statistical tests: Kolmogorov-Smirnov test, Population Stability Index (PSI). SYSTEM HEALTH: Latency: P50, P95, P99 response time. Throughput: Requests per second. Error rate: 4xx, 5xx errors. Resource usage: CPU, memory, GPU utilization. (2) Drift Detection: COVARIATE SHIFT (P(X) changes): Method: KS test on each feature. Compare production feature distribution (last 7 days) vs training distribution. If p-value <0.05 → significant drift. PSI: PSI = Σ (prod_pct - train_pct) × ln(prod_pct / train_pct). PSI >0.25 → major shift. Example: User age distribution in training: mean=35, std=10. In production: mean=45, std=12. KS test p=0.001 → drift detected. CONCEPT DRIFT (P(Y|X) changes): Method: Monitor model performance over time. If accuracy drops from 92% to 85% over 2 weeks → concept drift. Residual analysis: If errors concentrate in certain segments (e.g., new product category) → localized drift. LABEL SHIFT (P(Y) changes): Method: Track prediction distribution. If fraud rate predictions shift from 5% to 15% → label shift. (3) Alerting Strategy: Tiered alerts: INFO: PSI >0.1 (minor drift, log but no action). WARNING: PSI >0.2 or KS p <0.05 (drift detected, notify team). CRITICAL: Accuracy drop >5% or error rate >1% (page on-call, consider rollback). Thresholds: Set based on historical variability. If feature naturally varies ±10%, don't alert on 5% change. Require sustained drift (3+ consecutive days) to avoid false alarms from daily noise. Composite score: Combine multiple signals. Example: If 5+ features drifted AND accuracy dropped → high confidence alert. (4) Automated Actions: On WARNING: Trigger model retraining pipeline with recent data. Queue for next training cycle (daily). On CRITICAL: Auto-rollback to previous stable model version. Alert on-call engineer immediately. Pause A/B test if new model underperforming. Gradual degradation: If accuracy drops slowly (92% → 91% → 90% over months), schedule major retraining with fresh data and feature engineering. (5) Dashboards: Real-time dashboard (Grafana): Current accuracy, latency, throughput. Feature drift scores (PSI per feature, bar chart). Prediction distribution (histogram, compare to baseline). Alert history (recent warnings/criticals). Weekly report: Model performance trends (accuracy over 3 months). Drift analysis (which features drifted most). Business impact (revenue, false positive cost). Recommendations (retrain? add features?). DETECTING 2024 DEGRADATION: Setup: Model trained on 2023 data. Deployed Jan 1, 2024. Monitoring: Week 1 (Jan 1-7): Accuracy 92%, PSI avg 0.05 (normal). Week 4 (Jan 22-28): Accuracy 89%, PSI avg 0.18 (several features drifting). Week 8 (Feb 19-25): Accuracy 85%, PSI avg 0.35 (major drift). Analysis: Feature drift: User behavior changed (e.g., more mobile traffic in 2024). Concept drift: Relationship changed (e.g., new fraud patterns emerged). Action: Week 4: WARNING alert → schedule retraining. Week 8: CRITICAL alert → retrain immediately with 2024 data. Updated model deployed Week 9 → accuracy recovers to 91%. Lesson: Continuous monitoring caught degradation before major business impact. Quarterly retraining not enough—monthly or on-drift-detection better.",
    keyPoints: [
      'Track model performance (accuracy if ground truth available), proxy metrics (CTR), prediction distribution',
      'Detect drift: KS test for covariate shift (P(X)), performance monitoring for concept drift (P(Y|X)), PSI >0.25 major shift',
      'Tiered alerts: INFO (PSI >0.1), WARNING (PSI >0.2, retrain), CRITICAL (accuracy drop >5%, rollback)',
      'Automated actions: Retrain on WARNING, auto-rollback on CRITICAL, require sustained drift (3+ days)',
      '2024 degradation: Monitor weekly, PSI increases → accuracy drops → retrain with 2024 data, recover',
    ],
  },
  {
    id: 'mm-q-2',
    question:
      'Explain how to implement effective alerting for ML models in production. Address: (1) alert fatigue and how to avoid it, (2) choosing appropriate thresholds, (3) composite alerts vs single-metric alerts, (4) escalation policies, (5) actionable alerts. Why is monitoring accuracy alone insufficient?',
    sampleAnswer:
      'Effective Alerting: (1) Avoiding Alert Fatigue: Problem: Too many alerts → team ignores them → miss critical issues. Example: Alert on every 1% accuracy drop → 50 alerts/day → fatigue. Solutions: ACTIONABLE ALERTS: Every alert must have clear action. Bad: "Model accuracy is 91.5%". Good: "Model accuracy dropped to 87% (threshold: 90%). Action: Retrain with last 30 days of data." THRESHOLDED BY SEVERITY: Don\'t alert on minor fluctuations. Only alert if breach threshold for sustained period (3+ consecutive hours). RATE LIMITING: Max 1 alert per issue per hour. Don\'t spam "accuracy low" every minute. Group related alerts. INTELLIGENT AGGREGATION: Instead of alerting on 10 features drifting, one alert: "Data drift detected in 10/50 features". (2) Choosing Thresholds: Data-driven approach: Baseline period: Monitor model for 2 weeks in production. Calculate normal variability. Example: Accuracy fluctuates 90-92%. Set threshold: mean - 2×std = 92% - 2×1% = 90%. Alert if below 90% (2 sigma event, rare). Adjust for business: High-stakes decisions (medical diagnosis) → tight threshold. Low-stakes (movie recs) → loose threshold. Dynamic thresholds: Adjust based on time (weekend traffic different from weekday). Seasonality (Black Friday shopping patterns different). (3) Composite Alerts: Single-metric: Alert if accuracy <90%. Problem: False alarms. Daily noise causes brief dip. Composite (multi-signal): Alert if (accuracy <90%) AND (error rate >1%) AND (drift PSI >0.2). Higher confidence. Example: Accuracy 89% but error rate 0.2%, PSI 0.05 → don\'t alert (likely daily variance). Accuracy 89% AND error rate 1.2% AND PSI 0.3 → alert (multiple signals = real issue). Weighted scoring: Score = 0.5 × accuracy_drop + 0.3 × error_rate_increase + 0.2 × PSI. Alert if score >0.5. (4) Escalation Policies: Tier 1 (INFO/WARNING): Slack notification to ML team. Email daily summary. No immediate action required. Tier 2 (CRITICAL): Page on-call engineer (PagerDuty). Requires acknowledgment within 15 minutes. Tier 3 (EMERGENCY): Page manager + engineer. Automated rollback triggered. Example: Fraud model error rate >5% → emergency. Escalation timeline: Alert fired → 15 min (no ack) → escalate to manager. 30 min (no fix) → escalate to VP Engineering. (5) Actionable Alerts: Bad alert: "Model performance degraded." No action. Good alert: Title: "Fraud Model Accuracy <90% (CRITICAL)", Details: "Accuracy: 85% (threshold 90%), Error rate: 1.5%, PSI: 0.32 (drift detected in 8 features)", Action Items: "1. Retrain with last 30 days data, 2. Check feature drift dashboard (link), 3. If urgent: rollback to model_v2.1 (command: kubectl set image deployment/fraud-model model=v2.1)", Context: "Last retrained: 14 days ago. Possible cause: Black Friday shopping patterns (seasonality)." Runbook link: "https://wiki/ml-playbooks/fraud-model-degradation". WHY ACCURACY ALONE INSUFFICIENT: (1) Delayed feedback: Fraud outcome known in days, not real-time. Can\'t monitor accuracy immediately. (2) Subset degradation: Overall accuracy 92%, but accuracy on high-value transactions (>$1000) dropped to 70%. Need segmented metrics. (3) Business context missing: 92% accuracy but 10% false positive rate → declining good customers → revenue loss. (4) Proxy metrics needed: For recommendations, click-through rate (available immediately) proxies for accuracy. (5) Distribution shift without accuracy change: Input distribution changes (more mobile users) but accuracy stable. Model may be underconfident or overconfident—check calibration. COMPREHENSIVE MONITORING: Track: Accuracy (if ground truth available), Proxy metrics (CTR, conversion), Prediction distribution (calibration), Feature drift, Business metrics ($revenue, user satisfaction). Multi-dimensional alerting.',
    keyPoints: [
      'Avoid alert fatigue: Actionable alerts only, rate limiting, thresholds for severity, group related alerts',
      'Thresholds: Data-driven (mean - 2σ), adjust for business stakes, dynamic (seasonality), sustained breaches',
      'Composite alerts: Multiple signals (accuracy + error rate + drift PSI) → higher confidence, fewer false alarms',
      'Escalation: Tier 1 (Slack), Tier 2 (page on-call), Tier 3 (emergency, auto-rollback), timeline-based escalation',
      'Accuracy alone insufficient: Delayed feedback, subset degradation, need business metrics, proxy metrics, drift detection',
    ],
  },
  {
    id: 'mm-q-3',
    question:
      'Design a data quality monitoring system that runs continuously to detect issues before they affect model performance. Address: (1) schema validation, (2) statistical checks on features, (3) data freshness monitoring, (4) anomaly detection in data pipeline, (5) integration with model monitoring. Provide code examples for key checks.',
    sampleAnswer:
      'Data Quality Monitoring: (1) Schema Validation: Ensure incoming data matches expected schema. Checks: Column presence (all expected columns exist), Data types (age is int, email is string), Value constraints (age >0, email contains @), Required fields non-null. Tool: Great Expectations. Example: import great_expectations as ge; df_ge = ge.from_pandas(df); df_ge.expect_column_to_exist("age"); df_ge.expect_column_values_to_be_between("age", min_value=0, max_value=120); df_ge.expect_column_values_to_not_be_null("user_id"); results = df_ge.validate(); if not results.success: alert("Schema validation failed", results.results). On failure: Block data pipeline, alert data engineering team, use previous day\'s data as fallback. (2) Statistical Checks: Monitor feature distributions. Compare current batch vs historical baseline (training data). Checks: Mean/std within expected range, Percentiles (P1, P25, P50, P75, P99) stable, Outlier rate <1%, Null rate <0.1%. Example: import scipy.stats; def check_distribution_drift(current, baseline): # KS test; statistic, p_value = scipy.stats.ks_2samp(current, baseline); if p_value <0.05: alert(f"Feature drift detected: KS={statistic:.3f}, p={p_value:.4f}"); # PSI (Population Stability Index); def calculate_psi(current, baseline, bins=10): current_pct, _ = np.histogram(current, bins=bins); baseline_pct, _ = np.histogram(baseline, bins=bins); current_pct = current_pct / len(current); baseline_pct = baseline_pct / len(baseline); psi = np.sum((current_pct - baseline_pct) * np.log(current_pct / baseline_pct)); return psi; psi = calculate_psi(df["age",], baseline_age); if psi >0.25: alert(f"Major shift detected: PSI={psi:.3f}"). Run hourly on incoming data. (3) Data Freshness: Monitor pipeline lag. Data should be fresh (delay <1 hour). Checks: Max timestamp in data vs current time, Pipeline execution time (should complete in <30 minutes), Missing partitions (e.g., yesterday\'s data missing). Example: from datetime import datetime, timedelta; max_timestamp = df["timestamp",].max(); current_time = datetime.now(); lag = current_time - max_timestamp; if lag > timedelta(hours=2): alert(f"Data staleness: {lag.total_seconds()/3600:.1f} hours old"); # Airflow monitoring; from airflow import DAG; def check_dag_freshness(dag_id): last_run = get_last_dag_run(dag_id); if datetime.now() - last_run.execution_date > timedelta(hours=25): alert(f"DAG {dag_id} hasn\'t run in 25+ hours"). (4) Anomaly Detection in Pipeline: Detect pipeline failures or data corruption. Checks: Row count (expect ~1M rows/day, alert if <500K or >2M), Duplicate rate (alert if >1%), Data size (expect ~10GB, alert if <5GB), Processing time (expect 30 min, alert if >60 min). Example: def check_data_volume(df, expected_min=900000, expected_max=1100000): row_count = len(df); if row_count <expected_min: alert(f"Row count too low: {row_count} (expected ~1M)"); if row_count >expected_max: alert(f"Row count too high: {row_count} (spike in data?)"); def check_duplicates(df, key="user_id"): dup_rate = df.duplicated(subset=[key]).mean(); if dup_rate > 0.01: alert(f"High duplicate rate: {dup_rate*100:.2f}% on {key}"). (5) Integration with Model Monitoring: Data quality metrics feed into model monitoring dashboard. Example: If data drift detected → expect model performance degradation soon. Proactively retrain before accuracy drops. Flow: Data quality check (hourly) → Alert on drift → Trigger model evaluation on new data → If performance predicted to drop → Queue retraining → Deploy updated model. Dashboard: data_quality_score = 1.0 - (0.3 × psi_avg + 0.3 × null_rate + 0.2 × outlier_rate + 0.2 × freshness_lag); if data_quality_score <0.85: model_confidence = "LOW" → increase monitoring frequency. Complete Pipeline: Airflow DAG: Task 1: Extract data. Task 2: Data quality checks (schema, stats, freshness). Task 3: If checks pass → proceed. If fail → alert + block. Task 4: Feature engineering. Task 5: Model inference. Task 6: Prediction quality checks. Task 7: Write results. Monitoring: Great Expectations reports, Grafana dashboards (data quality metrics), Alerts (PagerDuty on failures).',
    keyPoints: [
      'Schema validation: Great Expectations for column presence, types, constraints, nulls, block pipeline on failure',
      'Statistical checks: KS test for drift, PSI for distribution shift, monitor mean/std/percentiles, run hourly',
      'Data freshness: Monitor lag (data <2h old), pipeline execution time, missing partitions, Airflow DAG monitoring',
      'Anomaly detection: Row count, duplicate rate, data size, processing time—alert on significant deviation',
      'Integration: Data quality score feeds model confidence, drift triggers proactive retraining, unified dashboard',
    ],
  },
];
