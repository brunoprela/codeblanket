export const abTestingForMlQuiz = [
  {
    id: 'abt-q-1',
    question:
      'Design a complete A/B testing framework for evaluating ML models in production. Address: (1) experiment design (sample size, randomization, control groups), (2) metrics selection (guardrail vs success metrics), (3) statistical significance testing, (4) duration and stopping criteria, (5) infrastructure for traffic splitting. How would you test a new recommendation model against the current production model?',
    sampleAnswer:
      'A/B Testing Framework: (1) Experiment Design: HYPOTHESIS: New model (B) increases click-through rate vs current model (A) by >5%. RANDOMIZATION: User-level split (not request-level). Each user consistently sees same model (A or B) for duration. Why: Avoid user confusion, enable session-based metrics. Implementation: hash (user_id) % 100. If <50 → A, else → B (50-50 split). CONTROL GROUP: Model A (current production). Safety net—if B fails, users in A unaffected. SAMPLE SIZE: Calculate required sample size. Formula: n = (Z_α/2 + Z_β)² × σ² / δ². Example: CTR_A=5%, expect CTR_B=5.25% (5% lift), α=0.05 (95% confidence), β=0.2 (80% power). Calculate: Need ~50,000 users per group. With 1M daily users → run for 5 days. STRATIFICATION: Ensure even distribution of user segments (new vs returning, mobile vs desktop). Prevent Simpson\'s paradox. (2) Metrics Selection: PRIMARY (Success metric): Click-through rate (CTR). Goal: B > A by >5%. SECONDARY: Time on site, Revenue per user, Bounce rate. GUARDRAIL (Safety): Ensure B doesn\'t harm. Error rate <1%, Latency P99 <100ms, User satisfaction (surveys). If guardrail violated → stop experiment. NOVELTY EFFECT: Track metrics over time. Week 1: Users explore new recs (novelty bias). Week 2-4: True behavior. (3) Statistical Testing: METHOD: Two-sample z-test for proportions (CTR is proportion). Null hypothesis: CTR_A = CTR_B. Alternative: CTR_B > CTR_A (one-tailed). Calculate: z = (CTR_B - CTR_A) / √(σ²_A/n_A + σ²_B/n_B). If z > 1.96 (α=0.05) → reject null, B is better. P-VALUE: If p <0.05 → statistically significant. CONFIDENCE INTERVAL: Calculate 95% CI for lift. Example: Lift = 0.3% ± 0.2%. If CI excludes 0 → significant. MULTIPLE TESTING: If testing 5 metrics, apply Bonferroni correction (α = 0.05/5 = 0.01) to avoid false positives. (4) Duration and Stopping: MINIMUM DURATION: Run until sample size reached (5 days). Even if significance achieved on day 2, run full duration (avoid peeking problem—early stopping inflates false positive rate). MAXIMUM DURATION: 4 weeks. If no significance after 4 weeks → B not better, end experiment. EARLY STOPPING (for harm): If B causes significant harm (error rate >2×, revenue drops >10%) → stop immediately. Sequential testing: Use sequential probability ratio test (SPRT) for ethical early stopping if B clearly wins or loses. (5) Infrastructure: TRAFFIC SPLITTING: Implemented in load balancer or application layer. Code: def get_model_variant (user_id): bucket = hash (user_id) % 100; return "model_a" if bucket <50 else "model_b". LOGGING: Every request logs: user_id, model_variant (A or B), timestamp, prediction, user action (click/no-click). Store in data warehouse for analysis. FEATURE FLAGS: Use LaunchDarkly or custom service. Can adjust split (50-50 → 90-10) without code deploy. A/B TEST RECOMMENDATION MODEL: Week 1: Deploy model B to 10% users (90% on A). Monitor guardrails (errors, latency). Week 2: If guardrails OK, increase to 50-50. Collect data. Week 3-4: Analyze: Model A: 100K users, CTR=5.0%, Model B: 100K users, CTR=5.3%. Z-test: z = (0.053 - 0.050) / √(0.05×0.95/100K + 0.053×0.947/100K) = 4.47. P-value <0.0001 → highly significant. Lift = 6% (exceeds 5% goal). Decision: Promote B to 100%. Week 5: Roll out B to all users. Monitor for regressions.',
    keyPoints: [
      'Experiment design: User-level randomization (hash), calculate sample size (power analysis), stratify user segments',
      'Metrics: Primary (CTR), secondary (revenue), guardrail (error rate, latency), watch for novelty effect',
      'Statistical testing: Z-test for proportions, p <0.05, confidence intervals, Bonferroni for multiple tests',
      'Duration: Minimum to reach sample size (avoid peeking), maximum 4 weeks, early stop only for harm',
      'Infrastructure: Hash-based traffic split, log user_id + variant + action, feature flags for dynamic adjustment',
    ],
  },
  {
    id: 'abt-q-2',
    question:
      "Explain the difference between A/B testing and multi-armed bandit algorithms for model evaluation. For each approach, discuss: (1) exploration vs exploitation trade-off, (2) regret and opportunity cost, (3) when to use which, (4) implementation complexity. If you're testing 5 different recommendation algorithms, which approach would you choose and why?",
    sampleAnswer:
      "A/B Testing vs Multi-Armed Bandits (MAB): A/B TESTING: (1) Exploration vs Exploitation: EXPLORATION: Test period (2-4 weeks). Split traffic 50-50, collect data. EXPLOITATION: After test, route 100% to winner. Clear separation. (2) Regret: During test, 50% users see worse model → opportunity cost (lost revenue/engagement). Example: Model B is 10% better. With 50-50 split for 4 weeks → 25% suboptimal traffic. (3) Use when: Few variants (2-3), long-term decision (model will be used for months), need statistical confidence (p-value, CI). (4) Implementation: Simple. Hash-based split, collect data, run statistical test at end. MULTI-ARMED BANDIT: (1) Exploration vs Exploitation: ADAPTIVE: Start 20% each (5 arms). As data accumulates, shift traffic to better-performing arms. After 2 weeks: Best arm gets 60%, others get 10% each. Dynamic optimization. (2) Regret: LOWER than A/B. Quickly reduces traffic to bad arms. Example: By week 2, bad model gets only 10% traffic (vs 50% in A/B). (3) Use when: Many variants (5+), want to minimize regret during experiment, online learning (continuous optimization). Example: Content ranking, ad bidding. (4) Implementation: Complex. Need real-time feedback, bandit algorithm (ε-greedy, UCB, Thompson Sampling). ALGORITHMS: ε-GREEDY: With probability ε (e.g., 0.1), explore (random arm). With probability 1-ε, exploit (best arm so far). Simple but fixed exploration rate. UCB (Upper Confidence Bound): Select arm with highest UCB = mean_reward + √(2×ln (t)/n_arm). Balances mean (exploitation) and uncertainty (exploration). High uncertainty → explore. THOMPSON SAMPLING: Bayesian approach. Maintain posterior distribution for each arm's reward. Sample from posteriors, pick highest. Automatically balances exploration/exploitation. Best theoretical guarantees. 5 RECOMMENDATION ALGORITHMS: SCENARIO: Test 5 recommendation algos (collaborative filtering, content-based, matrix factorization, neural network, hybrid). RECOMMENDATION: Multi-Armed Bandit (Thompson Sampling). REASONING: (1) Many variants (5): A/B testing requires 20% each → statistical power issue (need huge sample size). Bandit adapts traffic dynamically. (2) Minimize regret: 4 weeks A/B with 20% each → 80% traffic on suboptimal models. Bandit: After 1 week, shift 50% to best 2 models → lower regret. (3) Continuous learning: Recommendation patterns change. Bandit continues optimizing beyond initial test. A/B stops learning after test ends. (4) Business value: Every user counts. Bandit maximizes cumulative reward during test (higher engagement/revenue). IMPLEMENTATION (Thompson Sampling): Initialize: Each arm has Beta(α=1, β=1) prior (uniform). For each user: Sample θ_i ~ Beta(α_i, β_i) for each arm i. Select arm k = argmax θ_i. Show recommendation from algo k. Observe reward r (1 if click, 0 if not). Update: α_k += r, β_k += (1-r). Convergence: After ~10K samples, best arm gets 70-80% traffic. After experiment: Promote best arm to 100% or continue bandit (adaptive system). MONITORING: Track cumulative regret: Regret = t × best_arm_reward - actual_cumulative_reward. Track traffic distribution (should converge to best arm). Compare to theoretical optimal (if ran experiment with perfect knowledge). RESULT: Bandit identifies best model faster (1-2 weeks vs 4 weeks for A/B) with lower regret (fewer users saw bad models).",
    keyPoints: [
      'A/B testing: Fixed 50-50 split, high regret, clear winner after test, simple implementation, good for 2-3 variants',
      'MAB: Adaptive traffic (shift to winners), low regret, continuous learning, complex implementation, good for 5+ variants',
      'Regret: A/B wastes 50% traffic on worse model, MAB quickly reduces traffic to bad arms',
      'Thompson Sampling: Bayesian, Beta distribution, automatically balances exploration/exploitation, best theoretical guarantees',
      '5 algorithms: Use MAB (Thompson) to minimize regret, faster convergence, continuous optimization, higher cumulative reward',
    ],
  },
  {
    id: 'abt-q-3',
    question:
      'Design an experimentation platform that allows multiple teams to run concurrent A/B tests without interfering with each other. Address: (1) experiment isolation and orthogonalization, (2) handling experiment collisions, (3) shared infrastructure for traffic splitting, (4) centralized metrics and analysis, (5) governance and experiment approval process. How would you detect and resolve conflicts between experiments?',
    sampleAnswer:
      'Multi-Team Experimentation Platform: (1) Experiment Isolation: ORTHOGONAL EXPERIMENTS: Independent experiments should not interact. Example: Team A tests recommendation algo, Team B tests UI color. Orthogonal—no interference. METHOD: Orthogonal traffic splits. Each user independently assigned to treatment/control for each experiment. User 123: Experiment A (treatment), Experiment B (control). IMPLEMENTATION: Separate hash functions per experiment. def get_bucket (user_id, experiment_id, num_buckets): seed = f"{experiment_id}_{user_id}"; return hash (seed) % num_buckets. Each experiment gets independent randomization → orthogonal. NAMESPACES: Divide experiments into layers. Layer 1 (Ranking): Tests to ranking algo. Layer 2 (UI): Tests to UI. Layers independent. (2) Handling Collisions: DETECTION: If experiments touch same code/metric → potential collision. Example: Team A tests "recommendation algo", Team B tests "recommendation UI". Both affect CTR → collision. RESOLUTION OPTIONS: SERIAL: Run experiments sequentially (A then B). Safe but slow. PARALLEL (if low risk): Run both, but larger sample size needed (reduce statistical power). PRIORITY: High-priority experiment gets preference. Pause lower priority. HOLDOUT: Create holdout group (no experiments). Compare experimental groups to holdout to measure combined effect. EXAMPLE: Experiment A: New algo (50% users). Experiment B: New UI (50% users). Result: 4 groups: (A=control, B=control), (A=treatment, B=control), (A=control, B=treatment), (A=treatment, B=treatment). Analyze: Is interaction effect (A×B) significant? If yes → collision. (3) Shared Infrastructure: CENTRALIZED TRAFFIC SPLITTER: Microservice handles all experiment assignments. Input: user_id, experiment_id. Output: treatment/control. Benefits: Consistent randomization, audit log (which user saw what), easy experiment management. API: POST /assign {user_id: "123", experiment_id: "rec_algo_v2"} → {variant: "treatment"}. FEATURE FLAG SERVICE (LaunchDarkly, Optimizely): Configure experiments via UI (no code deploy). Adjust traffic splits dynamically (start at 10%, increase to 50%). Kill switch (turn off experiment immediately if issues). LOGGING PIPELINE: Every request logs: user_id, list of experiments, variant for each, timestamp, metrics (CTR, revenue). Centralized data warehouse for analysis. (4) Centralized Metrics: METRICS STORE: Define metrics once, reuse across experiments. Metric: CTR = clicks / impressions. Used by all teams. Pre-computed metrics: Revenue per user (RPU), Time on site. AUTOMATED ANALYSIS: Dashboard shows: Experiment status (running, paused, completed), Sample size progress (50K / 100K users), P-value, confidence interval, lift. Scorecard: Primary metric (CTR: +5%), secondary metrics, guardrail metrics (latency OK). STATISTICAL RIGOR: Automated p-value calculation, confidence intervals, multiple testing correction (if testing >1 metric). (5) Governance: EXPERIMENT PROPOSAL: Team submits: Experiment name, hypothesis, metrics (primary + guardrail), sample size, duration (2-4 weeks), impact analysis (which code touched, potential collisions). APPROVAL PROCESS: Review by experimentation platform team. Check: No collisions with running experiments, Metrics clearly defined, Sample size sufficient, Guardrails in place. Approve or request changes. EXPERIMENTATION CALENDAR: Public calendar shows all running experiments. Teams check before proposing new experiment to avoid collisions. CONFLICT DETECTION: Platform checks: Is new experiment\'s code path overlapping with running experiments? Are metrics shared? If yes → flag potential collision, recommend serial execution or larger sample size. EXAMPLE CONFLICT: Team A: Testing recommendation algo. Running, 50% users. Team B: Proposes test to recommendation ranking. CONFLICT: Both affect "recommendations shown". RESOLUTION: Option 1: Wait for A to finish (2 weeks), then run B. Option 2: Run B on remaining 50% users (not in A). But power reduced. Option 3: Pause A temporarily, run B, resume A. Platform suggests: Serial execution (Option 1) to avoid interaction effects. MONITORING: Dashboard tracks: Active experiments (15 running, 5 pending), Collision warnings (3 potential conflicts detected), Experiment health (2 experiments need attention—low traffic). Weekly report: Completed experiments (5), significant wins (2), no-ops (3), regressions (0).',
    keyPoints: [
      'Isolation: Orthogonal experiments use independent hash functions, namespaces (layers) for code separation',
      'Collisions: Detect overlapping code/metrics, resolve with serial execution or holdout groups, analyze interaction effects',
      'Shared infrastructure: Centralized traffic splitter (API), feature flags, logging pipeline to data warehouse',
      'Centralized metrics: Define once, reuse, automated analysis (p-values, CI, lift), scorecards per experiment',
      'Governance: Proposal → approval → conflict detection, experimentation calendar, monitor active experiments + collisions',
    ],
  },
];
