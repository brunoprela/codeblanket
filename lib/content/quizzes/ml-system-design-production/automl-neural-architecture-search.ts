export const automlNeuralArchitectureSearchQuiz = [
  {
    id: 'anas-q-1',
    question:
      "Design an AutoML system that can automatically train and select the best model for tabular classification problems. Address: (1) feature engineering automation, (2) model selection across different algorithms, (3) hyperparameter optimization strategy, (4) ensemble methods, (5) compute budget management. How would you ensure the AutoML system doesn't overfit to the validation set?",
    sampleAnswer:
      'Complete AutoML System: (1) Feature Engineering Automation: AUTOMATED TRANSFORMATIONS: Numeric features: Log transform, sqrt, polynomial (x²), binning. Categorical: One-hot encoding, target encoding, frequency encoding. DateTime: Extract year, month, day, hour, day_of_week, is_weekend. Text: TF-IDF, word count, sentiment scores. FEATURE GENERATION: Interactions: Multiply pairs of numeric features (feature1 × feature2). Aggregations: Group by categorical, compute mean/sum/count of numeric. Feature selection: Remove low-variance (<0.01), highly correlated (r>0.95), low importance (from tree model). Tools: Featuretools (automatic feature engineering), TPOT (genetic programming). Example: Auto-sklearn generates 100+ features, selects top 50 based on importance. (2) Model Selection: TEST MULTIPLE ALGORITHMS: Linear: Logistic Regression, Ridge, Lasso. Trees: Random Forest, XGBoost, LightGBM, CatBoost. Neural Networks: MLP (multilayer perceptron). Naive Bayes, SVM (for small datasets). SEQUENTIAL SELECTION: Start with fast models (Logistic Regression, 1 min). If accuracy <80%, try trees (Random Forest, 10 min). If still <85%, try boosting (XGBoost, 30 min). Budget-aware: Allocate more time to promising model families. EARLY STOPPING: If model accuracy <baseline after 10% of max_iter → stop training (save time). Example: XGBoost with 1000 trees. After 100 trees, accuracy 82%. Baseline 85% → stop (won\'t reach goal). (3) Hyperparameter Optimization: BAYESIAN OPTIMIZATION (Optuna): Sample hyperparameters guided by Gaussian Process. Example: learning_rate, max_depth, n_estimators. 50 trials, each 5 min → 250 min total. HYPERBAND: Early stopping for hyperparameter search. Train many configs for few iterations, keep best, train longer. Example: 81 configs × 10 iter each → keep best 27 → train 30 iter → keep best 9 → train 100 iter → best 3 → train 300 iter → winner. Saves 50% compute vs full grid search. METAFEATURES: Use dataset characteristics to initialize search. Example: If dataset has >100K rows, high capacity models (deep trees) likely better. If <1K rows, simpler models (linear) better. Warm-start search in promising region. (4) Ensemble Methods: STACKING: Train multiple models (XGBoost, LightGBM, Random Forest). Use predictions as features for meta-model (Logistic Regression). Meta-model learns to combine base models. Cross-validation: Train base models on fold 1-4, predict fold 5 → meta-features. Repeat for all folds. Prevents overfitting. BLENDING: Simple weighted average. weights = [0.5, 0.3, 0.2] for [XGBoost, LightGBM, RF]. Optimize weights on validation set. VOTING: Hard voting (majority vote for classification) or soft voting (average probabilities). Example: 3 models predict [0, 1, 1] → majority vote = 1. CARUANA\'S ENSEMBLE SELECTION: Greedily add models to ensemble. Start with best single model. Iterate: Add model that most improves ensemble performance. Repeat until no improvement. Automatically selects ensemble size and models. (5) Compute Budget Management: BUDGET ALLOCATION: Total budget: 1 hour. Allocate: 10 min feature engineering, 40 min model training, 10 min ensembling. Model training budget: Fast models (5 min each): Logistic, Ridge → 10 min. Medium models (10 min each): Random Forest, LightGBM → 20 min. Slow models (10 min each): XGBoost, Neural Net → 20 min. ADAPTIVE: If fast model scores >85% → skip slow models (goal achieved). PARALLEL: Train models in parallel (5 models × 8 min = 8 min if 5 GPUs). AVOIDING VALIDATION OVERFITTING: HOLDOUT SET: Split data: Train (60%), Validation (20%), Test (20%). AutoML uses Train + Validation for model selection. Final evaluation on Test (unseen by AutoML). Report Test accuracy (unbiased). NESTED CROSS-VALIDATION: Outer loop (5 folds): For each fold: Use 4 folds for AutoML (including model selection), Test on 5th fold. Inner loop (within AutoML): Use 4 folds for training, 1 for validation. Average Test accuracy across outer folds → unbiased estimate. REGULARIZATION: Limit model complexity. Max ensemble size: 5 models (prevent overfitting to validation). Early stopping in hyperparameter search. SIMPLE FINAL MODEL: If ensemble of 10 models has same accuracy as single XGBoost → choose simpler (interpretable, faster inference). Implementation (Auto-sklearn): from autosklearn.classification import AutoSklearnClassifier; automl = AutoSklearnClassifier (time_left_for_this_task=3600, per_run_time_limit=360, ensemble_size=5); automl.fit(X_train, y_train); predictions = automl.predict(X_test); print(f"Test Accuracy: {accuracy_score (y_test, predictions)}"); print(automl.show_models()) # Shows selected models. Result: AutoML finds best pipeline automatically. Compares: 15 feature eng configs × 6 algorithms × 50 hyperparameter configs = 4500 combinations. Evaluates 100 best (budget limit). Achieves 92% accuracy (vs 88% manual baseline).',
    keyPoints: [
      'Feature automation: Transforms (log, poly), interactions, aggregations, selection (low-var, correlation, importance)',
      'Model selection: Sequential (fast → medium → slow), early stopping if <baseline, budget-aware allocation',
      'Hyperparameter: Bayesian optimization (Optuna), Hyperband (early stopping), metafeatures (warm-start)',
      'Ensemble: Stacking (meta-model), blending (weighted avg), Caruana selection (greedy add models)',
      'Avoid overfitting: Holdout test set, nested CV, regularization (limit ensemble size), report test accuracy',
    ],
  },
  {
    id: 'anas-q-2',
    question:
      'Explain Neural Architecture Search (NAS) and how it differs from traditional AutoML. Address: (1) search space definition, (2) search strategies (random, reinforcement learning, evolutionary), (3) performance estimation (proxy tasks, early stopping), (4) computational cost, (5) when to use NAS vs traditional hyperparameter tuning. Would you recommend NAS for a production system?',
    sampleAnswer:
      "Neural Architecture Search (NAS): (1) Search Space Definition: NAS searches over neural network architectures (layers, connections, operations). MACRO SEARCH SPACE: Number of layers (5-20), layer types (conv, pooling, dense), connections (sequential, skip connections). Example: ResNet has skip connections, VGG is sequential. MICRO SEARCH SPACE (NASNet): Search for cell structure (repeated block). Cell = directed graph with nodes (feature maps) and edges (operations). Operations: 3×3 conv, 5×5 conv, max pool, identity. Once cell found, stack cells to form full network. ADVANTAGES: Smaller search space (one cell vs full architecture). Transfer learning: Cell found on CIFAR-10 transfers to ImageNet. Traditional AutoML: Searches hyperparameters of fixed architecture (learning rate, batch size, dropout). Doesn't change architecture. (2) Search Strategies: RANDOM SEARCH: Sample random architectures, evaluate, keep best. Baseline. Inefficient (may need 10,000+ architectures). REINFORCEMENT LEARNING (NAS paper, Google): Controller RNN generates architecture. Train architecture on task, get validation accuracy (reward). Update controller with REINFORCE to maximize reward. Example: Controller generates [Conv(64), Pool, Conv(128), Dense(10)]. Train → accuracy 85%. Controller learns to generate high-accuracy architectures. Cost: Requires 10,000+ architecture evaluations. EVOLUTIONARY ALGORITHMS (AmoebaNet): Population of architectures. Select top performers, mutate (add/remove layer), create offspring. Repeat for generations. Example: Start with 100 random architectures. Evolve for 50 generations. Best architecture emerges. DIFFERENTIABLE NAS (DARTS): Treat architecture search as continuous optimization. All operations active initially (weighted sum). Optimize weights (which operations to use). Prune low-weight operations → final architecture. Fast: 1 GPU-day vs 1800 GPU-days for RL-based NAS. (3) Performance Estimation: FULL TRAINING (expensive): Train each architecture for 100 epochs, evaluate. Cost: 10,000 architectures × 100 epochs = 1M epochs. Prohibitive. EARLY STOPPING: Train for 5 epochs, estimate final accuracy from learning curve. Cost: 50× reduction. Caveat: Early performance may not predict final performance. PROXY TASKS: Train on smaller dataset (CIFAR-10 instead of ImageNet) or lower resolution (32×32 vs 224×224). Transfer architecture to full task. WEIGHT SHARING (ENAS): Share weights across architectures. Treat all architectures as subgraphs of super-network. Train super-network, evaluate architectures by sampling subgraphs (no retraining needed). Cost: 1000× speedup. (4) Computational Cost: RL-BASED NAS: 1800 GPU-days (Google, 2017). Prohibitive for most. EVOLUTIONARY: 3000 GPU-days (AmoebaNet). DARTS (Differentiable): 1 GPU-day. Feasible. ONE-SHOT NAS (ENAS, Weight sharing): 0.5 GPU-days. COMPARISON: Traditional hyperparameter tuning: 10-100 GPU-hours (50 trials × 2 hours). NAS (DARTS): 1 GPU-day = 24 GPU-hours (comparable). NAS (RL): 1800 GPU-days = 43,200 GPU-hours (1000× more expensive). (5) NAS vs Traditional Tuning: USE NAS WHEN: Novel problem (no known good architecture). Example: New modality (3D medical images), new task (graph classification). Have large compute budget (100+ GPU-days) or use efficient NAS (DARTS). Want state-of-the-art performance (NAS can find better architectures than humans). Long-term deployment (one-time NAS cost amortized over years of use). USE TRADITIONAL TUNING WHEN: Established problem (image classification → use ResNet, text → use Transformer). Limited compute (<10 GPU-days). Need interpretability (NAS architectures are complex, hard to explain). Quick iteration (days, not weeks). Production recommendation: NO for most systems. (1) Cost: NAS expensive (even DARTS = 1 GPU-day = $200). Hyperparameter tuning cheaper. (2) Complexity: NAS architectures complex, hard to debug, maintain. Standard architectures (ResNet, BERT) well-understood. (3) Marginal gains: NAS may improve accuracy 1-2%. Often not worth complexity. (4) Transfer learning: Pre-trained models (ResNet on ImageNet) beat custom NAS architectures for most tasks. EXCEPTION: Use NAS for: Extremely high-value task (e.g., self-driving perception, medical diagnosis) where 1% accuracy = millions of dollars. Novel modality with no pre-trained models. Deployment on resource-constrained devices (NAS can find efficient architectures—MobileNet, EfficientNet were NAS-derived). Example: Google uses NAS for mobile vision models (MobileNet family). Worth investment for billions of devices.",
    keyPoints: [
      'NAS searches architecture (layers, connections), traditional AutoML searches hyperparameters of fixed architecture',
      'Search strategies: Random (baseline), RL (expensive, 1800 GPU-days), evolutionary, DARTS (differentiable, 1 GPU-day)',
      'Performance estimation: Early stopping (5 epochs), proxy tasks (CIFAR-10), weight sharing (ENAS, 1000× speedup)',
      'Cost: RL-based NAS 1800 GPU-days, DARTS 1 GPU-day, traditional tuning 10-100 GPU-hours',
      'Production: Generally NO (cost, complexity, marginal gains). YES for high-value tasks, novel modalities, mobile deployment',
    ],
  },
  {
    id: 'anas-q-3',
    question:
      'Design an automated feature engineering system for time series forecasting. Address: (1) lag feature generation, (2) rolling window statistics, (3) seasonal features, (4) automatic feature selection, (5) handling different time series characteristics (trend, seasonality, noise). How would you prevent data leakage in automated feature engineering?',
    sampleAnswer:
      'Automated Time Series Feature Engineering: (1) Lag Features: Generate lagged values: y_{t-1}, y_{t-2}, ..., y_{t-k}. Auto-determine k: ACF/PACF analysis. Example: If ACF significant up to lag 7 → create 7 lag features. Code: for lag in range(1, 8): df[f"lag_{lag}",] = df["value",].shift (lag). Remove first 7 rows (NaN due to shifts). External features: If predicting sales, lag features from weather, holidays. MULTI-STEP LAG: For horizon h=7 (predict 7 days ahead), use lags: y_{t-7}, y_{t-14} (multiples of horizon). Relevant for multi-step forecasting. (2) Rolling Window Statistics: Compute statistics over rolling windows: mean, std, min, max, median. Windows: 7-day, 30-day, 90-day (short, medium, long-term trends). Code: for window in [7, 30, 90]: df[f"rolling_mean_{window}",] = df["value",].rolling (window).mean(); df[f"rolling_std_{window}",] = df["value",].rolling (window).std(). Exponential moving average (EMA): Gives more weight to recent values. Code: df["ema_7",] = df["value",].ewm (span=7).mean(). Ensure no lookahead: Use .shift(1) to lag rolling stats. Example: df[f"rolling_mean_7",] = df["value",].rolling(7).mean().shift(1). (3) Seasonal Features: CALENDAR FEATURES: Month, day_of_week, day_of_month, quarter, is_weekend, is_holiday. Cyclical encoding: For month (1-12), use sin/cos: month_sin = sin(2π × month / 12), month_cos = cos(2π × month / 12). Preserves cyclicity (Dec and Jan are close). SEASONAL DECOMPOSITION (STL): Decompose series: y_t = Trend_t + Seasonal_t + Residual_t. Use decomposed components as features. Example: trend (long-term direction), seasonal (repeating pattern), residual (noise). FOURIER TERMS: For complex seasonality, use Fourier series. Example: Daily data with weekly seasonality (period=7): Features: sin(2πt/7), cos(2πt/7), sin(4πt/7), cos(4πt/7). Captures periodic patterns. (4) Automatic Feature Selection: CORRELATION-BASED: Remove highly correlated features (r>0.95). Example: rolling_mean_30 and rolling_mean_90 may be highly correlated. IMPORTANCE-BASED: Train tree model (XGBoost), rank features by importance. Keep top 50 features (remove low-importance). RECURSIVE FEATURE ELIMINATION: Start with all features. Remove least important iteratively. Stop when validation error increases. LASSO: L1 regularization shrinks unimportant features to zero. Automatically selects features. FORWARD SELECTION: Start with no features. Add feature that most improves validation error. Repeat until no improvement. (5) Handling Different Characteristics: TREND: If trend detected (ADF test p>0.05), difference series: y\'_t = y_t - y_{t-1}. Feature: is_uptrend = 1 if rolling_mean_30 > rolling_mean_90 else 0. SEASONALITY: If seasonal (ACF has periodic spikes), add seasonal lags (y_{t-7}, y_{t-14} for weekly). Add calendar features (month, day_of_week). NOISE: High noise (low signal-to-noise ratio): Use longer rolling windows (smooth out noise). Apply smoothing (EMA). PREVENTING DATA LEAKAGE: CRITICAL: Never use future information to predict past. SHIFT ALL FEATURES: Rolling statistics computed at time t include data from t-7 to t. But prediction at t should use data only up to t-1. Solution: df["rolling_mean_7",] = df["value",].rolling(7).mean().shift(1). Shifts window to t-8 to t-1. VALIDATION: Use walk-forward validation. Train on [t-365:t], predict [t+1:t+7], slide forward. Ensures no lookahead bias. FEATURE ENGINEERING IN PIPELINE: Apply same transformations in training and serving. No differences (cause train-test skew). Example Automated System: from tsfresh import extract_features; # Automated feature extraction; features = extract_features (df, column_id="series_id", column_sort="timestamp", column_value="value"); # Includes: mean, median, variance, skew, kurtosis, ACF, FFT coefficients, etc. # Feature selection (tsfresh has built-in); from tsfresh.feature_selection import select_features; features_filtered = select_features (features, y_train); # Keeps only significant features (p<0.05). Train model: model.fit (features_filtered, y_train). Comprehensive Pipeline: Auto-detect seasonality (STL, FFT) → generate appropriate lags. Auto-detect trend → difference if needed. Generate rolling stats with multiple windows. Add calendar features. Feature selection (importance-based). Validate: Walk-forward CV. Result: From raw series → 200+ features → 50 selected features → trained model. Automates 80% of feature engineering work.',
    keyPoints: [
      'Lag features: ACF/PACF to determine k, multi-step lags (y_{t-7}, y_{t-14}), shift to avoid lookahead',
      'Rolling stats: mean/std/min/max over 7-30-90 day windows, EMA, critical: shift(1) to prevent lookahead',
      'Seasonal: Calendar features (month, day_of_week), cyclical encoding (sin/cos), STL decomposition, Fourier terms',
      'Feature selection: Correlation filter, importance-based (XGBoost), RFE, LASSO, forward selection',
      'Prevent leakage: Shift all features by 1, walk-forward validation, apply same transformations in serving',
    ],
  },
];
