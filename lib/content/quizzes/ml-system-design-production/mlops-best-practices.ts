export const mlopsBestPracticesQuiz = [
  {
    id: 'mbp-q-1',
    question:
      'Design a complete CI/CD pipeline for ML models. Address: (1) automated testing (data validation, model performance, integration tests), (2) deployment strategies (canary, blue-green), (3) rollback mechanisms, (4) versioning (code, data, model), (5) approval workflows. How would you prevent a bad model from reaching production?',
    sampleAnswer:
      'ML CI/CD Pipeline: (1) Automated Testing: DATA VALIDATION TESTS: Schema validation: Columns, types, constraints. Great Expectations: expect_column_to_exist, expect_column_values_to_be_between. Distribution tests: KS test vs training data. PSI <0.25 (no major shift). Feature quality: Null rate <1%, outlier rate <0.1%. If tests fail → block pipeline, alert data team. MODEL PERFORMANCE TESTS: Accuracy threshold: Test set accuracy >85% (business requirement). F1-score >0.80. Fairness: Check performance across demographics. No bias. Latency: P99 inference time <100ms. Benchmark on test data. Comparison: New model accuracy ≥ production model accuracy. If worse → block. INTEGRATION TESTS: API contract: Test endpoint /predict accepts expected input, returns expected output. Load test: 10K requests/sec for 5 minutes. P99 latency <100ms, error rate <0.1%. Backwards compatibility: New model works with existing API. Shadow mode: Run new model alongside production (don\'t return results). Compare predictions for 10K requests. If disagreement >10% → investigate. (2) Deployment Strategies: CANARY: Deploy new model to 10% traffic. Monitor for 1 hour: If accuracy, latency, error rate OK → increase to 50%. Monitor 4 hours → 100%. If metrics degrade at any stage → automatic rollback. Implementation: K8s Deployment with two versions. Service routes 10% to v2, 90% to v1. Gradually adjust weights. BLUE-GREEN: Two identical environments (blue=v1, green=v2). Deploy v2 to green, run tests. Switch load balancer: 100% traffic blue → green (instant cutover). Keep blue running 24 hours (rollback option). If green stable → decommission blue. Implementation: Two K8s Deployments. Switch Service selector from blue to green. (3) Rollback Mechanisms: AUTOMATED ROLLBACK TRIGGERS: Accuracy drops >5% (compared to baseline). Error rate >1% for 5 minutes. P99 latency >150ms (1.5× threshold). Business metric drops (e.g., revenue per user). ROLLBACK PROCESS: Canary: Revert traffic split (10% → 0% for new model). Blue-green: Switch load balancer back to blue. K8s: kubectl rollout undo deployment/model-api. Time: <2 minutes for rollback. SAFEGUARDS: Keep last 3 model versions in registry (MLflow). Can rollback to v_n-1, v_n-2, v_n-3. Automated tests run on rollback (ensure old version still works). (4) Versioning: CODE: Git commit hash logged with every training run. Tag releases: git tag model-v2.3.1. To reproduce: git checkout <commit_hash>. DATA: DVC for dataset versioning. dvc add data/train.csv. Track with git (stores hash, not data). To reproduce: dvc checkout data/train.csv.dvc. DATA VERSION: v1.5.3 (semantic versioning). MODEL: MLflow Registry. Model versions auto-incremented (v1, v2, v3). Each version stores: Model artifacts, metrics, hyperparameters, linked training run (which has code + data version). LINEAGE: Model v2.3 trained with: Code v2.3.1 (commit abc123), Data v1.5.3, Hyperparameters: {lr: 0.001, ...}. Full reproducibility. (5) Approval Workflows: STAGES (MLflow Registry): None → Staging → Production → Archived. None: Newly trained model. No approval. Staging: Passed automated tests. Ready for manual review. Production: Approved by ML lead + product manager. Serving live traffic. APPROVAL PROCESS: Step 1: Model training completes. Automated tests run. If pass → promote to Staging. Step 2: ML engineer reviews: Model card (performance, fairness, limitations), comparison with production model (accuracy, latency), A/B test plan. If approved → request production promotion. Step 3: ML lead + PM review business impact, approve/reject. If approved → promote to Production. GOVERNANCE: All approvals logged (who, when, why). Audit trail for compliance. Pull requests for model code changes (peer review). PREVENTING BAD MODEL: Multi-layer defense: Layer 1: Automated tests (data, model, integration). 80% bad models caught. Layer 2: Shadow mode (compare with production). 15% caught. Layer 3: Canary deployment (10% traffic first). 4% caught. Layer 4: Monitoring + auto-rollback (if deployed bad model escapes). 1% caught. Result: <0.1% chance of bad model affecting all users. Example Pipeline (GitHub Actions + MLflow): # .github/workflows/ml-ci.yml; on: pull_request. jobs: data-validation: runs: pytest tests/test_data.py. model-training: runs: python train.py; model-tests: runs: pytest tests/test_model.py (accuracy, latency, fairness); shadow-mode: runs: python test_shadow.py (compare with production); promote-staging: if all tests pass: mlflow.set_tag("stage", "Staging"). On merge to main: deploy-canary: kubectl set image (10% traffic); monitor: Wait 1 hour, check metrics; promote-production: If OK, increase to 100%.',
    keyPoints: [
      'Automated tests: Data validation (schema, distribution, PSI), model performance (accuracy >85%, latency <100ms), integration (API, load)',
      'Deployment: Canary (10% → 50% → 100% with monitoring), blue-green (instant switch, keep old 24h)',
      'Rollback: Automated triggers (accuracy drops >5%, error rate >1%), <2min rollback, keep last 3 versions',
      'Versioning: Git (code), DVC (data), MLflow registry (model), full lineage (code+data+hyperparameters)',
      'Approval: Staging → Production requires ML lead + PM approval, automated tests gate, audit trail, multi-layer defense',
    ],
  },
  {
    id: 'mbp-q-2',
    question:
      'Explain ML-specific testing strategies. Address: (1) unit tests for data and models, (2) integration tests, (3) performance tests (latency, throughput), (4) fairness tests, (5) regression tests. How do ML tests differ from traditional software tests?',
    sampleAnswer:
      'ML Testing Strategies: (1) Unit Tests: DATA TESTS: Test data quality before training. Example: def test_no_nulls(): assert df.isnull().sum().sum() == 0. def test_age_range(): assert (df["age",] >= 0).all() and (df["age",] <= 120).all(). def test_no_duplicates(): assert not df.duplicated().any(). Run before every training. MODEL TESTS: Test model behavior on known inputs. Example: def test_fraud_model_on_known_fraud(): X = [[500, 1, 15]] # [amount, new_merchant, hour]; prediction = model.predict(X); assert prediction == 1 # Should predict fraud. def test_model_predictions_in_range(): predictions = model.predict_proba(X_test); assert (predictions >= 0).all() and (predictions <= 1).all(). FEATURE TESTS: Test feature engineering logic. Example: def test_age_binning(): assert age_to_bin(25) == "20-30"; assert age_to_bin(65) == "60+". Ensures features computed correctly. (2) Integration Tests: END-TO-END: Test full pipeline (data load → preprocess → predict → return). Example: def test_prediction_api(): response = requests.post("/predict", json={"user_id": 123, "amount": 100}); assert response.status_code == 200; assert "prediction" in response.json(); assert 0 <= response.json()["prediction",] <= 1. API CONTRACT: Test input/output schema. Example: def test_api_schema(): response = predict_api (test_input); assert "prediction" in response; assert "confidence" in response; assert isinstance (response["prediction",], int). BACKWARDS COMPATIBILITY: New model version works with old API. Example: def test_v2_model_with_v1_api(): # Deploy model v2; response = v1_api.predict (test_input); assert response["status",] == "success". (3) Performance Tests: LATENCY: Test inference time. Example: def test_inference_latency(): start = time.time(); for _ in range(100): model.predict(X_test[:1]); latency_ms = (time.time() - start) / 100 * 1000; assert latency_ms <10 # <10ms per prediction. THROUGHPUT: Test requests per second. Example: def test_throughput(): start = time.time(); requests_sent = 0; while time.time() - start <10: # 10 seconds; requests.post("/predict", ...); requests_sent += 1; throughput = requests_sent / 10; assert throughput >1000 # >1000 req/sec. LOAD TEST: Simulate production traffic. Example (Locust): from locust import HttpUser, task; class ModelUser(HttpUser): @task def predict(): self.client.post("/predict", json={...}). Run: locust -f loadtest.py --users 1000 --spawn-rate 100. (4) Fairness Tests: Test model doesn\'t discriminate. DEMOGRAPHIC PARITY: P(prediction=1 | group A) ≈ P(prediction=1 | group B). Example: def test_fairness_gender(): male_fraud_rate = model.predict(X_test[X_test["gender",]=="M",]).mean(); female_fraud_rate = model.predict(X_test[X_test["gender",]=="F",]).mean(); assert abs (male_fraud_rate - female_fraud_rate) <0.05 # <5% difference. EQUALIZED ODDS: True positive rate similar across groups. Example: def test_equalized_odds(): for group in ["A", "B",]: X_group = X_test[X_test["group",]==group]; y_group = y_test[X_test["group",]==group]; tpr = recall_score (y_group, model.predict(X_group)); assert tpr >0.7 # Both groups have TPR >70%. (5) Regression Tests: Test new model doesn\'t regress on important cases. GOLDEN DATASET: Curated set of critical examples. Example: fraud cases from past incidents, edge cases (high-value transactions). Test new model on golden dataset. Assert: Accuracy on golden set ≥ 95%. If new model performs worse → regression. COMPARISON WITH BASELINE: def test_new_model_vs_baseline(): baseline_accuracy = baseline_model.score(X_test, y_test); new_accuracy = new_model.score(X_test, y_test); assert new_accuracy >= baseline_accuracy - 0.02 # Allow 2% drop max. CRITICAL SEGMENTS: Test performance on important user segments. Example: High-value customers, new users, specific regions. def test_performance_on_high_value_users(): X_high_value = X_test[X_test["lifetime_value",] >1000]; y_high_value = y_test[X_test["lifetime_value",] >1000]; accuracy = new_model.score(X_high_value, y_high_value); assert accuracy >0.90 # Critical segment must have high accuracy. ML TESTS vs TRADITIONAL SOFTWARE TESTS: TRADITIONAL: Deterministic. Same input → same output. Test: assert add(2, 3) == 5. ML: Probabilistic. Same input → possibly different output (due to randomness in training). Test: assert 0.85 < model.score(X_test, y_test) <0.95 (range, not exact value). TRADITIONAL: Logic bugs (code crashes, wrong formula). ML: Data bugs (distribution shift, label noise), performance bugs (accuracy drops). TRADITIONAL: Test coverage (% of code executed). ML: Test coverage (% of data space covered—impossible to test all inputs). Use golden dataset + random sampling. TRADITIONAL: Regression = new code breaks old functionality. ML: Regression = new model performs worse than old model on critical cases. TRADITIONAL: Performance = execution speed. ML: Performance = both execution speed AND model accuracy. BEST PRACTICES: Automated: Run tests in CI/CD pipeline (GitHub Actions, Jenkins). Continuous: Test on every commit. Comprehensive: Data + model + integration + performance + fairness + regression. Fast: Unit tests <1 minute. Integration tests <5 minutes. Don\'t block development. Comprehensive test suite: 100+ tests covering all aspects. Result: Catch 95%+ issues before production.',
    keyPoints: [
      'Unit tests: Data quality (nulls, ranges, duplicates), model behavior (known inputs), feature engineering logic',
      'Integration: End-to-end pipeline, API contract (input/output schema), backwards compatibility',
      'Performance: Latency (<10ms), throughput (>1000 req/sec), load test (Locust with 1000 users)',
      'Fairness: Demographic parity (prediction rate similar across groups), equalized odds (TPR similar)',
      'ML vs traditional: Probabilistic (test ranges not exact), data bugs, test data space coverage, performance = speed + accuracy',
    ],
  },
  {
    id: 'mbp-q-3',
    question:
      'Design a model reproducibility strategy. Address: (1) what must be versioned (code, data, environment, config), (2) tools for each (Git, DVC, Docker, Hydra), (3) model registry and lineage tracking, (4) reproducing a model from 6 months ago, (5) challenges and solutions. Why is reproducibility critical for production ML?',
    sampleAnswer:
      'Model Reproducibility Strategy: (1) What Must Be Versioned: CODE: Training scripts, feature engineering, model architecture. VERSION: Git commit hash. MODEL_TRAINING_V2.3.1 (semantic versioning). DATA: Training dataset, validation dataset. VERSION: DVC hash or S3 path with timestamp. train_data_v1.5.3.csv. ENVIRONMENT: Python version, package versions (numpy, pytorch, scikit-learn). VERSION: requirements.txt (pinned versions), conda environment.yml, or Docker image tag. CONFIG: Hyperparameters, training config, data paths. VERSION: config.yaml (tracked in git). MODEL ARTIFACTS: Trained model weights, optimizer state, training history. VERSION: MLflow run_id. RANDOM SEEDS: Python seed, numpy seed, pytorch seed, CUDA seed. VERSION: Logged in MLflow. (2) Tools: GIT (Code): Track training scripts. Tag releases: git tag model-v2.3.1. Commit hash: abc123def456. DVC (Data): Track datasets. dvc add data/train.csv. Git stores data/train.csv.dvc (hash + metadata), S3 stores actual data. dvc checkout data/train.csv.dvc pulls exact version. DOCKER (Environment): Dockerfile: FROM python:3.9; COPY requirements.txt .; RUN pip install -r requirements.txt. Build: docker build -t ml-training:v2.3.1. Run: docker run ml-training:v2.3.1 python train.py. Guarantees exact environment. HYDRA (Config): config.yaml: model: type: xgboost; params: {learning_rate: 0.01, max_depth: 10}. Hydra loads config, overrides via CLI. Logs config with every run. MLFLOW (Model + Metadata): Tracks: run_id, metrics, parameters, artifacts, git_commit, data_version. Links everything. (3) Model Registry & Lineage: MLFLOW REGISTRY: Register model: mlflow.register_model("runs/{run_id}/model", "fraud_detection"). Versions: v1, v2, v3 (auto-incremented). Stages: None → Staging → Production. LINEAGE TRACKING: Model v2 lineage: Training run: run_id=abc123; Code version: git commit def456; Data version: DVC hash xyz789; Environment: Docker image ml-training:v2.3.1; Hyperparameters: {lr: 0.01, max_depth: 10}; Metrics: {accuracy: 0.92, F1: 0.88}; Timestamp: 2024-10-15 14:32:00. Full lineage stored in MLflow. (4) Reproducing Model from 6 Months Ago: SCENARIO: Model v2.0 trained on 2024-04-15. Need to reproduce exactly (debug issue, audit, retrain with more data). STEPS: Step 1: Retrieve lineage from MLflow. model_info = mlflow.get_model_version("fraud_detection", version=2); run = mlflow.get_run (model_info.run_id); git_commit = run.data.tags["git_commit",]; data_version = run.data.params["data_version",]; docker_image = run.data.tags["docker_image",]. Step 2: Restore code. git checkout {git_commit}. Step 3: Restore data. dvc checkout data/train.csv.dvc (pulls exact data from S3). Step 4: Restore environment. docker pull {docker_image} or conda env create -f environment.yml. Step 5: Restore config. Config stored in MLflow artifacts: mlflow.artifacts.download (run_id, "config.yaml"). Step 6: Re-run training. docker run {docker_image} python train.py --config config.yaml. Step 7: Verify. Compare metrics: Old model: accuracy 0.92, new training: accuracy 0.9201. Match within floating-point error (✓ reproduced). (5) Challenges & Solutions: CHALLENGE 1: Non-determinism. GPU operations non-deterministic (cuDNN). Same code → slightly different results. SOLUTION: Set deterministic mode: torch.backends.cudnn.deterministic=True, torch.backends.cudnn.benchmark=False. Accept small numerical differences (<0.1% accuracy). CHALLENGE 2: External dependencies. Model uses live API (e.g., weather API) during training. Can\'t reproduce (API data changed). SOLUTION: Log API responses during training. Store in MLflow artifacts. Replay during reproduction. CHALLENGE 3: Large data. 10TB training data. Can\'t store all versions (expensive). SOLUTION: Store data snapshots (monthly). Or store data transformations (not raw data). Trade-off: Cost vs reproducibility. CHALLENGE 4: Environment drift. Conda/pip dependency resolution changes over time. requirements.txt may install different versions. SOLUTION: Use Docker (exact environment, no drift). Or lock transitive dependencies: pip freeze >requirements.lock.txt. CHALLENGE 5: Human error. Engineer forgets to commit code before training. Model trained on uncommitted code. SOLUTION: Automated checks in CI. If git status --porcelain not empty → fail training. Force clean git state. WHY REPRODUCIBILITY CRITICAL: DEBUGGING: Model performs poorly in production. Need to reproduce training to debug (was it data? hyperparameters? code bug?). Without reproducibility: Can\'t debug, can\'t fix. AUDITING: Regulatory audit (financial model). Must prove model trained correctly. Need exact lineage (data, code, environment). Without reproducibility: Fail audit, legal issues. RETRAINING: Want to retrain model with more data (use same code/config). Need to reproduce setup. Without reproducibility: New model not comparable to old (different code/config). RESEARCH: Team member leaves. Need to understand their model. Reproducibility enables knowledge transfer. Without reproducibility: Model is black box. COMPLIANCE: GDPR right to explanation. Must explain how model made decision. Need reproducible lineage. Without reproducibility: Can\'t explain, violate GDPR. BEST PRACTICES: Automate tracking (MLflow auto-logs git commit, environment). Enforce clean git state (automated check before training). Use Docker (avoid environment drift). Test reproducibility (reproduce last week\'s model monthly—ensure system works). Document (README with reproduction steps). Result: Any model can be reproduced exactly anytime.',
    keyPoints: [
      'Version: Code (git commit), data (DVC hash), environment (Docker image), config (Hydra yaml), model (MLflow run_id), seeds',
      'Tools: Git (code), DVC (data), Docker (environment), Hydra (config), MLflow (model + lineage)',
      'Lineage: MLflow stores run_id → git commit, data version, Docker image, hyperparameters, metrics, timestamp',
      'Reproduce: Retrieve lineage → checkout code/data/environment → re-run training → verify metrics match',
      'Critical for: Debugging (reproduce to fix), auditing (regulatory compliance), retraining (consistent setup), knowledge transfer',
    ],
  },
];
