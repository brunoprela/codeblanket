export const modelServingDeploymentQuiz = [
  {
    id: 'msd-q-1',
    question:
      "Design a model serving infrastructure for a recommendation API serving 10,000 requests per second with P99 latency <50ms. Address: (1) model serving framework selection, (2) architecture for horizontal scaling, (3) caching strategy, (4) A/B testing capability, (5) monitoring and alerting. How would you handle a model that's 5GB in size?",
    sampleAnswer:
      "High-Throughput Model Serving: (1) Serving Framework: Use TensorFlow Serving or TorchServe for production. Why: Optimized inference (C++ backend), batching (group requests), versioning (serve multiple models), gRPC API (low latency). Alternative: FastAPI + ONNX Runtime for custom control. For 5GB model: Load model once in memory, share across requests. Don't reload per request. Use model warmup: Load at startup, test with dummy request to pre-cache. (2) Horizontal Scaling (Kubernetes): Deploy as K8s Deployment with 50+ replicas. Each pod: 1 model instance, 4 CPU cores, 8GB RAM. Load balancer (Ingress/ALB) distributes traffic. Auto-scaling: HPA (Horizontal Pod Autoscaler) based on CPU (>70%) or request count (>200 req/sec per pod). Scale up before saturation. Rolling updates: Deploy new model version gradually (10% → 50% → 100%). Readiness probes: Pod must pass health check (respond to /health in <100ms) before receiving traffic. (3) Caching: Multi-level caching. L1: Application cache (LRU, 10K entries). Check if request seen recently. If cache hit → return immediately (~1ms). L2: Redis (shared across pods). Popular items (top-100 products). TTL: 5 minutes. If cache hit → return (~3ms). L3: CDN (for geographic distribution). Cache responses by region. Cache key: hash(user_features + product_id). Invalidation: On model update, clear Redis cache (new predictions). Monitor cache hit rate (target: >40%). (4) A/B Testing: Deploy two model versions simultaneously. Load balancer routes traffic: 90% to model_v1, 10% to model_v2 (canary). Tag each request with model_version. Log predictions + model_version to analytics. After 1 day: Compare metrics (CTR, revenue per impression). If model_v2 performs better → promote to 100%. Implementation: K8s Services with weighted routing or feature flag (LaunchDarkly). (5) Monitoring: Metrics: Request rate (10K req/sec), latency (P50, P90, P99 <50ms), error rate (<0.1%), model version distribution (90-10 split during A/B), cache hit rate (>40%). Alerts: If P99 latency >50ms → scale up pods. If error rate >1% → investigate logs. If requests queued → auto-scale. Dashboard: Grafana showing latency histogram, request rate, errors by model version. Logs: Structured logs (JSON) with request_id, model_version, latency, prediction. Handling 5GB Model: Problem: 5GB per pod × 50 pods = 250GB total memory. Solution: (a) Model optimization: Quantization (INT8) → 4x smaller (~1.25GB). (b) Model distillation: Train smaller student model (1GB) that mimics large model. (c) Shared model serving: Use model server that loads model once, serves via RPC (TF Serving does this). (d) Feature reduction: Remove less important features. (e) Deploy on GPU instances: Faster inference, can handle larger models. Architecture: nginx (load balancer) → TF Serving (50 pods, each with quantized 1.25GB model) → Redis (caching) → Logging (Prometheus). Result: 10K req/sec → 200 req/sec per pod. Latency: Model inference 20ms + overhead 10ms = 30ms (P99). Cache hits: 40% reduce to 3ms. Meets <50ms requirement.",
    keyPoints: [
      'TF Serving/TorchServe for optimized inference, load 5GB model once, warmup at startup',
      'Kubernetes: 50+ pods, auto-scaling (HPA), load balancer, rolling updates, readiness probes',
      'Multi-level cache: In-memory (1ms), Redis (3ms), CDN, monitor hit rate >40%',
      'A/B testing: Deploy two versions, weighted routing (90-10), log model_version, compare metrics',
      '5GB model: Quantization (4x smaller), distillation, shared serving, GPU for large models',
    ],
  },
  {
    id: 'msd-q-2',
    question:
      'Compare batch inference vs real-time inference architectures. For a fraud detection system, explain: (1) when to use each approach, (2) infrastructure requirements, (3) latency and throughput trade-offs, (4) cost implications, (5) model update strategies. If you need to score 1 million transactions per day, which approach would you choose?',
    sampleAnswer:
      "Batch vs Real-Time Inference: BATCH INFERENCE: (1) Use when: Decision not time-sensitive. Examples: Email recommendations (send overnight), credit score updates (monthly), churn prediction (weekly). Fraud detection: Post-transaction analysis (after purchase cleared). (2) Infrastructure: Spark job runs nightly. Load transactions from database, apply model to all rows in parallel. Store predictions back to database. Example: 1M transactions × 10ms each = 10K seconds sequentially. With 100 parallel workers → 100 seconds. (3) Latency/Throughput: High throughput (process millions). High latency (hours from transaction to prediction). Throughput: 1M transactions in 1 hour = 278 transactions/sec. (4) Cost: Low. Run once per day. Use spot instances (70% discount). No always-on infrastructure. (5) Model updates: Update model weekly. Re-run batch job with new model. All predictions refreshed. REAL-TIME INFERENCE: (1) Use when: Decision needed immediately. Fraud detection: Approve/decline transaction before completing. Recommendation: Show on page load. (2) Infrastructure: Always-on API (FastAPI). Load balancer, 50 replicas, auto-scaling. (3) Latency/Throughput: Low latency (<100ms per request). Limited throughput per instance (200 req/sec). Scale with more instances. (4) Cost: High. Paying for servers 24/7, even during low traffic. (5) Model updates: Deploy new model with canary (gradual rollout). Zero-downtime updates. FRAUD DETECTION for 1M transactions/day: Calculate: 1M transactions/day ÷ 86,400 seconds/day = 11.6 transactions/sec average. Peak: 5x average = 58 transactions/sec. Choice: REAL-TIME (with caching). Reasoning: (1) Latency requirement: Must decide before transaction completes. Batch (hours delay) unacceptable. User waiting at checkout → need answer in <1 second. (2) Throughput: 58 transactions/sec peak manageable with real-time API. 5 server replicas (each handles 200 req/sec) → 1000 req/sec capacity (plenty of headroom). (3) Cost: Real-time more expensive but necessary. 5 replicas × $0.10/hour = $0.50/hour = $360/month. Reasonable for fraud prevention (saves $10K+/month in fraud). (4) Hybrid optimization: Use real-time for high-risk transactions (>$500, new merchant). Use batch for low-risk (<$50, known merchant). Saves compute. Implementation: (1) Real-time API: FastAPI with model loaded in memory. Endpoint: POST /predict {user_id, amount, merchant} → {fraud_probability, decision}. (2) Feature store: Pre-computed user features (30-day history) in Redis. Real-time lookup <5ms. (3) Decision logic: If fraud_prob >0.8 → Decline. If 0.3-0.8 → Manual review. If <0.3 → Approve. (4) Fallback: If API down → fail open (approve) or fail closed (decline). Depends on risk tolerance. (5) Monitoring: P99 latency <100ms, false positive rate <1% (don't decline good transactions), fraud catch rate >80%.",
    keyPoints: [
      'Batch: High throughput, hours latency, low cost (spot instances), good for non-urgent (email recs)',
      'Real-time: Low latency (<100ms), always-on cost, good for urgent decisions (fraud at checkout)',
      '1M transactions/day = 11.6/sec avg, 58/sec peak → real-time manageable (5 replicas)',
      'Hybrid optimization: Real-time for high-risk (>$500), batch for low-risk (<$50), save compute',
      'Infrastructure: FastAPI + Redis feature store + monitoring (P99 latency, FP rate, catch rate)',
    ],
  },
  {
    id: 'msd-q-3',
    question:
      'Design a blue-green deployment strategy for a production ML model. Explain: (1) infrastructure setup, (2) traffic routing mechanism, (3) testing and validation before cutover, (4) rollback procedure, (5) monitoring during transition. How would you handle a scenario where the new model has better accuracy but worse latency?',
    sampleAnswer:
      "Blue-Green Deployment: (1) Infrastructure Setup: Two identical environments: Blue (current production, model_v1) and Green (new version, model_v2). Each environment: K8s Deployment with 50 pods, Load balancer, Monitoring stack. Independent: No shared state. Both always running during deployment. (2) Traffic Routing: Use load balancer (ALB/nginx) with weighted routing. Initially: 100% traffic → Blue (model_v1), 0% → Green (model_v2). Cutover: Switch load balancer: 100% traffic → Green (model_v2). Instant switch (no gradual rollout in pure blue-green). Alternative: DNS switch (change DNS from blue.company.com to green.company.com). Slower propagation (TTL). (3) Pre-Cutover Validation: Smoke tests on Green: Health check (/health), prediction test (sample requests), load test (simulate 10K req/sec for 10 minutes), latency test (P99 <50ms), correctness test (compare predictions to Blue). Shadow mode: Route copy of traffic to Green (don't return results to users). Log predictions. Compare Green vs Blue predictions and latency for 1 hour. If discrepancy >5% → investigate before cutover. Automated checks: If all tests pass → proceed to cutover. If any fail → abort, fix Green, re-test. (4) Rollback Procedure: Keep Blue running for 24 hours after cutover. If issue detected in Green: (a) Immediate: Switch load balancer back to Blue (1-minute operation). (b) Alert on-call engineer. (c) Investigate Green logs. Automated rollback triggers: If Green error rate >1% for 5 minutes → auto-rollback. If Green P99 latency >100ms (2x threshold) → auto-rollback. If business metric drops (e.g., revenue per request) → manual rollback. After 24 hours of stable Green → decommission Blue (save costs). (5) Monitoring During Transition: Dashboards: Side-by-side comparison of Blue vs Green metrics. Metrics: Request rate, latency (P50, P95, P99), error rate, predictions distribution (mean/std), business metrics (CTR, conversion rate). Logs: Structured logs with environment tag (blue/green). Alerts: Any anomaly in Green → page on-call. Gradual verification: Monitor first 5 minutes closely. If stable after 1 hour → high confidence. SCENARIO: New model (Green) has better accuracy but worse latency. Accuracy: 92% (up from 90% in Blue). Latency: P99 = 80ms (up from 50ms in Blue). Violates <50ms SLA. DECISION TREE: (1) Quantify business impact: Does higher accuracy increase revenue? Example: 2% accuracy improvement → 5% more fraud caught → $100K/month saved. Does higher latency hurt UX? 80ms still acceptable for fraud detection. Not acceptable for real-time recommendations (user waiting). (2) If business value > latency cost: Accept trade-off. Proceed with Green. But: Investigate latency. Possible optimizations: Model quantization (INT8), batch inference (group requests), model pruning (remove layers), GPU inference (faster). Optimize Green to <50ms, then re-deploy. (3) If latency unacceptable: Reject Green. Rollback to Blue. Options: (a) Simplify model (fewer layers) to meet latency. (b) Train smaller model with knowledge distillation. (c) Improve infrastructure (faster GPUs, larger batch sizes). (4) Hybrid approach: Use Blue for latency-sensitive requests (real-time API). Use Green for batch inference (overnight scoring). Best of both worlds. Implementation: Deploy Green but don't switch load balancer. Run nightly batch jobs with Green (benefit from accuracy). Keep Blue for real-time API (meet latency SLA). Monitor: If latency optimization successful → switch real-time to Green later.",
    keyPoints: [
      'Blue-green: Two identical environments, instant traffic switch via load balancer, both running during deployment',
      'Pre-cutover: Smoke tests, shadow mode (compare predictions), load tests, automated validation',
      'Rollback: Keep Blue running 24h, instant switch back, auto-rollback triggers (error rate, latency)',
      'Monitor: Side-by-side metrics (latency, errors, business), structured logs, alerts on anomalies',
      'Accuracy vs latency: Quantify business impact, optimize (quantization, GPU), hybrid (batch with new, real-time with old)',
    ],
  },
];
