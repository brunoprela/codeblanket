export const testingFundamentalsPytestBasicsQuiz = [
  {
    id: 'tfpb-q-1',
    question:
      'You are building a payment processing service that handles monetary transactions. Your manager insists that comprehensive testing is too time-consuming and wants to ship quickly with minimal tests. Explain: (1) the specific risks of inadequate testing for a payment system with concrete examples of what could go wrong, (2) the cost-benefit analysis of writing tests vs. the cost of production bugs (use specific metrics), (3) which types of tests (unit, integration, E2E) you would prioritize given time constraints and why, (4) how you would demonstrate the value of testing to stakeholders, (5) a minimal but effective testing strategy that balances speed and safety.',
    sampleAnswer:
      `Inadequate testing in payment systems creates catastrophic risks: (1) Specific risks: Rounding errors: $0.001 rounding error × 1M transactions = $1,000 lost daily. Real example: Office Space movie plot—fractions of cents add up.` +
      `Data corruption: Incorrect decimal handling (float vs Decimal in Python) → $99.99 stored as $99.98999. Currency conversion errors: EUR→USD conversion using wrong rate → 5% loss on international payments.` +
      `Double charging: Payment processed twice due to race condition → customer anger, refunds, chargebacks ($25 fee per chargeback). Idempotency failures: Retry logic charges customer multiple times. Security vulnerabilities: SQL injection in payment lookup → entire database compromised.` +
      `Regulation violations: PCI-DSS requires transaction logging—missing logs → fines + loss of merchant account. (2) Cost-benefit analysis: Cost of writing tests: 3 days (1 developer) = $3,000 @ $1K/day. Coverage: 200 unit tests (2 days), 20 integration tests (0.5 days), 5 E2E tests (0.5 days).` +
      `Maintenance: 30 min/week = $250/month. Cost of production bugs: Minor bug (rounding error): 2 days investigation + 1 day fix + QA = $3K + customer trust loss. Medium bug (double charging): Refunds $10K + chargeback fees $5K + 1 week engineering = $15K + reputation damage.` +
      `Major bug (data corruption): Data recovery 2 weeks + customer communication + legal = $50K-$500K. Critical bug (security breach): PCI-DSS fine $50K-$500K + lawsuits + business closure. ROI calculation: Test investment: $3K initial + $3K/year maintenance = $6K/year.` +
      `Expected bug cost without tests: 5 minor bugs ($15K) + 2 medium ($30K) + 1 major ($50K) = $95K/year. Expected bug cost with tests: 1 minor bug (caught in QA) = $5K/year. ROI: ($95K - $5K) - $6K = $84K saved per year.` +
      `Payback period: <2 weeks. Break-even: Preventing just ONE medium bug pays for entire test suite. (3) Test prioritization with time constraints: If I have only 3 days: Day 1 (Unit tests - 60%): Payment validation: Amount must be positive, supported currency, valid format.` +
      `All decimal arithmetic: Addition, subtraction, rounding—NO float usage. State transitions: Pending→Completed→Refunded (valid transitions only). Error handling: Invalid input raises correct exceptions with clear messages.` +
      `Why prioritize: Fastest to write and run (<100ms), catch 70% of bugs, prevent arithmetic errors (highest risk in payments). Day 1.5 (Integration tests - 30%): Payment processing flow: Create payment → process → verify status → check database.` +
      `Refund flow: Complete payment → refund → verify money returned. Idempotency: Process same payment twice → only one charge. Database transactions: Payment fails mid-process → no partial data. Why prioritize: Catch component interaction bugs, verify business logic end-to-end, test database consistency.` +
      `Day 0.5 (Critical E2E - 10%): Happy path: User checkout → payment → confirmation → receipt email. Failure handling: Card declined → user sees error → can retry. Why minimal E2E: Slow (5-10s per test), brittle, but essential for user-facing flows. (4) Demonstrating value to stakeholders: Before deployment: Run mutation testing: Inject bugs into code (change + to -, remove validations), verify tests catch them.` +
      `Example: "Tests caught 47 out of 50 injected bugs (94% effectiveness)." Calculate potential loss: "Without test catching rounding error, we would have lost $1,000/day = $365K/year." Show competitor failures: Reference Knight Capital ($440M in 45 min) or AWS S3 outage (typo in command).` +
      `After deployment: Track production bugs: "Tests prevented 23 bugs from reaching production in Q1." Calculate avoided costs: "Estimated savings: $180K based on average bug fix cost." Monitor test coverage: "92% line coverage, 85% branch coverage—high confidence in code quality." Demo fast feedback: "Tests run in 2 minutes—developers know immediately if they broke something." Compare with/without tests: A/B test: One feature with tests (0 prod bugs), one without (5 prod bugs).` +
      `Show velocity increase: "Refactored payment module in 1 day—tests gave confidence to move fast." (5) Minimal but effective strategy (MVP Testing Strategy): Week 1: Core unit tests (2 days): Test every public method in payment processing.` +
      `Focus on arithmetic, validation, state transitions. Aim for 80% line coverage on payment module. Critical integration tests (1 day): End-to-end payment processing. Refund flow. Database transaction consistency.` +
      `Smoke E2E test (2 hours): One happy path test: Create account → add card → make payment → verify success. CI/CD setup (4 hours): Run tests on every commit. Block merges if tests fail. Week 2+: Add tests for every bug found (test-driven bug fixes).` +
      `Increase coverage to 90%+ over time. Add more E2E tests for critical user journeys. Review and metrics: Daily: Monitor test execution time (keep <5 min). Weekly: Review test coverage (aim for 90%+). Monthly: Calculate bugs prevented vs. bugs in production (should be 10:1 ratio).` +
      `Quarterly: Cost-benefit analysis (show ROI to stakeholders). Red flags that demand more testing: Any bug that reaches production → add test to prevent regression. Test suite takes >10 minutes → optimize or parallelize.` +
      `Coverage drops below 80% → freeze features until coverage restored. More than 1 production bug per month → insufficient testing. Philosophy: "Tests are not overhead—they are the foundation of reliable software." Payment systems demand higher testing standards than most applications.` +
      `Financial loss from bugs exceeds test investment by 10-100×. For payments, the question is not "Can we afford to test?" but "Can we afford NOT to test?" Every hour spent testing saves 10 hours debugging production issues.` +
      `Final recommendation: Start with minimal test suite (3 days investment), expand iteratively. Measure and communicate value continuously. Make testing non-negotiable for payment-critical code.`,
    keyPoints: [
      'Payment system risks: rounding errors ($1K/day), double charging ($15K/incident), data corruption ($50K-$500K)',
      'ROI: $6K test investment prevents $95K in bugs annually—break-even after preventing ONE medium bug',
      'Prioritize: 60% unit tests (arithmetic/validation), 30% integration (payment flows), 10% E2E (critical paths)',
      'Demonstrate value: mutation testing (94% bug detection), track prevented bugs (23 in Q1), show competitor failures',
      'MVP strategy: 3 days initial investment (unit + integration + CI/CD), iterate weekly, maintain 90%+ coverage',
    ],
  },
  {
    id: 'tfpb-q-2',
    question:
      'Compare pytest and unittest in Python for a large production application (100K+ lines of code). Address: (1) syntax differences with specific code examples showing the same test in both frameworks, (2) fixture systems and how they impact test maintainability and reusability, (3) performance differences for large test suites (1000+ tests), (4) plugin ecosystems and which plugins would be essential for production use, (5) learning curve and team adoption challenges. Which would you choose for a new project and why?',
    sampleAnswer:
      `pytest vs unittest for production applications: (1) Syntax differences—same test in both frameworks: unittest version (verbose): \`\`\`python import unittest from payment import PaymentProcessor, Payment class TestPaymentProcessor(unittest.TestCase): def setUp(self): \"\"\"Runs before each test\"\"\" self.processor = PaymentProcessor(api_key=\"test_key\") self.payment = Payment(amount=100.0, currency=\"USD\") def test_process_payment_success(self): \"\"\"Test successful payment\"\"\" result = self.processor.process_payment(self.payment) self.assertTrue(result) self.assertEqual(self.payment.status, \"completed\") self.assertEqual(len(self.processor.processed_payments), 1) def test_process_payment_invalid_amount(self): \"\"\"Test payment with invalid amount\"\"\" with self.assertRaises(ValueError) as context: Payment(amount=-100.0, currency=\"USD\") self.assertIn(\"positive\", str(context.exception)) def tearDown(self): \"\"\"Runs after each test\"\"\" self.processor = None self.payment = None if __name__ == \"__main__\": unittest.main() \`\`\` pytest version (concise): \`\`\`python import pytest from payment import PaymentProcessor, Payment @pytest.fixture def processor(): \"\"\"Reusable processor fixture\"\"\" return PaymentProcessor(api_key=\"test_key\") @pytest.fixture def payment(): \"\"\"Reusable payment fixture\"\"\" return Payment(amount=100.0, currency=\"USD\") def test_process_payment_success(processor, payment): \"\"\"Test successful payment\"\"\" result = processor.process_payment(payment) assert result is True assert payment.status == \"completed\" assert len(processor.processed_payments) == 1 def test_process_payment_invalid_amount(): \"\"\"Test payment with invalid amount\"\"\" with pytest.raises(ValueError, match=\"positive\"): Payment(amount=-100.0, currency=\"USD\") \`\`\` Line count: unittest = 34 lines, pytest = 21 lines (38% reduction).` +
      `Readability: pytest uses plain assert vs self.assertEqual (more Pythonic). pytest matches exception messages with regex (cleaner). No boilerplate: No class inheritance, setUp/tearDown, __main__ check. (2) Fixture systems—impact on maintainability: unittest fixtures (setUp/tearDown): Limited scope: setUp runs before each test (method-level only).` +
      `No reusability: Cannot share fixtures between test files easily. Teardown fragility: If test fails, tearDown might not run (use addCleanup instead). No dependency injection: Cannot compose fixtures. Example: \`\`\`python class TestPaymentIntegration(unittest.TestCase): def setUp(self): self.db = create_test_database() self.redis = create_test_redis() self.processor = PaymentProcessor(db=self.db, redis=self.redis) # Problem: Cannot reuse db or redis in other test classes # Problem: Must create all dependencies even if test doesn't need them \`\`\` pytest fixtures (dependency injection): Flexible scope: function, class, module, session-level fixtures.` +
      `Reusability: Define once in conftest.py, use everywhere. Composition: Fixtures can depend on other fixtures. Automatic cleanup: yield fixture ensures cleanup even on failure. Example: \`\`\`python # conftest.py(shared across all tests) @pytest.fixture(scope =\"session\") def db(): \"\"\"Session-level database (created once)\"\"\" db = create_test_database() yield db db.close() @pytest.fixture(scope=\"function\") def clean_db(db): \"\"\"Clean database for each test\"\"\" db.truncate_all_tables() return db @pytest.fixture def redis(): \"\"\"Redis for testing\"\"\" redis = create_test_redis() yield redis redis.flushall() redis.close() @pytest.fixture def processor(clean_db, redis): \"\"\"Processor with dependencies\"\"\" return PaymentProcessor(db=clean_db, redis=redis) # Test files can use any combination def test_payment_processing(processor): # Gets processor with clean_db and redis ... def test_database_only(clean_db): # Only needs database ... \`\`\` Maintainability impact: unittest: 100K LOC → duplicate setUp in 50+ test files → change db connection → update 50 files. pytest: 100K LOC → db fixture in conftest.py → change once → all tests updated. pytest saves 10-20 hours per year on fixture maintenance. (3) Performance for large test suites: unittest performance: Serial execution by default (no parallelization). 1000 tests × 0.5s each = 8.3 minutes.` +
      `Large projects: 30-60 minute test runs common. Workaround: Use pytest-xdist with unittest tests (yes, pytest can run unittest tests). pytest performance: Parallel execution: pytest -n auto (uses all CPU cores). 1000 tests × 0.5s with 8 cores = 1.0 minute (8× speedup).` +
      `Test collection: Fast test discovery (uses AST parsing). Selective execution: pytest -k \"payment\" runs only matching tests. pytest --lf runs last failed tests (fast feedback). Real-world example (from production): 5,000 tests in Django application: unittest (serial): 45 minutes. pytest (parallel, 8 cores): 6 minutes (7.5× speedup). pytest --lf after change: 30 seconds (only 50 failed tests rerun).` +
      `Developer productivity: 45 min → 6 min = 39 minutes saved per test run. 10 runs/day = 6.5 hours saved per developer per day. 10 developers = 65 hours saved per day = $16K/day @ $250/hour. (4) Plugin ecosystem—essential for production: unittest plugins: Limited ecosystem (unittest is basic).` +
      `Must build most tooling yourself. pytest plugins (800+): Essential plugins: pytest-cov: Coverage reporting. pytest --cov=src --cov-report=html. pytest-mock: Enhanced mocking (better than unittest.mock for fixtures). pytest-asyncio: Async test support. pytest-xdist: Parallel execution. pytest -n auto. pytest-timeout: Timeout hanging tests. pytest-randomly: Randomize test order (catch test dependencies). pytest-benchmark: Performance benchmarking. pytest-env: Set environment variables for tests. pytest-sugar: Beautiful progress bar and output.` +
      `Specialized plugins: pytest-django: Django integration. pytest-flask: Flask integration. pytest-postgresql: PostgreSQL test fixtures. pytest-redis: Redis test fixtures. pytest-freezegun: Mock datetime.now(). pytest-httpx: Mock HTTP requests.` +
      `Production setup example: \`\`\`bash pip install pytest pytest-cov pytest-mock pytest-asyncio pytest-xdist pytest-timeout pytest-randomly pytest-django # pytest.ini [pytest] addopts = -v --cov=src --cov-report=html --cov-report=term-missing -n auto --timeout=30 --randomly-seed=12345 \`\`\` (5) Learning curve and team adoption: unittest learning curve: Familiar to Java/JUnit developers (similar structure).` +
      `Requires learning class-based test structure, setUp/tearDown, assertion methods. Time to productivity: 1-2 days (if familiar with xUnit frameworks). pytest learning curve: Easier for Python beginners (just functions and assert).` +
      `Requires learning fixtures (1-2 days to master). Advanced features (parametrize, marks, hooks) take longer (1 week). Time to productivity: 1 day for basic tests, 1 week for advanced features. Team adoption challenges: Switching from unittest to pytest: Can run unittest tests with pytest (gradual migration).` +
      `Refactor tests incrementally to use pytest fixtures. Challenge: Unlearning setUp/tearDown patterns. Training: 1-day workshop on fixtures and best practices. Onboarding new developers: unittest: "Read unittest docs, then read our custom test utilities." pytest: "Read pytest docs—most patterns are standard." pytest has better documentation and community resources. (6) Decision: Which to choose for new project? Choose pytest for new projects because: Productivity: 38% less code, plain assert, powerful fixtures.` +
      `Performance: 7-8× faster with parallel execution. Ecosystem: 800+ plugins vs. minimal unittest ecosystem. Maintainability: Shared fixtures in conftest.py vs. duplicate setUp. Developer experience: Better error messages, beautiful output, faster feedback.` +
      `Industry standard: Most modern Python projects use pytest (Django, FastAPI, Flask all use pytest in their own test suites). Only choose unittest if: Required to use only standard library (no external dependencies).` +
      `Team has deep unittest expertise and refuses to learn pytest. Working on Python 2 codebase (but Python 2 is EOL—migrate to Python 3). Migration strategy: New projects: Start with pytest from day one. Existing unittest projects: Run tests with pytest (works out of the box).` +
      `Refactor high-value tests to pytest fixtures incrementally. Set up pre-commit hook to encourage pytest style for new tests. Complete migration in 3-6 months. ROI of switching: 100K LOC, 5K tests, 10 developers: Time saved: 6.5 hours/day × 10 devs = 65 hours/day.` +
      `Cost saved: 65 hours × $250/hour = $16,250/day. Migration cost: 1 week training + 2 weeks refactoring = $75K one-time. ROI: Pays for itself in 5 days, $4M/year savings thereafter. Final recommendation: pytest for all new Python projects.` +
      `Easier to write, faster to run, better to maintain.`,
    keyPoints: [
      'Syntax: pytest 38% less code (21 vs 34 lines), plain assert vs self.assertEqual, no class boilerplate',
      'Fixtures: pytest fixtures composable/reusable (define once in conftest.py), unittest setUp duplicated across 50+ files',
      'Performance: pytest 7.5× faster (6 min vs 45 min for 5K tests) with parallel execution, saves 6.5 hours/dev/day',
      'Plugins: pytest has 800+ plugins (cov, mock, asyncio, xdist), unittest has minimal ecosystem',
      'Choose pytest: Industry standard, better DX, $4M/year savings for large teams, migration pays for itself in 5 days',
    ],
  },
  {
    id: 'tfpb-q-3',
    question:
      'Design a test organization strategy for a microservices architecture with 15 services (each 10K-50K lines of Python code). Address: (1) directory structure for tests (where do integration tests go when testing multiple services?), (2) shared test utilities and fixtures (how to avoid duplication across services?), (3) CI/CD pipeline organization (which tests run when, parallelization strategy), (4) test data management (fixtures, factories, test databases per service?), (5) versioning and backward compatibility testing. Include specific examples of file structures and CI/CD configurations.',
    sampleAnswer:
      `Microservices test organization strategy (15 services): (1) Directory structure for tests: Per-service structure: \`\`\`service1/ ├── src/ │ ├── service1/ │ │ ├── api/ │ │ ├── models/ │ │ └── business_logic/ ├── tests/ │ ├── unit/ # Tests for service1 code only │ │ ├── test_api.py │ │ ├── test_models.py │ │ └── test_business_logic.py │ ├── integration/ # Tests service1 with its dependencies (DB, Redis) │ │ ├── test_database_integration.py │ │ └── test_redis_cache.py │ ├── contract/ # Contract tests (service1 as provider/consumer) │ │ ├── test_api_contract.py │ │ └── test_events_contract.py │ └── conftest.py # Service1-specific fixtures ├── pytest.ini └── requirements-test.txt \`\`\` Monorepo structure for cross-service tests: \`\`\`monorepo/ ├── services/ │ ├── service1/ │ ├── service2/ │ └── service3/ ├── tests/ │ ├── e2e/ # End-to-end tests across services │ │ ├── test_user_registration_flow.py # service1 → service2 → service3 │ │ ├── test_payment_flow.py │ │ └── conftest.py # E2E fixtures (start all services) │ └── cross_service_integration/ # Integration between 2-3 services │ ├── test_service1_to_service2.py │ └── test_event_propagation.py └── shared_test_utils/ # Shared across all services ├── fixtures/ ├── factories/ └── mocks/ \`\`\` Key decisions: Unit/integration tests: Live with service code (tests/unit, tests/integration).` +
      `E2E tests: Separate directory at repo root (test entire system). Cross-service integration: Separate directory (test 2-3 services together). Shared utilities: shared_test_utils/ imported by all services. (2) Shared test utilities—avoiding duplication: Problem: 15 services × duplicate fixtures = maintenance nightmare.` +
      `Solution: shared_test_utils package: \`\`\`shared_test_utils/ ├── __init__.py ├── fixtures/ │ ├── __init__.py │ ├── database.py # Database fixtures │ ├── redis.py # Redis fixtures │ └── kafka.py # Kafka fixtures ├── factories/ │ ├── __init__.py │ ├── user_factory.py # FactoryBoy factories │ └── payment_factory.py ├── mocks/ │ ├── __init__.py │ ├── mock_http.py # HTTP mocking utilities │ └── mock_events.py # Event mocking └── helpers/ ├── __init__.py └── assertions.py # Custom assertions \`\`\` Example—database.py: \`\`\`python import pytest from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker @pytest.fixture(scope=\"session\") def db_engine(): \"\"\"Shared database engine for all services\"\"\" engine = create_engine(\"postgresql://test:test@localhost/testdb\") yield engine engine.dispose() @pytest.fixture(scope=\"function\") def db_session(db_engine): \"\"\"Clean database session for each test\"\"\" connection = db_engine.connect() transaction = connection.begin() Session = sessionmaker(bind=connection) session = Session() yield session session.close() transaction.rollback() connection.close() \`\`\` Usage in service1/tests/conftest.py: \`\`\`python from shared_test_utils.fixtures.database import db_session, db_engine from shared_test_utils.factories.user_factory import UserFactory @pytest.fixture def user(db_session): \"\"\"Create test user\"\"\" return UserFactory.create(session=db_session) \`\`\` Benefits: Change db_session fixture once → all 15 services updated.` +
      `Consistent test data across services. Reduced maintenance: 1 place to update vs. 15. (3) CI/CD pipeline organization: Goal: Fast feedback (unit tests), comprehensive coverage (integration/E2E). Pipeline stages: Stage 1: Lint & Type Check (1 min): Run on every commit.` +
      `Parallel per service: ruff, black, mypy. Fast feedback on code quality. Stage 2: Unit Tests (2-3 min): Run per service in parallel (15 jobs). Only run tests for changed services (dependency detection).` +
      `Command: pytest tests/unit -n auto --cov=src --cov-fail-under=80. Stage 3: Integration Tests (5 min): Run per service in parallel. Requires Docker (spin up PostgreSQL, Redis). Command: docker-compose up -d && pytest tests/integration -n auto.` +
      `Stage 4: Contract Tests (3 min): Verify API contracts between services. Use Pact or Spring Cloud Contract. Ensures backward compatibility. Stage 5: Cross-Service Integration (10 min): Run tests in tests/cross_service_integration/.` +
      `Spin up 2-3 services with docker-compose. Test communication between services. Stage 6: E2E Tests (20 min): Run only on main branch or PR to main. Spin up all 15 services with docker-compose. Run critical user journeys (5-10 tests).` +
      `Runs nightly (too slow for every commit). GitHub Actions example (.github/workflows/test.yml): \`\`\`yaml name: Test Suite on: [push, pull_request] jobs: lint: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: \"3.11\" - run: pip install ruff black mypy - run: ruff check . - run: black --check . - run: mypy src unit-tests: runs-on: ubuntu-latest strategy: matrix: service: [service1, service2, service3, service4, service5] steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 - run: cd services/\${{ matrix.service }} && pip install -r requirements-test.txt - run: cd services/\${{ matrix.service }} && pytest tests/unit -n auto --cov=src --cov-fail-under=80 integration-tests: runs-on: ubuntu-latest needs: unit-tests strategy: matrix: service: [service1, service2, service3] steps: - uses: actions/checkout@v3 - run: cd services/\${{ matrix.service }} && docker-compose up -d - run: cd services/\${{ matrix.service }} && pytest tests/integration -n auto - run: docker-compose down e2e-tests: runs-on: ubuntu-latest needs: [unit-tests, integration-tests] if: github.ref == \"refs/heads/main\" steps: - uses: actions/checkout@v3 - run: docker-compose -f docker-compose.e2e.yml up -d - run: pytest tests/e2e -n auto --maxfail=1 - run: docker-compose -f docker-compose.e2e.yml down \`\`\` Parallelization strategy: Lint: 15 services × 1 min = 1 min (parallel).` +
      `Unit: 15 services × 2 min = 2 min (parallel). Integration: 15 services × 5 min = 5 min (parallel). E2E: 20 min (cannot parallelize easily—full system). Total: 28 minutes for full pipeline (vs. 180 min serial). (4) Test data management: Option 1: FactoryBoy factories (recommended): \`\`\`python # shared_test_utils/factories/user_factory.py import factory from service1.models import User class UserFactory(factory.alchemy.SQLAlchemyModelFactory): class Meta: model = User sqlalchemy_session_persistence = \"commit\" username = factory.Sequence(lambda n: f\"user{n}\") email = factory.LazyAttribute(lambda obj: f\"{obj.username}@example.com\") age = factory.Faker(\"random_int\", min=18, max=80) # Usage in tests def test_user_creation(db_session): user = UserFactory.create(username=\"alice\", age=30) assert user.username == \"alice\" assert user.age == 30 \`\`\` Benefits: Realistic test data with randomization (Faker).` +
      `Override specific fields, use defaults for others. Relationships handled automatically (UserFactory → OrderFactory). Option 2: Fixture files (for complex data): \`\`\`# tests/fixtures/users.json [ {\"username\": \"alice\", \"email\": \"alice@example.com\", \"age\": 30}, {\"username\": \"bob\", \"email\": \"bob@example.com\", \"age\": 25} ] # conftest.py import json @pytest.fixture def users(): with open(\"tests/fixtures/users.json\") as f: return json.load(f) \`\`\` Test databases per service: Each service has own test database: service1_test, service2_test, etc.` +
      `Avoids cross-contamination between service tests. Use Docker Compose to spin up test databases: \`\`\`yaml # docker-compose.test.yml services: postgres-service1: image: postgres:15 environment: POSTGRES_DB: service1_test POSTGRES_USER: test POSTGRES_PASSWORD: test ports: - \"5432:5432\" postgres-service2: image: postgres:15 environment: POSTGRES_DB: service2_test ports: - \"5433:5432\" \`\`\` (5) Versioning and backward compatibility testing: Problem: Service1 v2 deployed, Service2 still on v1 → must be compatible.` +
      `Contract testing with Pact: Service1 (provider) publishes contract (API schema). Service2 (consumer) tests against contract. CI fails if Service1 breaks Service2's expectations.Example: \`\`\`python # Service2 (consumer) contract test @pytest.mark.contract def test_service1_get_user_contract(): contract = PactContract(provider=\"service1\", consumer=\"service2\") contract.given(\"user exists\") .upon_receiving(\"GET /users/123\") .with_request(method=\"GET\", path=\"/users/123\") .will_respond_with(status=200, body={\"id\": 123, \"name\": \"Alice\"}) # Service1 (provider) verifies it satisfies contract def test_service1_satisfies_contract(): verify_pact(\"service2\", \"service1\", provider_url=\"http://localhost:8000\") \`\`\` Versioned API testing: Test multiple API versions simultaneously: \`\`\`python @pytest.mark.parametrize(\"api_version\", [\"v1\", \"v2\"]) def test_get_user_all_versions(api_version): response = requests.get(f\"http://service1/{api_version}/users/123\") assert response.status_code == 200 \`\`\` Database migration testing: Test migrations up and down: \`\`\`python def test_migration_0001_create_users_table(): # Apply migration alembic upgrade head # Verify table exists assert \"users\" in db.get_table_names() # Rollback alembic downgrade -1 assert \"users\" not in db.get_table_names() \`\`\` Summary: Structure: Unit / integration per service, E2E at repo root, shared_test_utils for common code.CI / CD: 6 stages(lint→unit→integration→contract→cross - service→E2E), 28 min total with parallelization.Test data: FactoryBoy for realistic data, JSON fixtures for complex scenarios, per - service test databases.Versioning: Contract tests(Pact) for backward compatibility, test multiple API versions, migration testing.This strategy scales to 100 + services while maintaining fast feedback and comprehensive coverage.`,
    keyPoints: [
      'Structure: per-service tests/ (unit/integration/contract), repo-level tests/e2e/, shared_test_utils/ package',
      'Shared utilities: Database/Redis/Kafka fixtures in shared_test_utils, FactoryBoy factories, import once use everywhere',
      'CI/CD: 6-stage pipeline (lint→unit→integration→contract→cross-service→E2E), 28 min with parallelization vs 180 min serial',
      'Test data: FactoryBoy for realistic data + Faker, per-service test databases (service1_test, service2_test)',
      'Versioning: Pact contract testing, parametrized multi-version tests, migration up/down testing for backward compatibility',
    ],
  },
];
