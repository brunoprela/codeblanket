export const parametrizedTestsQuiz = [
  {
    id: 'pt-q-1',
    question:
      'You have a function that validates credit card numbers using the Luhn algorithm. Design a comprehensive parametrized testing strategy that addresses: (1) generating test data (valid vs invalid card numbers, edge cases), (2) testing multiple card types (Visa, Mastercard, Amex, Discover), (3) performance optimization (testing 10,000 card numbers), (4) custom test IDs for clarity, (5) handling expected failures and skipped tests. Include specific examples of parametrize decorators and explain the tradeoffs between test coverage and execution time.',
    sampleAnswer: `Comprehensive parametrized testing strategy for credit card validation: (1) Generating test data: Valid card numbers: \`\`\`python# Valid credit card test data (real format, fake numbers for testing)valid_cards = [    pytest.param(\"4532015112830366\", \"Visa\", id=\"visa-16\"),    pytest.param(\"4916338506082832\", \"Visa\", id=\"visa-16-alt\"),    pytest.param(\"4532261615476013542\", \"Visa\", id=\"visa-19\"),  # 19 digits    pytest.param(\"5425233430109903\", \"Mastercard\", id=\"mastercard-16\"),    pytest.param(\"2221000010000015\", \"Mastercard\", id=\"mastercard-2series\"),    pytest.param(\"371449635398431\", \"Amex\", id=\"amex-15\"),    pytest.param(\"378282246310005\", \"Amex\", id=\"amex-test\"),    pytest.param(\"6011111111111117\", \"Discover\", id=\"discover-16\"),    pytest.param(\"6011000990139424\", \"Discover\", id=\"discover-alt\"),]@pytest.mark.parametrize(\"card_number, card_type\", valid_cards)def test_valid_credit_cards (card_number, card_type):    \"\"\"Test valid credit card numbers pass Luhn check\"\"\"    result = validate_credit_card (card_number)    assert result.is_valid is True    assert result.card_type == card_type\`\`\` Invalid card numbers (common errors): \`\`\`python invalid_cards = [    pytest.param(\"4532015112830367\", id=\"invalid-checksum\"),  # Last digit wrong    pytest.param(\"1234567890123456\", id=\"invalid-all-sequential\"),    pytest.param(\"0000000000000000\", id=\"all-zeros\"),    pytest.param(\"9999999999999999\", id=\"all-nines\"),    pytest.param(\"453201511283036\", id=\"too-short-15\"),  # Visa should be 16    pytest.param(\"45320151128303666\", id=\"too-long-17\"),    pytest.param(\"4532-0151-1283-0366\", id=\"contains-dashes\"),    pytest.param(\"4532 0151 1283 0366\", id=\"contains-spaces\"),    pytest.param(\"453201511283O366\", id=\"contains-letter-O\"),    pytest.param(\"\", id=\"empty-string\"),    pytest.param(\"abc\", id=\"non-numeric\"),]@pytest.mark.parametrize(\"card_number\", invalid_cards)def test_invalid_credit_cards (card_number):    \"\"\"Test invalid credit card numbers fail validation\"\"\"    result = validate_credit_card (card_number)    assert result.is_valid is False\`\`\` Edge cases: \`\`\`python edge_cases = [    pytest.param(\"00000000000000000000\", False, id=\"20-zeros\"),  # Too long    pytest.param(\"4532015112830366\" * 2, False, id=\"doubled-valid\"),  # 32 digits    pytest.param(\"4\", False, id=\"single-digit\"),    pytest.param(\"49\", False, id=\"two-digits\"),    pytest.param(None, marks=pytest.mark.xfail (raises=TypeError), id=\"none-input\"),]@pytest.mark.parametrize(\"card_number, expected_valid\", edge_cases)def test_edge_cases (card_number, expected_valid):    result = validate_credit_card (card_number)    assert result.is_valid == expected_valid\`\`\` (2) Testing multiple card types: Card type identification: \`\`\`python card_type_data = [    # Visa: starts with 4, length 13/16/19    (\"4532015112830366\", \"Visa\"),    (\"4532261615476013542\", \"Visa\"),    # Mastercard: starts with 51-55 or 2221-2720    (\"5425233430109903\", \"Mastercard\"),    (\"2221000010000015\", \"Mastercard\"),    # Amex: starts with 34 or 37, length 15    (\"371449635398431\", \"Amex\"),    (\"341111111111111\", \"Amex\"),    # Discover: starts with 6011 or 65    (\"6011111111111117\", \"Discover\"),    (\"6500000000000002\", \"Discover\"),]@pytest.mark.parametrize(\"card_number, expected_type\", card_type_data, ids=lambda x: f\"{x[1]}-{x[0][:4]}\")def test_card_type_detection (card_number, expected_type):    \"\"\"Test card type is correctly identified\"\"\"    result = validate_credit_card (card_number)    assert result.card_type == expected_type\`\`\` Test card-type-specific rules: \`\`\`python @pytest.mark.parametrize(\"card_type, valid_lengths, starts_with\", [    (\"Visa\", [13, 16, 19], [\"4\"]),    (\"Mastercard\", [16], [\"51\", \"52\", \"53\", \"54\", \"55\", \"222\"]),  # 2221-2720    (\"Amex\", [15], [\"34\", \"37\"]),    (\"Discover\", [16], [\"6011\", \"65\"]),])def test_card_type_rules (card_type, valid_lengths, starts_with):    \"\"\"Test each card type follows its validation rules\"\"\"    # Test would generate card numbers matching these rules    # and verify validation works correctly    ...\`\`\` (3) Performance optimization (10,000 card numbers): Problem: Testing 10,000 cards × 10ms validation = 100 seconds. Strategy 1—Sample representative cases: \`\`\`python # Instead of all 10,000, test representative samplesample_sizes = [10, 100, 1000, 10000]  # Test with increasing sizes@pytest.mark.parametrize(\"sample_size\", sample_sizes)def test_batch_validation_performance (sample_size):    \"\"\"Test performance with increasing batch sizes\"\"\"    import random    # Generate sample_size random valid cards    cards = [generate_valid_card() for _ in range (sample_size)]        start = time.time()    results = [validate_credit_card (card) for card in cards]    duration = time.time() - start        assert all (r.is_valid for r in results)    assert duration < sample_size * 0.02  # <20ms per card    print(f\"{sample_size} cards validated in {duration:.2f}s ({duration/sample_size*1000:.1f}ms per card)\")\`\`\` Strategy 2—Mark slow tests for selective execution: \`\`\`python# Generate 10,000 test cases but mark as slow@pytest.fixture (params=[generate_valid_card() for _ in range(10000)], ids=lambda x: f\"card-{x[:4]}\")def large_card_dataset (request):    return request.param@pytest.mark.slow  # Mark as slow@pytest.mark.parametrize(\"card_number\", [generate_valid_card() for _ in range(10000)])def test_comprehensive_validation (card_number):    \"\"\"Comprehensive test with 10K cards (run nightly)\"\"\"    result = validate_credit_card (card_number)    assert result.is_valid is True # Run fast tests (development): pytest -m \"not slow\"# Run all tests (CI nightly): pytest\`\`\` Strategy 3—Parallelize with pytest-xdist: \`\`\`bash# Run tests in parallel (8 cores)pytest -n 8 # 10,000 tests / 8 cores = 1,250 tests per core# If each test takes 10ms: 1,250 × 10ms = 12.5s per core# Total: 12.5s (vs 100s serial) = 8× speedup\`\`\` Strategy 4—Property-based testing (better approach): \`\`\`python from hypothesis import given, strategiesimport hypothesis.strategies as st@given (st.from_regex (r\"[0-9]{13,19}\", fullmatch=True))def test_validation_with_hypothesis (card_number):    \"\"\"Hypothesis generates thousands of test cases automatically\"\"\"    result = validate_credit_card (card_number)    # Result should never raise exception    assert result.is_valid in [True, False]    # If valid, Luhn checksum must pass    if result.is_valid:        assert luhn_check (card_number) is True# Hypothesis runs this 100-10,000 times with different inputs# More efficient than manual parametrization for large datasets\`\`\` (4) Custom test IDs for clarity: Without custom IDs (bad): \`\`\`python@pytest.mark.parametrize(\"card_number, card_type\", [    (\"4532015112830366\", \"Visa\"),    (\"5425233430109903\", \"Mastercard\"),])def test_valid_cards (card_number, card_type):    ...# Output: # test_valid_cards[4532015112830366-Visa]  # Hard to distinguish# test_valid_cards[5425233430109903-Mastercard]\`\`\` With custom IDs (good): \`\`\`python@pytest.mark.parametrize(\"card_number, card_type\", [    (\"4532015112830366\", \"Visa\"),    (\"5425233430109903\", \"Mastercard\"),], ids=[\"visa-valid-16-digit\", \"mastercard-valid-16-digit\"])def test_valid_cards (card_number, card_type):    ...# Output:# test_valid_cards[visa-valid-16-digit]  # Clear and readable# test_valid_cards[mastercard-valid-16-digit]\`\`\` Dynamic ID generation: \`\`\`python def card_id (params):    \"\"\"Generate readable ID from card data\"\"\"    card_number, card_type = params    return f\"{card_type.lower()}-{card_number[:4]}...{card_number[-4:]}\"@pytest.mark.parametrize(\"card_number, card_type\", [    (\"4532015112830366\", \"Visa\"),    (\"5425233430109903\", \"Mastercard\"),], ids=card_id)# Output:# test_valid_cards[visa-4532...0366]# test_valid_cards[mastercard-5425...9903]\`\`\` (5) Handling expected failures and skipped tests: Expected failures (known bugs): \`\`\`python@pytest.mark.parametrize(\"card_number, card_type\", [    (\"4532015112830366\", \"Visa\"),    pytest.param(\"6011111111111117\", \"Discover\",         marks=pytest.mark.xfail (reason=\"Bug #123: Discover cards not supported yet\"),        id=\"discover-unsupported\"),    pytest.param(\"3566002020360505\", \"JCB\",        marks=pytest.mark.xfail (reason=\"JCB cards not implemented\"),        id=\"jcb-not-implemented\"),])def test_all_card_types (card_number, card_type):    result = validate_credit_card (card_number)    assert result.card_type == card_type# Output:# test_all_card_types[visa-...] PASSED# test_all_card_types[discover-unsupported] XFAIL (expected to fail)# test_all_card_types[jcb-not-implemented] XFAIL\`\`\` Skipped tests (environment-dependent): \`\`\`python import sys@pytest.mark.parametrize(\"card_number\", [    \"4532015112830366\",    pytest.param(\"5425233430109903\",        marks=pytest.mark.skipif (sys.version_info < (3, 10), reason=\"Requires Python 3.10+\"),        id=\"py310-only\"),    pytest.param(\"371449635398431\",        marks=pytest.mark.skipif (not has_amex_validation_lib(), reason=\"Amex lib not installed\"),        id=\"amex-requires-lib\"),])def test_validation (card_number):    assert validate_credit_card (card_number).is_valid# Output:# test_validation[...] PASSED# test_validation[py310-only] SKIPPED (Requires Python 3.10+)# test_validation[amex-requires-lib] SKIPPED (Amex lib not installed)\`\`\` Tradeoffs: Test coverage vs execution time: Full coverage (10,000 cards): Coverage: 100% (all edge cases tested). Time: 100 seconds serial, 12.5 seconds parallel. When: Nightly CI runs, before releases. Sampled coverage (100 representative cards): Coverage: 95% (covers most edge cases). Time: 1 second serial. When: Every commit, local development. Minimal coverage (10 critical cards): Coverage: 80% (major card types and errors). Time: 0.1 seconds. When: Pre-commit hook (fast feedback). Recommendation: Development: 10 critical cases (0.1s, fast feedback). PR/CI: 100 representative cases (1s, good coverage). Nightly: 10,000 comprehensive cases (12.5s parallel, full coverage). Production: Property-based testing with Hypothesis (generates thousands of cases automatically). Summary: Test data: Valid cards (9 cases), invalid cards (11 cases), edge cases (5 cases) = 25 core tests. Card types: Parametrize by type (Visa, Mastercard, Amex, Discover) with type-specific rules. Performance: Sample strategy (10/100/10K), parallelize (8× speedup), mark slow tests, use property-based testing. Custom IDs: Readable IDs (\"visa-valid-16\"), dynamic generation (card_id function), clear test output. Failures: pytest.param with marks=pytest.mark.xfail (known bugs), skipif (environment deps). Final result: 25 fast tests (1s) for development, 10,000 comprehensive tests (12.5s parallel) for CI, clear test output with custom IDs, expected failures documented.`,
    keyPoints: [
      'Test data: 9 valid (Visa/MC/Amex/Discover), 11 invalid (checksum/format/length), 5 edge cases (empty/special chars)',
      'Card types: Parametrize by type, test type-specific rules (Visa 13/16/19 digits, Amex 15 digits, etc.)',
      'Performance: Sample strategy (10 dev, 100 CI, 10K nightly), parallelize 8× speedup, mark slow tests, property-based testing',
      'Custom IDs: Readable (\"visa-valid-16\"), dynamic generation (card_id function), clear output for debugging',
      'Failures: pytest.param marks=xfail (known bugs), skipif (environment deps), 25 fast core tests + 10K comprehensive',
    ],
  },
  {
    id: 'pt-q-2',
    question:
      'Compare different parametrization strategies for testing an API with 50 endpoints, each supporting 4 HTTP methods (GET, POST, PUT, DELETE) with various authentication levels (anonymous, user, admin). Address: (1) single parametrize vs multiple parametrize decorators, (2) parametrized fixtures vs parametrized tests, (3) indirect parametrization for complex setup, (4) test organization (one file vs multiple files), (5) measuring and optimizing test execution time. Provide specific examples showing the tradeoffs.',
    sampleAnswer: `API testing parametrization strategies comparison: Setup: 50 endpoints × 4 methods × 3 auth levels = 600 possible test combinations. (1) Single vs multiple parametrize decorators: Strategy A—Single parametrize (explicit combinations): \`\`\`python# Explicit test cases (tedious but clear)test_cases = [    (\"/users\", \"GET\", \"anonymous\", 401),  # Anonymous forbidden    (\"/users\", \"GET\", \"user\", 200),         # User can list    (\"/users\", \"GET\", \"admin\", 200),        # Admin can list    (\"/users\", \"POST\", \"anonymous\", 401),   # Anonymous forbidden    (\"/users\", \"POST\", \"user\", 403),        # User forbidden    (\"/users\", \"POST\", \"admin\", 201),       # Admin can create    # ... 594 more combinations (tedious!)  ]@pytest.mark.parametrize(\"endpoint, method, auth_level, expected_status\", test_cases)def test_api_access (api_client, endpoint, method, auth_level, expected_status):    client = get_authenticated_client (api_client, auth_level)    response = client.request (method, endpoint)    assert response.status_code == expected_status# Pros: Explicit control over each combination, can skip specific cases# Cons: 600 lines of test data (tedious, error-prone), hard to maintain\`\`\` Strategy B—Multiple parametrize (matrix expansion): \`\`\`python@pytest.mark.parametrize(\"endpoint\", [\"/users\", \"/posts\", \"/comments\"])@pytest.mark.parametrize(\"method\", [\"GET\", \"POST\", \"PUT\", \"DELETE\"])@pytest.mark.parametrize(\"auth_level\", [\"anonymous\", \"user\", \"admin\"])def test_api_access_matrix (api_client, endpoint, method, auth_level):    client = get_authenticated_client (api_client, auth_level)    response = client.request (method, endpoint)    # Determine expected status based on endpoint, method, auth    expected = get_expected_status (endpoint, method, auth_level)    assert response.status_code == expected# Pros: Concise (8 lines generates 3×4×3=36 tests), easy to add endpoints# Cons: All combinations tested (might test invalid combinations like DELETE /users as anonymous)\`\`\` Strategy C—Hybrid (group by logic): \`\`\`python# Public endpoints (accessible to all)public_endpoints = [\"/health\", \"/docs\", \"/login\"]@pytest.mark.parametrize(\"endpoint\", public_endpoints)@pytest.mark.parametrize(\"method\", [\"GET\"])@pytest.mark.parametrize(\"auth_level\", [\"anonymous\", \"user\", \"admin\"])def test_public_endpoints (api_client, endpoint, method, auth_level):    response = api_client.get (endpoint)    assert response.status_code == 200# User-only endpoints@pytest.mark.parametrize(\"endpoint\", [\"/profile\", \"/settings\"])@pytest.mark.parametrize(\"method\", [\"GET\", \"PUT\"])def test_user_endpoints_authenticated (user_api_client, endpoint, method):    response = user_api_client.request (method, endpoint)    assert response.status_code in [200, 201]def test_user_endpoints_anonymous (api_client, endpoint, method):    response = api_client.request (method, endpoint)    assert response.status_code == 401# Admin-only endpoints@pytest.mark.parametrize(\"endpoint\", [\"/admin/users\", \"/admin/reports\"])@pytest.mark.parametrize(\"method\", [\"GET\", \"POST\", \"DELETE\"])def test_admin_endpoints_admin (admin_api_client, endpoint, method):    response = admin_api_client.request (method, endpoint)    assert response.status_code in [200, 201, 204]def test_admin_endpoints_non_admin (user_api_client, endpoint, method):    response = user_api_client.request (method, endpoint)    assert response.status_code == 403# Pros: Logical grouping (public, user, admin), only valid combinations tested# Cons: More test functions (but each is clear and focused)\`\`\` Recommendation: Use Strategy C (hybrid) for clarity and maintainability. (2) Parametrized fixtures vs parametrized tests: Parametrized tests (test-level): \`\`\`python@pytest.mark.parametrize(\"endpoint\", [\"/users\", \"/posts\"])def test_endpoints (api_client, endpoint):    response = api_client.get (endpoint)    assert response.status_code == 200# Pros: Clear what's being tested, each test independent# Cons: Test function repeated for each parameter\`\`\` Parametrized fixtures (fixture-level): \`\`\`python@pytest.fixture (params = [\"/users\", \"/posts\"])def endpoint (request):    return request.paramdef test_get_endpoint (api_client, endpoint):    response = api_client.get (endpoint)    assert response.status_code == 200def test_post_endpoint (api_client, endpoint):    response = api_client.post (endpoint, json={})    assert response.status_code in [200, 201]# Pros: Both tests run with both endpoints (2 tests × 2 endpoints = 4 total)# Cons: Less explicit, can be confusing what's being tested\`\`\` When to use each: Parametrized tests: When testing different inputs to same function. Parametrized fixtures: When multiple tests need same variations of setup. Example: Multiple auth levels: \`\`\`python@pytest.fixture (params=[\"anonymous\", \"user\", \"admin\"])def api_client_with_auth (request, api_client):    auth_level = request.param    if auth_level == \"user\":        api_client.authenticate (create_user())    elif auth_level == \"admin\":        api_client.authenticate (create_admin())    return api_client, auth_level# All tests using this fixture run 3× (once per auth level)def test_users_endpoint (api_client_with_auth):    client, auth = api_client_with_auth    response = client.get(\"/users\")    # Runs 3× (anonymous, user, admin)def test_posts_endpoint (api_client_with_auth):    client, auth = api_client_with_auth    response = client.get(\"/posts\")    # Also runs 3× (anonymous, user, admin)# Total: 2 tests × 3 auth levels = 6 test executions\`\`\` (3) Indirect parametrization for complex setup: Problem: Need different client configurations per test. Direct parametrization (limited): \`\`\`python@pytest.mark.parametrize(\"auth_level\", [\"user\", \"admin\"])def test_endpoint (api_client, auth_level):    # Must set up authentication inside test (repetitive)    if auth_level == \"user\":        api_client.authenticate (create_user())    elif auth_level == \"admin\":        api_client.authenticate (create_admin())    response = api_client.get(\"/users\")    ...\`\`\` Indirect parametrization (clean): \`\`\`python@pytest.fixture def authenticated_client (request, api_client):    \"\"\"Fixture that receives parameter and creates authenticated client\"\"\"    auth_level = request.param    if auth_level == \"user\":        user = create_user()        api_client.authenticate (user)    elif auth_level == \"admin\":        admin = create_admin()        api_client.authenticate (admin)    return api_client@pytest.mark.parametrize(\"authenticated_client\", [\"user\", \"admin\"], indirect=True)def test_endpoint (authenticated_client):    \"\"\"Parameter goes through fixture (indirect=True)\"\"\"    response = authenticated_client.get(\"/users\")    assert response.status_code == 200# Pros: Clean test function, complex setup in fixture, reusable# Cons: Slightly less explicit (parameter goes through fixture)\`\`\` Partial indirect (mix direct and indirect): \`\`\`python@pytest.fixture def database (request):    db_type = request.param    return create_database (db_type)@pytest.mark.parametrize(\"database, endpoint\", [    (\"postgresql\", \"/users\"),    (\"mysql\", \"/posts\"),], indirect=[\"database\"])  # Only database goes through fixturedef test_endpoint (database, endpoint):    \"\"\"database indirect, endpoint direct\"\"\"    # database created via fixture    # endpoint used directly    ...\`\`\` (4) Test organization (one file vs multiple files): Option A—Single file (simple but messy): \`\`\`python# test_api.py (1000 lines, hard to navigate)def test_users_get_anonymous(): ...def test_users_get_user(): ...def test_users_get_admin(): ...def test_users_post_anonymous(): ...# ... 596 more tests\`\`\` Option B—File per endpoint (organized): \`\`\`python# tests/api/test_users.py (100 lines)@pytest.mark.parametrize(\"method, auth, expected\", [    (\"GET\", \"anonymous\", 401),    (\"GET\", \"user\", 200),    (\"GET\", \"admin\", 200),    # ... user-specific cases])def test_users_endpoint (api_client, method, auth, expected): ...# tests/api/test_posts.py (100 lines)# ... posts-specific tests# tests/api/test_comments.py# ... comments-specific tests\`\`\` Option C—File per auth level (by access pattern): \`\`\`python# tests/api/test_public_endpoints.py@pytest.mark.parametrize(\"endpoint\", public_endpoints)def test_public_access (api_client, endpoint): ...# tests/api/test_user_endpoints.py@pytest.mark.parametrize(\"endpoint\", user_endpoints)def test_user_access (user_api_client, endpoint): ...# tests/api/test_admin_endpoints.py@pytest.mark.parametrize(\"endpoint\", admin_endpoints)def test_admin_access (admin_api_client, endpoint): ...\`\`\` Recommendation: Option B (file per endpoint) for API testing—matches API structure. (5) Measuring and optimizing execution time: Baseline: 600 tests × 100ms each = 60 seconds. Measure current performance: \`\`\`bash# Install pytest-benchmark and pytest-profilingpip install pytest-benchmark pytest-profiling# Run with timingpytest --durations=10  # Show 10 slowest tests# Profile test executionpytest --profile\`\`\` Results: 60 seconds total. Slowest 10 tests take 40 seconds (67% of time). Database setup: 10s. Authentication setup: 15s. Actual API calls: 35s. Optimization 1—Fixture scope: \`\`\`python# Before: function-scoped database (created 600×)@pytest.fixture def db():    return create_database()  # 10ms × 600 = 6s# After: session-scoped database (created 1×)@pytest.fixture (scope=\"session\")def db_engine():    engine = create_database()  # 10ms × 1 = 0.01s    yield engine    engine.dispose()# Savings: 6s → 0.01s (600× faster)\`\`\` Optimization 2—Parallelize: \`\`\`bash# Run tests in parallel (8 cores)pytest -n 8# Time: 60s / 8 = 7.5s# Savings: 60s → 7.5s (8× faster)\`\`\` Optimization 3—Mock slow operations: \`\`\`python# Before: Real API call (100ms)def test_endpoint (api_client):    response = api_client.get(\"/external-api\")  # Slow!    ...# After: Mock external API (<1ms)def test_endpoint (api_client, mock_external_api):    mock_external_api.return_value = {\"status\": \"success\"}    response = api_client.get(\"/external-api\")  # Fast!    ...# Savings: 100ms → 1ms per test (100× faster for tests with external calls)\`\`\` Final performance: Baseline: 60s. After fixture scoping: 54s (saved 6s). After parallelization: 6.75s (saved 47.25s). After mocking: 3s (saved 3.75s). Total: 60s → 3s (20× faster). Summary: Single vs multiple: Use hybrid (group by logic) for 50 endpoints—clear and maintainable. Fixtures vs tests: Parametrize tests for inputs, parametrize fixtures for shared setup (auth clients). Indirect: Use for complex setup (authentication, database), keeps tests clean, setup in fixtures. Organization: File per endpoint (50 files), matches API structure, easy to navigate and maintain. Performance: Scope fixtures (6s saved), parallelize 8 cores (47s saved), mock external (4s saved) → 60s to 3s (20× faster).`,
    keyPoints: [
      'Strategies: Single parametrize (600 explicit combinations), Multiple (matrix 50×4×3), Hybrid (group by access: public/user/admin)',
      'Fixtures vs tests: Parametrize tests for inputs, parametrize fixtures for shared variations (auth client runs all tests 3×)',
      'Indirect parametrization: Clean tests, complex setup in fixture, partial indirect (mix direct/indirect parameters)',
      'Organization: File per endpoint (50 files) matches API structure, easier navigation than single 1000-line file',
      'Performance: Scope fixtures (6s), parallelize 8 cores (47s), mock external APIs (4s) → 60s to 3s (20× speedup)',
    ],
  },
  {
    id: 'pt-q-3',
    question:
      'Design a parametrized testing strategy for a data processing pipeline that transforms CSV files with 50+ columns and millions of rows. The pipeline has 12 transformation steps (validation, cleansing, enrichment, aggregation). Address: (1) test data generation (realistic vs minimal datasets), (2) parametrizing by transformation step vs end-to-end, (3) property-based testing with Hypothesis for edge cases, (4) performance testing with parametrized data sizes, (5) regression testing (comparing outputs across code versions). Include specific examples and discuss test coverage vs execution time tradeoffs.',
    sampleAnswer: `Data pipeline parametrized testing strategy: Context: CSV with 50+ columns, millions of rows, 12 transformation steps, complex business logic. (1) Test data generation: Challenge: Real data is large (GB), slow to process, contains PII. Strategy A—Minimal synthetic data (fast): \`\`\`python# Minimal test data (10 rows, 5 columns)minimal_data = [    {\"user_id\": 1, \"name\": \"Alice\", \"age\": 30, \"city\": \"NYC\", \"revenue\": 100.0},    {\"user_id\": 2, \"name\": \"Bob\", \"age\": 25, \"city\": \"LA\", \"revenue\": 200.0},    {\"user_id\": 3, \"name\": \"Charlie\", \"age\": 35, \"city\": \"Chicago\", \"revenue\": 150.0},    # Edge cases    {\"user_id\": 4, \"name\": None, \"age\": -1, \"city\": \"\", \"revenue\": 0.0},  # Null/invalid    {\"user_id\": 5, \"name\": \"Dave\" * 100, \"age\": 200, \"city\": \"X\", \"revenue\": -100.0},  # Extreme]@pytest.fixture (params=[minimal_data])def input_data (request):    return pd.DataFrame (request.param)def test_validation_step (input_data):    result = validation_step (input_data)    assert len (result) <= len (input_data)  # Some rows may be filtered    assert \"is_valid\" in result.columns# Pros: Fast (10ms), easy to understand, catches basic errors# Cons: Doesn't catch performance issues, doesn't test full schema (50 columns)\`\`\` Strategy B—Realistic sample data (balanced): \`\`\`python# Realistic sample (1000 rows, 50 columns)@pytest.fixture (params=[\"sample_1k\", \"sample_10k\"])def sample_data (request):    sample_size = int (request.param.split(\"_\")[1].replace(\"k\", \"000\"))    return generate_realistic_data (num_rows=sample_size, num_cols=50)def generate_realistic_data (num_rows, num_cols):    \"\"\"Generate realistic test data with Faker\"\"\"    from faker import Faker    fake = Faker()        data = {        \"user_id\": range (num_rows),        \"name\": [fake.name() for _ in range (num_rows)],        \"age\": [fake.random_int (min=18, max=80) for _ in range (num_rows)],        \"email\": [fake.email() for _ in range (num_rows)],        \"city\": [fake.city() for _ in range (num_rows)],        \"revenue\": [fake.pyfloat (min_value=0, max_value=10000) for _ in range (num_rows)],        # ... 44 more columns    }    return pd.DataFrame (data)def test_pipeline_with_realistic_data (sample_data):    result = run_full_pipeline (sample_data)    assert len (result) > 0    assert len (result.columns) >= 50  # All columns present# Pros: Realistic (catches issues minimal doesn't), medium speed (100ms-1s)# Cons: Slower than minimal, still not full production scale\`\`\` Strategy C—Layered approach (recommended): \`\`\`python# Layer 1: Minimal data (fast, unit tests)minimal_fixtures = [    pytest.param (generate_minimal_data(10), id=\"minimal-10\"),    pytest.param (generate_edge_cases(), id=\"edge-cases\"),]# Layer 2: Realistic sample (medium, integration tests)sample_fixtures = [    pytest.param (generate_realistic_data(1000), id=\"sample-1k\"),    pytest.param (generate_realistic_data(10000), id=\"sample-10k\"),]# Layer 3: Production scale (slow, performance tests)@pytest.mark.slowproduction_fixtures = [    pytest.param (load_real_data_sample(100000), id=\"prod-100k\"),    pytest.param (load_real_data_sample(1000000), id=\"prod-1m\"),]@pytest.mark.parametrize(\"input_data\", minimal_fixtures)def test_validation_unit (input_data):    \"\"\"Fast unit tests with minimal data\"\"\"    result = validation_step (input_data)    assert result is not None@pytest.mark.parametrize(\"input_data\", sample_fixtures)def test_validation_integration (input_data):    \"\"\"Integration tests with realistic data\"\"\"    result = validation_step (input_data)    assert len (result) > 0@pytest.mark.slow@pytest.mark.parametrize(\"input_data\", production_fixtures)def test_validation_performance (input_data):    \"\"\"Performance tests with production-scale data\"\"\"    import time    start = time.time()    result = validation_step (input_data)    duration = time.time() - start        assert result is not None    assert duration < 10.0  # Must process 100K rows in <10s# Run strategy:# Development: pytest -m \"not slow\"  # Only minimal (fast)# CI: pytest -m \"not slow\"  # Minimal + realistic (medium)# Nightly: pytest  # All including production (slow)\`\`\` (2) Parametrizing by step vs end-to-end: Option A—Per-step parametrization (unit tests): \`\`\`python# Test each transformation step independentlypipeline_steps = [    (\"validation\", validation_step, {\"expected_cols\": [\"is_valid\"]}),    (\"cleansing\", cleansing_step, {\"expected_nulls\": 0}),    (\"enrichment\", enrichment_step, {\"expected_cols\": [\"country\", \"timezone\"]}),    (\"aggregation\", aggregation_step, {\"expected_groups\": [\"city\", \"age_bucket\"]}),    # ... 8 more steps]@pytest.mark.parametrize(\"step_name, step_func, expectations\", pipeline_steps, ids=lambda x: x[0])def test_pipeline_step (input_data, step_name, step_func, expectations):    \"\"\"Test each step independently\"\"\"    result = step_func (input_data)        # Common assertions    assert result is not None    assert len (result) > 0        # Step-specific assertions    for key, value in expectations.items():        if key == \"expected_cols\":            for col in value:                assert col in result.columns        elif key == \"expected_nulls\":            assert result.isnull().sum().sum() == value# Pros: Fast feedback (which step failed), easy to debug, parallel execution# Cons: Doesn't catch integration issues between steps\`\`\` Option B—End-to-end parametrization: \`\`\`python@pytest.mark.parametrize(\"input_data, expected_output_size\", [    (minimal_data, 8),  # Expect 2 invalid rows filtered    (sample_1k, 950),   # Expect ~5% filtered    (sample_10k, 9500), # Expect ~5% filtered])def test_full_pipeline (input_data, expected_output_size):    \"\"\"Test complete pipeline end-to-end\"\"\"    result = run_full_pipeline (input_data)        assert len (result) == expected_output_size    assert \"final_score\" in result.columns  # Final output column    assert result[\"final_score\"].notna().all()  # No nulls# Pros: Tests real workflow, catches integration issues# Cons: Slow (12 steps), hard to debug (which step failed?)\`\`\` Option C—Hybrid (recommended): \`\`\`python# Unit tests per step (fast)@pytest.mark.parametrize(\"step_name, step_func\", pipeline_steps)def test_step (step_name, step_func, minimal_data):    result = step_func (minimal_data)    assert result is not None# Integration tests for step chains (medium)step_chains = [    (\"validation+cleansing\", [validation_step, cleansing_step]),    (\"enrichment+aggregation\", [enrichment_step, aggregation_step]),]@pytest.mark.parametrize(\"chain_name, steps\", step_chains)def test_step_chain (chain_name, steps, sample_1k):    data = sample_1k    for step in steps:        data = step (data)    assert data is not None# End-to-end test (slow, comprehensive)def test_full_pipeline_e2e (sample_10k):    result = run_full_pipeline (sample_10k)    assert len (result) > 0\`\`\` (3) Property-based testing with Hypothesis: Challenge: Edge cases hard to enumerate (null values, duplicates, extreme values, special chars). Solution—Hypothesis generates test cases: \`\`\`python from hypothesis import given, strategies as st# Define data strategiesrow_strategy = st.fixed_dictionaries({    \"user_id\": st.integers (min_value=1, max_value=1000000),    \"name\": st.one_of (st.text (min_size=1, max_size=50), st.none()),  # Can be null    \"age\": st.integers (min_value=-10, max_value=200),  # Allow invalid    \"email\": st.one_of (st.emails(), st.text(), st.none()),  # Valid, invalid, null    \"revenue\": st.floats (min_value=-1000, max_value=100000, allow_nan=True),})@given (st.lists (row_strategy, min_size=1, max_size=100))def test_validation_with_hypothesis (rows):    \"\"\"Hypothesis generates thousands of test cases with edge cases\"\"\"    df = pd.DataFrame (rows)        # Should never raise exception (even with invalid data)    try:        result = validation_step (df)        assert result is not None    except Exception as e:        pytest.fail (f\"validation_step raised {e}\")        # Valid rows should be marked    if result is not None:        assert \"is_valid\" in result.columns        assert result[\"is_valid\"].dtype == bool# Hypothesis runs this 100-10,000 times with different data# Automatically finds edge cases: nulls, NaNs, negatives, special chars\`\`\` Edge cases Hypothesis finds: \`\`\`python# Example cases Hypothesis generated that broke the code:# Case 1: All null row{\"user_id\": None, \"name\": None, \"age\": None, \"email\": None, \"revenue\": None}# Case 2: Extreme values{\"user_id\": 999999999, \"name\": \"A\" * 1000, \"age\": -999, \"email\": \"@@@\", \"revenue\": float(\"inf\")}# Case 3: Unicode and special characters{\"user_id\": 1, \"name\": \"josé\", \"age\": 30, \"email\": \"test@tëst.com\", \"revenue\": 100.0}\`\`\` (4) Performance testing with parametrized data sizes: \`\`\`python data_sizes = [    pytest.param(100, 0.01, id=\"tiny-100\"),        # 100 rows, expect <10ms    pytest.param(1000, 0.1, id=\"small-1k\"),      # 1K rows, expect <100ms    pytest.param(10000, 1.0, id=\"medium-10k\"),   # 10K rows, expect <1s    pytest.param(100000, 10.0, id=\"large-100k\"), # 100K rows, expect <10s    pytest.param(1000000, 100.0, id=\"xlarge-1m\", marks=pytest.mark.slow),  # 1M rows, <100s]@pytest.mark.parametrize(\"num_rows, max_duration\", data_sizes)def test_pipeline_performance (num_rows, max_duration):    \"\"\"Test pipeline scales linearly with data size\"\"\"    import time        # Generate data    data = generate_realistic_data (num_rows)        # Measure execution time    start = time.time()    result = run_full_pipeline (data)    duration = time.time() - start        # Assertions    assert result is not None    assert len (result) <= num_rows    assert duration < max_duration, f\"Processed {num_rows} rows in {duration:.2f}s (max {max_duration}s)\"        # Calculate throughput    throughput = num_rows / duration    print(f\"Throughput: {throughput:.0f} rows/second\")# Output:# test_pipeline_performance[tiny-100] PASSED (10,000 rows/s)# test_pipeline_performance[small-1k] PASSED (11,000 rows/s)# test_pipeline_performance[medium-10k] PASSED (12,000 rows/s)# test_pipeline_performance[large-100k] PASSED (10,500 rows/s)# test_pipeline_performance[xlarge-1m] PASSED (11,200 rows/s)# → Performance scales linearly (good!)\`\`\` (5) Regression testing (comparing outputs across versions): \`\`\`python # Save reference outputs from v1.0@pytest.fixture (scope=\"session\")def reference_outputs():    \"\"\"Load reference outputs from previous version\"\"\"    return {        \"minimal\": pd.read_parquet(\"tests/reference/v1.0/minimal_output.parquet\"),        \"sample_1k\": pd.read_parquet(\"tests/reference/v1.0/sample_1k_output.parquet\"),    }test_cases = [    (\"minimal\", generate_minimal_data(10)),    (\"sample_1k\", generate_realistic_data(1000)),]@pytest.mark.parametrize(\"case_name, input_data\", test_cases)def test_regression (case_name, input_data, reference_outputs):    \"\"\"Test output matches reference from v1.0\"\"\"    # Run current version    current_output = run_full_pipeline (input_data)        # Load reference output    reference_output = reference_outputs[case_name]        # Compare    pd.testing.assert_frame_equal(        current_output.sort_index (axis=1),  # Sort columns        reference_output.sort_index (axis=1),        check_dtype=False,  # Allow minor type differences        atol=0.01,  # Allow 1% numerical tolerance    )# If this fails → breaking change detected\`\`\` Summary: Test data: Layered (minimal 10 rows for speed, realistic 1K for integration, production 1M for performance). Per-step vs E2E: Hybrid (unit tests per step, integration chains, E2E comprehensive)—balance speed and coverage. Property-based: Hypothesis generates thousands of edge cases automatically (nulls, extremes, Unicode). Performance: Parametrize by size (100/1K/10K/100K/1M rows), measure throughput, assert <max_duration. Regression: Save reference outputs (v1.0), compare with current (pd.testing.assert_frame_equal), detect breaking changes. Trade-offs: Fast (minimal, per-step, 10ms) vs Comprehensive (production E2E, 100s)—run fast in dev, comprehensive nightly.`,
    keyPoints: [
      'Test data: Layered (minimal 10 rows, realistic 1K/10K, production 100K/1M), Faker for generation, edge cases included',
      'Per-step vs E2E: Hybrid best (unit per-step fast, integration chains medium, E2E comprehensive slow)',
      'Hypothesis: Property-based testing finds edge cases (nulls, extremes, Unicode) automatically, 100-10K generated cases',
      'Performance: Parametrize sizes (100/1K/10K/100K/1M), measure duration and throughput, assert linear scaling',
      'Regression: Save reference outputs (v1.0), pd.testing.assert_frame_equal with tolerance, detect breaking changes early',
    ],
  },
];
