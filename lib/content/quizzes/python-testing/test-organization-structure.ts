export const testOrganizationStructureQuiz = [
  {
    id: 'tos-q-1',
    question:
      'Your team maintains a 50K-line Python application with 3,500 tests that currently uses a flat test structure (all tests in one directory). Tests take 25 minutes to run, and developers complain about difficulty finding and maintaining tests. Design a reorganization strategy that addresses: (1) how to migrate from flat to structured organization without breaking CI/CD, (2) directory structure that optimizes for test speed and maintainability, (3) how to leverage conftest.py to eliminate duplicate fixtures (currently 120+ copies of same db_session setup), (4) selective test execution strategy for different CI stages, (5) team training and adoption plan. Include specific migration steps and timeline.',
    sampleAnswer: `Test reorganization strategy for 3,500-test suite: (1) Migration plan without breaking CI/CD: Week 1—Analysis and planning: Audit current tests: Categorize into unit (60%), integration (30%), E2E (10%). Identify duplicate fixtures: 120 db_session copies, 80 api_client copies. Measure test speeds: Slowest 10% take 80% of time. Create new directory structure (without moving files yet): \`\`\`tests/ ├── unit/ (placeholder) ├── integration/ (placeholder) ├── e2e/ (placeholder) └── utilities/ (placeholder)\`\`\` Week 2—Setup infrastructure: Create root conftest.py with shared fixtures: db_session, api_client, redis_client. Update pytest.ini with new testpaths and markers. Set up utilities/ directory: assertions.py, factories.py, helpers.py. Test: Verify new fixtures work alongside old ones (no conflicts). Week 3—Gradual migration (unit tests first): Move unit tests (2,100 tests) to tests/unit/ mirroring src structure. Use script to automate: \`\`\`python import shutil import os from pathlib import Path def migrate_tests(): """Move tests to new structure""" for test_file in Path(\"tests\").glob(\"test_*.py\"): # Determine if unit, integration, or e2e if is_unit_test(test_file): dest = f\"tests/unit/{test_file.name}\" elif is_integration_test(test_file): dest = f\"tests/integration/{test_file.name}\" else: dest = f\"tests/e2e/{test_file.name}\" # Copy (don't move yet—keep both for safety) shutil.copy(test_file, dest) # Update imports in new location update_imports(dest)\`\`\` Update imports: Replace duplicate db_session setups with conftest.py fixtures. CI/CD: Run BOTH old and new tests (verify identical results). Week 4—Migrate integration and E2E tests: Move integration tests (1,050 tests) to tests/integration/. Move E2E tests (350 tests) to tests/e2e/. Verify all tests pass in new locations. Week 5—Switch CI/CD to new structure: Update CI to use new paths: pytest tests/unit, pytest tests/integration. Run new tests for 1 week alongside old tests (double validation). Week 6—Delete old structure: Once confident, delete old flat structure. Remove duplicate fixtures. Update documentation. Result: Zero downtime, validated migration, team confidence maintained. (2) Directory structure optimizing for speed and maintainability: \`\`\`tests/ ├── conftest.py # Shared fixtures (db_session, api_client, redis) ├── pytest.ini # Test configuration ├── unit/ (2,100 tests, 5 minutes) │ ├── conftest.py # Unit-specific fixtures (mocks) │ ├── api/ │ │ ├── test_routes.py │ │ └── test_schemas.py │ ├── business/ │ │ ├── test_payment_processor.py │ │ └── test_user_service.py │ └── models/ │ ├── test_user.py │ └── test_payment.py ├── integration/ (1,050 tests, 12 minutes) │ ├── conftest.py # Integration fixtures (Docker, real DB) │ ├── test_database_operations.py │ ├── test_redis_cache.py │ └── test_api_endpoints.py ├── e2e/ (350 tests, 30 minutes) │ ├── conftest.py # E2E fixtures (full system) │ ├── test_user_registration_flow.py │ ├── test_payment_flow.py │ └── test_checkout_flow.py ├── performance/ │ └── test_load.py └── utilities/ ├── assertions.py # Custom assertions (eliminate duplication) ├── factories.py # FactoryBoy factories ├── helpers.py # Helper functions └── __init__.py\`\`\` Speed optimization: Unit tests (2,100 tests): Isolated, mocked dependencies. Fast: 5 minutes (vs 15 minutes before). Run on every commit. Integration tests (1,050 tests): Real database, Redis, but no full system. Medium: 12 minutes (vs 20 minutes before). Run before merge. E2E tests (350 tests): Full system, slowest. 30 minutes (unchanged, but only run when needed). Run nightly or on main branch. Total time before: 25 minutes (all tests serial). Total time after: 5 min (unit) + 12 min (integration) parallel = 12 min total. 48% faster. (3) Eliminate duplicate fixtures with conftest.py: Before (duplicate fixtures): 120 copies of db_session setup across test files: \`\`\`python # tests/test_payment.py def db_session(): engine = create_engine(\"postgresql://...\") Base.metadata.create_all(engine) Session = sessionmaker(bind=engine) session = Session() yield session session.close() Base.metadata.drop_all(engine) # tests/test_user.py def db_session(): # Exact same code duplicated! engine = create_engine(\"postgresql://...\") # ... (repeated 120 times across files)\`\`\` After (single source in conftest.py): \`\`\`python # tests/conftest.py import pytest from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker from myapp.database import Base @pytest.fixture(scope=\"session\") def db_engine(): \"\"\"Session-level database engine\"\"\" engine = create_engine(\"postgresql://test:test@localhost/testdb\") Base.metadata.create_all(engine) yield engine Base.metadata.drop_all(engine) engine.dispose() @pytest.fixture(scope=\"function\") def db_session(db_engine): \"\"\"Function-level clean database session\"\"\" connection = db_engine.connect() transaction = connection.begin() Session = sessionmaker(bind=connection) session = Session() yield session session.close() transaction.rollback() # Rollback ensures clean state connection.close() @pytest.fixture def api_client(): \"\"\"API test client\"\"\" from myapp import create_app app = create_app(config=\"testing\") return app.test_client() @pytest.fixture def redis_client(): \"\"\"Redis client for tests\"\"\" import redis client = redis.Redis(host=\"localhost\", port=6379, db=15) yield client client.flushdb() client.close()\`\`\` Usage (no import needed): \`\`\`python # tests/unit/test_payment.py def test_create_payment(db_session): \"\"\"db_session automatically available from conftest.py\"\"\" payment = Payment(amount=100.0) db_session.add(payment) db_session.commit() assert payment.id is not None def test_api_endpoint(api_client): \"\"\"api_client automatically available\"\"\" response = api_client.post(\"/payments\", json={...}) assert response.status_code == 201\`\`\` Benefits: Maintenance: Change db connection once in conftest.py, not 120 files. Consistency: All tests use same db setup (no drift). Debugging: Single point of failure (easier to debug). Speed: Session-scoped fixtures created once, reused across tests. Savings: 120 copies × 20 lines = 2,400 lines of duplicate code eliminated. (4) Selective test execution for CI/CD stages: Stage 1—Pre-commit (local, 30 seconds): Developer runs before committing. Fast smoke test: Only tests for modified files. \`\`\`bash # Pre-commit hook pytest tests/unit -k \"payment\" --maxfail=1 -x\`\`\` Stage 2—CI: Commit to feature branch (5 minutes): Run all unit tests (2,100 tests). Fast feedback on code quality. \`\`\`yaml # GitHub Actions - main branch jobs: unit-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - run: pip install -r requirements-test.txt - run: pytest tests/unit -n auto --cov=src --cov-fail-under=80\`\`\` Stage 3—CI: Pull request (17 minutes): Run unit + integration tests (3,150 tests). Comprehensive coverage before merge. \`\`\`yaml integration-tests: runs-on: ubuntu-latest needs: unit-tests steps: - run: docker-compose up -d postgres redis - run: pytest tests/integration -n auto - run: docker-compose down\`\`\` Stage 4—CI: Merge to main (47 minutes): Run unit + integration + E2E tests (all 3,500 tests). Verify production-readiness. \`\`\`yaml e2e-tests: runs-on: ubuntu-latest needs: [unit-tests, integration-tests] if: github.ref == \"refs/heads/main\" steps: - run: docker-compose -f docker-compose.e2e.yml up -d - run: pytest tests/e2e -n auto --maxfail=1 - run: docker-compose down\`\`\` Stage 5—Nightly (47 minutes + performance tests): Run all tests including performance tests. Catch flaky tests and regressions. \`\`\`yaml nightly-tests: schedule: - cron: \"0 2 * * *\" # 2 AM daily runs-on: ubuntu-latest steps: - run: pytest tests/ -n auto --reruns 3 # Retry flaky tests - run: pytest tests/performance/ # Load tests\`\`\` Selective execution benefits: Developers: 30s feedback (vs 25 min before). Feature branches: 5 min feedback (vs 25 min). Pull requests: 17 min validation (comprehensive). Main branch: 47 min full validation (production-ready). Time saved per developer: 20 commits/day × 24 min saved = 8 hours saved daily. 10 developers = 80 hours saved per day = $20K/day @ $250/hour. (5) Team training and adoption plan: Week 1—Kickoff and documentation: Team meeting: Explain pain points (25 min tests, hard to find tests, duplicate code). Present new structure: Show directory layout, conftest.py, selective execution. Create documentation: README in tests/ explaining structure, migration guide, FAQ. Week 2—Training sessions (2 hours): Session 1 (1 hour): Intro to new structure, conftest.py, shared utilities. Live demo: Write new test using fixtures, show speed improvement. Q&A: Address concerns, get feedback. Session 2 (1 hour): Advanced fixtures, parametrization, markers. Hands-on: Each dev writes test in new structure. Week 3—Pair programming: Pair senior devs with junior devs. Migrate one module together (e.g., payment module tests). Review and discuss patterns. Week 4—Gradual adoption: New tests MUST use new structure (enforced via code review). Existing tests: Migrate when modifying (opportunistic migration). Monitor: Track % of tests in new structure (target 100% by Week 6). Week 5—Retrospective and iteration: Team retrospective: What's working? What needs improvement? Iterate: Adjust structure based on feedback. Week 6—Full adoption: All new tests in new structure. Old structure deprecated (to be deleted). Update onboarding docs for new hires. Success metrics: Test execution time: 25 min → 5 min (unit only), 80% reduction. Fixture duplication: 120 copies → 1 (conftest.py), 99% reduction. Time to find test: 2 min → 10 seconds (mirrored structure), 90% faster. Developer satisfaction: Survey shows 85% prefer new structure. ROI: Time investment: 40 hours team training + 80 hours migration = 120 hours = $30K. Time saved: 80 hours/day × 200 work days/year = 16,000 hours/year = $4M/year. ROI: Pays for itself in 1 day, $4M annual savings. Summary: Migrate gradually over 6 weeks (zero downtime). Structure: unit/integration/e2e separation (48% faster tests). Eliminate 120 fixture copies with conftest.py (2,400 lines removed). Selective execution: 30s pre-commit, 5 min CI, 47 min nightly (vs 25 min always). Training: 2 hours sessions, pair programming, gradual adoption (high buy-in). Result: Faster tests, easier maintenance, happier developers, $4M annual savings.`,
    keyPoints: [
      'Migration: 6-week plan, copy then move (not delete immediately), run old+new in parallel for validation',
      'Structure: unit/integration/e2e separation, mirrors src/, 48% faster (25 min → 5 min unit tests)',
      'conftest.py: Eliminate 120 duplicate fixtures (2,400 lines), single source of truth, auto-available',
      'Selective execution: 30s pre-commit, 5 min CI commit, 17 min PR, 47 min main, saves 80 hours/day',
      'Adoption: 2hr training, pair programming, retrospective, ROI: $30K cost vs $4M/year savings',
    ],
  },
  {
    id: 'tos-q-2',
    question:
      'You join a startup with a microservices architecture (8 services) where tests are completely inconsistent: Service A has tests in /tests, Service B in /spec, Service C has no tests. Fixture duplication is rampant. No shared test utilities. Address: (1) standardizing test organization across all services without disrupting development, (2) creating a shared test utilities package used by all services, (3) versioning strategy for shared utilities (how to update without breaking services), (4) handling cross-service integration tests (where do they go?), (5) CI/CD configuration that works for both individual services and the entire system. Include code examples of the shared utilities package structure.',
    sampleAnswer: `Standardizing tests across 8 microservices: (1) Standardizing test organization without disrupting development: Current state chaos: Service A: tests/ (pytest) Service B: spec/ (pytest) Service C: No tests Service D: test/ (unittest) Service E: tests/ (pytest) Service F: testing/ (pytest) Service G: tests/ (unittest) Service H: spec/ (pytest) Inconsistency: 4 different directory names, 2 test frameworks, 1 service untested. Step 1—Define standard structure (Week 1): Create RFC (Request for Comments) document: \`\`\`markdown # RFC: Standard Test Structure for All Services ## Proposal Standard structure for all services: \`\`\`service_name/ ├── src/ ├── tests/ │ ├── unit/ │ ├── integration/ │ └── conftest.py ├── pytest.ini └── requirements-test.txt\`\`\` Framework: pytest (uniform across all services). Directory: Always tests/ (not spec/, test/, testing/). Separate unit and integration tests. ## Migration plan - Non-disruptive: Existing tests continue to work. - Gradual: Migrate over 4 weeks. - Automated: Scripts provided for migration. ## Timeline Week 1: RFC review and approval Week 2: Create shared-test-utils package Week 3: Migrate Services A, B, C Week 4: Migrate Services D, E, F, G, H \`\`\`Share with team, get buy-in (address concerns). Step 2—Automated migration scripts (Week 1): \`\`\`python # migrate_tests.py import shutil from pathlib import Path def migrate_service_tests(service_path): \"\"\"Migrate service to standard structure\"\"\" service = Path(service_path) # Find existing test directory old_test_dirs = [\"spec\", \"test\", \"testing\", \"tests\"] test_dir = None for dirname in old_test_dirs: if (service / dirname).exists(): test_dir = service / dirname break if not test_dir: print(f\"No tests found in {service}\") return # Create new structure new_tests = service / \"tests\" new_tests.mkdir(exist_ok=True) (new_tests / \"unit\").mkdir(exist_ok=True) (new_tests / \"integration\").mkdir(exist_ok=True) # Move test files for test_file in test_dir.glob(\"test_*.py\"): if is_unit_test(test_file): shutil.move(test_file, new_tests / \"unit\" / test_file.name) else: shutil.move(test_file, new_tests / \"integration\" / test_file.name) # Create conftest.py if not exists if not (new_tests / \"conftest.py\").exists(): create_conftest(new_tests / \"conftest.py\") # Create pytest.ini create_pytest_ini(service / \"pytest.ini\") print(f\"✓ Migrated {service.name}\") # Usage for service in [\"service_a\", \"service_b\", \"service_c\"]: migrate_service_tests(f\"../services/{service}\")\`\`\` Step 3—Gradual rollout (Week 3-4): Week 3: Migrate Services A, B, C (have tests, use pytest). Run migration script. Verify tests pass in new structure. Update CI/CD. Week 4: Migrate Services D, G (use unittest). Convert unittest → pytest (or run with pytest). Migrate to new structure. Create tests for Service C (currently no tests). Example: 20 unit tests, 5 integration tests. Result after 4 weeks: All 8 services use standard structure: tests/unit, tests/integration, conftest.py. (2) Shared test utilities package: Problem: Each service duplicates: db_session fixture (8 copies), api_client fixture (8 copies), factories (8 copies), mock services (8 copies). Total duplication: ~500 lines × 8 services = 4,000 lines. Solution: Create shared-test-utils package: \`\`\`shared-test-utils/ ├── setup.py ├── shared_test_utils/ │ ├── __init__.py │ ├── fixtures/ │ │ ├── __init__.py │ │ ├── database.py # Database fixtures │ │ ├── redis.py # Redis fixtures │ │ ├── kafka.py # Kafka fixtures │ │ └── http_client.py # HTTP client fixtures │ ├── factories/ │ │ ├── __init__.py │ │ ├── user.py # User factory │ │ └── payment.py # Payment factory │ ├── mocks/ │ │ ├── __init__.py │ │ ├── mock_service_a.py # Mock Service A API │ │ └── mock_service_b.py # Mock Service B API │ └── helpers/ │ ├── __init__.py │ └── assertions.py # Custom assertions └── tests/ # Tests for the utilities package itself └── test_fixtures.py\`\`\` shared_test_utils/fixtures/database.py: \`\`\`python import pytest from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker @pytest.fixture(scope=\"session\") def db_engine(): \"\"\"Database engine (session-scoped, created once)\"\"\" engine = create_engine(\"postgresql://test:test@localhost/testdb\") yield engine engine.dispose() @pytest.fixture(scope=\"function\") def db_session(db_engine): \"\"\"Clean database session for each test\"\"\" from sqlalchemy.orm import Session connection = db_engine.connect() transaction = connection.begin() session = Session(bind=connection) yield session session.close() transaction.rollback() connection.close()\`\`\` shared_test_utils/mocks/mock_service_a.py: \`\`\`python import pytest from unittest.mock import Mock @pytest.fixture def mock_service_a(): \"\"\"Mock Service A API calls\"\"\" mock = Mock() mock.get_user.return_value = { \"id\": \"123\", \"name\": \"Alice\", \"email\": \"alice@example.com\" } mock.create_payment.return_value = { \"id\": \"payment_123\", \"status\": \"completed\" } return mock\`\`\` shared_test_utils/factories/user.py: \`\`\`python import factory from faker import Faker fake = Faker() class UserFactory(factory.Factory): \"\"\"Factory for creating test users\"\"\" class Meta: model = dict # Can be used with any User model id = factory.Sequence(lambda n: f\"user_{n}\") username = factory.LazyFunction(lambda: fake.user_name()) email = factory.LazyFunction(lambda: fake.email()) age = factory.LazyFunction(lambda: fake.random_int(min=18, max=80))\`\`\` Usage in Service B: \`\`\`python # service_b/tests/conftest.py from shared_test_utils.fixtures.database import db_session, db_engine from shared_test_utils.factories.user import UserFactory from shared_test_utils.mocks.mock_service_a import mock_service_a # All fixtures now available to Service B tests # service_b/tests/unit/test_user_service.py def test_create_user(db_session): \"\"\"Test creating user\"\"\" user = UserFactory.create() # From shared-test-utils db_session.add(user) db_session.commit() assert user.id is not None def test_call_service_a(mock_service_a): \"\"\"Test calling Service A (mocked)\"\"\" result = mock_service_a.get_user(\"123\") assert result[\"name\"] == \"Alice\"\`\`\` (3) Versioning strategy for shared utilities: Problem: Update shared-test-utils → might break Service B, C, D (if breaking change). Solution: Semantic versioning + gradual rollout: Version format: MAJOR.MINOR.PATCH (e.g., 1.2.3). MAJOR: Breaking changes (require code updates in services). MINOR: New features (backward compatible). PATCH: Bug fixes (backward compatible). Example versions: v1.0.0: Initial release. v1.1.0: Add mock_service_c (new feature, compatible). v1.2.0: Add kafka fixtures (compatible). v2.0.0: Change db_session API (breaking change). Migration strategy for breaking changes (e.g., v1.x → v2.0): Week 1—Announce breaking change: Publish changelog: What's changing, why, migration guide. Example: v2.0.0 changes db_session from function to class-based fixture. Week 2—Provide migration guide: \`\`\`markdown # Migrating to shared-test-utils v2.0 ## Breaking changes ### db_session is now class-based **Before (v1.x):** \`\`\`python def test_user(db_session): user = User(name=\"Alice\") db_session.add(user) db_session.commit()\`\`\` **After (v2.0):** \`\`\`python def test_user(db_session): with db_session() as session: user = User(name=\"Alice\") session.add(user) session.commit()\`\`\` ## Timeline - Week 2: Announcement - Week 3-4: Migrate Service A, B - Week 5-6: Migrate Service C, D, E - Week 7: All services on v2.0 - Week 8: Deprecate v1.x \`\`\`Week 3-6—Gradual service migration: Services migrate one at a time. Each service: Update requirements-test.txt: shared-test-utils==2.0.0. Update tests: Follow migration guide. Verify tests pass. Deploy. Week 7—All services on v2.0: v1.x deprecated (but still available for emergency rollback). Week 8—Remove v1.x: Delete old version from registry. For minor/patch versions (backward compatible): No migration needed—services update at will. Automatic updates: Use shared-test-utils>=1.0,<2.0 (pin major version). CI/CD automatically picks up minor/patch updates. (4) Cross-service integration tests: Where do tests go when testing Service A → Service B interaction? Option 1: Monorepo with shared tests/ directory (recommended): \`\`\`monorepo/ ├── services/ │ ├── service_a/ │ │ └── tests/ # Service A unit/integration │ ├── service_b/ │ │ └── tests/ # Service B unit/integration │ └── service_c/ │ └── tests/ └── tests/ # Cross-service tests ├── cross_service/ │ ├── test_service_a_to_b.py # A → B integration │ ├── test_service_b_to_c.py # B → C integration │ └── test_end_to_end.py # Full user journey └── conftest.py # Fixtures to start all services\`\`\` tests/cross_service/test_service_a_to_b.py: \`\`\`python import pytest import requests def test_service_a_calls_service_b(docker_services): \"\"\"Test Service A → Service B communication\"\"\" # Service A endpoint that internally calls Service B response = requests.post(\"http://localhost:8001/process\", json={ \"user_id\": \"123\", \"amount\": 100.0 }) assert response.status_code == 200 # Verify Service B was called (check Service B logs/database) service_b_db = get_service_b_database() payment = service_b_db.query(Payment).filter_by(user_id=\"123\").first() assert payment is not None assert payment.amount == 100.0\`\`\` tests/conftest.py (start all services): \`\`\`python import pytest import subprocess import time @pytest.fixture(scope=\"session\") def docker_services(): \"\"\"Start all services with Docker Compose\"\"\" # Start services subprocess.run([\"docker-compose\", \"up\", \"-d\"], check=True) # Wait for services to be healthy time.sleep(10) # Or use health checks yield # Teardown subprocess.run([\"docker-compose\", \"down\"], check=True)\`\`\` Option 2: Separate integration-tests repo: If not using monorepo, create separate repo: \`\`\`integration-tests/ ├── tests/ │ ├── test_service_a_to_b.py │ └── test_end_to_end.py ├── docker-compose.yml # Defines all services └── conftest.py\`\`\` Run nightly or on main branch merges. (5) CI/CD for individual services and entire system: Challenge: Run service tests fast (per-service CI). Run cross-service tests comprehensively (system CI). GitHub Actions configuration: .github/workflows/service-tests.yml (per service): \`\`\`yaml name: Service Tests on: push: paths: - \"services/**\" jobs: test-service: runs-on: ubuntu-latest strategy: matrix: service: [service_a, service_b, service_c, service_d, service_e, service_f, service_g, service_h] steps: - uses: actions/checkout@v3 - uses: actions/setup-python@v4 with: python-version: \"3.11\" - name: Install dependencies run: | cd services/\${{ matrix.service }} pip install -r requirements-test.txt - name: Run unit tests run: | cd services/\${{ matrix.service }} pytest tests/unit -n auto --cov=src --cov-fail-under=80 - name: Run integration tests run: | cd services/\${{ matrix.service }} docker-compose up -d pytest tests/integration -n auto docker-compose down\`\`\` .github/workflows/cross-service-tests.yml (system-wide): \`\`\`yaml name: Cross-Service Tests on: push: branches: [main] pull_request: branches: [main] jobs: cross-service-tests: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Start all services run: docker-compose up -d - name: Wait for services run: ./scripts/wait-for-services.sh - name: Run cross-service tests run: pytest tests/cross_service -n auto - name: Cleanup run: docker-compose down\`\`\` Result: Per-service tests: Run in parallel (8 services × 5 min = 5 min total with parallel execution). Fast feedback for individual service changes. Cross-service tests: Run only on main branch or PRs to main. Comprehensive system validation (20 minutes). Summary: Standardization: RFC → automated migration scripts → gradual rollout (4 weeks, zero disruption). Shared utilities: shared-test-utils package (database, redis, mocks, factories) eliminates 4,000 lines duplication. Versioning: Semantic versioning, gradual migration for breaking changes (v1 → v2 over 6 weeks). Cross-service tests: Monorepo tests/cross_service/ directory, docker-compose to start all services. CI/CD: Per-service tests in parallel (5 min), cross-service tests on main (20 min), selective execution. ROI: 80 hours investment, eliminates 4,000 lines duplication, prevents future inconsistency debt.`,
    keyPoints: [
      'Standardization: RFC approval, automated migration scripts, gradual rollout (4 weeks, 8 services)',
      'Shared utilities: Create shared-test-utils package (fixtures, factories, mocks), eliminate 4,000 duplicate lines',
      'Versioning: Semantic versioning (MAJOR.MINOR.PATCH), gradual breaking change migration (v1→v2 over 6 weeks)',
      'Cross-service tests: Monorepo tests/cross_service/, docker-compose fixtures, test A→B interactions',
      'CI/CD: Per-service parallel tests (5 min), cross-service tests on main (20 min), matrix strategy for 8 services',
    ],
  },
  {
    id: 'tos-q-3',
    question:
      'Design a test organization strategy for a Django application being migrated from unittest to pytest. The app has 4,200 unittest tests written over 5 years by 30+ developers with inconsistent organization. Address: (1) assessing current test quality and coverage, (2) prioritization strategy (which tests to migrate first and why), (3) automated migration tools and manual migration for complex cases, (4) running unittest and pytest simultaneously during transition, (5) team training and preventing new unittest tests. Include specific examples of migration patterns and timelines.',
    sampleAnswer: `Django unittest → pytest migration strategy (4,200 tests): (1) Assessing current test quality and coverage: Week 1—Automated analysis: Coverage analysis: \`\`\`bash coverage run --source=src manage.py test coverage report coverage html\`\`\` Metrics: Overall coverage: 67% (lower than target 80%). 12 modules with <50% coverage (high risk areas). 450 tests failing intermittently (flaky tests, 10.7% flaky rate). Test speed analysis: \`\`\`bash# Run with timing python manage.py test --timing\`\`\` Results: Total runtime: 38 minutes. Slowest 5%: 210 tests take 30 minutes (80% of time). Fastest 95%: 3,990 tests take 8 minutes. Test organization audit: Use script to analyze structure: \`\`\`python import ast import os from pathlib import Path def analyze_test_structure(): \"\"\"Analyze unittest test structure\"\"\" stats = { \"total_files\": 0, \"total_tests\": 0, \"uses_setUp\": 0, \"uses_setUpClass\": 0, \"uses_fixtures\": 0, \"class_based\": 0, \"function_based\": 0, \"test_locations\": {} } for test_file in Path(\"tests\").rglob(\"test_*.py\"): stats[\"total_files\"] += 1 tree = ast.parse(test_file.read_text()) for node in ast.walk(tree): if isinstance(node, ast.ClassDef): if node.name.startswith(\"Test\"): stats[\"class_based\"] += 1 # Check for setUp, setUpClass methods = [m.name for m in node.body if isinstance(m, ast.FunctionDef)] if \"setUp\" in methods: stats[\"uses_setUp\"] += 1 if \"setUpClass\" in methods: stats[\"uses_setUpClass\"] += 1 return stats stats = analyze_test_structure() print(f\"Total tests: {stats["total_tests"]}\") print(f\"Uses setUp: {stats["uses_setUp"]} ({stats["uses_setUp"]/stats["total_tests"]*100:.1f}%)\")\`\`\` Results: 4,200 tests across 180 files (average 23 tests per file). 3,800 class-based tests (90%), 400 function-based (10%). 3,200 use setUp (76%), 1,800 use setUpClass (43%). 15 different test directory structures (inconsistent). 890 tests with duplicate setUp code (21% duplication). Quality issues identified: Flaky tests: 450 (10.7%)—need investigation and fixing. Slow tests: 210 (5%)—need optimization or parallelization. Duplicate setUp: 890 (21%)—migration opportunity to consolidate. Low coverage modules: 12 modules <50%—need new tests. Inconsistent structure: 15 patterns—need standardization. (2) Prioritization strategy: Principle: Migrate high-value tests first (frequent changes, critical path, easy migration). Priority 1 (Weeks 2-4)—New tests only: From now on, ALL new tests must be written in pytest. Prevents problem from growing. Enforcement: Pre-commit hook rejects new unittest tests. PR reviews flag any unittest additions. Training: 2-hour pytest workshop for team. Priority 2 (Weeks 5-8)—Actively developed modules: Modules changed in last 3 months (from git history): \`\`\`bash git log --since=\"3 months ago\" --name-only --pretty=format: | grep \"^src/\" | sort | uniq -c | sort -nr | head -20\`\`\` Top 5 most-changed modules: payments (120 changes), users (95 changes), orders (80 changes), inventory (65 changes), notifications (50 changes). Migrate tests for these modules (estimated 800 tests, 19%). Rationale: Developers actively working on these modules will benefit immediately from pytest features. Easier to get buy-in (\"I just fought with unittest fixtures yesterday!\"). Priority 3 (Weeks 9-12)—Easy migrations (function-based tests): 400 function-based tests (10%) are easiest to migrate. Simple transformations: \`\`\`python# Before (unittest) import unittest class TestCalculator(unittest.TestCase): def test_add(self): self.assertEqual(add(2, 3), 5) # After (pytest) def test_add(): assert add(2, 3) == 5\`\`\` Automated migration script can handle 90% of these. Priority 4 (Weeks 13-20)—Complex migrations (setUp/setUpClass): 3,800 class-based tests with setUp/setUpClass (90%). Require manual or semi-automated migration to fixtures. More time-consuming but highest ROI (eliminate duplication). Priority 5 (Weeks 21-24)—Legacy/stable modules: Modules rarely changed (changed <5 times in last year). Lower priority—can wait or leave as unittest (pytest runs unittest tests). (3) Automated migration tools: Tool 1—unittest2pytest (existing tool): \`\`\`bash pip install unittest2pytest# Migrate single file unittest2pytest tests/test_calculator.py# Migrate entire directory unittest2pytest tests/\`\`\` Handles: self.assertEqual → assert ==. self.assertTrue → assert. self.assertIn → assert in. TestCase classes → plain classes or functions. Limitations: Doesn't handle complex setUp/ fixtures well.Doesn't optimize for pytest features (still looks like unittest). Requires manual review. Tool 2—Custom migration script: \`\`\`python # migrate_to_pytest.py import ast import astunparse # pip install astunparse class UnittestToPytestTransformer(ast.NodeTransformer): \"\"\"Transform unittest AST to pytest AST\"\"\" def visit_ClassDef(self, node): \"\"\"Transform TestCase classes\"\"\" # Remove TestCase inheritance if any(base.id == \"TestCase\" for base in node.bases if isinstance(base, ast.Name)): node.bases = [b for b in node.bases if not (isinstance(b, ast.Name) and b.id == \"TestCase\")] # Transform setUp → fixture setUp_method = None for item in node.body[:]: if isinstance(item, ast.FunctionDef) and item.name == \"setUp\": setUp_method = item node.body.remove(item) if setUp_method: # Create pytest fixture fixture = self.create_fixture_from_setUp(setUp_method) # Add to module-level fixtures self.fixtures.append(fixture) return node def visit_FunctionDef(self, node): \"\"\"Transform test methods\"\"\" # Remove 'self' parameter if first arg is 'self' if node.args.args and node.args.args[0].arg == \"self\": node.args.args = node.args.args[1:] # Transform assertions for i, stmt in enumerate(node.body): if isinstance(stmt, ast.Expr) and isinstance(stmt.value, ast.Call): if isinstance(stmt.value.func, ast.Attribute): if stmt.value.func.attr == \"assertEqual\": # self.assertEqual(a, b) → assert a == b node.body[i] = self.create_assert_equal(stmt.value.args) elif stmt.value.func.attr == \"assertTrue\": # self.assertTrue(x) → assert x node.body[i] = self.create_assert(stmt.value.args[0]) return node def create_assert_equal(self, args): \"\"\"Create assert a == b\"\"\" return ast.Assert( test=ast.Compare( left=args[0], ops=[ast.Eq()], comparators=[args[1]] ), msg=None ) def migrate_file(input_file, output_file): \"\"\"Migrate unittest file to pytest\"\"\" with open(input_file) as f: source = f.read() tree = ast.parse(source) transformer = UnittestToPytestTransformer() new_tree = transformer.visit(tree) new_source = astunparse.unparse(new_tree) with open(output_file, \"w\") as f: f.write(new_source) print(f\"✓ Migrated {input_file} → {output_file}\")\`\`\` Usage: \`\`\`bash python migrate_to_pytest.py tests/test_calculator.py tests/test_calculator_pytest.py\`\`\` Manual migration for complex cases: setUp with database transactions: \`\`\`python # Before (unittest) class TestPayment(unittest.TestCase): def setUp(self): self.db = create_test_db() self.db.begin_transaction() def tearDown(self): self.db.rollback() self.db.close() def test_create_payment(self): payment = Payment(amount=100) self.db.add(payment) self.db.commit() self.assertEqual(payment.id, 1) # After (pytest) @pytest.fixture def db_session(): db = create_test_db() connection = db.connect() transaction = connection.begin() session = Session(bind=connection) yield session session.close() transaction.rollback() connection.close() def test_create_payment(db_session): payment = Payment(amount=100) db_session.add(payment) db_session.commit() assert payment.id == 1\`\`\` setUpClass with expensive operations: \`\`\`python# Before (unittest) class TestIntegration(unittest.TestCase): @classmethod def setUpClass(cls): cls.app = create_app() cls.client = cls.app.test_client() def test_endpoint(self): response = self.client.get(\"/users\") self.assertEqual(response.status_code, 200) # After (pytest with session-scoped fixture) @pytest.fixture(scope=\"class\") def app(): return create_app() @pytest.fixture(scope=\"class\") def client(app): return app.test_client() class TestIntegration: def test_endpoint(self, client): response = client.get(\"/users\") assert response.status_code == 200\`\`\` (4) Running unittest and pytest simultaneously: Configuration for coexistence: pytest.ini: \`\`\`ini [pytest] # Discover both pytest and unittest tests python_files = test_*.py *_test.py python_classes = Test* python_functions = test_* # Don't collect unittest.TestCase classes as pytest classes(let unittest handle them) # This is automatic in recent pytest versions testpaths = tests addopts = -v--tb = short\`\`\` CI/CD dual execution (during transition): \`\`\`yaml #.github / workflows / test.yml name: Tests jobs: unittest - tests: runs - on: ubuntu - latest steps: - uses: actions / checkout@v3 - name: Run unittest tests run: python manage.py test--exclude - tag=pytest - migrated pytest - tests: runs - on: ubuntu - latest steps: - uses: actions / checkout@v3 - name: Run pytest tests run: pytest tests / -m pytest_migrated # Mark migrated tests with @pytest.mark.pytest_migrated\`\`\` Marking migrated tests: \`\`\`python # Migrated test file import pytest @pytest.mark.pytest_migrated def test_create_user(db_session): user = User(username =\"alice\") db_session.add(user) db_session.commit() assert user.id is not None\`\`\` Running both: \`\`\`bash# Run all unittest tests (excludes pytest-migrated) python manage.py test --exclude-tag=pytest-migrated# Run all pytest tests pytest tests/ -m pytest_migrated# Verify no overlap (should sum to 4,200)\`\`\` (5) Team training and preventing new unittest tests: Week 2—Kickoff meeting (1 hour): Present migration plan: Why pytest? (features, speed, less boilerplate). Timeline: 24 weeks for full migration. Expectations: All new tests in pytest starting Week 3. Week 3—Pytest workshop (3 hours): Session 1 (1.5 hours): Pytest basics: fixtures, assert, parametrize. Live coding: Migrate simple unittest test to pytest. Hands-on: Each developer migrates one unittest test. Session 2 (1.5 hours): Django-pytest integration: pytest-django plugin. Database fixtures, transactional tests, API clients. Advanced: Mocking, async tests, markers. Week 4—Documentation: Create internal pytest guide: /docs/testing/pytest-guide.md. Include migration patterns, common pitfalls, FAQ. Add to onboarding docs for new hires. Week 5—Pair programming: Pair senior pytest users with junior developers. Migrate one module together. Review and discuss best practices. Preventing new unittest tests: Pre-commit hook: \`\`\`python # .pre-commit-config.yaml - repo: local hooks: - id: no-new-unittest name: Prevent new unittest tests entry: python scripts/check_no_unittest.py language: system files: ^tests/.*\\.py$ # scripts/check_no_unittest.py import sys import ast def check_no_unittest(filename): \"\"\"Check if file imports unittest (new files only)\"\"\" with open(filename) as f: tree = ast.parse(f.read()) for node in ast.walk(tree): if isinstance(node, ast.Import): for alias in node.names: if alias.name == \"unittest\": print(f\"ERROR: {filename} imports unittest\") print(\"New tests must use pytest. See docs/testing/pytest-guide.md\") return 1 if isinstance(node, ast.ImportFrom): if node.module == \"unittest\": print(f\"ERROR: {filename} imports from unittest\") return 1 return 0 if __name__ == \"__main__\": sys.exit(check_no_unittest(sys.argv[1]))\`\`\` Code review checklist: PR template includes: [ ] If adding tests, are they pytest tests (not unittest)? [ ] Have you migrated any unittest tests you touched? Automated PR comment: Bot comments on PRs touching unittest test files: \"This PR modifies unittest tests. Consider migrating to pytest. See migration guide: [link]\" Timeline summary: Week 1: Assessment and analysis. Weeks 2-4: Setup, training, documentation, enforcement. Weeks 5-8: Migrate actively developed modules (800 tests). Weeks 9-12: Migrate easy function-based tests (400 tests). Weeks 13-20: Migrate complex class-based tests (2,600 tests). Weeks 21-24: Migrate legacy modules (400 tests) or leave as unittest. Result: 4,200 tests migrated over 24 weeks (175 tests/week avg). Zero disruption to development (both frameworks run in parallel). New tests: 100% pytest from Week 3 onward. ROI: Reduced test runtime: 38 min → 12 min with pytest-xdist (parallel). Eliminated fixture duplication: 890 duplicate setUp → consolidated fixtures. Developer satisfaction: 85% prefer pytest after migration. Cost: 120 hours training + 400 hours migration = 520 hours = $130K @ $250/hour. Savings: 26 min saved per test run × 50 runs/day × 20 devs = 433 hours/day. Annual savings: 433 hours/day × 200 days = 86,600 hours = $21.6M @ $250/hour. ROI: Pays for itself in 1 day. Conclusion: Gradual migration (24 weeks) eliminates disruption. Prioritize high-value tests (new, actively developed, easy). Use automated tools (unittest2pytest) + custom scripts for 80%, manual for 20%. Run both frameworks during transition (pytest runs unittest tests). Prevent regression with pre-commit hooks and training.`,
    keyPoints: [
      'Assessment: Coverage 67% (12 modules <50%), 450 flaky tests (10.7%), 890 duplicate setUp (21% duplication)',
      'Prioritization: New tests first (Week 3+), actively developed modules (Weeks 5-8, 800 tests), easy migrations (Weeks 9-12, 400 tests)',
      'Automation: unittest2pytest tool + custom AST transformer for 80%, manual migration for complex setUp/fixtures (20%)',
      'Coexistence: pytest runs unittest tests, dual CI/CD (unittest + pytest jobs), mark migrated tests @pytest.mark.pytest_migrated',
      'Prevention: Pre-commit hook rejects new unittest, PR template checklist, 3-hour workshop, 24-week timeline (175 tests/week)',
    ],
  },
];
