export const fixturesDeepDiveQuiz = [
  {
    id: 'fdd-q-1',
    question:
      'Design a fixture strategy for a Django REST Framework application with complex testing requirements: (1) database fixtures that handle both PostgreSQL (production) and SQLite (CI), (2) API client fixtures with different authentication levels (anonymous, user, admin, superuser), (3) test data factories for 12 models with complex relationships, (4) fixture scope optimization to reduce test time from 45 minutes to <10 minutes, (5) handling fixture cleanup when tests fail mid-execution. Include specific code examples and performance analysis.',
    sampleAnswer:
      'Comprehensive fixture strategy for Django REST Framework: (1) Multi-database fixtures (PostgreSQL + SQLite): Challenge: Production uses PostgreSQL, CI uses SQLite (faster). Need fixtures that work with both. Solution—Conditional database fixture: ```python # conftest.py import pytest import os from django.conf import settings from django.db import connection @pytest.fixture(scope=\"session\") def django_db_setup(django_db_setup, django_db_blocker): \"\"\"Setup test database (PostgreSQL or SQLite)\"\"\" with django_db_blocker.unblock(): # Create schema if os.environ.get(\"CI\"): # CI: Use SQLite (faster) settings.DATABASES[\"default\"] = { \"ENGINE\": \"django.db.backends.sqlite3\", \"NAME\": \":memory:\", # In-memory (even faster) } else: # Local: Use PostgreSQL (matches production) settings.DATABASES[\"default\"] = { \"ENGINE\": \"django.db.backends.postgresql\", \"NAME\": \"testdb\", \"USER\": \"test\", \"PASSWORD\": \"test\", \"HOST\": \"localhost\", } # Create all tables from django.core.management import call_command call_command(\"migrate\", verbosity=0, interactive=False) @pytest.fixture(scope=\"function\") def db(django_db_setup, django_db_blocker): \"\"\"Database fixture with transaction rollback\"\"\" with django_db_blocker.unblock(): # Start transaction connection.set_autocommit(False) yield # Rollback transaction (clean database) connection.rollback() connection.set_autocommit(True)``` Performance: SQLite in-memory: 45 min → 12 min (4× faster). PostgreSQL on local: Matches production exactly. Best of both: Fast CI, accurate local testing. (2) API client fixtures with authentication levels: Problem: Need to test endpoints with different permissions: anonymous, authenticated user, admin, superuser. Solution—Tiered authentication fixtures: ```python # conftest.py import pytest from rest_framework.test import APIClient from django.contrib.auth import get_user_model User = get_user_model() @pytest.fixture def api_client(): \"\"\"Unauthenticated API client\"\"\" return APIClient() @pytest.fixture def user(db): \"\"\"Standard user (no special permissions)\"\"\" return User.objects.create_user( username=\"testuser\", email=\"test@example.com\", password=\"password123\" ) @pytest.fixture def admin_user(db): \"\"\"Admin user (staff permissions)\"\"\" return User.objects.create_user( username=\"admin\", email=\"admin@example.com\", password=\"password123\", is_staff=True ) @pytest.fixture def superuser(db): \"\"\"Superuser (all permissions)\"\"\" return User.objects.create_superuser( username=\"super\", email=\"super@example.com\", password=\"password123\" ) @pytest.fixture def user_api_client(api_client, user): \"\"\"API client authenticated as regular user\"\"\" api_client.force_authenticate(user=user) return api_client @pytest.fixture def admin_api_client(api_client, admin_user): \"\"\"API client authenticated as admin\"\"\" api_client.force_authenticate(user=admin_user) return api_client @pytest.fixture def superuser_api_client(api_client, superuser): \"\"\"API client authenticated as superuser\"\"\" api_client.force_authenticate(user=superuser) return api_client``` Usage: ```python def test_anonymous_access(api_client): \"\"\"Anonymous users can list products\"\"\" response = api_client.get(\"/api/products/\") assert response.status_code == 200 def test_user_can_create_order(user_api_client): \"\"\"Authenticated users can create orders\"\"\" response = user_api_client.post(\"/api/orders/\", {\"product_id\": 1}) assert response.status_code == 201 def test_admin_can_delete_user(admin_api_client): \"\"\"Admins can delete users\"\"\" response = admin_api_client.delete(\"/api/users/1/\") assert response.status_code == 204 def test_superuser_can_access_admin_panel(superuser_api_client): \"\"\"Superusers can access admin endpoints\"\"\" response = superuser_api_client.get(\"/api/admin/settings/\") assert response.status_code == 200``` Benefits: Clear permission testing: Each test explicitly states required permission. Reusable: 4 authentication levels cover all scenarios. Composable: Build higher-level fixtures on top. (3) Test data factories for 12 models with complex relationships: Problem: 12 models (User, Product, Order, OrderItem, Payment, Address, Review, Cart, CartItem, Category, Tag, Coupon) with foreign keys and many-to-many relationships. Solution—Factory Boy factories: ```python # conftest.py import pytest import factory from factory import fuzzy from decimal import Decimal # ========== Factories ========== class UserFactory(factory.django.DjangoModelFactory): class Meta: model = \"auth.User\" username = factory.Sequence(lambda n: f\"user{n}\") email = factory.LazyAttribute(lambda obj: f\"{obj.username}@example.com\") password = factory.PostGenerationMethodCall(\"set_password\", \"password123\") class CategoryFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Category\" name = factory.Sequence(lambda n: f\"Category {n}\") slug = factory.LazyAttribute(lambda obj: obj.name.lower().replace(\" \", \"-\")) class TagFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Tag\" name = factory.Sequence(lambda n: f\"Tag {n}\") class ProductFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Product\" name = factory.Faker(\"word\") description = factory.Faker(\"text\") price = fuzzy.FuzzyDecimal(10.0, 1000.0, 2) stock = fuzzy.FuzzyInteger(0, 100) category = factory.SubFactory(CategoryFactory) @factory.post_generation def tags(self, create, extracted, **kwargs): \"\"\"Add tags to product\"\"\" if not create: return if extracted: for tag in extracted: self.tags.add(tag) else: self.tags.add(TagFactory()) class AddressFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Address\" user = factory.SubFactory(UserFactory) street = factory.Faker(\"street_address\") city = factory.Faker(\"city\") state = factory.Faker(\"state_abbr\") zipcode = factory.Faker(\"zipcode\") class OrderFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Order\" user = factory.SubFactory(UserFactory) status = \"pending\" total = Decimal(\"0.00\") shipping_address = factory.SubFactory(AddressFactory) class OrderItemFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.OrderItem\" order = factory.SubFactory(OrderFactory) product = factory.SubFactory(ProductFactory) quantity = fuzzy.FuzzyInteger(1, 10) price = factory.LazyAttribute(lambda obj: obj.product.price) class PaymentFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Payment\" order = factory.SubFactory(OrderFactory) amount = factory.LazyAttribute(lambda obj: obj.order.total) status = \"completed\" transaction_id = factory.Faker(\"uuid4\") class ReviewFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Review\" product = factory.SubFactory(ProductFactory) user = factory.SubFactory(UserFactory) rating = fuzzy.FuzzyInteger(1, 5) comment = factory.Faker(\"text\") class CouponFactory(factory.django.DjangoModelFactory): class Meta: model = \"shop.Coupon\" code = factory.Sequence(lambda n: f\"COUPON{n}\") discount_percent = fuzzy.FuzzyInteger(5, 50) active = True # ========== Fixture Wrappers ========== @pytest.fixture def user_factory(): return UserFactory @pytest.fixture def product_factory(): return ProductFactory @pytest.fixture def order_factory(): return OrderFactory @pytest.fixture def order_item_factory(): return OrderItemFactory @pytest.fixture def payment_factory(): return PaymentFactory @pytest.fixture def review_factory(): return ReviewFactory @pytest.fixture def coupon_factory(): return CouponFactory # ========== Common Fixtures ========== @pytest.fixture def user(user_factory): \"\"\"Standard test user\"\"\" return user_factory.create() @pytest.fixture def product(product_factory): \"\"\"Standard test product\"\"\" return product_factory.create(price=Decimal(\"99.99\"), stock=10) @pytest.fixture def order_with_items(order_factory, order_item_factory): \"\"\"Order with 3 items\"\"\" order = order_factory.create() items = [ order_item_factory.create(order=order, quantity=2), order_item_factory.create(order=order, quantity=1), order_item_factory.create(order=order, quantity=3), ] # Calculate total order.total = sum(item.price * item.quantity for item in items) order.save() return order``` Usage: ```python def test_create_order(user, product_factory, order_factory): \"\"\"Test order creation with products\"\"\" # Create 3 products product1 = product_factory.create(price=Decimal(\"10.00\")) product2 = product_factory.create(price=Decimal(\"20.00\")) product3 = product_factory.create(price=Decimal(\"30.00\")) # Create order order = order_factory.create(user=user) # Add items OrderItem.objects.create(order=order, product=product1, quantity=2, price=product1.price) OrderItem.objects.create(order=order, product=product2, quantity=1, price=product2.price) OrderItem.objects.create(order=order, product=product3, quantity=1, price=product3.price) # Calculate total order.total = Decimal(\"70.00\") # (10*2) + (20*1) + (30*1) order.save() assert order.total == Decimal(\"70.00\")``` Benefits: Realistic data: Faker generates realistic names, addresses, etc. Relationships: SubFactory handles foreign keys automatically. Customizable: Override any field (product_factory.create(price=Decimal(\"50.00\"))). Reusable: 12 factories × 100 tests = 1,200 uses (define once). (4) Fixture scope optimization (45 min → <10 min): Problem: All fixtures use default function scope → expensive setup repeated 2,500 times. Analysis: ```python# Before (all function-scoped) @pytest.fixture def db_session(): # Created 2,500 times (once per test) return create_database_connection() @pytest.fixture def api_client(): # Created 2,500 times return APIClient() # Total time: 45 minutes``` Solution—Strategic scope optimization: ```python # After (optimized scopes) @pytest.fixture(scope=\"session\") def django_db_setup(): # Created ONCE per test session \"\"\"Setup database schema\"\"\" # 30 seconds setup (once) @pytest.fixture(scope=\"module\") def api_client(): # Created once per file \"\"\"API client (reused across file)\"\"\" # 0.1 seconds × 25 files = 2.5 seconds return APIClient() @pytest.fixture(scope=\"function\") def user(db): # Created per test (needs isolation) \"\"\"Test user (clean for each test)\"\"\" return UserFactory.create() @pytest.fixture(scope=\"function\") def order_with_items(user): # Depends on function-scoped user \"\"\"Order with items (clean for each test)\"\"\" return OrderFactory.create(user=user)``` Performance optimization strategy: Identify expensive fixtures: Database setup: 30s → session scope (created once). API client: 0.1s × 2,500 = 250s → module scope (created 25 times). User factory: 0.01s × 2,500 = 25s → function scope (needs isolation). Guidelines: Session scope: Database schema, browser launch (very expensive, read-only). Module scope: API clients, service instances (expensive, mostly stateless). Function scope: Test data (cheap, needs isolation). Avoid session scope for stateful fixtures: Can cause test interdependence. Results: Before: 45 minutes (function scope for everything). After: Database setup: 30s (once) instead of 75 minutes. API client: 2.5s (25× ) instead of 250s. Test data: 25s (function-scoped, as needed). Total: 30s + 2.5s + 25s + test execution (7 min) = 9.5 minutes. Speedup: 45 min → 9.5 min (4.7× faster). (5) Fixture cleanup when tests fail: Problem: Test fails mid-execution → resources not cleaned up (database connections, files, processes). Solution—Guaranteed cleanup with yield and finalizers: ```python @pytest.fixture def database_connection(request): \"\"\"Database connection with guaranteed cleanup\"\"\" print(\"Opening database connection...\") conn = DatabaseConnection() conn.connect() # Method 1: yield (preferred) yield conn # Cleanup runs even if test fails print(\"Closing database connection...\") conn.close() @pytest.fixture def temporary_file(request, tmp_path): \"\"\"Temporary file with finalizer\"\"\" file_path = tmp_path / \"test_data.txt\" file_path.write_text(\"test data\") # Method 2: request.addfinalizer def cleanup(): print(f\"Deleting {file_path}\") file_path.unlink(missing_ok=True) request.addfinalizer(cleanup) return file_path @pytest.fixture def background_process(): \"\"\"Background process with guaranteed termination\"\"\" import subprocess proc = subprocess.Popen([\"python\", \"server.py\"]) try: yield proc finally: # Guaranteed cleanup (even on exception) print(\"Terminating process...\") proc.terminate() proc.wait(timeout=5)``` Test with failure: ```python def test_that_fails(database_connection, temporary_file): \"\"\"Test that fails mid-execution\"\"\" # Do some work data = database_connection.query(\"SELECT * FROM users\") # Test fails here raise Exception(\"Something went wrong!\") # database_connection.close() still called # temporary_file still deleted # Cleanup guaranteed```  Advanced error handling: ```python @pytest.fixture def complex_fixture(): \"\"\"Fixture with multiple cleanup steps\"\"\" resources = [] try: # Setup step 1 db = create_database() resources.append((\"db\", db)) # Setup step 2 cache = create_cache() resources.append((\"cache\", cache)) # Setup step 3 queue = create_queue() resources.append((\"queue\", queue)) yield {\"db\": db, \"cache\": cache, \"queue\": queue} finally: # Cleanup in reverse order (LIFO) for name, resource in reversed(resources): try: print(f\"Cleaning up {name}...\") resource.close() except Exception as e: # Log but don\'t fail cleanup print(f\"Error cleaning {name}: {e}\")``` Summary: Multi-database: Conditional fixture (PostgreSQL local, SQLite CI), 4× faster CI. Authentication: Tiered fixtures (anonymous, user, admin, superuser), clear permission testing. Factories: Factory Boy for 12 models, handles relationships, Faker for realistic data. Scope optimization: Strategic scoping (session/module/function), 45 min → 9.5 min (4.7× faster). Cleanup: yield and finalizers guarantee cleanup even on failure, prevents resource leaks. Result: Robust, fast, maintainable test suite for Django REST Framework application.',
    keyPoints: [
      'Multi-database: Conditional fixture (PostgreSQL local, SQLite in-memory CI), 4× faster CI while matching production',
      'Authentication tiers: api_client, user_api_client, admin_api_client, superuser_api_client (4 permission levels)',
      'Factory Boy: 12 model factories with SubFactory for relationships, Faker for realistic data, define once use 1,200×',
      'Scope optimization: Session (DB schema, 30s once), Module (API client, 2.5s), Function (test data, 25s) → 45 min to 9.5 min',
      'Guaranteed cleanup: yield in fixtures, request.addfinalizer, finally blocks—cleanup runs even when tests fail',
    ],
  },
  {
    id: 'fdd-q-2',
    question:
      'Compare fixture scope strategies for different testing scenarios: (1) function scope vs. module scope for database fixtures, (2) when session scope is appropriate vs. dangerous, (3) performance vs. test isolation tradeoffs with concrete examples, (4) handling scope conflicts (function-scoped fixture depending on session-scoped fixture), (5) measuring and optimizing fixture execution time. For a test suite with 5,000 tests taking 60 minutes, provide a systematic approach to reduce to <15 minutes without sacrificing test quality.',
    sampleAnswer:
      'Fixture scope analysis and optimization: (1) Function vs. module scope for database fixtures: Function scope (default): ```python @pytest.fixture(scope=\"function\") def db_session(): \"\"\"New database session for each test\"\"\" engine = create_engine(\"postgresql://...\") Session = sessionmaker(bind=engine) session = Session() yield session session.close()``` Pros: Perfect test isolation: Each test gets clean database. No test interdependence: Test order doesn\'t matter. Can be parallelized: Tests don\'t interfere. Cons: Slow: 5,000 tests × 0.1s setup = 500s (8.3 minutes) just for setup. Expensive: Creating connection for each test. Module scope (shared within file): ```python @pytest.fixture(scope=\"module\") def db_session(): \"\"\"Shared database session for all tests in file\"\"\" engine = create_engine(\"postgresql://...\") Session = sessionmaker(bind=engine) session = Session() yield session session.rollback() # Clean up any uncommitted changes session.close()``` Pros: Faster: 200 test files × 0.1s = 20s setup (vs. 500s). Reuses connection: Amortized cost. Cons: Risk of test interdependence: If test doesn\'t clean up, affects next test. Harder to debug: Failure in test 50 might be caused by test 10. Parallelization complex: Can\'t run tests in same file in parallel. Best practice—Hybrid approach: ```python @pytest.fixture(scope=\"session\") def db_engine(): \"\"\"Database engine (created once)\"\"\" engine = create_engine(\"postgresql://...\") Base.metadata.create_all(engine) yield engine Base.metadata.drop_all(engine) engine.dispose() @pytest.fixture(scope=\"function\") def db_session(db_engine): \"\"\"Clean session per test (using shared engine)\"\"\" connection = db_engine.connect() transaction = connection.begin() Session = sessionmaker(bind=connection) session = Session() yield session session.close() transaction.rollback() # Rollback = clean state connection.close()``` Benefits: Fast: Engine created once (expensive), connections per-test (cheap). Isolated: Transaction rollback ensures clean state per test. Parallelizable: Each test independent. Performance: Engine creation: 10s once (instead of 50,000s total). Connection per test: 5,000 × 0.01s = 50s. Total: 10s + 50s = 60s (vs. 500s before, 8× faster). (2) When session scope is appropriate vs. dangerous: Session scope appropriate for: Read-only expensive setup: ```python @pytest.fixture(scope=\"session\") def database_schema(): \"\"\"Create database schema (read-only)\"\"\" engine = create_engine(\"postgresql://...\") Base.metadata.create_all(engine) # Create tables (once) yield engine Base.metadata.drop_all(engine)``` Browser launch (E2E tests): ```python @pytest.fixture(scope=\"session\") def browser(): \"\"\"Launch browser once\"\"\" from selenium import webdriver driver = webdriver.Chrome() yield driver driver.quit()``` Configuration loading: ```python @pytest.fixture(scope=\"session\") def app_config(): \"\"\"Load config once\"\"\" return load_config(\"config.yaml\")``` Session scope DANGEROUS for: Mutable state: ```python # DANGEROUS @pytest.fixture(scope=\"session\") def user_list(): \"\"\"Shared list across all tests\"\"\" return [] # Problem: Tests modify this list, affecting other tests def test_1(user_list): user_list.append(\"Alice\") # Modifies shared state! def test_2(user_list): assert len(user_list) == 0 # FAILS if test_1 ran first``` Test data with state: ```python # DANGEROUS @pytest.fixture(scope=\"session\") def test_user(): \"\"\"Shared user across all tests\"\"\" return User(name=\"Alice\", balance=100) # Problem: Tests modify user, affecting other tests def test_spend_money(test_user): test_user.balance -= 50 # Modifies shared user! def test_check_balance(test_user): assert test_user.balance == 100 # FAILS if test_spend_money ran first``` Rule: Session scope only for immutable/read-only resources. (3) Performance vs. isolation tradeoffs: Scenario 1—Unit tests (fast, isolated): ```python @pytest.fixture(scope=\"function\") def calculator(): \"\"\"New calculator for each test (fast)\"\"\" return Calculator() # 0.001s per test # 1,000 tests × 0.001s = 1s setup # Isolation: Perfect # Performance: Excellent``` Verdict: Function scope appropriate (setup is cheap, isolation critical). Scenario 2—Integration tests (database): ```python # Option A: Function scope (isolated but slow) @pytest.fixture(scope=\"function\") def db(): return create_database() # 0.1s per test # 1,000 tests × 0.1s = 100s setup # Isolation: Perfect # Performance: Slow # Option B: Module scope (fast but risky) @pytest.fixture(scope=\"module\") def db(): return create_database() # 0.1s per file # 50 files × 0.1s = 5s setup # Isolation: Risky (if tests don\'t clean up) # Performance: Excellent (20× faster) # Option C: Hybrid (best of both) @pytest.fixture(scope=\"session\") def db_engine(): return create_engine() # 10s once @pytest.fixture(scope=\"function\") def db_session(db_engine): connection = db_engine.connect() transaction = connection.begin() session = Session(bind=connection) yield session session.close() transaction.rollback() # Isolation: Perfect (transaction rollback) # Performance: Good (10s + 1,000 × 0.01s = 20s)``` Verdict: Hybrid approach (session engine + function session) best. Scenario 3—E2E tests (browser): ```python # Option A: Function scope (isolated but extremely slow) @pytest.fixture(scope=\"function\") def browser(): return webdriver.Chrome() # 5s per test # 100 tests × 5s = 500s (8.3 minutes) # Isolation: Perfect # Performance: Terrible # Option B: Session scope (fast but dangerous) @pytest.fixture(scope=\"session\") def browser(): return webdriver.Chrome() # 5s once # Isolation: Dangerous (shared state in browser) # Performance: Excellent # Option C: Module scope with cleanup (balanced) @pytest.fixture(scope=\"module\") def browser(): driver = webdriver.Chrome() yield driver driver.delete_all_cookies() # Clean state driver.refresh() # Reset page # 10 files × 5s = 50s setup # Isolation: Good (if cleanup thorough) # Performance: Good (10× faster)``` Verdict: Module scope with cleanup (balance). (4) Handling scope conflicts: Problem: Function-scoped fixture depends on session-scoped fixture. ```python @pytest.fixture(scope=\"session\") def app(): \"\"\"Application instance (session-scoped)\"\"\" return create_app() @pytest.fixture(scope=\"function\") def client(app): \"\"\"API client (function-scoped, depends on session-scoped app)\"\"\" return app.test_client() # Works fine: Lower scope can depend on higher scope # Each test gets new client, but reuses same app``` Problem: Session-scoped fixture depends on function-scoped fixture. ```python @pytest.fixture(scope=\"function\") def config(): \"\"\"Config (function-scoped)\"\"\" return {\"debug\": True} @pytest.fixture(scope=\"session\") def app(config): # ERROR! \"\"\"App depends on function-scoped config\"\"\" return create_app(config) # ERROR: ScopeMismatch # Session-scoped fixture cannot depend on function-scoped fixture # (Session fixture created once, function fixture created per-test)``` Solution 1—Elevate dependency scope: ```python @pytest.fixture(scope=\"session\") # Changed to session def config(): \"\"\"Config (session-scoped)\"\"\" return {\"debug\": True} @pytest.fixture(scope=\"session\") def app(config): return create_app(config) # Works now``` Solution 2—Use indirect parametrization: ```python @pytest.fixture(scope=\"session\") def app(): return create_app() @pytest.fixture(scope=\"function\") def configured_client(app, request): \"\"\"Client configured per-test\"\"\" config = request.param # Get config from parametrize app.config.update(config) return app.test_client() @pytest.mark.parametrize(\"configured_client\", [{\"debug\": True}], indirect=True) def test_with_debug(configured_client): ...``` (5) Systematic approach to reduce 60 min to <15 min (5,000 tests): Step 1—Measure current fixture execution (Week 1): ```python # Install pytest-benchmark pip install pytest-profiling # Run with profiling pytest --profile # Analyze fixture execution time``` Results: Total: 60 minutes. Database setup: 5,000 tests × 0.1s = 500s (8.3 min). API client creation: 5,000 tests × 0.05s = 250s (4.2 min). Test data factories: 5,000 tests × 0.02s = 100s (1.7 min). Test execution: 5,000 tests × 0.4s = 2,000s (33.3 min). Teardown: 5,000 tests × 0.05s = 250s (4.2 min). Total: 3,100s (51.7 min). Step 2—Optimize fixture scopes (Week 2): Database: Function → Session engine + Function session: ```python # Before @pytest.fixture def db_session(): engine = create_engine() session = Session(bind=engine) yield session session.close() engine.dispose() # 5,000 × 0.1s = 500s # After @pytest.fixture(scope=\"session\") def db_engine(): engine = create_engine() yield engine engine.dispose() # Once: 10s @pytest.fixture def db_session(db_engine): connection = db_engine.connect() transaction = connection.begin() session = Session(bind=connection) yield session session.close() transaction.rollback() # 5,000 × 0.01s = 50s # Savings: 500s → 60s (440s saved)``` API client: Function → Module: ```python # Before @pytest.fixture def api_client(): return APIClient() # 5,000 × 0.05s = 250s # After @pytest.fixture(scope=\"module\") def api_client(): return APIClient() # 200 files × 0.05s = 10s # Savings: 250s → 10s (240s saved)``` Step 3—Parallelize test execution (Week 3): ```bash # Install pytest-xdist pip install pytest-xdist # Run with 8 cores pytest -n 8 # Parallelization speedup # Test execution: 2,000s / 8 = 250s # Teardown: 250s / 8 = 31s # Total execution: 250s + 31s = 281s (4.7 min)``` Step 4—New total time: Database: 60s (session engine + function sessions). API client: 10s (module-scoped). Factories: 100s (unchanged, per-test needed). Test execution: 281s (parallelized). Teardown: 31s (parallelized). Total: 60 + 10 + 100 + 281 + 31 = 482s (8 minutes). Step 5—Further optimizations (Week 4): Use in-memory SQLite for CI: ```python @pytest.fixture(scope=\"session\") def db_engine(): if os.environ.get(\"CI\"): engine = create_engine(\"sqlite:///:memory:\") # 10× faster else: engine = create_engine(\"postgresql://...\") yield engine``` Savings: 60s → 6s in CI (54s saved). Reduce test data creation: ```python # Use cached factories @pytest.fixture(scope=\"module\") def standard_user(): \"\"\"Standard user (reused across module)\"\"\" return UserFactory.create(name=\"Standard User\")``` Savings: 50s (50% of factory time). Final time: Database: 6s (SQLite in-memory in CI). API client: 10s (module-scoped). Factories: 50s (cached). Test execution: 281s (parallelized). Teardown: 31s. Total: 6 + 10 + 50 + 281 + 31 = 378s (6.3 minutes). Summary: Week 1: Measure (identify bottlenecks: database 8.3 min, API client 4.2 min). Week 2: Optimize scopes (session engine, module client: save 680s). Week 3: Parallelize (8 cores: save 1,719s). Week 4: Further optimize (SQLite in-memory, cached factories: save 104s). Result: 60 minutes → 6.3 minutes (9.5× faster). Quality: Test isolation maintained (transaction rollback). Parallelization enabled (independent tests).',
    keyPoints: [
      'Function vs. module DB: Hybrid best (session engine + function session w/ rollback), 8× faster with isolation',
      'Session scope: Safe for read-only (schema, browser, config), DANGEROUS for mutable state (shared lists, test data)',
      'Scope conflicts: Lower scope can depend on higher (function → session OK), higher → lower ERROR (session → function fails)',
      'Systematic optimization: Measure (60 min), optimize scopes (save 680s), parallelize (save 1,719s), SQLite in-memory (save 54s)',
      'Result: 60 min → 6.3 min (9.5× faster) while maintaining isolation via transaction rollback and independent tests',
    ],
  },
  {
    id: 'fdd-q-3',
    question:
      'Design a fixture architecture for testing a microservices system with 10 services where tests need: (1) mock vs. real service dependencies, (2) fixture sharing across services (shared utilities package), (3) docker-compose integration for integration tests, (4) test data that spans multiple services, (5) fixture versioning (when shared fixtures change). Include specific examples of fixture composition, dependency injection, and backward compatibility strategies.',
    sampleAnswer:
      'Microservices fixture architecture (10 services): (1) Mock vs. real service dependencies: Challenge: Service A depends on Service B, C, D. Should we mock dependencies or use real services? Strategy—Layered testing: Unit tests: Mock all dependencies (fast, isolated). Integration tests: Real dependencies (accurate, slow). Contract tests: Mock with verified contracts. Unit tests with mocks: ```python # service_a/tests/unit/test_payment_service.py import pytest from unittest.mock import Mock @pytest.fixture def mock_user_service(): \"\"\"Mock Service B (User Service)\"\"\" mock = Mock() mock.get_user.return_value = { \"id\": \"user_123\", \"name\": \"Alice\", \"email\": \"alice@example.com\" } return mock @pytest.fixture def mock_inventory_service(): \"\"\"Mock Service C (Inventory Service)\"\"\" mock = Mock() mock.check_stock.return_value = {\"available\": True, \"quantity\": 10} mock.reserve_stock.return_value = True return mock @pytest.fixture def payment_service(mock_user_service, mock_inventory_service): \"\"\"Payment service with mocked dependencies\"\"\" from service_a.payment_service import PaymentService return PaymentService( user_service=mock_user_service, inventory_service=mock_inventory_service ) def test_process_payment_success(payment_service): \"\"\"Test payment processing (mocked dependencies)\"\"\" result = payment_service.process_payment( user_id=\"user_123\", product_id=\"prod_456\", amount=100.0 ) assert result.status == \"success\" # Fast: <100ms (no real services)``` Integration tests with real services: ```python # service_a/tests/integration/test_payment_integration.py import pytest import requests @pytest.fixture(scope=\"module\") def docker_services(docker_compose_file): \"\"\"Start all services with docker-compose\"\"\" import subprocess # Start services subprocess.run([\"docker-compose\", \"-f\", docker_compose_file, \"up\", \"-d\"], check=True) # Wait for health checks import time time.sleep(10) # Or use proper health check polling yield # Teardown subprocess.run([\"docker-compose\", \"down\"], check=True) def test_payment_with_real_services(docker_services): \"\"\"Test payment with real User and Inventory services\"\"\" # Create user in User Service response = requests.post(\"http://localhost:8001/users\", json={ \"name\": \"Alice\", \"email\": \"alice@example.com\" }) user_id = response.json()[\"id\"] # Add product to Inventory Service requests.post(\"http://localhost:8002/products\", json={ \"id\": \"prod_456\", \"stock\": 10 }) # Process payment in Payment Service response = requests.post(\"http://localhost:8003/payments\", json={ \"user_id\": user_id, \"product_id\": \"prod_456\", \"amount\": 100.0 }) assert response.status_code == 200 assert response.json()[\"status\"] == \"success\" # Verify inventory was decremented inventory = requests.get(\"http://localhost:8002/products/prod_456\").json() assert inventory[\"stock\"] == 9 # Slow: ~1s (real services)``` Contract tests: ```python # service_a/tests/contract/test_user_service_contract.py import pytest from pact import Consumer, Provider @pytest.fixture def user_service_contract(): \"\"\"Contract for User Service API\"\"\" pact = Consumer(\"PaymentService\").has_pact_with(Provider(\"UserService\")) pact.given(\"user exists\") .upon_receiving(\"GET user by ID\") .with_request(method=\"GET\", path=\"/users/user_123\") .will_respond_with(status=200, body={ \"id\": \"user_123\", \"name\": \"Alice\", \"email\": \"alice@example.com\" }) return pact def test_get_user_contract(user_service_contract): \"\"\"Verify Payment Service uses User Service correctly\"\"\" with user_service_contract: response = user_service.get_user(\"user_123\") assert response[\"name\"] == \"Alice\"``` (2) Fixture sharing across services (shared utilities): Problem: 10 services × duplicate fixtures = 10 copies of same db_session, api_client, factories. Solution—shared-test-utils package: ```monorepo/ ├── services/ │ ├── service_a/ │ ├── service_b/ │ └── service_c/ └── shared-test-utils/ ├── setup.py ├── shared_test_utils/ │ ├── fixtures/ │ │ ├── database.py │ │ ├── http_client.py │ │ └── docker.py │ ├── factories/ │ │ ├── user.py │ │ └── product.py │ └── mocks/ │ ├── service_a.py │ ├── service_b.py │ └── service_c.py └── tests/ └── test_fixtures.py``` shared_test_utils/fixtures/database.py: ```python import pytest from sqlalchemy import create_engine from sqlalchemy.orm import sessionmaker @pytest.fixture(scope=\"session\") def db_engine(): \"\"\"Database engine (shared across all services)\"\"\" engine = create_engine(\"postgresql://test:test@localhost/testdb\") yield engine engine.dispose() @pytest.fixture(scope=\"function\") def db_session(db_engine): \"\"\"Clean database session\"\"\" connection = db_engine.connect() transaction = connection.begin() Session = sessionmaker(bind=connection) session = Session() yield session session.close() transaction.rollback() connection.close()``` shared_test_utils/mocks/service_b.py: ```python import pytest from unittest.mock import Mock @pytest.fixture def mock_service_b(): \"\"\"Mock Service B API\"\"\" mock = Mock() mock.get_data.return_value = {\"status\": \"success\"} return mock``` Usage in Service A: ```python # service_a/conftest.py from shared_test_utils.fixtures.database import db_engine, db_session from shared_test_utils.mocks.service_b import mock_service_b # All fixtures now available to Service A tests # service_a/tests/test_something.py def test_with_shared_fixtures(db_session, mock_service_b): \"\"\"Use shared fixtures (no duplication)\"\"\" user = User(name=\"Alice\") db_session.add(user) db_session.commit() result = mock_service_b.get_data() assert result[\"status\"] == \"success\"``` Benefits: Define once, use 10× (in all services). Consistency: All services use same fixtures. Maintainability: Change once, propagates to all services. (3) Docker-compose integration for integration tests: docker-compose.test.yml: ```yaml version: \'3.8\' services: service_a: build: ./services/service_a ports: - \"8001:8000\" environment: - DATABASE_URL=postgresql://test:test@postgres/testdb - SERVICE_B_URL=http://service_b:8000 depends_on: - postgres - service_b service_b: build: ./services/service_b ports: - \"8002:8000\" environment: - DATABASE_URL=postgresql://test:test@postgres/testdb depends_on: - postgres service_c: build: ./services/service_c ports: - \"8003:8000\" postgres: image: postgres:15 environment: - POSTGRES_DB=testdb - POSTGRES_USER=test - POSTGRES_PASSWORD=test ports: - \"5432:5432\" redis: image: redis:7 ports: - \"6379:6379\"``` Fixture for docker-compose: ```python # shared_test_utils/fixtures/docker.py import pytest import subprocess import time import requests @pytest.fixture(scope=\"session\") def docker_compose_services(): \"\"\"Start all services with docker-compose\"\"\" # Start services print(\"Starting docker-compose services...\") subprocess.run( [\"docker-compose\", \"-f\", \"docker-compose.test.yml\", \"up\", \"-d\"], check=True ) # Wait for services to be healthy def wait_for_service(url, timeout=60): start = time.time() while time.time() - start < timeout: try: response = requests.get(f\"{url}/health\") if response.status_code == 200: return True except requests.exceptions.ConnectionError: pass time.sleep(1) raise TimeoutError(f\"Service {url} not healthy after {timeout}s\") wait_for_service(\"http://localhost:8001\") # Service A wait_for_service(\"http://localhost:8002\") # Service B wait_for_service(\"http://localhost:8003\") # Service C print(\"All services ready\") yield # Teardown print(\"Stopping docker-compose services...\") subprocess.run([\"docker-compose\", \"-f\", \"docker-compose.test.yml\", \"down\"], check=True)``` Usage: ```python # tests/integration/test_cross_service.py def test_payment_flow_across_services(docker_compose_services): \"\"\"Test flow spanning Service A, B, C\"\"\" # Create user in Service B response = requests.post(\"http://localhost:8002/users\", json={\"name\": \"Alice\"}) user_id = response.json()[\"id\"] # Process payment in Service A response = requests.post(\"http://localhost:8001/payments\", json={ \"user_id\": user_id, \"amount\": 100.0 }) payment_id = response.json()[\"id\"] assert response.status_code == 200 # Verify notification sent by Service C notifications = requests.get(f\"http://localhost:8003/notifications?user_id={user_id}\").json() assert len(notifications) > 0``` (4) Test data spanning multiple services: Challenge: Test requires user in Service B, product in Service C, payment in Service A. Solution—Cross-service test data factories: ```python # shared_test_utils/factories/cross_service.py import requests class CrossServiceFactory: \"\"\"Factory for creating test data across services\"\"\" def __init__(self, base_urls): self.base_urls = base_urls def create_user(self, name=\"Test User\", email=None): \"\"\"Create user in User Service (Service B)\"\"\" response = requests.post(f\"{self.base_urls[\'user_service\']}/users\", json={ \"name\": name, \"email\": email or f\"{name}@example.com\" }) return response.json() def create_product(self, name=\"Test Product\", price=100.0, stock=10): \"\"\"Create product in Inventory Service (Service C)\"\"\" response = requests.post(f\"{self.base_urls[\'inventory_service\']}/products\", json={ \"name\": name, \"price\": price, \"stock\": stock }) return response.json() def create_payment(self, user_id, product_id, amount): \"\"\"Create payment in Payment Service (Service A)\"\"\" response = requests.post(f\"{self.base_urls[\'payment_service\']}/payments\", json={ \"user_id\": user_id, \"product_id\": product_id, \"amount\": amount }) return response.json() @pytest.fixture def cross_service_factory(docker_compose_services): \"\"\"Factory for cross-service test data\"\"\" return CrossServiceFactory(base_urls={ \"user_service\": \"http://localhost:8002\", \"inventory_service\": \"http://localhost:8003\", \"payment_service\": \"http://localhost:8001\" })``` Usage: ```python def test_end_to_end_purchase(cross_service_factory): \"\"\"Test complete purchase flow across services\"\"\" # Create test data user = cross_service_factory.create_user(name=\"Alice\") product = cross_service_factory.create_product(name=\"Widget\", price=50.0) payment = cross_service_factory.create_payment( user_id=user[\"id\"], product_id=product[\"id\"], amount=product[\"price\"] ) assert payment[\"status\"] == \"success\"``` (5) Fixture versioning (backward compatibility): Problem: Update shared-test-utils fixture → breaks Service C, D, E. Solution—Semantic versioning + deprecation warnings: shared-test-utils v1.0.0: ```python @pytest.fixture def db_session(): \"\"\"Old API\"\"\" session = Session() yield session session.close()``` shared-test-utils v2.0.0 (breaking change): ```python # Provide both old and new API @pytest.fixture def db_session(): \"\"\"New API with transaction rollback\"\"\" connection = engine.connect() transaction = connection.begin() session = Session(bind=connection) yield session session.close() transaction.rollback() @pytest.fixture def db_session_v1(): \"\"\"Deprecated: Use db_session instead\"\"\" import warnings warnings.warn( \"db_session_v1 is deprecated. Use db_session instead.\", DeprecationWarning, stacklevel=2 ) # Old behavior (for backward compatibility) session = Session() yield session session.close()``` Migration guide: ```markdown # Migrating to shared-test-utils v2.0.0 ## Breaking changes ### db_session now uses transaction rollback **Before (v1.x):** ```python def test_user(db_session): user = User(name=\"Alice\") db_session.add(user) db_session.commit() # Manual commit required``` **After (v2.0):** ```python def test_user(db_session): user = User(name=\"Alice\") db_session.add(user) db_session.commit() # Still works, but rollback automatic``` No code changes required if using commit. Automatic rollback after test ensures clean state. ## Deprecations - `db_session_v1`: Use `db_session` instead. ## Timeline - Week 1-2: Update Service A, B - Week 3-4: Update Service C, D, E - Week 5: All services on v2.0 - Week 6: Remove v1 compatibility ```Summary: Mock vs. real: Unit tests mock (fast), integration tests real (docker-compose), contract tests verify mocks. Shared fixtures: shared-test-utils package (database, mocks, factories), import in each service, eliminate duplication. Docker-compose: docker-compose.test.yml with all services, wait_for_service health checks, session-scoped fixture. Cross-service data: CrossServiceFactory creates data across services (user in B, product in C, payment in A). Versioning: Semantic versioning (MAJOR.MINOR.PATCH), deprecation warnings, migration guide, gradual rollout. Result: Consistent, maintainable, scalable test architecture for 10-service microservices system.',
    keyPoints: [
      'Mock vs. real: Unit tests mock all deps (fast <100ms), integration tests docker-compose real services (slow ~1s), contract tests verify',
      'Shared fixtures: shared-test-utils package (db_session, mock_service_b, factories), import in conftest.py, eliminate 10× duplication',
      'Docker-compose: docker-compose.test.yml, wait_for_service() health checks, session-scoped fixture starts all services once',
      'Cross-service data: CrossServiceFactory().create_user/product/payment spans services, handles API calls, consistent test data',
      'Versioning: Semantic versioning v1→v2, deprecation warnings (db_session_v1), migration guide, gradual rollout (Week 1-6)',
    ],
  },
];
