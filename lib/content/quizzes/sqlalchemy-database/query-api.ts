// import { MultipleChoiceQuestion } from '@/lib/types';

export const queryApiQuiz = [
  {
    id: 'sql-query-q-1',
    question:
      'Design a complex analytics query for an e-commerce platform to find: (1) top 10 products by revenue in the last 30 days, (2) with their category name, (3) total units sold, (4) average order value, (5) percentage of total revenue. Include JOINs, aggregations, window functions, and subqueries. Explain why you chose each technique and how to optimize for performance.',
    sampleAnswer: `E-commerce analytics query: (1) Base query with JOINs: SELECT p.name, c.name as category, SUM(oi.quantity * oi.price_at_purchase) as revenue, SUM(oi.quantity) as units_sold, AVG(oi.price_at_purchase) as avg_price FROM products p JOIN order_items oi ON p.id = oi.product_id JOIN orders o ON oi.order_id = o.id JOIN categories c ON p.category_id = c.id WHERE o.created_at >= NOW() - INTERVAL '30 days' GROUP BY p.id, p.name, c.name. (2) Window function for percentage: WITH product_revenue AS (previous query), total_revenue AS (SELECT SUM(revenue) as total FROM product_revenue) SELECT pr.*, pr.revenue / tr.total * 100 as pct_of_total, RANK() OVER (ORDER BY pr.revenue DESC) as rank FROM product_revenue pr CROSS JOIN total_revenue tr ORDER BY pr.revenue DESC LIMIT 10. (3) SQLAlchemy implementation: from sqlalchemy import func, over, select, literal_column. product_revenue = select(Product.name, Category.name.label("category"), func.sum(OrderItem.quantity * OrderItem.price_at_purchase).label("revenue"), func.sum(OrderItem.quantity).label("units_sold"), func.avg(OrderItem.price_at_purchase).label("avg_price")).select_from(Product).join(OrderItem).join(Order).join(Category).where(Order.created_at >= func.now() - func.text("interval '30 days'")).group_by(Product.id, Product.name, Category.name).cte("product_revenue"). total_revenue_cte = select (func.sum (product_revenue.c.revenue).label("total")).cte("total_revenue"). stmt = select (product_revenue, (product_revenue.c.revenue / total_revenue_cte.c.total * 100).label("pct_of_total"), func.rank().over (order_by=product_revenue.c.revenue.desc()).label("rank")).select_from (product_revenue).cross_join (total_revenue_cte).order_by (product_revenue.c.revenue.desc()).limit(10). (4) Optimization: Index strategy: CREATE INDEX idx_orders_created ON orders (created_at) - filter 30 days. CREATE INDEX idx_order_items_product ON order_items (product_id, order_id) - JOIN optimization. CREATE INDEX idx_products_category ON products (category_id) - category JOIN. Partitioning: Partition orders table by created_at (monthly) - scan only relevant partition. Materialized views: CREATE MATERIALIZED VIEW product_revenue_30d with daily refresh. Caching: Cache result for 1 hour (data changes slowly). Query plan: EXPLAIN ANALYZE shows index scans, no sequential scans. Total execution: < 100ms with indexes.`,
    keyPoints: [
      'CTEs: Break complex query into readable steps (product_revenue, total_revenue)',
      'Window functions: RANK() for ranking, percentage calculation without self-join',
      'JOINs: Link products → order_items → orders → categories',
      'Optimization: Indexes on foreign keys + created_at, partition orders table',
      'Production: Materialized view with refresh, cache results, monitor query time',
    ],
  },
  {
    id: 'sql-query-q-2',
    question:
      'Explain the N+1 query problem with a concrete example, how to detect it, and three different solutions using SQLAlchemy. Include: (1) code showing the N+1 problem, (2) how to identify it in logs, (3) eager loading with joinedload, (4) subquery loading with selectinload, (5) when to use each approach.',
    sampleAnswer: `N+1 query problem: (1) Problem code: users = session.query(User).limit(10).all(); for user in users: print(user.posts). This executes: SELECT * FROM users LIMIT 10 (1 query), then SELECT * FROM posts WHERE user_id = ? for each user (10 queries). Total: 1 + 10 = 11 queries (N+1). As N grows, performance degrades linearly. (2) Detection: Enable SQL logging: engine = create_engine (url, echo=True). Look for repeated similar queries with different parameters. Use query counter: from sqlalchemy import event; query_count = 0; @event.listens_for(Engine, "before_cursor_execute"); def receive_before_cursor_execute (conn, cursor, statement, parameters, context, executemany): global query_count; query_count += 1. Production: Use APM tools (Datadog, New Relic) to detect high query counts per request. (3) Solution 1 - joinedload (single query with JOIN): from sqlalchemy.orm import joinedload; users = session.query(User).options (joinedload(User.posts)).limit(10).all(). SQL: SELECT users.*, posts.* FROM users LEFT OUTER JOIN posts ON users.id = posts.user_id LIMIT 10. Single query, but returns cartesian product (user repeated for each post). Good for: Small collections (< 10 items per parent). One-to-one relationships. When you need all related objects immediately. (4) Solution 2 - selectinload (2 queries with IN): from sqlalchemy.orm import selectinload; users = session.query(User).options (selectinload(User.posts)).limit(10).all(). SQL: SELECT * FROM users LIMIT 10, then SELECT * FROM posts WHERE user_id IN (1,2,3,4,5,6,7,8,9,10). Two queries total, no cartesian product. Good for: Large collections. Multiple relationships. Default recommended approach. (5) Solution 3 - subqueryload (2 queries with JOIN): from sqlalchemy.orm import subqueryload; users = session.query(User).options (subqueryload(User.posts)). SQL: SELECT * FROM users LIMIT 10, then SELECT posts.* FROM posts WHERE posts.user_id IN (SELECT users.id FROM users LIMIT 10). Legacy approach, selectinload is better. (6) When to use: joinedload: One-to-one, small one-to-many (< 10), need single query. selectinload: Default choice, one-to-many, multiple relationships. lazy="immediate": Load relationship in same query as parent (2.0+). Performance: N+1: 1000 users = 1001 queries (10+ seconds). joinedload: 1 query (100ms), but 10KB result for 1000 users × 10 posts each. selectinload: 2 queries (50ms each), 5KB total. Winner: selectinload for most cases.`,
    keyPoints: [
      'N+1: 1 query for parents, N queries for children (linearly worse with scale)',
      'Detection: echo=True in engine, query counter, APM tools showing high query count',
      'joinedload: Single query with JOIN, cartesian product, good for small collections',
      'selectinload: 2 queries with IN clause, no cartesian, default recommendation',
      'Benchmark: N+1 (10s) vs joinedload (100ms) vs selectinload (100ms) for 1000 records',
    ],
  },
  {
    id: 'sql-query-q-3',
    question:
      'Write a pagination system that handles large datasets efficiently. Address: (1) basic offset pagination and its problems at scale, (2) cursor-based pagination implementation, (3) keyset pagination for ordered queries, (4) how to include total count without full table scan, (5) performance comparison of all approaches.',
    sampleAnswer: `Pagination strategies: (1) Offset pagination (basic): def paginate_offset (page: int, page_size: int): offset = (page - 1) * page_size; users = session.query(User).offset (offset).limit (page_size).all(); total = session.query (func.count(User.id)).scalar(); return {"items": users, "total": total, "page": page}. SQL: SELECT * FROM users LIMIT 20 OFFSET 1000. Problem at scale: OFFSET 1000000 requires database to scan 1,000,000 rows then discard them. Performance degrades linearly: page 1 (10ms), page 1000 (1s), page 100000 (100s). Total count requires full table scan (slow for large tables). (2) Cursor-based pagination: def paginate_cursor (cursor: int | None, page_size: int): stmt = select(User); if cursor: stmt = stmt.where(User.id > cursor); users = session.execute (stmt.order_by(User.id).limit (page_size)).scalars().all(); next_cursor = users[-1].id if users else None; return {"items": users, "next_cursor": next_cursor}. SQL: SELECT * FROM users WHERE id > 1234 ORDER BY id LIMIT 20. Index scan (fast), no offset, but: cannot jump to page N, total count unavailable, UI shows "Load More" not page numbers. (3) Keyset pagination (ordered by created_at): def paginate_keyset (after_id: int | None, after_created: datetime | None, page_size: int): stmt = select(User).order_by(User.created_at.desc(), User.id.desc()); if after_created: stmt = stmt.where((User.created_at < after_created) | ((User.created_at == after_created) & (User.id < after_id))); users = session.execute (stmt.limit (page_size)).scalars().all(); return {"items": users, "next": {"id": users[-1].id, "created_at": users[-1].created_at}}. Index: (created_at DESC, id DESC). Fast seeks, but complex WHERE clause for non-unique sort keys. (4) Efficient total count: Option A - Approximate count (PostgreSQL): SELECT reltuples::bigint FROM pg_class WHERE relname = "users" (instant, 95% accurate). Option B - Cached count: Redis: SET users_count (update on insert/delete via trigger). Option C - Limit count query: SELECT COUNT(*) FROM users LIMIT 10000 (cap at reasonable number). Option D - Count only first page: if page == 1: calculate count, else: return cached/approximate. (5) Performance comparison (1M rows): Offset page 1: 10ms, page 1000: 500ms, page 10000: 5s (linear degradation). Cursor: 10ms (all pages, constant time). Keyset: 15ms (all pages, constant with composite index). Total count: Full scan (1s), approximate (< 1ms), cached (< 1ms). Recommendation: Use cursor/keyset for infinite scroll. Use offset with count cap for traditional pagination (max 100 pages). Cache/approximate total count.`,
    keyPoints: [
      'Offset pagination: Simple but O(N) degradation, OFFSET 1000000 scans 1M rows',
      'Cursor pagination: WHERE id > cursor, O(1) seeks, no page numbers, good for infinite scroll',
      'Keyset pagination: WHERE (created_at, id) > (val1, val2), needs composite index',
      'Total count: Approximate (reltuples), cached (Redis), or cap at reasonable max',
      'Production: Cursor for APIs/infinite scroll, offset for admin (with max page limit)',
    ],
  },
];
