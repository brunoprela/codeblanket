export const sqlalchemyCoreQuiz = [
  {
    id: 'sql-core-q-1',
    question:
      'Design a production database connection setup for a FastAPI application that will handle 1000 concurrent requests. Address: (1) engine configuration (pool size, overflow, timeouts), (2) session management pattern, (3) how to inject database sessions into endpoints, (4) error handling and rollback strategy, (5) monitoring connection pool health. Include complete code implementation.',
    sampleAnswer:
      'Production FastAPI database setup: (1) Engine configuration: Create singleton engine with appropriate pool settings. pool_size=20 (based on CPU cores × 2-4), max_overflow=40 (burst capacity for spikes), pool_timeout=30 (wait for connection), pool_recycle=3600 (refresh connections hourly, prevents stale connections), pool_pre_ping=True (test connection before use, handles database restarts). Code: engine = create_engine("postgresql://...", pool_size=20, max_overflow=40, pool_timeout=30, pool_recycle=3600, pool_pre_ping=True). (2) Session management: SessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=True, expire_on_commit=True). Use dependency injection pattern. (3) Dependency injection: def get_db(): session = SessionLocal(); try: yield session; finally: session.close(). Use in endpoints: @app.get("/users") def get_users(db: Session = Depends(get_db)): return db.query(User).all(). FastAPI automatically closes session after request. (4) Error handling: FastAPI handles exceptions automatically—session closed in finally block even on error. For explicit transactions: with db.begin(): operations. Automatically commits on success, rolls back on exception. Use try/except in business logic: try: db.add(user); db.commit() except IntegrityError: db.rollback(); raise HTTPException(409, "User exists"). (5) Monitoring: Create /health endpoint: def check_pool_health(): pool = engine.pool; stats = {"size": pool.size(), "checked_out": pool.checkedout(), "overflow": pool.overflow()}; if stats["checked_out"] / (pool.size() + pool.overflow()) > 0.8: log warning. Use prometheus_client to export metrics: pool_connections_total, pool_connections_in_use. Alert if pool exhaustion approaching. Rule of thumb: pool_size = (number of app instances) × (connections per instance) < max_connections on database (default PostgreSQL: 100). For 5 app instances with pool_size=20: 5 × 20 = 100 total connections.',
    keyPoints: [
      'pool_size based on CPU cores, max_overflow for burst capacity',
      'Dependency injection: yield session in get_db(), automatically closes',
      'FastAPI handles cleanup automatically, use context managers for explicit transactions',
      'Monitor pool health: checked_out / total > 0.8 indicates saturation',
      'Total connections across all instances must not exceed database max_connections',
    ],
  },
  {
    id: 'sql-core-q-2',
    question:
      'Explain the difference between SQLAlchemy Core and ORM. When would you use each? Provide: (1) concrete code examples for both, (2) performance implications, (3) scenarios where Core is superior, (4) scenarios where ORM is superior, (5) how to mix both approaches in the same application.',
    sampleAnswer:
      'Core vs ORM comparison: (1) Code examples: Core uses Table objects and SQL expressions: users = Table("users", metadata, Column("id", Integer), Column("email", String)); stmt = select(users).where(users.c.email == "test"); result = conn.execute(stmt). Returns Row objects (tuple-like). ORM uses classes and objects: class User(Base): __tablename__ = "users"; id = Column(Integer, primary_key=True); email = Column(String). stmt = select(User).where(User.email == "test"); users = session.execute(stmt).scalars().all(). Returns User instances with methods, relationships. (2) Performance: Core is 5-15% faster (no object creation overhead), better for bulk operations. ORM adds overhead but negligible for I/O-bound operations. For 10,000 row insert: Core/raw SQL 10-100x faster than ORM (use bulk_insert_mappings). (3) Core superior for: (a) ETL pipelines: Transform data without needing domain models. (b) Data migrations: Schema changes, bulk operations. (c) Analytics queries: Complex aggregations, window functions, CTEs clearer in SQL. (d) Performance-critical paths: Trading systems, real-time processing. (e) Database-agnostic libraries: Don\'t assume ORM models. (4) ORM superior for: (a) Application development: CRUD operations, type safety, IDE autocomplete. (b) Relationships: Automatic JOIN handling, lazy/eager loading. (c) Business logic: Methods on models, validation, computed properties. (d) Migrations (with Alembic): Automatic migration generation from model changes. (e) Testing: Easier to mock objects than rows. (5) Mixing approaches: Use repository pattern: class UserRepository: def create(self, email): user = User(email=email); session.add(user); return user (ORM). def get_daily_signups(self): return session.execute(text("SELECT DATE(created_at), COUNT(*) FROM users GROUP BY 1")).all() (Core/raw SQL). Keep 80% ORM (productivity), 20% Core (performance). SQLAlchemy ORM is built on Core—can use Core constructs in ORM queries: session.query(User).from_statement(text("SELECT * FROM users WHERE email = :email")).params(email="test").',
    keyPoints: [
      'Core: Table objects, SQL expressions, 5-15% faster, better for ETL/analytics',
      'ORM: Classes with relationships, methods, type safety, better for app development',
      'Core superior for: bulk ops, migrations, analytics, performance-critical',
      'ORM superior for: CRUD, relationships, business logic, testing',
      'Mix with repository pattern: ORM for CRUD, Core for complex queries',
    ],
  },
  {
    id: 'sql-core-q-3',
    question:
      'Transaction isolation levels are critical for correctness but impact performance. Explain: (1) all four isolation levels (READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE), (2) what anomalies each prevents, (3) performance trade-offs, (4) when to use each in production, (5) how to handle serialization failures. Provide code examples and real-world scenarios.',
    sampleAnswer:
      'Isolation levels explained: (1) Levels and anomalies: READ UNCOMMITTED: Can see uncommitted changes from other transactions (dirty reads). Fastest, but dangerous—rarely used. READ COMMITTED: Only see committed changes. Prevents dirty reads. Allows non-repeatable reads (same query returns different results within transaction) and phantom reads (new rows appear). Default for PostgreSQL, MySQL. REPEATABLE READ: Same query returns same results within transaction. Prevents dirty reads and non-repeatable reads. Allows phantom reads (MySQL prevents, PostgreSQL prevents). Default for MySQL. SERIALIZABLE: Full isolation, as if transactions run serially. Prevents all anomalies. Slowest due to locking. (2) Anomalies: Dirty read: T1 updates row, T2 reads uncommitted value, T1 rolls back. T2 saw data that never existed. Non-repeatable read: T1 reads row, T2 updates row, T1 reads again—different value. Phantom read: T1 queries for age > 30, T2 inserts age=35, T1 queries again—new row appears. (3) Performance trade-offs: READ UNCOMMITTED < READ COMMITTED < REPEATABLE READ < SERIALIZABLE (left = faster, right = more isolation). Higher isolation = more locking = less concurrency. SERIALIZABLE can cause serialization failures requiring retry. (4) When to use: READ COMMITTED: Default for most applications. Good balance of correctness and performance. Use for: standard CRUD, reads of committed data only. REPEATABLE READ: Analytics queries, reports requiring consistent snapshot. Example: Generate monthly report—don\'t want data changing mid-report. SERIALIZABLE: Financial transactions requiring absolute correctness. Example: Transferring money, allocating limited inventory. Use when race conditions unacceptable. (5) Handling serialization failures: SERIALIZABLE can fail with "could not serialize access". Code: from sqlalchemy.exc import OperationalError; MAX_RETRIES = 3; for attempt in range(MAX_RETRIES): try: with engine.begin() as conn: conn.execution_options(isolation_level="SERIALIZABLE"); transfer_money(conn); break except OperationalError as e: if "could not serialize" in str(e) and attempt < MAX_RETRIES - 1: continue; raise. Real-world: E-commerce inventory: Use SERIALIZABLE to prevent overselling. Product has 5 units, two customers buy simultaneously. Without SERIALIZABLE, both transactions read 5 units, both decrement—result: -1 units (oversold). With SERIALIZABLE: Second transaction fails, retry or show "out of stock".',
    keyPoints: [
      'READ UNCOMMITTED: dirty reads possible, rarely used',
      'READ COMMITTED: default, prevents dirty reads, good for most apps',
      'REPEATABLE READ: consistent snapshot, use for reports/analytics',
      'SERIALIZABLE: full isolation, use for financial/inventory, handle retries',
      'Higher isolation = more correctness, less concurrency, may need retry logic',
    ],
  },
];

