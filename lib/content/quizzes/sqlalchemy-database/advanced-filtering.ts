import { MultipleChoiceQuestion } from '@/lib/types';

export const advancedFilteringQuiz = [
  {
    id: 'sql-filter-q-1',
    question:
      'Design a product search system with JSON metadata. Requirements: (1) Products have JSONB column with specs (brand, category, price, features array). (2) Implement search by brand, price range, and features. (3) Add appropriate indexes. (4) Implement faceted search (show available filters). (5) Optimize for 1M products. Include complete schema, queries, and index strategy.',
    sampleAnswer: `Product search system: (1) Schema: class Product(Base): id, name, metadata = Column(JSONB) storing {\"brand\": \"Apple\", \"category\": \"laptop\", \"price\": 1999, \"features\": [\"touchscreen\", \"backlit-keyboard\"]}. (2) Search queries: Brand filter: SELECT * FROM products WHERE metadata @> '{\"brand\": \"Apple\"}'. Price range: SELECT * FROM products WHERE (metadata->>'price')::numeric BETWEEN 1000 AND 2000. Features: SELECT * FROM products WHERE metadata->'features' @> '[\"touchscreen\"]'. Combined: WHERE metadata @> '{\"brand\": \"Apple\"}' AND (metadata->>'price')::numeric BETWEEN 1000 AND 2000 AND metadata->'features' @> '[\"touchscreen\"]'. SQLAlchemy: stmt = select(Product).where(Product.metadata.contains({\"brand\": \"Apple\"}), type_coerce(Product.metadata[\"price\"], Numeric).between(1000, 2000), Product.metadata[\"features\"].contains([\"touchscreen\"])). (3) Index strategy: CREATE INDEX idx_product_metadata_gin ON products USING gin (metadata jsonb_path_ops) - for containment (@>). CREATE INDEX idx_product_brand ON products ((metadata->>'brand')) - B-tree for exact brand. CREATE INDEX idx_product_price ON products (((metadata->>'price')::numeric)) - B-tree for price range. CREATE INDEX idx_product_features_gin ON products USING gin((metadata->'features')) - GIN for array features. Why multiple indexes? GIN jsonb_path_ops handles containment but not field-specific range queries. B-tree indexes handle =, <, >, BETWEEN efficiently. Query planner chooses best index. (4) Faceted search: SELECT metadata->>'brand' as brand, COUNT(*) FROM products GROUP BY brand - show available brands with counts. SELECT UNNEST((metadata->'features')::text[]) as feature, COUNT(*) FROM products GROUP BY feature - show all features. Combine with current filters: WHERE conditions AND brand IN (selected_brands) to show refined facets. (5) Performance optimization: jsonb_path_ops index 30% smaller than default GIN. Partial indexes for common filters: CREATE INDEX idx_laptops ON products USING gin (metadata) WHERE metadata->>'category' = 'laptop' - fast for laptop searches. Materialized views for facets: CREATE MATERIALIZED VIEW product_facets AS SELECT brand, category, COUNT(*) FROM products GROUP BY brand, category. REFRESH MATERIALIZED VIEW CONCURRENTLY product_facets (hourly). Caching: Cache facet counts in Redis (TTL 1 hour). Cache individual product metadata. Result: 1M products, < 50ms query time with proper indexes.`,
    keyPoints: [
      'JSONB with @> containment, GIN jsonb_path_ops index for fast lookups',
      'Expression indexes: ((metadata->>',
      ')::type) for specific field queries',
      'Faceted search: UNNEST + GROUP BY for dynamic filters, materialized views for counts',
      'Multiple indexes: GIN for containment, B-tree for ranges, partial for categories',
      'Optimization: jsonb_path_ops (30% smaller), materialized views, Redis caching',
    ],
  },
  {
    id: 'sql-filter-q-2',
    question:
      'Implement full-text search for a blog platform with 100K articles. Address: (1) schema with tsvector column and triggers, (2) weighted search (title > content), (3) ranking and relevance, (4) highlighting matched terms, (5) handling typos and stemming, (6) performance optimization. Compare with Elasticsearch and explain trade-offs.',
    sampleAnswer: `Full-text search implementation: (1) Schema: class Article(Base): id, title, content, language, search_vector = Column(TSVECTOR). Trigger: CREATE TRIGGER tsvectorupdate BEFORE INSERT OR UPDATE ON articles FOR EACH ROW EXECUTE FUNCTION tsvector_update_trigger (search_vector, 'pg_catalog.english', title, content). Auto-updates search_vector on changes. (2) Weighted search: ALTER FUNCTION to use setweight: setweight (to_tsvector('english', title), 'A') || setweight (to_tsvector('english', content), 'B'). Weights: A (title) = 1.0, B (content) = 0.4, C = 0.2, D = 0.1. Ranking prioritizes title matches. (3) Ranking: stmt = select(Article, func.ts_rank_cd(Article.search_vector, func.to_tsquery('python & database'), 32).label('rank')).where(Article.search_vector.match('python & database')).order_by('rank DESC'). ts_rank vs ts_rank_cd: ts_rank = simple frequency, ts_rank_cd = cover density (accounts for term proximity). Normalization flag 32 = length normalization (prevents long documents dominating). (4) Highlighting: func.ts_headline('english', Article.content, func.to_tsquery('python'), 'MaxWords=50, MinWords=20, HighlightAll=false') returns snippet with <b>python</b>. Customize: StartSel=<mark>, StopSel=</mark>. (5) Typos and stemming: Stemming automatic: \"running\" matches \"run\", \"databases\" matches \"database\". Fuzzy matching: Use pg_trgm: SELECT * FROM articles WHERE similarity (title, 'pythno') > 0.3 - handles typos. Combined: Full-text search first (fast), then similarity search on no results (slower). Prefix matching: to_tsquery('pyth:*') matches \"python\", \"pythonic\". (6) Performance optimization: CREATE INDEX idx_search_gin ON articles USING gin (search_vector) - essential, 10-100x speedup. Partial index: CREATE INDEX idx_published_search ON articles USING gin (search_vector) WHERE published = true. Query optimization: Use setweight in trigger (not query) - compute once at insert/update. Limit results: LIMIT 100 (don't fetch all matches). Caching: Cache popular queries in Redis. Materialized views for auto-complete: SELECT word, count FROM ts_stat('SELECT search_vector FROM articles') ORDER BY count DESC. (7) PostgreSQL vs Elasticsearch: PostgreSQL FTS: Pros: Built-in, ACID transactions, join with other tables, simpler stack. Cons: Single-node (no distributed), slower for > 10M documents, limited features (no fuzzy matching built-in). Elasticsearch: Pros: Distributed, scales horizontally, advanced features (fuzzy, autocomplete, aggregations), < 10ms queries. Cons: Eventual consistency, separate system, complex operations, memory-hungry. Trade-offs: Use PostgreSQL FTS for: < 1M documents, need ACID, want simple stack, budget-constrained. Use Elasticsearch for: > 1M documents, need fuzzy/autocomplete/aggregations, multi-language advanced, have DevOps capacity. Hybrid: PostgreSQL for exact search + structured filters, Elasticsearch for fuzzy/autocomplete. Performance: PostgreSQL FTS: 100K articles, 20-50ms query time. Elasticsearch: 100K articles, 5-15ms query time.`,
    keyPoints: [
      'tsvector with trigger auto-update, weighted (title=A, content=B) for relevance',
      'ts_rank_cd for ranking with cover density, normalization prevents long doc bias',
      'Highlighting: ts_headline with MaxWords/MinWords, fuzzy with pg_trgm similarity',
      'GIN index essential (10-100x speedup), partial for published, cache popular queries',
      'PostgreSQL FTS: < 1M docs, simple stack. Elasticsearch: > 1M docs, advanced features',
    ],
  },
  {
    id: 'sql-filter-q-3',
    question:
      'Create a location-based search system for a real estate app. Requirements: (1) Store property locations (latitude, longitude). (2) Search within radius (e.g., 5km). (3) Search within polygon (drawn on map). (4) Order by distance. (5) Handle 1M properties with < 50ms query time. Compare PostgreSQL PostGIS vs application-level Haversine formula. Include indexing strategy.',
    sampleAnswer: `Location-based search: (1) Schema with PostGIS: Enable extension: CREATE EXTENSION postgis. class Property(Base): id, address, location = Column(Geometry('POINT', srid=4326)) - SRID 4326 = WGS84 (GPS coordinates). Store: property.location = func.ST_SetSRID(func.ST_MakePoint (longitude, latitude), 4326). Without PostGIS: lat = Column(Numeric(10, 7)), lng = Column(Numeric(10, 7)). (2) Radius search (PostGIS): stmt = select(Property).where (func.ST_DWithin(Property.location, func.ST_SetSRID(func.ST_MakePoint (lng, lat), 4326), 5000)) - 5000 meters. Uses spatial index (fast). Without PostGIS (Haversine): stmt = select(Property).where (func.acos (func.sin (func.radians (lat)) * func.sin (func.radians(Property.lat)) + func.cos (func.radians (lat)) * func.cos (func.radians(Property.lat)) * func.cos (func.radians(Property.lng) - func.radians (lng))) * 6371 < 5). Haversine formula, 6371 = Earth radius km. (3) Polygon search: polygon = [[lng1, lat1], [lng2, lat2], ...]. stmt = select(Property).where (func.ST_Within(Property.location, func.ST_GeomFromText (f'POLYGON(({points}))', 4326))). (4) Order by distance: stmt = select(Property, func.ST_Distance(Property.location, func.ST_SetSRID(func.ST_MakePoint (lng, lat), 4326)).label('distance')).order_by('distance'). Returns distance in meters. (5) Indexing: CREATE INDEX idx_property_location ON properties USING gist (location) - GIST spatial index, essential for PostGIS. Without PostGIS: CREATE INDEX idx_property_lat ON properties (lat); CREATE INDEX idx_property_lng ON properties (lng). Bounding box optimization: Calculate bounding box (lat ± delta, lng ± delta), filter with B-tree indexes first, then calculate distance. stmt = select(Property).where(Property.lat.between (lat - 0.05, lat + 0.05), Property.lng.between (lng - 0.05, lng + 0.05)). Then apply Haversine in Python. (6) Performance comparison: PostGIS (1M properties): < 50ms with GIST index, exact calculations, supports complex geometries. Haversine (1M properties): Bounding box + B-tree: ~100ms, full table scan: 10+ seconds. PostGIS advantages: Spatial indexes (GIST, BRIN), accurate geodesic calculations, complex operations (polygon, intersection), scales to millions. Haversine advantages: No extension needed, simple queries, good for < 100K properties with bounding box optimization. (7) Production optimization: GIST index essential: CREATE INDEX idx_location_gist ON properties USING gist (location). Partial index for active listings: WHERE status = 'active'. Cluster table by location: CLUSTER properties USING idx_location_gist - groups nearby properties on disk. Caching: Cache popular areas (city centers) in Redis. Materialized views for regions: CREATE MATERIALIZED VIEW properties_by_city AS SELECT city, COUNT(*), AVG(ST_X(location)) as avg_lng, AVG(ST_Y(location)) as avg_lat FROM properties GROUP BY city. Result: 1M properties, < 50ms queries with PostGIS + GIST index.`,
    keyPoints: [
      'PostGIS: Geometry(POINT, srid=4326), ST_DWithin for radius, ST_Within for polygon',
      'GIST spatial index essential (< 50ms vs 10+ seconds without)',
      'Haversine: Bounding box first (B-tree indexes), then distance calc (100ms for 1M)',
      'PostGIS advantages: Spatial indexes, exact geodesic, complex geometries, scales',
      'Optimization: GIST index, CLUSTER by location, cache popular areas, materialized views',
    ],
  },
];
