/**
 * Multiple choice questions for Fine-Tuning OpenAI Models section
 */

export const fineTuningOpenAIModelsMultipleChoice = [
  {
    id: 'openai-fine-tune-mc-1',
    question:
      'You uploaded 10,000 training examples for OpenAI fine-tuning, but the job fails with "invalid format error". What is the MOST likely cause?',
    options: [
      'Training file is too large (over size limit)',
      'Missing system message or incorrect message role order',
      'Learning rate is too high',
      'Not enough validation data',
    ],
    correctAnswer: 1,
    explanation:
      'Option B (format error) is most likely. OpenAI requires specific JSONL format: Each line must be valid JSON, Each example must have "messages" key, Messages must include valid roles ("system", "user", "assistant"), Messages must be in logical order (system first, then user/assistant alternating). Common errors: Missing system message, Role typos ("assitant" instead of "assistant"), Empty content fields, Wrong nesting structure. Example fix: {"messages": [{"role": "system", "content": "..."}, {"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}. Option A is unlikely—size limit is very high (1GB+). Option C doesn\'t cause upload failures. Option D is not required (validation is optional).',
  },
  {
    id: 'openai-fine-tune-mc-2',
    question:
      'OpenAI fine-tuning costs $0.008/1K tokens. You have 5,000 examples averaging 200 tokens each, training for 3 epochs. What is the training cost?',
    options: ['$8', '$24', '$40', '$120'],
    correctAnswer: 1,
    explanation:
      'Option B ($24) is correct. Calculation: Total tokens = examples × avg_tokens × epochs = 5,000 × 200 × 3 = 3,000,000 tokens. Cost = (3,000,000 / 1,000) × $0.008 = 3,000 × $0.008 = $24. Important: Training cost is one-time, but inference costs more for fine-tuned models ($0.003 input, $0.006 output vs $0.0015/$0.002 for base). For 1M requests/month: Fine-tuned costs ~$450/month vs ~$250/month for base (2x more). Only worth it if performance improvement justifies cost.',
  },
  {
    id: 'openai-fine-tune-mc-3',
    question:
      'After fine-tuning, your model performs great in testing but poorly in production. Users report responses are too formal and verbose. What is the issue?',
    options: [
      'Fine-tuning overfitted to test data',
      "Training data style doesn't match desired production style",
      'Need to fine-tune longer (more epochs)',
      'System message is wrong',
    ],
    correctAnswer: 1,
    explanation:
      "Option B (training data mismatch) is the issue. Model learned style from training data—if training examples are formal/verbose, model will be formal/verbose even if you want casual/concise. This is \"you get what you train on\". Solution: Re-examine training data examples, if most are formal→verbose, that's what model learned. Fix: Collect or create examples with desired style (casual, concise), retrain with corrected data. Option D (system message) can help but won't override learned training style. Option A (overfit to test) doesn't explain style mismatch. Option C (more epochs) will make problem worse. Key lesson: Training data quality and style are everything—model mirrors what it sees.",
  },
  {
    id: 'openai-fine-tune-mc-4',
    question:
      'You want to fine-tune for multiple tasks: customer support, sales, and technical docs. What is the BEST approach?',
    options: [
      'Train three separate fine-tuned models (one per task)',
      'Train one model with all tasks mixed, use system message to specify task',
      'Train one model, switch tasks with different temperature settings',
      "Use base model with task-specific prompts (don't fine-tune)",
    ],
    correctAnswer: 1,
    explanation:
      'Option B (one model, task-specific system messages) is best. Multi-task fine-tuning: Mix all training data (support + sales + technical), use system message to specify task at inference: "You are a customer support agent" or "You are a technical writer". Benefits: One model to maintain (simpler), shared learning across tasks (improves all), lower cost (one fine-tuning job + one model), can handle task blends. Implementation: Tag training examples with task-specific system messages, model learns to adapt based on system message. Option A (separate models) costs 3x more (training + inference + maintenance). Option C (temperature) doesn\'t change capabilities. Option D (prompting only) loses fine-tuning benefits. Multi-task fine-tuning is underutilized but very effective!',
  },
  {
    id: 'openai-fine-tune-mc-5',
    question:
      'Your fine-tuned model ID is "ft:gpt-3.5-turbo:org:suffix:abc123". What does each part mean?',
    options: [
      'ft=fine-tuned, gpt-3.5-turbo=base model, org=your organization, suffix=your chosen name, abc123=unique ID',
      'All parts are random IDs generated by OpenAI',
      'ft=file type, gpt-3.5-turbo=model version, rest is metadata',
      'The ID format is not standardized',
    ],
    correctAnswer: 0,
    explanation:
      'Option A is correct. ID structure: ft: prefix means "fine-tuned", gpt-3.5-turbo: base model used for fine-tuning, org: your OpenAI organization ID, suffix: your custom name (specified during fine-tuning), abc123: unique job ID. Why this matters: You can identify which base model (important for deprecation), you can organize models by suffix (e.g., "customer-support-v2"), you can track which org owns the model. Example usage: client.chat.completions.create(model="ft:gpt-3.5-turbo:org:support-v2:abc123"). Suffix tip: Use descriptive names with versions (task-v1, task-v2) for easy tracking.',
  },
];
