/**
 * Design News Feed Multiple Choice Questions
 */

export const newsFeedMultipleChoice = [
  {
    question:
      'A user with 5000 followers creates a post. Using the push model (fanout-on-write), the system writes the post_id to Redis lists for each follower: `LPUSH feed:{follower_id} {post_id}`. If each LPUSH operation takes 2ms and batch writes (pipelining) process 100 followers at once, how long does the complete fanout take?',
    options: [
      '50 batches × 2ms = 100ms total fanout time',
      '5000 operations × 2ms = 10,000ms = 10 seconds total fanout time',
      '50 batches × 2ms per operation × 100 operations per batch = 10,000ms = 10 seconds',
      '50 batches × (2ms / 100) = 1ms total fanout time due to pipelining efficiency',
    ],
    correctAnswer: 0,
    explanation:
      "The correct answer is 50 batches × 2ms = 100ms total fanout time. Here's the detailed reasoning: The user has 5000 followers. With batch writes (Redis pipelining), we group 100 followers per batch: 5000 / 100 = 50 batches. Each batch sends 100 LPUSH commands to Redis in a single pipeline, and Redis processes them efficiently. The key insight about pipelining: instead of 100 round-trips (100 × 2ms = 200ms per batch), we make 1 round-trip that processes all 100 commands together. The '2ms' stated in the question refers to the total batch processing time, not per-operation within the batch. Therefore, total fanout time = 50 batches × 2ms per batch = 100ms. Option B incorrectly treats each LPUSH as a separate 2ms operation without pipelining, which would indeed take 10 seconds—this is why pipelining is critical. Option C makes a calculation error, multiplying 2ms × 100 operations incorrectly (batch time is already given as 2ms for all 100 operations). Option D incorrectly divides by 100, suggesting batching makes it faster than physically possible. Real-world implications: 100ms fanout means the post appears in all 5000 followers' feeds within 100ms—acceptable latency for most use cases. For users with 10K+ followers, fanout time grows proportionally: 100 batches × 2ms = 200ms, still fast. However, for celebrities with 1M followers, 10,000 batches × 2ms = 20 seconds, which is why the hybrid model switches to pull mode at the 10K threshold. The 10K threshold is chosen precisely because fanout remains <1 second, maintaining good UX.",
  },
  {
    question:
      'In a news feed ranking algorithm, you calculate affinity as: affinity = (interactions_with_author / total_interactions). User A has interacted 500 times total in the last 30 days, with 50 interactions with User B. User B posts frequently (10 posts/day = 300 posts/month). What problem might occur with this affinity calculation, and how should it be adjusted?',
    options: [
      "The calculation is correct; high interaction count (50) reflects strong affinity and User B's posts should be prioritized",
      "User B's frequent posting inflates the interaction count artificially; adjust affinity using interactions_per_post: (50 interactions / 300 posts) = 0.167 interactions/post",
      "The calculation doesn't account for interaction type; weight likes (1×), comments (2×), shares (3×) differently: weighted_affinity = Σ(interactions × weight) / total",
      'Both B and C: adjust for posting frequency AND weight interaction types for more accurate affinity measurement',
    ],
    correctAnswer: 3,
    explanation:
      "The correct answer is D: both adjustments are necessary. Let's analyze the problem: User B posts 10× more than average user (1 post/day), so naturally, User A has more opportunities to interact with User B's content—this inflates the raw interaction count (50) without reflecting true affinity strength. If User B posted only 1× per day (30 posts/month), User A might have 5 interactions—same interaction rate (0.167 interactions/post) but 10× lower raw count. The raw affinity calculation (50/500 = 0.1) would unfairly favor frequent posters. Adjustment 1 (frequency normalization): Calculate interactions_per_post = 50 / 300 = 0.167. Compare this to the average: if average user interaction rate is 0.2 interactions/post, User B's content is actually slightly below average affinity despite 50 total interactions. Adjustment 2 (weighted interactions): Not all interactions are equal. Scenario: User A liked 48 of User B's posts (passive), commented on 1 (engaged), shared 1 (strong signal). Raw count: 50 interactions. Weighted: (48 × 1) + (1 × 2) + (1 × 3) = 53 weighted interactions vs potentially (25 × 1) + (20 × 2) + (5 × 3) = 80 for another user with fewer but deeper interactions. The final formula should be: affinity = (weighted_interactions / author_post_count) / (total_weighted_interactions / average_post_count_across_all_authors). This normalizes for both posting frequency and interaction depth. Real-world example: Facebook's EdgeRank algorithm uses a similar approach, computing affinity as a function of interaction recency, frequency, and type, normalized by content volume. Without these adjustments, spammy frequent posters dominate feeds, degrading UX. The lesson: raw counts mislead; normalize for opportunity (post count) and weight by signal strength (interaction type).",
  },
  {
    question:
      'Your news feed system uses cursor-based pagination: `GET /feed?cursor=2025-01-15T10:00:00&limit=50` returns posts with `created_at < cursor`. A user loads their feed at 10:00 AM, scrolls to offset 100 (cursor=2025-01-15T09:30:00), then a new post from their friend is created at 09:35:00 (between 09:30 and 10:00). What happens when the user continues scrolling?',
    options: [
      "The new post (09:35:00) appears in the next batch since it's within the cursor range",
      'The new post is skipped because cursor-based pagination only includes posts that existed at initial load time',
      "The new post appears, but pagination breaks because the cursor logic includes posts created_at < 09:30:00, so 09:35:00 doesn't match",
      'The system returns duplicate posts because the cursor logic is disrupted by newly inserted content',
    ],
    correctAnswer: 2,
    explanation:
      "The correct answer is C: the new post appears, but it breaks pagination. Here's the detailed scenario: Initial load (10:00 AM): query `created_at < 2025-01-15T10:00:00 LIMIT 50` returns posts from 09:59:59 → 09:45:00 (50 posts). Cursor returned: 09:45:00. User scrolls: query `created_at < 2025-01-15T09:45:00 LIMIT 50` returns posts from 09:44:59 → 09:30:01 (50 posts). Cursor returned: 09:30:01. Meanwhile, at 09:35:00, friend posts new content. User continues scrolling: query `created_at < 2025-01-15T09:30:01 LIMIT 50` returns posts from 09:30:00 → 09:15:00 (50 posts). The new post at 09:35:00 doesn't match this query (09:35:00 is NOT less than 09:30:01), so it's skipped entirely. However, if the user refreshes the feed or loads a new cursor range that includes 09:35:00, the post appears, but out of order—user already scrolled past that time range, causing confusion. This is the 'phantom read' problem in pagination. The issue: cursor-based pagination assumes immutable data (no new inserts in already-paginated ranges). When new content is inserted, it can be skipped or cause duplicates depending on cursor design. Solutions: (1) Snapshot isolation: generate a snapshot of post IDs at initial load time (e.g., store in Redis with user session key), pagination only includes posts from that snapshot. New posts appear only on refresh. (2) Offset + snapshot timestamp: include a snapshot_timestamp in pagination: `GET /feed?offset=100&snapshot_timestamp=2025-01-15T10:00:00`. Query: `created_at < snapshot_timestamp ORDER BY created_at DESC LIMIT 50 OFFSET 100`. New posts created after 10:00 AM are excluded from this session's pagination. (3) Hybrid: use real-time feed for initial load (show new posts at top with 'Load more recent posts' banner), but cursor-based pagination for scrolling back in time, explicitly ignoring new posts until user refreshes. Option A is wrong—cursor logic excludes the post. Option B is partially correct (snapshot approach), but cursor-based pagination itself doesn't enforce this without additional logic. Option D (duplicates) can occur if cursor is based on row offset instead of timestamp, but our query uses timestamp, so skipping is the actual outcome. Production systems (Twitter, Instagram) use snapshot isolation or hybrid approaches to avoid confusing users with inconsistent pagination.",
  },
  {
    question:
      "A news feed fanout worker consumes events from Kafka and writes to followers' Redis feeds. The worker crashes after processing 2000 of 5000 followers for a post. Kafka offset was not committed. What happens when the worker restarts?",
    options: [
      "The fanout restarts from follower 2001, continuing where it left off due to worker's internal state",
      'The fanout restarts from follower 1, resulting in 2000 duplicate feed entries for the first 2000 followers',
      'Kafka redelivers the post event, fanout restarts from follower 1, but Redis LPUSH is idempotent so no duplicates occur',
      'Kafka redelivers the post event, fanout restarts from follower 1, resulting in duplicate entries unless the worker implements deduplication logic',
    ],
    correctAnswer: 3,
    explanation:
      "The correct answer is D: Kafka redelivers the event, fanout restarts from follower 1, causing duplicate entries unless deduplication is implemented. Here's the detailed analysis: Kafka offset tracking: consumers commit offsets to mark 'message processed.' If the worker crashes before committing, Kafka considers the message unprocessed and redelivers it on restart (at-least-once delivery guarantee). In our scenario: worker receives post event (post_id=12345), begins fanout to 5000 followers, writes to 2000 feeds, crashes before committing offset. On restart, Kafka redelivers the same event (post_id=12345), and the worker starts fanout from follower 1 again. The problem: Redis LPUSH is NOT idempotent. `LPUSH feed:user123 12345` executed twice results in: `[12345, 12345, ...]`—duplicate entries in the feed. User will see the same post twice in their feed. Option A is wrong because worker internal state (which followers were processed) is lost on crash—Kafka only tracks offset, not worker's progress within a message. Option C is incorrect: LPUSH is not idempotent; adding the same post_id twice creates duplicates. Option B is technically what happens, but the key is whether duplicates occur (they do without deduplication). Solutions: (1) Idempotent writes using Redis sets instead of lists: `SADD feed:user123 12345`. SADD is idempotent—adding the same element twice has no effect. Trade-off: sets don't preserve order (need to sort by timestamp when reading). (2) Conditional write with check: Before `LPUSH feed:user123 12345`, check `LRANGE feed:user123 0 0` (most recent post). If already 12345, skip the write. Overhead: extra read per write. (3) Deduplication key in Redis: Store a temporary key `fanout:post:12345:user:123` with 1-hour TTL. Before writing to feed, check `EXISTS fanout:post:12345:user:123`. If exists, skip. If not, write to feed and set the deduplication key. After fanout completes successfully, commit Kafka offset. If crash occurs before commit but after partial fanout, restart won't create duplicates (deduplication keys prevent re-writing). (4) Exactly-once semantics: Use Kafka transactions with idempotent producers (Kafka 0.11+) and Redis Streams for fanout tracking. Complex but guarantees no duplicates. Production choice: Option 3 (deduplication keys) balances simplicity and reliability. The pattern: before any non-idempotent operation in a Kafka consumer, check a deduplication key (Redis, database, or in-memory cache) to detect redelivery and skip processing. This is critical for financial transactions, notifications, and feed systems where duplicates degrade UX.",
  },
  {
    question:
      'To reduce feed generation latency from 300ms to 100ms, you decide to cache entire feed responses in Redis: key=`feed_response:user:123`, value=`{serialized_feed_json}`. TTL=5 minutes. A user follows a new account. When should the cache be invalidated?',
    options: [
      "Immediately upon follow action—delete `feed_response:user:123` to force fresh generation including new followee's posts",
      "No invalidation needed—the new followee's posts will appear after the 5-minute TTL expires naturally",
      'Invalidate only if the new followee has posted in the last 24 hours; otherwise, cache remains valid (no new content to show)',
      "Partial invalidation: append new followee's recent posts to cached feed without regenerating entire feed",
    ],
    correctAnswer: 0,
    explanation:
      "The correct answer is A: invalidate immediately. Here's why: When a user follows a new account, their expectation is to see that account's content in their feed right away (or at least on the next refresh). If the cache remains valid for 5 more minutes (Option B), the user will load their feed, not see the new followee's posts, and likely think the follow action didn't work or the system is broken—poor UX. The follow action should trigger immediate cache invalidation: `DEL feed_response:user:123`. Next feed request will be a cache miss (taking full 300ms to generate) but will include the new followee's posts, meeting user expectations. Option C (conditional invalidation based on recent posts) is clever but has edge cases: what if the new followee posts 1 minute after being followed? The cached feed (still valid for 4 more minutes) won't include that new post until TTL expires. Users expect real-time feed updates for new follows. Option D (partial invalidation / append) sounds efficient but is complex and risky: appending new followee's posts to a serialized JSON blob requires deserializing, modifying, re-serializing—introduces bugs (e.g., posts out of order if not sorted by timestamp), and the cached ranking scores might be stale. Simpler to regenerate the feed. Trade-offs: immediate invalidation reduces cache hit rate (every follow action = cache miss). If a user follows 10 accounts in rapid succession, that's 10 cache misses over 5 minutes. However, this is rare behavior (users don't follow many accounts frequently), so impact is minimal. Optimization: batch invalidation—if user follows 10 accounts within 1 minute, invalidate cache only once after the batch completes. Alternative design: instead of caching entire feed response, cache feed components: (1) list of post_ids (from Redis feed list), (2) post data (from database/cache), (3) ranking scores (computed). When user follows new account, only invalidate component (1) (post_id list), keeping post data and some ranking scores cached. This reduces regeneration time from 300ms to ~150ms. Real-world systems (Facebook, Twitter) invalidate feed caches aggressively on any user action that affects feed content (follow, unfollow, mute, block) to maintain consistency and user trust. The lesson: caching improves performance, but invalidation logic is critical for correctness—better to over-invalidate (slightly lower cache hit rate) than under-invalidate (stale data, confused users).",
  },
];
