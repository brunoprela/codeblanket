import { ModuleSection } from '@/lib/types';

const logAnalyticsSection: ModuleSection = {
  id: 'log-analytics',
  title: 'Log Analytics',
  content: `
# Log Analytics

## Introduction

**Log analytics** is the process of collecting, storing, searching, and analyzing log data generated by applications, infrastructure, and security systems. In modern distributed systems, logs are the primary mechanism for understanding system behavior, debugging issues, detecting security threats, and ensuring compliance.

Every second, a typical large-scale application generates gigabytes of log data across thousands of servers. Without proper log analytics infrastructure, this data is uselessâ€”or worse, it becomes a liability (storage costs without value). With the right tools and practices, logs become your most powerful diagnostic and operational intelligence tool.

## Why Log Analytics Matters

### Modern Challenges

**Volume**: 
- Large application: 10TB of logs per day
- 5,000 microservices Ã— 100 requests/sec Ã— 200 bytes = 100GB/sec
- Storage costs: $230/TB/month (hot storage)

**Distribution**:
- Logs scattered across thousands of servers
- Multiple clouds, on-premise, edge locations
- No single place to search

**Velocity**:
- Need to find errors in real-time (not batch at night)
- Security incidents require immediate investigation
- Debugging production issues can't wait hours

**Variety**:
- Different formats: JSON, plain text, syslog, binary
- Different sources: Applications, databases, load balancers, firewalls
- Different languages: Python, Java, Go, JavaScript

### What Logs Provide

1. **Debugging**: "Why did this specific request fail?"
2. **Monitoring**: "How many errors in the last hour?"
3. **Security**: "Was this login attempt suspicious?"
4. **Compliance**: "Show all accesses to PII data in Q4 2023"
5. **Business Analytics**: "What features are users actually using?"
6. **Capacity Planning**: "Which endpoints are getting more traffic?"

## ELK Stack (Elasticsearch, Logstash, Kibana)

The ELK stack is the industry-standard open-source log analytics platform.

### Architecture

\`\`\`
Application Servers (1000s)
    â†“
  Filebeat (lightweight shipper on each server)
    â†“
  Logstash (centralized processing, parsing, enrichment)
    â†“
  Elasticsearch (storage, indexing, search)
    â†“
  Kibana (visualization, dashboards, alerting)
\`\`\`

### Components in Detail

#### 1. Beats: Data Shippers

**Filebeat**: Ships log files
\`\`\`yaml
# filebeat.yml
filebeat.inputs:
  - type: log
    enabled: true
    paths:
      - /var/log/application/*.log
    fields:
      service: api-server
      environment: production
      datacenter: us-east-1

output.logstash:
  hosts: ["logstash:5044"]
  compression_level: 3
  worker: 2
\`\`\`

**Key features**:
- Lightweight (10MB RAM, minimal CPU)
- Handles file rotation
- At-least-once delivery (no data loss)
- Backpressure handling (won't overwhelm Logstash)

**Other Beats**:
- **Metricbeat**: System metrics (CPU, memory, disk)
- **Packetbeat**: Network traffic analysis
- **Auditbeat**: Security audit data
- **Heartbeat**: Uptime monitoring

#### 2. Logstash: Processing Pipeline

Logstash transforms raw logs into structured, searchable data.

**Pipeline stages**: Input â†’ Filter â†’ Output

\`\`\`ruby
# logstash.conf

input {
  beats {
    port => 5044
  }
}

filter {
  # Parse Apache/NGINX logs
  if [fields][service] == "web-server" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}" 
      }
    }
    
    # Parse timestamp
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    
    # Convert status code to integer
    mutate {
      convert => { "response" => "integer" }
    }
    
    # Classify error severity
    if [response] >= 500 {
      mutate {
        add_field => { "severity" => "critical" }
      }
    } else if [response] >= 400 {
      mutate {
        add_field => { "severity" => "warning" }
      }
    }
  }
  
  # Parse JSON logs
  if [fields][service] == "api-server" {
    json {
      source => "message"
    }
    
    # GeoIP enrichment
    geoip {
      source => "client_ip"
      target => "geoip"
    }
    
    # User agent parsing
    useragent {
      source => "user_agent"
      target => "user_agent_parsed"
    }
  }
  
  # Drop noisy logs
  if [message] =~ /health check/ {
    drop { }
  }
}

output {
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "logs-%{[fields][service]}-%{+YYYY.MM.dd}"
    document_type => "_doc"
  }
  
  # Also send critical errors to alerting system
  if [severity] == "critical" {
    http {
      url => "https://alerting-service/webhook"
      format => "json"
    }
  }
}
\`\`\`

**Key filters**:

**Grok**: Parse unstructured text using patterns
\`\`\`ruby
# Common patterns
%{IP:client_ip}           # IP address
%{WORD:method}            # HTTP method
%{NUMBER:response_time}   # Number
%{QUOTEDSTRING:message}   # Quoted string
%{TIMESTAMP_ISO8601:ts}   # ISO timestamp

# Custom pattern
%{IP:ip} - - \\[%{HTTPDATE:timestamp}\\] "%{WORD:method} %{URIPATH:path}" %{NUMBER:status} %{NUMBER:bytes}
\`\`\`

**Mutate**: Transform fields
\`\`\`ruby
mutate {
  lowercase => ["user_id"]
  remove_field => ["temp", "debug_info"]
  rename => { "old_name" => "new_name" }
  convert => { "duration_ms" => "integer" }
}
\`\`\`

**GeoIP**: IP geolocation
\`\`\`ruby
geoip {
  source => "client_ip"
  target => "geoip"
}
# Adds: city, country, lat/lon, timezone
\`\`\`

#### 3. Elasticsearch: Storage and Search

Elasticsearch is a distributed search and analytics engine.

**Index structure**:
\`\`\`
logs-api-server-2024.01.15
â”œâ”€â”€ Shard 0 (primary)
â”œâ”€â”€ Shard 1 (primary)
â”œâ”€â”€ Shard 2 (primary)
â””â”€â”€ Replicas (for fault tolerance)
\`\`\`

**Mapping** (schema):
\`\`\`json
{
  "mappings": {
    "properties": {
      "@timestamp": { "type": "date" },
      "level": { "type": "keyword" },
      "message": { "type": "text" },
      "user_id": { "type": "keyword" },
      "request_id": { "type": "keyword" },
      "duration_ms": { "type": "integer" },
      "service": { "type": "keyword" },
      "environment": { "type": "keyword" }
    }
  }
}
\`\`\`

**Field types**:
- **keyword**: Exact matching, aggregations (user_id, status codes)
- **text**: Full-text search (log messages, error descriptions)
- **date**: Timestamps
- **integer/long**: Numbers (durations, counts)
- **ip**: IP addresses
- **geo_point**: Latitude/longitude

**Why it matters**: Wrong field type = queries fail or perform poorly.

#### 4. Kibana: Visualization and Exploration

Kibana provides the UI for searching, visualizing, and alerting on log data.

**Key features**:
- **Discover**: Search and explore logs
- **Visualizations**: Charts, graphs, maps
- **Dashboards**: Combine multiple visualizations
- **Alerting**: Notify on conditions
- **Dev Tools**: Execute Elasticsearch queries directly

## Log Parsing with Grok

Grok is the primary tool for parsing unstructured logs into structured data.

### Why Parsing Matters

**Before parsing**:
\`\`\`
192.168.1.1 - - [15/Jan/2024:14:30:05 +0000] "GET /api/users/123 HTTP/1.1" 200 1234 0.045
\`\`\`

You can search for "192.168.1.1" but can't:
- Filter by status code (200)
- Aggregate average response time (0.045)
- Group by endpoint (/api/users/*)

**After parsing**:
\`\`\`json
{
  "client_ip": "192.168.1.1",
  "timestamp": "2024-01-15T14:30:05Z",
  "method": "GET",
  "path": "/api/users/123",
  "status_code": 200,
  "bytes": 1234,
  "response_time": 0.045
}
\`\`\`

Now you can:
- Filter: "Show all 500 errors"
- Aggregate: "Average response time per endpoint"
- Alert: "Response time > 1 second"

### Built-in Patterns

\`\`\`ruby
# Apache/NGINX Combined Log
%{COMBINEDAPACHELOG}

# Syslog
%{SYSLOGBASE}

# Java stack trace
%{JAVASTACKTRACEPART}
\`\`\`

### Custom Patterns

**Example 1: Application log**
\`\`\`
Raw log:
[2024-01-15 14:30:05] INFO user_service: User login successful user_id=12345 ip=192.168.1.1 duration_ms=23

Pattern:
\\[%{TIMESTAMP_ISO8601:timestamp}\\] %{LOGLEVEL:level} %{WORD:service}: %{GREEDYDATA:message} user_id=%{INT:user_id} ip=%{IP:client_ip} duration_ms=%{INT:duration_ms}

Result:
{
  "timestamp": "2024-01-15 14:30:05",
  "level": "INFO",
  "service": "user_service",
  "message": "User login successful",
  "user_id": "12345",
  "client_ip": "192.168.1.1",
  "duration_ms": "23"
}
\`\`\`

**Example 2: Custom application format**
\`\`\`
Raw log:
order_placed|2024-01-15T14:30:05Z|order_123|user_456|amount=99.99|items=3

Pattern:
%{WORD:event_type}\\|%{TIMESTAMP_ISO8601:timestamp}\\|%{NOTSPACE:order_id}\\|%{NOTSPACE:user_id}\\|amount=%{NUMBER:amount}\\|items=%{INT:item_count}

Result:
{
  "event_type": "order_placed",
  "timestamp": "2024-01-15T14:30:05Z",
  "order_id": "order_123",
  "user_id": "user_456",
  "amount": "99.99",
  "item_count": "3"
}
\`\`\`

### Testing Grok Patterns

Use the **Grok Debugger** in Kibana:
1. Paste sample log line
2. Enter grok pattern
3. See parsed result immediately
4. Iterate until correct

**Pro tip**: Start simple, add complexity incrementally.

## Structured Logging (JSON)

Instead of parsing unstructured logs, emit structured logs from the start.

### Traditional (Unstructured)

\`\`\`python
logger.info (f"User {user_id} logged in from {ip} in {duration}ms")
# Output: "User 12345 logged in from 192.168.1.1 in 23ms"
\`\`\`

**Problems**:
- Requires grok parsing (error-prone)
- Format changes break parsing
- Hard to query structured data

### Structured (JSON)

\`\`\`python
import structlog

logger = structlog.get_logger()

logger.info(
    "user_login",
    user_id=user_id,
    client_ip=ip,
    duration_ms=duration,
    success=True
)

# Output:
# {
#   "event": "user_login",
#   "user_id": "12345",
#   "client_ip": "192.168.1.1",
#   "duration_ms": 23,
#   "success": true,
#   "timestamp": "2024-01-15T14:30:05.123Z",
#   "level": "info",
#   "service": "auth-service",
#   "host": "server-42"
# }
\`\`\`

**Benefits**:
- No parsing needed (JSON filter in Logstash)
- Query by any field immediately
- Add fields without breaking anything
- Consistent across services

### Best Practices for Structured Logging

**1. Include context fields automatically**:
\`\`\`python
logger = logger.bind(
    service="auth-service",
    environment="production",
    version="2.3.1",
    host=socket.gethostname()
)
\`\`\`

**2. Add request context**:
\`\`\`python
def process_request (request):
    logger = logger.bind(
        request_id=request.headers.get("X-Request-ID"),
        user_id=request.user.id,
        endpoint=request.path
    )
    # All subsequent logs include these fields
\`\`\`

**3. Log errors with stack traces**:
\`\`\`python
try:
    process_payment (order)
except Exception as e:
    logger.error(
        "payment_failed",
        order_id=order.id,
        amount=order.total,
        error=str (e),
        stack_trace=traceback.format_exc()
    )
    raise
\`\`\`

## Index Lifecycle Management (ILM)

Logs have different access patterns over time. ILM automates transitions between storage tiers.

### The Problem

- **Recent logs** (last 7 days): Searched frequently, need fast access (SSD)
- **Old logs** (30-90 days): Searched rarely, can be slower (HDD)
- **Ancient logs** (>90 days): Compliance only, archive (S3)

Storing everything on expensive SSDs wastes money. Deleting old logs violates compliance.

### ILM Phases

\`\`\`
Hot (SSD, indexed) â†’ Warm (SSD, read-only) â†’ Cold (HDD) â†’ Delete/Archive
  0-7 days              7-30 days            30-90 days     >90 days
\`\`\`

### ILM Policy

\`\`\`json
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d",
            "max_docs": 100000000
          },
          "set_priority": {
            "priority": 100
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "forcemerge": {
            "max_num_segments": 1
          },
          "shrink": {
            "number_of_shards": 1
          },
          "allocate": {
            "require": {
              "data": "warm"
            }
          },
          "set_priority": {
            "priority": 50
          }
        }
      },
      "cold": {
        "min_age": "30d",
        "actions": {
          "freeze": {},
          "allocate": {
            "require": {
              "data": "cold"
            }
          },
          "set_priority": {
            "priority": 10
          }
        }
      },
      "delete": {
        "min_age": "90d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
\`\`\`

**Breakdown**:

**Hot phase** (0-7 days):
- **Rollover**: Create new index when current reaches 50GB or 1 day
- **Priority**: 100 (highest, recover first on restart)
- **Hardware**: Fast SSDs
- **Replicas**: 1 (fault tolerance)

**Warm phase** (7-30 days):
- **Force merge**: Merge segments (optimize for reads)
- **Shrink**: Reduce shards from 5 to 1 (less overhead)
- **Move to warm nodes**: Cheaper SSDs or even HDDs
- **Read-only**: No more writes
- **Priority**: 50 (lower priority)

**Cold phase** (30-90 days):
- **Freeze**: Rarely accessed, very slow searches OK
- **Move to cold nodes**: Cheapest storage
- **Priority**: 10 (recover last)

**Delete phase** (>90 days):
- **Delete**: Permanently remove (or snapshot to S3 first)

### Applying ILM Policy

\`\`\`bash
# Create policy
PUT _ilm/policy/logs_policy
{ <policy JSON above> }

# Create index template with ILM
PUT _index_template/logs_template
{
  "index_patterns": ["logs-*"],
  "template": {
    "settings": {
      "index.lifecycle.name": "logs_policy",
      "index.lifecycle.rollover_alias": "logs",
      "number_of_shards": 5,
      "number_of_replicas": 1
    }
  }
}

# Create initial index
PUT logs-000001
{
  "aliases": {
    "logs": {
      "is_write_index": true
    }
  }
}
\`\`\`

**Result**: Logs automatically transition through phases, reducing storage costs by 50-80%.

## Log Aggregation at Scale

### Architecture: Kafka as Buffer

\`\`\`
Applications (5,000 instances)
    â†“
  Filebeat (on each instance)
    â†“
  Kafka (buffer, 7-day retention)
    â†“
  Logstash (scale independently, 50 instances)
    â†“
  Elasticsearch (100 nodes)
    â†“
  Kibana
\`\`\`

### Why Kafka?

**Problem**: Applications produce logs at variable rates (spike during deployments, traffic bursts). Elasticsearch has fixed ingestion capacity.

**Without Kafka**: Log spike â†’ Elasticsearch overloaded â†’ Logs dropped â†’ Blind to issues

**With Kafka**:
1. Logs written to Kafka (never blocks, never drops)
2. Kafka buffers spikes (7-day retention)
3. Logstash consumes at Elasticsearch\'s max rate
4. After spike, Logstash catches up from Kafka

**Benefits**:
- **Durability**: No log loss during spikes
- **Backpressure**: Producers never block
- **Replay**: Reprocess with new Logstash config
- **Multiple consumers**: Send to ES, S3, analytics simultaneously

### Configuration

**Kafka topic**:
\`\`\`bash
kafka-topics.sh --create \\
  --topic logs \\
  --partitions 100 \\
  --replication-factor 3 \\
  --config retention.ms=604800000  # 7 days
\`\`\`

**Filebeat â†’ Kafka**:
\`\`\`yaml
output.kafka:
  hosts: ["kafka1:9092", "kafka2:9092", "kafka3:9092"]
  topic: logs
  partition.hash:
    reachable_only: true
  compression: gzip
  max_message_bytes: 1000000
\`\`\`

**Logstash â† Kafka**:
\`\`\`ruby
input {
  kafka {
    bootstrap_servers => "kafka1:9092,kafka2:9092,kafka3:9092"
    topics => ["logs"]
    group_id => "logstash-consumers"
    consumer_threads => 4
    codec => json
  }
}
\`\`\`

### Monitoring

**Key metrics**:
- **Kafka consumer lag**: How far behind is Logstash?
- **Elasticsearch indexing rate**: docs/sec
- **Logstash pipeline throughput**: events/sec
- **Error rate**: Failed documents

\`\`\`bash
# Check Kafka consumer lag
kafka-consumer-groups.sh --describe --group logstash-consumers

TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG
logs            0          1523891         1523950         59
logs            1          1489234         1489300         66
...

# Lag > 100k = problem (falling behind)
\`\`\`

## Querying Logs

### Elasticsearch Query DSL

**Simple search**:
\`\`\`json
GET logs-*/_search
{
  "query": {
    "match": {
      "message": "database connection failed"
    }
  }
}
\`\`\`

**Boolean query** (AND, OR, NOT):
\`\`\`json
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "ERROR" } },
        { "range": { "@timestamp": { "gte": "now-1h" } } }
      ],
      "filter": [
        { "term": { "service": "api-server" } }
      ],
      "must_not": [
        { "match": { "message": "expected error" } }
      ]
    }
  }
}
\`\`\`

**Aggregations** (group by, count):
\`\`\`json
{
  "query": {
    "range": { "@timestamp": { "gte": "now-24h" } }
  },
  "aggs": {
    "errors_by_service": {
      "filter": { "term": { "level": "ERROR" } },
      "aggs": {
        "services": {
          "terms": {
            "field": "service",
            "size": 10
          }
        }
      }
    }
  }
}
\`\`\`

**Result**:
\`\`\`json
{
  "aggregations": {
    "errors_by_service": {
      "doc_count": 15234,
      "services": {
        "buckets": [
          { "key": "api-server", "doc_count": 8934 },
          { "key": "auth-service", "doc_count": 3421 },
          { "key": "payment-service", "doc_count": 2879 }
        ]
      }
    }
  }
}
\`\`\`

### KQL (Kibana Query Language)

Simpler syntax for common queries:

\`\`\`
# Errors in last hour
level:ERROR and @timestamp >= now-1h

# Specific user
user_id:"12345" and service:auth-service

# Status codes 500-599
status_code >= 500 and status_code < 600

# Wildcard
path:/api/users/* and method:POST

# Phrase search
message:"database connection timeout"
\`\`\`

## Alerting

Proactive monitoring via alerts on log patterns.

### Elasticsearch Watcher (X-Pack)

\`\`\`json
{
  "trigger": {
    "schedule": { "interval": "1m" }
  },
  "input": {
    "search": {
      "request": {
        "indices": ["logs-*"],
        "body": {
          "query": {
            "bool": {
              "must": [
                { "match": { "level": "ERROR" } },
                { "range": { "@timestamp": { "gte": "now-5m" } } }
              ],
              "filter": [
                { "term": { "service": "payment-service" } }
              ]
            }
          }
        }
      }
    }
  },
  "condition": {
    "compare": { "ctx.payload.hits.total": { "gt": 10 } }
  },
  "actions": {
    "notify_slack": {
      "webhook": {
        "url": "https://hooks.slack.com/services/XXX/YYY/ZZZ",
        "method": "POST",
        "body": "{\\"text\\": \\"ðŸš¨ {{ctx.payload.hits.total}} payment errors in last 5 minutes\\"}"
      }
    },
    "create_ticket": {
      "webhook": {
        "url": "https://ticketing-system/api/tickets",
        "method": "POST",
        "body": "{...}"
      }
    }
  }
}
\`\`\`

**Trigger**: Every 1 minute
**Input**: Search for errors in last 5 minutes
**Condition**: If >10 errors found
**Actions**: Send Slack notification AND create ticket

### Alert Best Practices

**1. Avoid alert fatigue**:
- Don't alert on every error (some errors are expected)
- Alert on rate, not absolute count
- Use thresholds based on historical data

**2. Alert on what matters**:
- Business impact, not technical metrics
- "Payment processing failing" not "Redis connection error"

**3. Include actionable context**:
- Link to logs, dashboards, runbooks
- Show recent trend (increasing? decreasing?)
- Suggest next steps

**4. Test alerts**:
- Inject test errors to verify alerts fire
- Ensure notifications reach the right people

## Cost Optimization

Log storage is expensive. Optimize aggressively.

### Strategy 1: Hot-Warm-Cold Architecture

\`\`\`
Hot (SSD): 7 days Ã— 1TB/day = 7TB Ã— $0.15/GB = $1,050/month
Warm (HDD): 23 days Ã— 1TB/day = 23TB Ã— $0.05/GB = $1,150/month
Cold (S3): 60 days Ã— 1TB/day = 60TB Ã— $0.023/GB = $1,380/month
Total: $3,580/month

vs.

All on SSD: 90 days Ã— 1TB/day = 90TB Ã— $0.15/GB = $13,500/month

Savings: 73% ($9,920/month)
\`\`\`

### Strategy 2: Sampling

**Keep 100% of**:
- ERROR and WARN logs
- Authentication events
- Payment transactions
- Security events

**Sample 10% of**:
- DEBUG logs
- Health checks
- Successful requests (keep only 1 in 10)

**Result**: 90% reduction in volume for low-value logs

**Implementation** (Logstash):
\`\`\`ruby
filter {
  # Drop 90% of successful requests
  if [level] == "INFO" and [path] =~ /health/ {
    ruby {
      code => "event.cancel if rand() < 0.9"
    }
  }
}
\`\`\`

### Strategy 3: Field Reduction

Don't store everything.

**Before**:
\`\`\`json
{
  "message": "User login successful",
  "user_id": "12345",
  "username": "alice",
  "email": "alice@example.com",
  "ip": "192.168.1.1",
  "user_agent": "Mozilla/5.0...",  // 200 bytes
  "headers": { ... },  // 500 bytes
  "request_body": { ... },  // 1000 bytes
  "response_body": { ... },  // 5000 bytes
  "stack_trace": "..."  // 3000 bytes (empty for success)
}
\`\`\`

**After**:
\`\`\`json
{
  "message": "User login successful",
  "user_id": "12345",
  "ip": "192.168.1.1"
}
\`\`\`

**Result**: 9.7KB â†’ 150 bytes (98% reduction)

**For errors, keep more**:
\`\`\`ruby
filter {
  if [level] == "ERROR" {
    # Keep stack trace, request/response bodies
  } else {
    mutate {
      remove_field => ["stack_trace", "request_body", "response_body", "headers"]
    }
  }
}
\`\`\`

### Strategy 4: Compression

Enable best_compression codec:
\`\`\`json
PUT logs-template
{
  "settings": {
    "index.codec": "best_compression"
  }
}
\`\`\`

**Result**: 30-50% storage reduction (slightly slower queries)

### Total Cost Optimization

\`\`\`
Before:
10TB/day Ã— 30 days Ã— $0.15/GB = $45,000/month

After optimizations:
- Sampling (90% of INFO): 10TB â†’ 2TB
- Field reduction: 2TB â†’ 0.5TB
- Compression: 0.5TB â†’ 0.25TB
- Hot-warm-cold: 0.25TB Ã— $0.08/GB avg = $2,000/month

Savings: 95.5% ($43,000/month)
\`\`\`

## Distributed Tracing Integration

Correlate logs across microservices using trace IDs.

### The Problem

**User reports**: "Payment failed"

**Your logs** (50 microservices):
- API Gateway: Request ID abc123
- Auth Service: Session ID xyz789
- Order Service: Order ID order456
- Payment Service: Transaction ID txn999
- Notification Service: Job ID job111

**Question**: Which logs are related to this user's request?

### The Solution: Trace ID Propagation

\`\`\`
User Request
    â†“ (generate trace_id: "trace-abc-123")
API Gateway [trace_id=trace-abc-123]
    â†“ (propagate via header: X-Trace-ID)
Auth Service [trace_id=trace-abc-123]
    â†“
Order Service [trace_id=trace-abc-123]
    â†“
Payment Service [trace_id=trace-abc-123]
    â†“
Notification Service [trace_id=trace-abc-123]
\`\`\`

**Implementation**:

**1. API Gateway generates trace ID**:
\`\`\`python
import uuid

@app.before_request
def before_request():
    trace_id = request.headers.get('X-Trace-ID') or str (uuid.uuid4())
    g.trace_id = trace_id
    g.logger = logger.bind (trace_id=trace_id)
\`\`\`

**2. Each service logs with trace ID**:
\`\`\`python
g.logger.info("order_created", order_id=order.id, user_id=user.id)
# Output: {"trace_id": "trace-abc-123", "event": "order_created", ...}
\`\`\`

**3. Propagate to downstream services**:
\`\`\`python
response = requests.post(
    "https://payment-service/charge",
    headers={"X-Trace-ID": g.trace_id},
    json=payment_data
)
\`\`\`

**4. Query all logs for trace ID**:
\`\`\`
GET logs-*/_search
{
  "query": {
    "term": { "trace_id": "trace-abc-123" }
  },
  "sort": [{ "@timestamp": "asc" }]
}
\`\`\`

**Result**: Single query returns all logs across all services for this request, chronologically ordered.

## Security and Compliance

### PII (Personally Identifiable Information)

**Problem**: Logs contain sensitive data (SSNs, credit cards, passwords)

**Solution**: Mask or redact before indexing

\`\`\`ruby
filter {
  # Redact credit card numbers
  mutate {
    gsub => [
      "message", "\\b\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}[\\s-]?\\d{4}\\b", "****-****-****-****"
    ]
  }
  
  # Remove password fields
  mutate {
    remove_field => ["password", "password_hash", "api_key"]
  }
  
  # Hash emails (for uniqueness without exposing email)
  ruby {
    code => "
      if event.get('email')
        event.set('email_hash', Digest::SHA256.hexdigest (event.get('email')))
        event.remove('email')
      end
    "
  }
}
\`\`\`

### Access Control

**Role-based access** in Elasticsearch:

\`\`\`json
{
  "roles": {
    "developer": {
      "indices": [
        {
          "names": ["logs-app-*"],
          "privileges": ["read"],
          "query": "{\"term\": {\"environment\": \"staging\"}}"
        }
      ]
    },
    "security_team": {
      "indices": [
        {
          "names": ["logs-security-*", "logs-audit-*"],
          "privileges": ["read", "write"]
        }
      ]
    }
  }
}
\`\`\`

**Result**: Developers see only staging logs, security team sees all security logs.

### Audit Logging

Log access to logs (meta!):

\`\`\`json
{
  "timestamp": "2024-01-15T14:30:05Z",
  "event": "log_access",
  "user": "alice@company.com",
  "query": "user_id:12345",
  "indices": ["logs-production-*"],
  "result_count": 234
}
\`\`\`

**Use case**: "Who accessed logs for user 12345 during investigation?"

## Best Practices

### 1. Use Structured Logging (JSON)

Emit JSON from applications, avoid grok parsing.

### 2. Include Trace IDs

Propagate trace_id through all services for correlation.

### 3. Set Retention Policies (ILM)

Hot (7d) â†’ Warm (30d) â†’ Cold (90d) â†’ Delete/Archive

### 4. Sample Non-Critical Logs

Keep 100% of errors, 10% of info logs.

### 5. Monitor Kafka Lag

Alert when consumer lag > threshold (falling behind).

### 6. Index by Time

Use date-based indices: logs-2024.01.15 (fast deletion, clear retention).

### 7. Avoid High-Cardinality Fields

Don't index: request_id (billions of unique values) â†’ Slow queries, huge indices.
Do index: user_id (millions), service (dozens) â†’ Fast queries.

### 8. Alert on Anomalies, Not Thresholds

Use ML to detect unusual error rates, not fixed thresholds.

### 9. Redact PII

Mask sensitive data before indexing (can't un-index later).

### 10. Test Log Ingestion Pipeline

Inject test logs, verify they appear in Elasticsearch with correct fields.

## Summary

Log analytics involves:

**Collection**: Filebeat (lightweight shippers on each server)
**Buffering**: Kafka (absorbs spikes, enables replay)
**Processing**: Logstash (parse, enrich, filter)
**Storage**: Elasticsearch (indexed, searchable)
**Visualization**: Kibana (dashboards, alerts)

**Key concepts**:
- **Structured logging**: Emit JSON, avoid parsing
- **ILM**: Hot-warm-cold-delete lifecycle (reduce costs 50-80%)
- **Trace IDs**: Correlate logs across microservices
- **Sampling**: Keep 100% errors, 10% info (90% volume reduction)
- **Alerting**: Proactive monitoring on log patterns

**Cost optimization**:
- Tiered storage (hot/warm/cold)
- Sampling non-critical logs
- Field reduction (store only what's needed)
- Compression

With proper log analytics infrastructure, logs become your most valuable operational intelligence toolâ€”enabling fast debugging, proactive monitoring, security incident response, and compliance.
`,
};

export default logAnalyticsSection;
